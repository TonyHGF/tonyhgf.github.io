[{"id": "2025_0001", "x": 3.611, "y": 3.576, "title": "A Composite Alignment-Aware Framework for Myocardial Lesion Segmentation in Multi-sequence CMR Images", "abstract": "Accurate segmentation of myocardial lesions from multisequence cardiac magnetic resonance imaging is essential for cardiac disease diagnosis and treatment planning. However, achieving optimal feature correspondence is challenging due to intensity variations across modalities and spatial misalignment caused by inconsistent slice acquisition protocols. We propose CAA-Seg, a composite alignment-aware framework that addresses these challenges through a two-stage approach. First, we introduce a selective slice alignment method that dynamically identifies and aligns anatomically corresponding slice pairs while excluding mismatched sections, ensuring reliable spatial correspondence between sequences. Second, we develop a hierarchical alignment network that processes multi-sequence features at different semantic levels, i.e., local deformation correction modules address geometric variations in lowlevel features, while global semantic fusion blocks enable semantic fusion at high levels where intensity discrepancies diminish. We validate our method on a large-scale dataset comprising 397 patients. Experimental results show that our proposed CAA-Seg achieves superior performance on most evaluation metrics, with particularly strong results in myocardial infarction segmentation, representing a substantial 5.54% improvement over state-of-the-art approaches. The code is available at https://github. com/yifangao112/CAA-Seg.", "filename": "2025_0001.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Yifan Gao", "Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0002", "x": 4.023, "y": 2.354, "title": "A Curvature-Guided Diffeomorphic Mesh Deformation Framework for Lifespan Brain Cortical Surface Reconstruction", "abstract": "Accurate and automatic lifespan brain cortical surface reconstruction (CSR) is crucial for analyzing brain development and aging. Traditional pipelines involve multiple processing steps, which are timeintensive and inefficient for handling larger datasets. While deep learningbased methods can accelerate reconstruction speed and produce highquality meshes compared to traditional approaches, they are often constrained to a single time point. The limitation arises from the significant variations in cortical surfaces across age groups, particularly in folding patterns. In this paper, we propose a novel curvature-guided diffeomorphic mesh deformation framework for lifespan brain CSR. Specifically, to preserve correct topology structure and uniformity, the framework employs multiple deformation blocks to gradually warp a simple smooth template mesh to a complex target surface with high folding. Considering that curvature is closely associated with folding patterns, we introduce curvature map prediction as an auxiliary task to guide the deformation process, enhancing the anatomical accuracy to facilitate subsequent cortical morphometry. Notably, incorporating curvature can also expedite model convergence. Our method is evaluated on a large-scale brain dataset with 2,132 subjects spanning ages 0 to 100 years. Experimental results show that our reconstructed surfaces have fewer geometric errors and optimal mesh regularity while being several orders of magnitude faster than traditional pipelines. Our code is available at https://github. com/TL9792/CCF.", "filename": "2025_0002.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Lin Teng", "Shen Zhao", "Feng Shi", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0003", "x": 4.577, "y": 4.954, "title": "A Holistic Time-Aware Classification Model for Multimodal Longitudinal Patient Data", "abstract": "Current prognostic and diagnostic AI models for healthcare often limit informational input capacity by being time-agnostic and focusing on single modalities, therefore lacking the holistic perspective clinicians rely on. To address this, we introduce a Time-Aware MultiModal Transformer Encoder (TAMME) for longitudinal medical data. Unlike most state-of-the-art models, TAMME integrates longitudinal imaging, textual, numerical, and categorical data together with temporal information. Each element is represented as the sum of embeddings for high-level categorical type, further specification of this type, time-related data, and value. This composition overcomes limitations of a closed input vocabulary, enabling generalization to novel data. Additionally, with temporal context including the delta to the preceding element, we eliminate the requirement for evenly sampled input sequences. For long-term EHRs, the model employs a novel summarization mechanism that processes sequences piecewise and prepends recent data with history representations in end-to-end training. This enables balancing recent information with historical signals via self-attention. We demonstrate TAMME's capabilities using data from 431k+ hospital stays, 73k ICU stays, and 425k Emergency Department (ED) visits from the MIMIC dataset for clinical classification tasks: prediction of triage acuity, length of stay, and readmission. We show superior performance over state-ofthe-art approaches especially gained from long-term data. Overall, our approach provides versatile processing of entire patient trajectories as a whole to enhance predictive performance on clinical tasks. Code is available at github.com/go31glX57/tamme.", "filename": "2025_0003.pdf", "year": 2025, "institution": "Technical University of Munich (TUM)", "country": "Germany", "authors": ["Tobias Susetzky", "Huaqi Qiu", "Rickmer Braren", "Daniel Rueckert"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0004", "x": 4.597, "y": 2.814, "title": "Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound", "abstract": "Accurate fetal birth weight (FBW) estimation is essential for optimizing delivery decisions and reducing perinatal mortality. However, clinical methods for FBW estimation are inefficient, operator-dependent, and challenging to apply in cases of complex fetal anatomy. Existing deep learning methods are based on 2D standard ultrasound (US) images or videos that lack spatial information, limiting their prediction accuracy. In this study, we propose the first method for directly estimating FBW from 3D fetal US volumes. Our approach integrates a multi-scale feature fusion network (MFFN) and a synthetic sample-based learning framework (SSLF). The MFFN effectively extracts and fuses multi-scale features under sparse supervision by incorporating channel attention, spatial attention, and a ranking-based loss function. SSLF generates synthetic samples by simply combining fetal head and abdomen data from different fetuses, utilizing semi-supervised learning to improve prediction performance. Experimental results demonstrate that our method achieves superior performance, with a mean absolute error of 166.4 ± 155.9 g and a mean absolute percentage error of 5.1 ± 4.6%, outperforming existing methods and approaching the accuracy of a senior doctor. Code is available at: https://github.com/Qioy-i/EFW.", "filename": "2025_0004.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Jian Wang", "Qiongying Ni", "Hongkui Yu", "Ruixuan Yao", "Jinqiao Ying", "Bin Zhang", "Xingyi Yang", "Jin Peng", "Jiongquan Chen", "Junxuan Yu", "Wenlong Shi", "Chaoyu Chen", "Zhongnuo Yan", "Mingyuan Luo", "Gaocheng Cai", "Dong Ni", "Jing Lu", "Xin Yang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0005", "x": 4.48, "y": 6.571, "title": "Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling", "abstract": "Spatial transcriptomics (ST) is a promising technique that characterizes the spatial gene profiling patterns within the tissue context. Comprehensive ST analysis depends on consecutive slices for 3D spatial insights, whereas the missing intermediate tissue sections and high costs limit the practical feasibility of generating multi-slice ST. In this paper, we propose C2-STi, the first attempt for interpolating missing ST slices at arbitrary intermediate positions between adjacent ST slices. Despite intuitive, effective ST interpolation presents significant challenges, including 1) limited continuity across heterogeneous tissue sections, 2) complex intrinsic correlation across genes, and 3) intricate cellular structures and biological semantics within each tissue section. To mitigate these challenges, in C2-STi, we design 1) a distance-aware local structural modulation module to adaptively capture cross-slice deformations and enhance positional correlations between ST slices, 2) a pyramid gene co-expression correlation module to capture multi-scale biological associations among genes, and 3) a cross-modal alignment module that integrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter and align the essential cellular features across ST and H&E images. Extensive experiments on the public dataset demonstrate our superiority over state-of-the-art approaches on both single-slice and multi-slice ST interpolation. Codes are available at https://github.com/ XiaofeiWang2018/C2-STi.", "filename": "2025_0005.pdf", "year": 2025, "institution": "University of Dundee", "country": "UK", "authors": ["Ningfeng Que", "Xiaofei Wang", "Jingjing Chen", "Yixuan Jiang", "Chao Li"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0006", "x": 1.198, "y": 3.474, "title": "Anatomy-Aware Frequency-Attention Transformer Networks for Liver Couinaud CT/MR Segmentation", "abstract": "Accurate Couinaud segmentation of liver CT/MR is essential in helping surgeons perceive the positional relationship between liver anatomy and intrahepatic lesions to make surgical planning. Unfortunately, current conventional and deep-learning based methods remain challenges in accurate Couinaud segmentation since the segmentation boundaries of different categories depending on hepatic vascular information are hard to predict. This work proposes a new deeply learned framework called anatomy-aware frequency-attention transformer networks (AFATN) for Couinaud segmentation of liver anatomy which contains the hybrid anatomy-aware preprocessing and frequency-attention transformer networks (FATN). Specifically, our framework first uses hybrid anatomy-aware preprocessing to integrate the hybrid cues of liver contour and hepatic venous centerline, then effectively utilizes hybrid cues for accurate Couinaud segmentation through the frequency-attention transformer networks with omission re-detected loss function. Our segmentation model FATN uses transformers to extract local structure and global semantic features and further focus on the hybrid cues with frequency-attention mechanisms. The proposed method was evaluated on clinical CT data and compared with currently available deep learning approaches, with the experimental results demonstrating that our method outperforms other approaches especially in accurately segmenting the Couinaud boundaries.", "filename": "2025_0006.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Wenkang Fan", "Hao Fang", "Rui Li", "Yanduan Lin", "Chao An", "Xiongbiao Luo"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0007", "x": 1.655, "y": 4.188, "title": "Attention-Guided Vector Quantized Variational Autoencoder for Brain Tumor Segmentation", "abstract": "Precise brain tumor segmentation is critical for effective treatment planning and radiotherapy. Existing methods rely on voxellevel supervision and often struggle to accurately delineate tumor boundaries, increasing potential surgical risks. We propose an Attention-Guided Vector Quantized Variational Autoencoder (AG-VQ-VAE)-a two-stage network specifically designed for boundary-focused tumor segmentation. Stage 1 comprises a VQ-VAE which learns a compact, discrete latent representation of segmentation masks. In stage 2, a conditional network extracts contextual features from MRI scans and aligns them with discrete mask embeddings to facilitate precise structural correspondence and improved segmentation fidelity. Additionally, we propose an attention scaling module to reinforce discriminative feature learning and a soft masking module to refine attention in uncertain tumor regions. Comprehensive evaluations on BraTS 2021 demonstrate that our AG-VQ-VAE sets a new benchmark, improving the HD95 metric by 4.83 mm (Whole Tumor), 2.14 mm (Tumor Core), and 2.39 mm (Enhancing Tumor), compared to state-of-the-art methods, while achieving a 0.23% improvement in Dice score for whole tumor. Furthermore, our qualitative results and ablation study demonstrate that featurelevel supervision significantly enhances boundary delineation compared to voxel-level approaches. The code is available at https://github.com/ danishali6421/AG-VQVAE-MICCAI.", "filename": "2025_0007.pdf", "year": 2025, "institution": "The University of Western Australia", "country": "Australia", "authors": ["Danish Ali", "Ajmal Mian", "Naveed Akhtar", "Ghulam Mubashar Hassan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0008", "x": 3.804, "y": 4.319, "title": "Blood Pressure Assisted Cerebral Microbleed Segmentation via Meta-matching", "abstract": "Cerebral microbleeds (CMBs) are small hemorrhagic lesions that pose significant challenges for accurate segmentation due to the high rate of false positives and false negatives. CMBs have two subtypes: lobar and deep microbleeds (MBs). Motivated by the strong association between deep MBs and hypertension, we propose a blood pressuredriven nnU-Net (BP-nnUNet) that integrates blood pressure (BP) prompt into the state-of-the-art nnU-Net framework through three key strategies. First, we estimate BP using the pre-trained Meta-matching model, that requires only MRI images. This allows our method to be successfully applied to public datasets with missing clinical demographics. Second, we categorize CMBs into lobar and deep MB, enriching input text prompts with multiple classes while constraining the BP effect to deep MBs. Lastly, we introduce a novel anatomically-aware joint prompt fusion module that combines lobar and deep MB prompts. Experiments on both in-house and public datasets demonstrate that our BP-nnUNet outperforms existing CMB segmentation models and universal models incorporating medical prompts. Ablation studies validate the effectiveness of integrating subtype-level and case-level prompts, as well as our fusion module. Our method paves the way for the incorporation of clinically relevant information into a segmentation framework. Our code is available at https://github.com/junmokwon/BP-nnUNet", "filename": "2025_0008.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "South Korea", "authors": ["Junmo Kwon", "Jonghun Kim", "Taehyeon Kim", "Sang Won Seo", "Hwan-Ho Cho", "Hyunjin Park"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0009", "x": 1.851, "y": 4.096, "title": "C2MAOT: Cross-modal Complementary Masked Autoencoder with Optimal Transport for Cancer Segmentation in PET-CT Images", "abstract": "Accurate cancer segmentation in PET-CT images is crucial for oncology, yet remains challenging due to lesion diversity, data scarcity, and modality heterogeneity. Existing methods often struggle to effectively fuse cross-modal information and leverage self-supervised learning for improved representation. In this paper, we introduce C 2 MAOT, a Cross-modal Complementary Masked Autoencoder with Optimal Transport framework for PET-CT cancer segmentation. Our method employs a novel modality-complementary masking strategy during pre-training to explicitly encourage cross-modal learning between PET and CT encoders. Furthermore, we integrate an optimal transport loss to guide the alignment of feature distributions across modalities, facilitating robust multi-modal fusion. Experimental results on two datasets demonstrate that C 2 MAOT outperforms existing state-of-the-art methods, achieving significant improvements in segmentation accuracy across five cancer types. These results establish our proposed method as an effective approach for tumor segmentation in PET-CT imaging. Our code is available at https://github.com/hjj194/c2maot.", "filename": "2025_0009.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Jiaju Huang", "Shaobin Chen", "Xinglong Liang", "Xiao Yang", "Zhuoneng Zhang", "Yue Sun", "Ying Wang", "Tao Tan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0010", "x": 6.187, "y": 3.659, "title": "CATVis: Context-Aware Thought Visualization", "abstract": "EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 27.08% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fréchet Inception Distance by 36.61%, indicating superior semantic alignment and image quality. The code is available at https://github.com/CVLABLUMS/CATVis.", "filename": "2025_0010.pdf", "year": 2025, "institution": "Lahore University of Management Sciences", "country": "Pakistan", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0011", "x": 6.276, "y": 3.594, "title": "CBrain: Cross-Modal Learning for Brain Vigilance Detection in Resting-State fMRI", "abstract": "Detecting human vigilance states (e.g., natural shifts between alertness and drowsiness) from functional magnetic resonance imaging (fMRI) data can provide novel insight into the whole-brain patterns underlying these critical states. Moreover, as a person's vigilance levels are closely tied to their behavior and brain activity, vigilance state can strongly influence the results of fMRI studies. Therefore, the ability to annotate fMRI scans with vigilance information can also enable clearer and more robust results in fMRI research. However, well-established vigilance indicators are derived from other modalities such as behavioral responses, electroencephalography (EEG), and pupillometry, which are not typically available in fMRI data collection. While previous works indicate the promise of distinguishing vigilance states from fMRI alone, EEG data can provide reliable vigilance indicators that complement and augment fMRI domain information. Here, we propose CBrain: Cross-modal learning for Brain vigilance detection in resting-state fMRI. Our model transfers EEG vigilance information into an fMRI latent space in training, and predicts human vigilance states using only fMRI data in testing, addressing the need for external vigilance indicators. Experimental results demonstrate CBrain's ability to predict vigilance states across different individuals at a granularity of 10-fMRI-frames with an 81.07% mF 1 score on a test set of unseen subjects. Additionally, our generalization experiments highlight the model's potential to estimate vigilance in an unseen task and in restingstate fMRI scans collected with a different scanner at a different site. Source code: https://github.com/neurdylab/CBrain.", "filename": "2025_0011.pdf", "year": 2025, "institution": "Vanderbilt University", "country": "USA", "authors": ["Chang Li", "Yamin Li", "Haatef Pourmotabbed", "Shengchao Zhang", "Jorge A Salas", "Sarah E Goodale", "Roza G Bayrak", "Catie Chang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0012", "x": 0.953, "y": 4.359, "title": "CENet: Context Enhancement Network for Medical Image Segmentation", "abstract": "Medical image segmentation, particularly in multi-domain scenarios, requires precise preservation of anatomical structures across diverse representations. While deep learning has advanced this field, existing models often struggle with accurate boundary representation, variability in organ morphology, and information loss during downsampling, limiting their accuracy and robustness. To address these challenges, we propose the Context Enhancement Network (CENet), a novel segmentation framework featuring two key innovations. First, the Dual Selective Enhancement Block (DSEB) integrated into skip connections enhances boundary details and improves the detection of smaller organs in a context-aware manner. Second, the Context Feature Attention Module (CFAM) in the decoder employs a multi-scale design to maintain spatial integrity, reduce feature redundancy, and mitigate overly enhanced representations. Extensive evaluations on both radiology and dermoscopic datasets demonstrate that CENet outperforms state-of-theart (SOTA) methods in multi-organ segmentation and boundary detail preservation, offering a robust and accurate solution for complex medical image analysis tasks. The source code is publicly available at https:// github.com/xmindflow/cenet.", "filename": "2025_0012.pdf", "year": 2025, "institution": "University of Regensburg", "country": "Germany", "authors": ["Afshin Bozorgpour", "Sina Ghorbani Kolahi", "Reza Azad", "Ilker Hacihaliloglu", "Dorit Merhof"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0013", "x": 1.286, "y": 5.199, "title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation", "abstract": "Histopathology image analysis is critical yet challenged by the demand of segmenting tissue regions and nuclei instances for tumor microenvironment and cellular morphology analysis. Existing studies focused on tissue semantic segmentation or nuclei instance segmentation separately, but ignored the inherent relationship between these two tasks, resulting in insufficient histopathology understanding. To address this issue, we propose a Co-Seg framework for collaborative tissue and nuclei segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing tissue and nuclei segmentation tasks to mutually enhance each other. To this end, we first devise a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and instance region prompts as prior constraints. Moreover, we design a mutual prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, collaboratively computing semantic and instance segmentation masks. Extensive experiments on the PUMA dataset demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the semantic, instance and panoptic segmentation of tumor tissues and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg", "filename": "2025_0013.pdf", "year": 2025, "institution": "University of Lincoln", "country": "UK", "authors": ["Qing Xu", "Wenting Duan", "Zhen Chen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0014", "x": 0.986, "y": 5.144, "title": "Cycle Context Verification for In-Context Medical Image Segmentation", "abstract": "In-context learning (ICL) is emerging as a promising technique for achieving universal medical image segmentation, where a variety of objects of interest across imaging modalities can be segmented using a single model. Nevertheless, its performance is highly sensitive to the alignment between the query image and in-context image-mask pairs. In a clinical scenario, the scarcity of annotated medical images makes it challenging to select optimal in-context pairs, and fine-tuning foundation ICL models on contextual data is infeasible due to computational costs and the risk of catastrophic forgetting. To address this challenge, we propose Cycle Context Verification (CCV), a novel framework that enhances ICL-based medical image segmentation by enabling self-verification of predictions and accordingly enhancing contextual alignment. Specifically, CCV employs a cyclic pipeline in which the model initially generates a segmentation mask for the query image. Subsequently, the roles of the query and an in-context pair are swapped, allowing the model to validate its prediction by predicting the mask of the original in-context image. The accuracy of this secondary prediction serves as an implicit measure of the initial query segmentation. A query-specific prompt is introduced to alter the query image and updated to improve the measure, thereby enhancing the alignment between the query and in-context pairs. We evaluated CCV on seven medical image segmentation datasets using two ICL foundation models, demonstrating its superiority over existing methods. Our results highlight CCV's ability to enhance ICLbased segmentation, making it a robust solution for universal medical image segmentation. The code will be available at https://github.com/ ShishuaiHu/CCV. S. Hu and Z. Liao-Contributed equally.", "filename": "2025_0014.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Shishuai Hu", "Zehui Liao", "Liangli Zhen", "Huazhu Fu", "Yong Xia"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0015", "x": 2.113, "y": 2.058, "title": "DGMIR: Dual-Guided Multimodal Medical Image Registration Based on Multi-view Augmentation and On-Site Modality Removal", "abstract": "Multi-modal medical image registration integrates complementary information from various modalities to deliver comprehensive visual insights for disease diagnosis, treatment planning, surgical navigation, etc. However, current methods often suffer from artifacts, computational overhead, or insufficient handling of modality-specific interference. Moreover, they still rely on specialized modules, such as generative trans-modal units, additional encoders, or handcrafted modalityinvariant operators, without fully exploiting the inherent potential of registration features. To address these drawbacks in multimodal medical image registration, we propose a novel registration framework. First, a plug-and-play architecture is proposed to directly process multi-scale heterogeneous features, with active guidance only during deformation field generation stage. Second, we introduce a multi-view feature reorganization module that dynamically optimizes feature distributions via adaptive relation computation and global calibration. Finally, an in-network modality removal module is introduced to leverage multi-scale adaptive convolutions to explicitly eliminate modality-specific interference. Extensive experiments on the BraTS2018 and Learn2Reg2021 datasets confirm that our proposed method achieves state-of-the-art performance on multiple multimodal medical image registration metrics. (https://github. com/St-Antonio/DGMIR).", "filename": "2025_0015.pdf", "year": 2025, "institution": "Chongqing University of Posts and Telecommunications", "country": "China", "authors": ["Gao Le", "Yucheng Shu", "Lihong Qiao", "Lijian Yang", "Bin Xiao", "Weisheng Li", "Xinbo Gao"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0016", "x": 2.753, "y": 3.346, "title": "Diffusion-Based Virtual Staining from Polarimetric Mueller Matrix Imaging", "abstract": "Polarization, as a new optical imaging tool, has been explored to assist in the diagnosis of pathology. Moreover, converting the polarimetric Mueller Matrix (MM) to standardized stained images becomes a promising approach to help pathologists interpret the results. However, existing methods for polarization-based virtual staining are still in the early stage, and the diffusion-based model, which has shown great potential in enhancing the fidelity of the generated images, has not been studied yet. In this paper, a Regulated Bridge Diffusion Model (RBDM) for polarization-based virtual staining is proposed. RBDM utilizes the bidirectional bridge diffusion process to learn the mapping from polarization images to other modalities such as H&E and fluorescence. And to demonstrate the effectiveness of our model, we conduct the experiment on our manually collected dataset, which consists of 18,000 paired polarization, fluorescence and H&E images, due to the unavailability of the public dataset. The experiment results show that our model greatly outperforms other benchmark methods. Our data and code are available at https://github.com/xiaoyu-z/RBDM/", "filename": "2025_0016.pdf", "year": 2025, "institution": "Hong Kong University of Science and Technology", "country": "China", "authors": ["Xiaoyu Zheng", "Jing Wen", "Jiaxin Zhuang", "Yao Du", "Jing Cong", "Limei Guo", "Lin Luo", "Chao He", "Hao Chen"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0017", "x": 3.983, "y": 3.424, "title": "Dual Correlation-Aware Mamba for Microvascular Obstruction Identification in Non-contrast Cine Cardiac Magnetic Resonance", "abstract": "Microvascular obstruction (MVO) is a key prognostic factor in acute myocardial infarction, with affected patients experiencing higher mortality rates. Currently, late gadolinium enhancement cardiac magnetic resonance (CMR) is the gold standard for MVO identification. However, it is unsuitable for patients with renal impairment, who make up 20% of all patients. Recent studies have demonstrated the feasibility of using non-contrast cine CMR to identify MVO. Despite this, existing methods struggle to effectively learn crucial motion features, as they implicitly model motion dynamics while overlooking regional wall motion abnormalities, which are important for MVO identification. To this end, we introduce a Dual Correlation-aware Mamba, which includes an Adjacent Frame Correlation (AFC) module and a Diastolic Frame Correlation (DFC) module to address these limitations. The AFC module calculates the correlations through adjacent frames to explicitly model the motion dynamics. The DFC module learns correlations between the diastolic frame and others. Leveraging the diastolic frame as a reference, this module highlights regional abnormalities and guides motion learning. Experimental results demonstrate that our method outperforms competing methods, potentially providing a non-contrast tool for MVO identification. The code is available at https://github.com/code-koukai/Dual-Correlation-Mamba.", "filename": "2025_0017.pdf", "year": 2025, "institution": "Nanyang Technological University", "country": "Singapore", "authors": ["Yige Yan", "Jun Cheng", "Xulei Yang", "Shuang Leng", "Ru San Tan", "Liang Zhong", "Jagath C Rajapakse"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0018", "x": 2.019, "y": 4.007, "title": "EdgeANet: A Transformer-based Edge Representation Learning Network for Canine X-ray Verification", "abstract": "Artificial intelligence (AI) has shown great potential in medical imaging, yet its adoption in veterinary medicine remains limited due to data scarcity and anatomical complexity. This study introduces a novel transformer-based edge representation learning network for verifying rotated vertebral bodies in canine thoracic X-ray images. The proposed method integrates a localization module to identify the spinous process, a transformer encoder for global feature extraction using a selfattention mechanism, and an edge encoder to enhance feature extraction of fine-grained details, improving classification performance. Experimental results demonstrate that our method achieves superior accuracy, precision, and recall, outperforming state-of-the-art (SOTA) methods with a classification accuracy of 0.7838. Furthermore, the ablation study confirms that including the proposed encoders significantly impacts performance, demonstrating their effectiveness in improving classification accuracy. These findings highlight the importance of multi-scale feature extraction in veterinary imaging and suggest that EdgeANet can be a valuable tool for AI-assisted X-ray verification in veterinary and human medical applications.", "filename": "2025_0018.pdf", "year": 2025, "institution": "Chungbuk National University", "country": "Republic of Korea", "authors": ["In-Gyu Lee", "Jun-Young Oh", "Hyewon Choi", "Tae-Eui Kam", "Namsoon Lee", "Sang-Hwan Hyun", "Euijong Lee", "Ji-Hoon Jeong"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0019", "x": 5.8, "y": 3.905, "title": "EEG-DINO: Learning EEG Foundation Models via Hierarchical Self-distillation", "abstract": "Electroencephalography (EEG) provides a non-invasive window into the brain's electrical activity, playing an essential role in various brain-computer interface (BCI) and healthcare applications. In this paper, we propose EEG-DINO, a novel foundation model for EEG encoding based on a hierarchical self-distillation framework. By multi-view semantic alignment, the model is able to extract multi-level semantic features from EEG data, which captures a wide range of semantic information, increasing the robustness against noise and variances inherent in complex EEG signals. Moreover, acknowledging the unique heterogeneous spatial-temporal dependencies in EEG signals, we design a channel-aware sampling mechanism and a decoupled positional embedding scheme. They independently address spatial and temporal dimensions, enabling the model to capture the intricate structural characteristics of EEG signals. We train EEG-DINO on a large-scale EEG corpus spanning over 9000 h, which consistently achieves state-of-the-art performance on multiple downstream tasks (The pre-trained weights and code for fine-tuning are available at: https://huggingface.co/eegdino/ EEG-DINO). These results demonstrate the great effectiveness of our self-distillation framework for EEG encoding.", "filename": "2025_0019.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Xujia Wang", "Xuhui Liu", "Xi Liu", "Qian Si", "Zhaoliang Xu", "Yang Li", "Xiantong Zhen"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0020", "x": 1.846, "y": 3.423, "title": "Frequency-Enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation", "abstract": "Automated and accurate segmentation of individual vertebra in 3D CT and MRI images is essential for various clinical applications. Due to the limitations of current imaging techniques and the complexity of spinal structures, existing methods still struggle with reducing the impact of image blurring and distinguishing similar vertebrae. To alleviate these issues, we introduce a Frequency-enhanced Multigranularity Context Network (FMC-Net) to improve the accuracy of vertebrae segmentation. Specifically, we first apply wavelet transform for lossless downsampling to reduce the feature distortion in blurred images. The decomposed high and low-frequency components are then processed separately. For the high-frequency components, we apply a High-frequency Feature Refinement (HFR) to amplify the prominence of key features and filter out noises, restoring fine-grained details in blurred images. For the low-frequency components, we use a Multi-granularity State Space Model (MG-SSM) to aggregate feature representations with different receptive fields, extracting spatially-varying contexts while capturing long-range dependencies with linear complexity. The utilization of multi-granularity contexts is essential for distinguishing similar vertebrae and improving segmentation accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets. The source code is publicly available at https://github.com/anaanaa/FMCNet.", "filename": "2025_0020.pdf", "year": 2025, "institution": "Dalian University of Technology", "country": "China", "authors": ["Jian Shi", "Tianqi You", "Pingping Zhang", "Hongli Zhang", "Rui Xu", "Haojie Li"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0021", "x": 4.607, "y": 3.707, "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG", "abstract": "An electrocardiogram (ECG) is a widely used, cost-effective tool for detecting electrical abnormalities in the heart. However, it cannot directly measure functional parameters, such as ventricular volumes and ejection fraction, which are crucial for assessing cardiac function. Cardiac magnetic resonance (CMR) is the gold standard for these measurements, providing detailed structural and functional insights, but is expensive and less accessible. To bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive Learning), a multimodal contrastive learning framework that enhances ECG representations by integrating spatio-temporal information from CMR. PTACL uses global patient-level contrastive loss and local temporal-level contrastive loss. The global loss aligns patient-level representations by pulling ECG and CMR embeddings from the same patient closer together, while pushing apart embeddings from different patients. Local loss enforces finegrained temporal alignment within each patient by contrasting encoded ECG segments with corresponding encoded CMR frames. This approach enriches ECG representations with diagnostic information beyond electrical activity and transfers more insights between modalities than global alignment alone, all without introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL achieves better performance in two clinically relevant tasks: (1) retrieving patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac function parameters, such as ventricular volumes and ejection fraction. Our results highlight the potential of PTACL to enhance non-invasive cardiac diagnostics using ECG. The code is available at: https://github. com/alsalivan/ecgcmr.", "filename": "2025_0021.pdf", "year": 2025, "institution": "Technical University of Munich (TUM)", "country": "Germany", "authors": ["Alexander Selivanov", "Philip Müller", "Özgün Turgut", "Nil Stolt-Ansó", "Daniel Rueckert"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0022", "x": 4.372, "y": 6.696, "title": "HAGE: Hierarchical Alignment Gene-Enhanced Pathology Representation Learning with Spatial Transcriptomics", "abstract": "Histopathology images capture tissue morphology, while spatial transcriptomics (ST) provides spatially resolved gene expression, offering complementary molecular insights. However, acquiring ST data is costly and time-consuming, limiting its practical use. To address this, we propose HAGE (Hierarchical Alignment Gene-Enhanced), a framework that enhances pathology representation learning by predicting gene expression directly from histological images and integrating molecular context into the pathology model. HAGE leverages genetype embeddings, which encode relationships among genes, guiding the model in learning biologically meaningful expression patterns. To further improve alignment between histology and gene expression, we introduce a hierarchical clustering strategy that groups image patches based on molecular and visual similarity, capturing both local and global dependencies. HAGE consistently outperforms existing methods across six datasets. In particular, on the HER2+ breast cancer cohort, it significantly improves the Pearson correlation coefficient by 8.0% and achieves substantial reductions in mean squared error and mean absolute error by 18.1% and 38.0%, respectively. Beyond gene expression prediction, HAGE improves downstream tasks, such as patch-level cancer classification and whole-slide image diagnostics, demonstrating its broader applicability. To the best of our knowledge, HAGE is the first framework to integrate gene co-expression as prior knowledge into a pathology image encoder via a cross-attention mechanism, enabling more biologically informed and accurate pathology representations. https://github. com/uta-smile/gene_expression.", "filename": "2025_0022.pdf", "year": 2025, "institution": "The University of Texas at Arlington", "country": "USA", "authors": ["Thao M Dang", "Haiqing Li", "Yuzhi Guo", "Hehuan Ma", "Feng Jiang", "Yuwei Miao", "Qifeng Zhou", "Jean Gao", "Junzhou Huang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0023", "x": 2.932, "y": 6.121, "title": "Hierarchical Self-supervised Adversarial Training for Robust Vision Models in Histopathology", "abstract": "Adversarial attacks pose significant challenges for vision models in critical fields like healthcare, where reliability is essential. Although adversarial training has been well studied in natural images, its application to biomedical and microscopy data remains limited. Existing self-supervised adversarial training methods overlook the hierarchical structure of histopathology images, where patient-slide-patch relationships provide valuable discriminative signals. To address this, we propose Hierarchical Self-Supervised Adversarial Training (HSAT), which exploits these properties to craft adversarial examples using multi-level contrastive learning and integrate it into adversarial training for enhanced robustness. We evaluate HSAT on multiclass histopathology dataset Open-SRH and the results show that HSAT outperforms existing methods from both biomedical and natural image domains. HSAT enhances robustness, achieving an average gain of 54.31% in the white-box setting and reducing performance drops to 3-4% in the black-box setting, compared to 25-30% for the baseline. These results set a new benchmark for adversarial training in this domain, paving the way for more robust models. Code and models are available at https://github.com/HashmatShadab/ HSAT.", "filename": "2025_0023.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Hashmat Shadab Malik", "Shahina Kunhimon", "Muzammal Naseer", "Fahad Shahbaz Khan", "Salman Khan"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0024", "x": 3.679, "y": 5.833, "title": "High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection", "abstract": "Automatic detection of abnormal cervical cells from Thinprep Cytologic Test (TCT) images is a critical component in the development of intelligent computer-aided diagnostic systems. However, existing algorithms typically fail to effectively model the correlations of visual features, while these spatial correlation features actually contain critical diagnostic information. Furthermore, no detection algorithm has the ability to integrate inter-correlation features of cells with intra-discriminative features of cells, lacking a fusion strategy for the end-to-end detection model. In this work, we propose a hypergraph-based cell detection network that effectively fuses different types of features, combining spatial correlation features and deep discriminative features. Specifically, we use a Multi-level Fusion Sub-network (MLF-SNet) to enhance feature extraction capabilities. Then we introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation module (CLFFS-HC), to integrate mixed features. Finally, we conducted experiments on three publicly available datasets, and the results demonstrate that our method significantly improves the performance of cervical abnormal cell detection. The code is publicly available at https://github.com/ddddoreen/ HyperMF2-Cell-Detection.", "filename": "2025_0024.pdf", "year": 2025, "institution": "Nantong University", "country": "China", "authors": ["Jincheng Li", "Danyang Dong", "Menglin Zheng", "Jingbo Zhang", "Yueqin Hang", "Lichi Zhang", "Lili Zhao"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0025", "x": 3.009, "y": 5.129, "title": "Hybrid-View Attention Network for Clinically Significant Prostate Cancer Classification in Transrectal Ultrasound", "abstract": "Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, and accurate identification of clinically significant PCa (csPCa) is critical for timely intervention. Transrectal ultrasound (TRUS) is widely used for prostate biopsy; however, its low contrast and anisotropic spatial resolution pose diagnostic challenges. To address these limitations, we propose a novel hybrid-view attention (HVA) network for csPCa classification in 3D TRUS that leverages complementary information from transverse and sagittal views. Our approach integrates a CNN-transformer hybrid architecture, where convolutional layers extract fine-grained local features and transformer-based HVA models global dependencies. Specifically, the HVA comprises intra-view attention to refine features within a single view and cross-view attention to incorporate complementary information across views. Furthermore, a hybridview adaptive fusion module dynamically aggregates features along both channel and spatial dimensions, enhancing the overall representation. Experiments are conducted on an in-house dataset containing 590 subjects who underwent prostate biopsy. Comparative and ablation results prove the efficacy of our method. The code is available at https://github. com/mock1ngbrd/HVAN.", "filename": "2025_0025.pdf", "year": 2025, "institution": "Shenzhen University Medical School", "country": "China", "authors": ["Zetian Feng", "Juan Fu", "Xuebin Zou", "Hongsheng Ye", "Hong Wu", "Jianhua Zhou", "Yi Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0026", "x": 1.062, "y": 4.278, "title": "IKAN: Interactive KAN with Modulation Fusion for Medical Image Segmentation", "abstract": "Interactive segmentation in medical imaging remains challenged by progressive loss of crucial interaction cues (click responsiveness, boundary fidelity) in deep networks. To address this limitation, we propose Interactive Kolmogorov-Arnold Network with adaptive modulation (IKAN ), a unified framework that synergistically preserves interaction signals through spline-activated basis functions while enabling iterative anatomical refinement. The architecture achieves enhanced diagnostic fidelity by integrating three core components: hierarchical multi-scale feature extraction through Hierarchical Inception and Channel Attention Module (HICAM), dual-branch adaptive probability modulation for backbone/side-feature fusion, and click density-guided prediction sharpening. By dynamically correlating user-provided clicks with multi-modal data patterns, our method resolves ambiguous boundaries in complex clinical scenarios. Evaluated across OCT, BUSI, and AISD datasets, our method demonstrates enhanced segmentation accuracy in complex clinical scenarios, outperforming state-of-the-art approaches through systematic preservation and amplification of diagnostic interaction cues. The code is available at https://github.com/Umbrellaliu/iKAN.", "filename": "2025_0026.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Sihan Liu", "Tonghua Wan", "Yuxin Cai", "Shengcai Chen", "Bo Hu", "Yan Wan", "Wu Qiu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0027", "x": -0.236, "y": 4.961, "title": "Knowledge Bridges the Intent Gap: Contextual Fusion in Medical Fine-Grained Segmentation", "abstract": "Segment Anything Model (SAM) has been widely used in common medical image segmentation for its great zero-shot generalization by providing points or box as prompt. However, we find that SAM and its variants do not cope well with complex fine-grained segmentation tasks such as kidney anatomical structure segmentation due to the discrepancy between the model's interpretation of the task and the actual intent conveyed by the prompts. This paper introduces a new approach called Knowledge SAM (KSAM). By providing a pair of example image and corresponding fine-grained segmentation mask as the knowledge prompt, model can utilize the contextual information to better understand the meaning of the unseen fine-grained segmentation task. To accommodate knowledge prompts, we design two modules specifically designed for knowledge prompt feature fusion. KSAM outperforms the SAM models based on different prompts across both our proposed kidney anatomical structure dataset and REFUGE. Notably, our approach demonstrates competitive performance while offering better extensibility on new tasks compared with prompt-free methods.", "filename": "2025_0027.pdf", "year": 2025, "institution": "National University of Defense Technology", "country": "China", "authors": ["Hengyuan Zhang", "Peng Qiao", "Wenyu Li", "Yan Jia", "Yong Dou"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0028", "x": 1.932, "y": 6.338, "title": "Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification", "abstract": "The pursuit of decision safety in clinical applications highlights the potential of transparent methods in medical imaging. While concept-based models offer local concept explanations (instance-level), they often neglect the global decision logic (dataset-level). Moreover, these models often suffer from concept leakage, where unintended information within soft concept representations undermines both interpretability and generalizability. To address these limitations, we propose Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules from binary visual concepts. CRL employs logical layers to capture concept correlations and extract clinically meaningful rules, thereby providing both local and global interpretability. The results from two tasks demonstrate that CRL achieves competitive performance with existing interpretable methods while improving generalizability to outof-distribution data. The code of our work is available at https://github. com/obiyoag/crl", "filename": "2025_0028.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Yibo Gao", "Hangqi Zhou", "Zheyao Gao", "Bomin Wang", "Shangqi Gao", "Sihan Wang", "Xiahai Zhuang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0029", "x": 0.26, "y": 5.299, "title": "Location-Aware Parameter Fine-Tuning for Multimodal Image Segmentation", "abstract": "Accurate segmentation of lung infection regions is critical for early diagnosis and quantitative assessment of disease severity. However, existing segmentation methods largely depend on high-quality, manually annotated data. Although some approaches have attempted to alleviate the reliance on detailed annotations by leveraging radiology reports, their complex model architectures often hinder practical training and widespread clinical deployment. With the advent of large-scale pretrained foundation models, efficient and lightweight segmentation frameworks have become feasible. In this work, we propose a novel segmentation framework that utilizes CLIP to generate multimodal high-quality prompts, including coarse mask, point, and text prompts, which are subsequently fed into the Segment Anything Model 2 (SAM2) to produce the final segmentation results. To fully exploit the informative content of medical reports, we introduce a localization loss that extracts positional cues from the text to guide the model in localizing potential lesion regions. Experiments on the CT dataset MosMedData+ and the X-ray dataset QaTa-COV19 demonstrate that our method achieves state-ofthe-art performance while requiring only minimal parameter fine-tuning. These results highlight the effectiveness and clinical potential for pulmonary infection segmentation.", "filename": "2025_0029.pdf", "year": 2025, "institution": "The University of New South Wales", "country": "Australia", "authors": ["Sicong Gao", "Maurice Pagnucco", "Yang Song"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0030", "x": 3.903, "y": 6.026, "title": "Lymph Node Metastasis Classification with Prototype-Guided Multiple Instance Aggregation and Heterogeneous Feature Fusion", "abstract": "Lymph node metastasis diagnosis in computed tomography (CT) scans is an essential yet very challenging task for esophageal cancer staging and treatment planning. Recent advances in deep learning have markedly improved the performance in lymph node (LN) metastasis classification. However, these methods often focus more on the averaged features of all CT slices containing a 3D LN instance, lacking effective fusion of key slice-wise features, which is important in the LN metastasis analysis by physicians. In addition, existing deep learning models are trained using CT scans in an end-to-end fashion, thus lacking the explicit incorporation of clinically relevant meta-imaging features (i.e., morphological and radiomic features). Meta-imaging features play a crucial role in LN assessment and may not be effectively captured by direct endto-end deep learning models. To address these issues, we formulate the 3D LN metastasis classification as a multiple instance learning (MIL) problem by extracting and fusing slice-level features (instance) into a comprehensive bag representation. Building on this, we propose a twostreamed MIL framework with a prototype-guided aggregation method that effectively captures LN characteristics at both local and global scales. Furthermore, a multi-scale multi-source fusion module is introduced to integrate the heterogeneous meta-imaging features with deep learning features, enhancing the comprehensive representation of LN. Five-fold cross-validation on a cohort of 284 esophageal cancer patients with 809 pathology-confirmed LN instances demonstrate the superiority of our methods compared to the state-of-the-art approaches with +2.66% in AUROC and +4.81% in sensitivity improvements.", "filename": "2025_0030.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Haoshen Li", "Tashan Ai", "Yirui Wang", "Zhanghexuan Ji", "Qinji Yu", "Le Lu", "Bin Dong", "Li Zhang", "Xianghua Ye", "Kuaile Zhao", "Dakai Jin"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0031", "x": 4.184, "y": 5.807, "title": "Lymphoma Prognosis with Lesion-Anatomy Context Fusion and Attention-Based Multi-lesion Aggregation", "abstract": "Early identification of lymphoma patients with poor prognosis is crucial to determining personalized treatment plans and improving prognosis. Currently, commonly used prognostic biomarkers include clinical variables such as International Prognostic Index. Quantitative parameters based on PET/CT and deep learning methods have also shown promising results. However, there are still several challenges in PET/CTbased prognostic studies: heterogeneity in the number and location of lesions, insufficient representation of lesion features, and the lack of anatomical context modeling of the lesions. We propose a novel framework named LAMP, with lesion-anatomy context fusion and attentionbased multi-lesion aggregation as its two key components. The former takes into account information about the surrounding anatomical organs of the lesions to improve their representation. The latter treats each lesion region as an instance, assigning attention scores that reflect the contribution of each lesion, and aggregates them accordingly. A total of 229 lymphoma patients were collected to evaluate our model. In prediction tasks for progression-free survival and overall survival, the 5-fold crossvalidation C-index is 0.791 and 0.828, respectively, outperforming existing models based on clinical variables and deep learning. LAMP has the potential to become a clinical auxiliary tool to differentiate patients with varying risk levels, facilitating the development of personalized treatment plans.", "filename": "2025_0031.pdf", "year": 2025, "institution": "Alibaba Group", "country": "China", "authors": ["Song Zhang", "Jiajin Zhang", "Liheng Qiu", "Wei Liu", "Dakai Jin", "Le Lu", "Shenmiao Yang", "Ke Yan"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0032", "x": 5.76, "y": 3.85, "title": "MambaMER: Adaptive EEG-Guided Multimodal Emotion Recognition with Mamba", "abstract": "In recent years, multimodal emotion recognition has gradually become a research hotspot. Although existing methods have achieved significant results by integrating information from different modalities, irrelevant or conflicting emotional information across modalities often limits performance improvement. Inspired by Mamba's ability to effectively filter irrelevant information and model long-range dependencies with linear complexity, we propose a new paradigm for EEG-guided adaptive multimodal emotion recognition with Mamba. This paradigm effectively addresses the interference caused by cross-modal information conflicts, enhancing the performance of multimodal emotion recognition. Firstly, to alleviate the interference caused by conflicts between different modalities, we design a multi-scale EEG-guided conflict suppression module. Guided by multi-scale EEG features, this module uses a selective cross state space model to suppress irrelevant information and conflicts in eye movement features, thereby obtaining enhanced eye movement features. Secondly, to deeply integrate the complementary features between the EEG modality and the enhanced eye movement modality, we propose a novel cross-modal fusion mechanism, consisting of Mutual-Cross-Mamba and Merge-Mamba, which effectively captures long-range dependencies in the fused features, thereby enhancing the integration and utilization of cross-modal information. Experimental results on the SEED, SEED-IV, and SEED-V datasets demonstrate that our method significantly surpasses current state-of-the-art methods.", "filename": "2025_0032.pdf", "year": 2025, "institution": "Shandong Normal University", "country": "China", "authors": ["Xiangle Ping", "Wenhui Huang", "Yuanjie Zheng"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0033", "x": 3.919, "y": 1.818, "title": "Meta-learning Physics-Informed Neural Networks for Personalized Cardiac Modeling", "abstract": "The advancement of personalized cardiac modeling, particularly through digital cardiac twins, enables tailored treatments based on the physiology of the individual patient. Traditional physics-based methods for optimizing the parameters of these cardiac models face challenges in clinical adoption due to their computational cost. Recent shifts towards data-driven approaches offer improved efficiency, but struggle with generalization and integration of core electrophysiological principles. The emerging use of physics-informed neural networks (PINNs) has the potential to combine the advantages of these two approaches, although still requiring retraining from scratch for each subject. This paper introduces a novel framework for meta-learning PINNs to ov ercome these challenges, enabling rapid personalization of a PINN to new subjects' data via simple feedforward computation. We instantiate this meta-PINN framework using the Eikonal model as the governing physics, demonstrating its efficacy in significantly reducing computational demands while improving the predictive accuracy of personalized cardiac models. Source code available at https://github.com/temporary- repos/MICCAI2025", "filename": "2025_0033.pdf", "year": 2025, "institution": "Rochester Institute of Technology (RIT)", "country": "USA", "authors": ["Maryam Toloubidokhti", "Ryan Missel", "Shichang Lian", "Linwei Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0034", "x": 0.633, "y": 4.86, "title": "MG-UNet: A Memory-Guided UNet for Lesion Segmentation in Chest Images", "abstract": "Lesion segmentation in medical images is a key task for the intelligent diagnosis of lung diseases. Although existing multimodal methods have achieved significant progress in medical image segmentation by combining image and text information, these methods still rely on textual input during the inference phase, limiting their applicability in real-world scenarios. To address this limitation, this paper proposes an innovative Memory-Guided UNet model (MG-UNet). MG-UNet introduces a learnable memory bank that automatically extracts and stores textual information during the training phase. In the decoding stage, the proposed memory-guided decoder retrieves knowledge relevant to the current image from the memory bank, thereby eliminating the need for textual input during inference. Extensive experiments were conducted on the QaTa-Cov19 and MosMedData+ datasets to validate the effectiveness of MG-UNet. The experimental results demonstrate that MG-UNet not only outperforms existing unimodal and multimodal methods in terms of segmentation performance but also excels in text-free inference scenarios using only 15% of the training data, surpassing the current best unimodal methods. This characteristic significantly reduces the reliance on annotated data for medical image segmentation, offering greater flexibility and scalability for practical clinical applications. The code will be available soon.", "filename": "2025_0034.pdf", "year": 2025, "institution": "Chongqing Normal University", "country": "China", "authors": ["Shuaipeng Ding", "Mingyong Li", "Chao Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0035", "x": 3.87, "y": 5.566, "title": "MIBF-Net: Multi-Modal Information Balanced Fusion Network for Clinical Diagnosis via Patient Narratives and Lesion Image", "abstract": "Accurate clinical diagnosis requires comprehensive analysis of medical imaging and patient narratives. However, current computeraided diagnosis methods focus primarily on imaging modalities while neglecting the integration of patient-reported clinical narratives, due to the scarcity of high-quality patient narratives and the limitations in multimodal information fusion. To address these issues, we propose a dualcomponent framework consisting of: 1) a Retrieval Augmented Patient Narratives Generation Module (RANGM) that employs a retrievalenhanced mechanism to guide pre-trained large language models in generating clinically plausible patient narratives; and 2) a Multimodal Information Balanced Fusion Network (MIBF-Net) incorporating our novel Information Balanced Fusion Attention (IBFA) module for effective cross-modal integration, along with a Modal Prediction-Div ergent Loss (MP-Loss) to enhance the model's ability to diagnose samples that have ambiguous single modal prediction distribution. Owing to the plug-andplay design, our MIBF-Net can integrate with existing imaging-based state-of-the-art methods. Extensive experiments demonstrate significant performance improvements of 2.3%-4.6% on the HAM10000 dataset and 3.8%-6.4% on the ISIC2019 dataset. Our code is publicly available at https://github.com/sysu19351118/MIBF-Net.", "filename": "2025_0035.pdf", "year": 2025, "institution": "Sun Yat-Sen U niversity", "country": "China", "authors": ["Zixuan Tang", "Bai Sun", "Shidan He", "Yuan Hong", "Dongdong Yu", "Zhenzhong Liu", "Mengtang Li", "Bin Chen", "Shen Zhao"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0036", "x": 3.77, "y": 6.873, "title": "MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis", "abstract": "Multiple instance learning (MIL) has shown significant promise in histopathology whole slide image (WSI) analysis for cancer diagnosis and prognosis. However, the inherent spatial heterogeneity of WSIs presents critical challenges, as morphologically similar tissue types are often dispersed across distant anatomical regions. Conventional MIL methods struggle to model these scattered tissue distributions and capture cross-regional spatial interactions effectively. To address these limitations, we propose a novel Multiple instance learning framework with Context-Aware Clustering (MiCo), designed to enhance cross-regional intra-tissue correlations and strengthen inter-tissue semantic associations in WSIs. MiCo begins by clustering instances to distill discriminative morphological patterns, with cluster centroids serving as semantic anchors. To enhance cross-regional intra-tissue correlations, MiCo employs a Cluster Route module, which dynamically links instances of the same tissue type across distant regions via feature similarity. These semantic anchors act as con textual hubs, propagating semantic relationships to refine instance-level representations. To eliminate semantic fragmentation and strengthen inter-tissue semantic associations, MiCo integrates a Cluster Reducer module, which consolidates redundant anchors while enhancing information exchange between distinct semantic groups. Extensive experiments on two challenging tasks across nine large-scale public cancer datasets demonstrate the effectiveness of MiCo, showcasing its superiority over state-of-the-art methods. The code is available at https://github.com/junjianli106/MiCo.", "filename": "2025_0036.pdf", "year": 2025, "institution": "Central South University", "country": "China", "authors": ["Junjian Li", "Jin Liu", "Hulin Kuang", "Hailin Yue", "Mengshen He", "Jianxin Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0037", "x": 3.76, "y": 6.825, "title": "MicroMIL: Graph-Based Multiple Instance Learning for Context-Aware Diagnosis with Microscopic Images", "abstract": "Cancer diagnosis has greatly benefited from the integration of whole-slide images (WSIs) with multiple instance learning (MIL), enabling high-resolution analysis of tissue morphology. Graph-based MIL (GNN-MIL) approaches have emerged as powerful solutions for capturing contextual information in WSIs, thereby improving diagnostic accuracy. However, WSIs require significant computational and infrastructural resources, limiting accessibility in resource-constrained settings. Conventional light microscopes offer a cost-effective alternative, but applying GNN-MIL to such data is challenging due to extensive redundant images and missing spatial coordinates, which hinder contextual learning. To address these issues, we introduce MicroMIL, the first weaklysupervised MIL framework specifically designed for images acquired from conventional light microscopes. MicroMIL leverages a representative image extractor (RIE) that employs deep cluster embedding (DCE) and hard Gumbel-Softmax to dynamically reduce redundancy and select representative images. These images serve as graph nodes, with edges computed via cosine similarity, eliminating the need for spatial coordinates while preserving contextual information. Extensive experiments on a real-world colon cancer dataset and the BreakHis dataset demonstrate that MicroMIL achieves state-of-the-art performance, improving both diagnostic accuracy and robustness to redundancy. The code is available at https://github.com/kimjongwoo-cell/MicroMIL.", "filename": "2025_0037.pdf", "year": 2025, "institution": "Korea Advanced Institute of Science and Technology", "country": "South Korea", "authors": ["Jongwoo Kim", "Bryan Wong", "Huazhu Fu", "Willmer Rafell Quiñones Robles", "Young Sin Ko", "Mun Yong Yi"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0038", "x": 6.643, "y": 3.773, "title": "MNM: Multi-level Neuroimaging Meta-analysis with Hyperbolic Brain-Text Representations", "abstract": "Various neuroimaging studies suffer from small sample size problem which often limit their reliability. Meta-analysis addresses this challenge by aggregating findings from different studies to identify consistent patterns of brain activity. However, traditional approaches based on keyword retrieval or linear mappings often overlook the rich hierarchical structure in the brain. In this work, we propose a novel framework that leverages hyperbolic geometry to bridge the gap between neuroscience literature and brain activation maps. By embedding text from research articles and corresponding brain images into a shared hyperbolic space via the Lorentz model, our method captures both semantic similarity and hierarchical organization inherent in neuroimaging data. In the hyperbolic space, our method performs multi-level neuroimaging meta-analysis (MNM) by 1) aligning brain and text embeddings for semantic correspondence, 2) guiding hierarchy between text and brain activations, and 3) preserving the hierarchical relationships within brain activation patterns. Experimental results demonstrate that our model outperforms baselines, offering a robust and interpretable paradigm of multi-level neuroimaging meta-analysis via hyperbolic brain-text representation.", "filename": "2025_0038.pdf", "year": 2025, "institution": "Pohang University of Science and Technology", "country": "South Korea", "authors": ["Seunghun Baek", "Jaejin Lee", "Jaeyoon Sim", "Minjae Jeong", "Won Hwa Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0039", "x": 0.614, "y": 3.413, "title": "MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion During Ultrasound-Guided Aspiration Biopsy", "abstract": "Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally invasive diagnostic procedure. However, an aspiration needle tracker addressing rapid reciprocating motion is still missing. MrTrack, an aspiration needle tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a Mamba-based register extractor to sequentially distill global context from each historical search map, storing these temporal cues in a register bank. The Mamba-based register retriever then retrieves temporal prompts from the register bank to provide external cues when current vision features are temporarily unusable due to rapid reciprocating motion and imaging degradation. A self-supervised register div ersify loss is proposed to encourage feature diversity and dimension independence within the learned register, mitigating feature collapse. Comprehensive experiments conducted on both robotic and manual aspiration biopsy datasets demonstrate that MrTrack not only outperforms state-of-the-art trackers in accuracy and robustness but also achieves superior inference efficiency. Project page: https://github.com/PieceZhang/MrTrack.", "filename": "2025_0039.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "Hong Kong", "authors": ["Yuelin Zhang", "Qingpeng Ding", "Long Lei", "Yongxuan Feng", "Raymond Shing-Yan Tang", "Shing Shin Cheng"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0040", "x": 6.683, "y": 3.79, "title": "MSGFlowNet: Learning Effective Connectivity Network Based on Sparse Generative Flow Network from fMRI and EEG Data", "abstract": "Brain effective connectivity (EC) is key to understanding causal neural interactions and brain organization. However, learning EC from single-modal brain data, such as functional magnetic resonance imaging (fMRI) or electroencephalography (EEG), is limited by the inability to simultaneously capture sparse temporal and spatial information. This paper proposes a novel multimodal sparse generative flow network (MSGFlowNet), which integrates fMRI and EEG data through an attention-guided encoder and employs a multi-head self-attention sparse Transformer to extract features from the fused data. These features are then processed by two output heads of the generative flow network: one computes state transition probabilities and updates the mask, while the other determines the probability of generating a termination state. Experiments on synthetic and real-world datasets demonstrate that MSGFlowNet significantly outperforms state-of-the-art methods.", "filename": "2025_0040.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Zhihao Su", "Jihao Zhai", "Junzhong Ji", "Jinduo Liu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0041", "x": 4.39, "y": 4.162, "title": "Multi-agent Reasoning for Cardiovascular Imaging Phenotype Analysis", "abstract": "Identifying the associations between imaging phenotypes and disease risk factors and outcomes is essential for understanding disease mechanisms and improving diagnosis and prognosis models. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, nonlinear dependencies among imaging phenotypes and other multi-modal data. To address this, we introduce a Multi-agent Exploratory Synergy for the Heart (MESHAgents) framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies, using cardiovascular imaging as a proof of concept. Specifically, we orchestrate a multi-disciplinary team of AI agents, which spontaneously generate and converge on insights through iterative, self-organizing reasoning. The framework dynamically synthesizes statistical correlations with multi-expert consensus, providing an automated pipeline for phenome-wide association studies (Phe-WAS). We demonstrate the system's capabilities through a populationbased study of imaging phenotypes of the heart and aorta. MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as .-0.004±0.010 on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types. Our framework provides clinically relevant imaging phenoty pes with transparent reasoning, offering a scalable alternative to expert-driven methods.Weitong Zhang and Mengyun Qiao:-Equal contribution.", "filename": "2025_0041.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Weitong Zhang", "Mengyun Qiao", "Chengqi Zang", "Steven Niederer", "Paul M Matthews", "Wenjia Bai", "Bernhard Kainz"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0042", "x": 5.725, "y": 3.861, "title": "Multi-expert Collaboration and Knowledge Enhancement Network for Multimodal Emotion Recognition", "abstract": "Emotion recognition leveraging multimodal data plays a pivotal role in human-computer interaction and clinical applications, such as depression, mania, Parkinson's Disease, etc. However, existing emotion recognition methods are susceptible to heterogeneous feature representations across modalities. Additionally, complex emotions involve multiple dimensions, which presents challenges for achieving highly trustworthy decisions. To address these challenges, in this paper, we propose a novel multi-expert collaboration and knowledge enhancement network for multimodal emotion recognition. First, we devise a cross-modal fusion module to dynamically aggregate complementary features from EEG and facial expressions through attention-guided. Second, our approach incorporates a feature prototype alignment module to enhance the consistency of multimodal feature represen tations. Then, we design a prior knowledge enhancement module that injects original dynamic brain networks into feature learning to enhance the feature representation. Finally, we introduce a multi-expert collaborative decision module designed to refine predictions, enhancing the robustness of classification results. Experimental results on the DEAP dataset demonstrate that our proposed method surpasses several state-of-the-art emotion recognition techniques.", "filename": "2025_0042.pdf", "year": 2025, "institution": "Nanjing University of Aeronautics and Astronautics", "country": "China", "authors": ["Kun Wang", "Junyong Zhao", "Liying Zhang", "Qi Zhu", "Daoqiang Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0043", "x": 3.958, "y": 5.701, "title": "Multimodal Fusion Network with Distribution-Based Tumor-Marker Imputation for Multi-origin Metastatic Cervical Lymphadenopathy Classification", "abstract": "Accurate identification the primary tumor of metastatic cervical lymphadenopathy (CLA) is crucial for guiding clinical treatment, yet clinical diagnosis remains challenging due to the complexity of tracing multi-potential origins using ultrasound images and incomplete clinical information. Existing deep learning methods typically utilize the imaging semantic features from B-mode ultrasound (BUS) and color Doppler flow imaging (CDFI), or incorporate basic clinical information, neglecting the importance of patient-specific features such as tumor markers (TMs) in clinical diagnosis. To address these limitations, we propose a new multimodal imaging-features and distribution-based tumor-marker fusion network (MDFN) for five categories of CLA metastatic origins (thyroid, head and neck, respiratory, female reproductive, and digestive). First, a distribution-based TM imputation method is proposed to reconstruct missing TMs, which treats the available clinical information of each patient (such as sex, age, neck region, etc.) as a vector to construct data distributions between TMs a nd avoid the data bias issues. Building on these personalized TMs, we propose the first population-personalized fusion framework, which integrates semantic features related to lymph node morphology from BUS images, semantic features related to vascular distribution from CDFI images, and TM features consistent with individualized patient data, thereby simulating clinical reasoning patterns. The effectiveness of the proposed MDFN method was evaluated using extensive experimental results from 3,100 multi-origin metastatic CLA cases, achieving an area under the receiver operating characteristic R. Li, C. L i-Equal contribution.", "filename": "2025_0043.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Rui Li", "Chunyan Li", "Xi Lin", "Jinfeng Xu", "Fang Li", "Yao Lu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0044", "x": 4.079, "y": 5.248, "title": "Non-invasive TB Detection Using Acoustic and Semantic Features from Cough Sounds", "abstract": "We present a novel dual-stream deep learning architecture, AcouSem-AFNet, for automated tuberculosis (TB) detection using acoustic analysis of respiratory sounds. The proposed architecture utilizes two complementary pathways to extract distinct semantic and acoustic characteristics essential for identifying TB-related respiratory patterns. Specifically, the semantic stream employs a Whisper encoder to model structured patterns in respiratory events, while the acoustic stream leverages WavLM to capture detailed temporal dynamics characteristic of TB cough sounds. These distinct features are fused through a specialized backbone with squeeze-excitation mechanisms and residual connections, designed explicitly to maintain discriminative capabilities and mitigate overfitting challenges typical of limited medical datasets. Evaluated on the CODA-TB challenge dataset, our approach achieves state-of-the-art performance with an accuracy of 78.10% and an AUC of 0.79, demonstrating improvements of 3% in AUC and 2% in accuracy over leading baseline methods. Our framework enables rapid, noninvasive TB screening, particularly beneficial for resource-limited settings, demonstrating the feasibility of deep learning-based acoustic analysis as a scalable, preliminary diagnostic tool to enhance global TB screening accessibility. The code and models are publicly available at https://github.com/IAB-IITJ/AcouSem-AFNet", "filename": "2025_0044.pdf", "year": 2025, "institution": "Indian Institute of Technology Jodhpur", "country": "India", "authors": ["Yasmeena Akhter", "Rishabh Ranjan", "Bikash Dutta", "Mayank Vatsa", "Richa Singh"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0045", "x": 0.67, "y": 4.489, "title": "Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation", "abstract": "Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for enhanced disease diagnosis, particularly in computational pathology, offering rich spectral information that aids in identifying subtle biochemical properties of tissues. Despite these advantages, effectively fusing both spatial-dimensional and spectral-dimensional information from MHSIs remains challenging due to its high dimensionality and spectral redundancy inherent characteristics. To solve the above challenges, we propose a novel spatial-spectral omni-fusion network for hyperspectral image segmentation, named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature fusion operations, including (1) a cross-dimensional enhancement module that refines both spatial and spectral features through bidirectional attention mechanisms; (2) a spectral-guided spatial query selection to select the most spectralrelated spatial feature as the query; and (3) a t wo-stage cross-dimensional decoder which dynamically guide the model's attention towards the selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains efficient in execution. Experiments on two microscopic hyperspectral image datasets show that our approach can significantly improve the segmentation performance compared with the state-of-the-art methods, with over 5.73% improvement in DSC. Code available at: https:// github.com/DeepMed-Lab-ECNU/Omni-Fuse. Q. Zhang and G. Pei-Denotes e qual contribution.", "filename": "2025_0045.pdf", "year": 2025, "institution": "East C hina Normal University", "country": "China", "authors": ["Qing Zhang", "Guoquan Pei", "Yan Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0046", "x": 4.038, "y": 5.842, "title": "PDF-Net: Prototype-Aware Dynamic Fusion Network for Nasopharyngeal Carcinoma T-Staging Classification with Epstein-Barr Virus DNA", "abstract": "Accurate T-staging classification of nasopharyngeal carcinoma (NPC) is crucial for guiding individualized treatment strategies and predicting patient prognosis. However, this task remains challenging due to the limitations of unimodal approaches, which often fail to capture the full complexity of NPC progression, and the severe class imbalance in clinical datasets, where early-stage cases (T1/T2 stage) are significantly underrepresented. In this paper, we propose a Prototype-Aware Dynamic Fusion Network (PDF-Net), a novel multimodal framework that integrates MR images with Epstein-Barr virus (EBV) DNA tabular data to improve NPC T-staging classification. Our framework introduces two key components: (1) the Dynamic Multi-Modal Alignment (DMMA) module, which aligns MR imaging features with EBV DNA data to capture complementary information across modalities, and (2) the Optimal Prototype-Aware Transport (OPAT) module, which incorporates a Prototypical Constraint to enhance the representation of T2-staging features and mitigate class imbalance. To the best of our knowledge, PDF-Net is the first framework to leverage EBV DNA data as an auxiliary tool for T-staging classification, significantly improving accuracy and robustness. Experimental results in a real clinical dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an accuracy of 0.8006 ± 0.0488 and an AUC of 0.8191 ± 0.0551 for T1C images, highlighting its potential to advance NPC diagnosis and personalized treatment s trategies.", "filename": "2025_0046.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Wantong Lu", "Xu Han", "Yibo Wei", "Zanting Ye", "Lijun Lu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0047", "x": 3.729, "y": 1.726, "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac $$T_1$$ Mapping", "abstract": "Spin-lattice relaxation time (T 1) is an important biomarker in cardiac parametric mapping for characterizing myocardial tissue and diagnosing cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI) acquires 11 breath-hold baseline images with interleaved rest periods to ensure mapping accuracy. However, prolonged scanning can be challenging for patients with poor breathholds, often leading to motion artifacts that degrade image quality. In addition, T1 mapping requires a voxel-wise nonlinear fitting to a signal recovery model involving an iterative estimation process. Recent studies have proposed deep-learning approaches for rapid T1 mapping using shortened sequences to reduce acquisition time for patient comfort. Nevertheless, existing methods overlook important physics constraints, limiting interpretability and generalization. In this work, we present an accelerated, end-to-end T1 mapping framework leveraging Physics-Informed Neural Ordinary Differential Equations (ODEs) to model temporal dynamics and address these challenges. Our m ethod achieves high-accuracy T1 estimation from a sparse subset of baseline images and ensures efficient null index estimation at the test time. Specifically, we develop a continuoustime LSTM-ODE model to enable selective Look-Locker (LL) data acquisition with arbitrary time lags. Experimental results show superior performance in T1 estimation for both native and post-contrast sequences and demonstrate the strong benefit of our physics-based formulation over direct data-driven T1 priors.", "filename": "2025_0047.pdf", "year": 2025, "institution": "Delft University of T echnology", "country": "The Netherlands", "authors": ["Nuno Capitão", "Yi Zhang", "Yidong Zhao", "Qian Tao"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0048", "x": 2.866, "y": 5.153, "title": "PolarDETR: Enhancing Interpretability in Multi-modal Methods for Jawbone Lesion Detection in CBCT", "abstract": "Rapidly advancing multi-modal learning shows great promise in medical image analysis, but challenges remain in the detection of jawbone lesions. Existing general-purpose models fail to capture the relationships between anatomical contexts and spatial locations in CBCT images, and the complexity of these models hinders interpretability. We propose PolarDETR, a novel framework combining anatomical priors and multi-modal alignment through: 1) Polar Text-Position Encoding (PTPE), which links text to spatial coordinates via polar mapping, 2) Anatomical Constraint Learning, ensuring lesion detection within anatomically plausible regions, and 3) Position Matching Optimization for spatial consistency. Evaluated on 180 clinical cases (6929 CBCT slices), our method achieves a state-of-the-art mAP of 93.66%, outperforming both single-modal (e.g., DETR at 89.35%) and multimodal models (e.g., CORA at 91.52%). Additionally, PolarDETR excels in interpretability, with an ACS of 84.12% and PMS of 80.45%, demonstrating its potential to enhance both detection performance and clinical usability in real-world applications. Our code is available at https:// github.com/Cxxxsky/PolarDETR.", "filename": "2025_0048.pdf", "year": 2025, "institution": "Beijing Jiaotong University", "country": "China", "authors": ["Yuxuan Yang", "Chen Zhong", "Xinyue Zhang", "Ruohan Ma", "Gang Li", "Yong Guo", "Jupeng Li"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0049", "x": 2.036, "y": 2.93, "title": "Predicting Femoral Head Collapse Risk in Osteonecrosis Using Label Tokenization: A Multi-modality Survival Analysis Approach", "abstract": "Collapse of the femoral head is a critical event in osteonecrosis (ONFH) that often leads to debilitating hip pain and necessitates total hip arthroplasty. Early and accurate prediction of collapse risk is crucial for personalized treatment planning. While many studies focus on the automated diagnosis of ONFH, prognosis remains less explored. In this study, we propose a robust tri-stream deep learning framework that extracts features from T1-weighted MRI, region-of-interest (ROI) labels, and ONFH grades to estimate patient-specific collapse risk. We introduce an independent Spatial Label Encoder (SLE) module that tokenizes discrete ROI labels into dense, context-rich embeddings, thereby facilitating multi-modality model training. Experiments on 92 hips (70 patients) show that our approach performs competitively with state-ofthe-art (SOTA) methods across most metrics, ac hieving a concordance index (CI) of 0.847±0.087 and an integrated AUC of 0.884 in 5-fold crossvalidation. Notably, the SLE module enhances long-term discrimination by up to 2.4% on AUC at 60 months compared to our base network. These findings highlight the potential benefits of late-fusion strategies with label tokenization for predicting femoral head collapse in ONFH, contributing to improved early intervention and prognosis.", "filename": "2025_0049.pdf", "year": 2025, "institution": "Nara Institute of S cience and Technology", "country": "Japan", "authors": ["Ganping Li", "Yoshito Otake", "Yuito Kameda", "Keisuke Uemura", "Kazuma Takashima", "Hirokazu Mae", "Sotaro Kono", "Hidetoshi Hamada", "Seiji Okada", "Nobuhiko Sugano", "Yoshinobu Sato"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0050", "x": 2.965, "y": 5.167, "title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning", "abstract": "Retinal foundation models have significantly advanced retinal image analysis by leveraging self-supervised learning to reduce dependence on labeled data while achieving strong generalization. Many recent approaches enhance retinal image understanding using report supervision, but obtaining clinical reports is often costly and challenging. In contrast, metadata (e.g., age, gender) is widely available and serves as a valuable resource for analyzing disease progression. To effectively incorporate patient-specific information, we propose PRETI, a retinal foundation model that integrates metadata-aware learning with robust self-supervised representation learning. We introduce Learnable Metadata Embedding (LME), which dynamically refines metadata representations. Additionally, we construct patient-level data pairs, associating images from the same individual to improve robustness against nonclinical variations. To further optimize retinal image representation, we propose Retina-Aw are Adaptive Masking (RAAM), a strategy that selectively applies masking within the retinal region and dynamically adjusts the masking ratio during training. PRETI captures both global structures and fine-grained pathological details, resulting in superior diagnostic performance. Extensive experiments demonstrate that PRETI achieves state-of-the-art results across diverse diseases and biomarker predictions using in-house and public data, indicating the importance of metadata-guided foundation models in retinal disease analysis. Our code and pretrained model are available at https://github.com/MICV- yonsei/PRETI", "filename": "2025_0050.pdf", "year": 2025, "institution": "Yonsei University", "country": "South Korea", "authors": ["Yeonkyung Lee", "Woojung Han", "Youngjun Jun", "Hyeonmin Kim", "Jungkyung Cho", "Seong Jae Hwang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0051", "x": 4.396, "y": 5.003, "title": "Pre-to-Post Operative MRI Generation with Retrieval-Based Visual In-Context Learning", "abstract": "Glioblastoma is an aggressive brain tumor requiring precise treatment planning. Magnetic resonance imaging (MRI) is essential for pre-operative assessment, surgical resection planning, and postoperative monitoring. Therefore, generating post-operative MRI from pre-operative MRI can assist neurosurgeons in many ways, such as predicting surgical outcomes and guiding treatment planning. However, generating post-operative MRI from pre-operative MRI is challenging, as the resection extent depends on tumor location and infiltration to minimize potential complications, necessitating consideration of surgical outcomes based on tumor location and shape. Furthermore, post-operative MRI differs significantly from pre-operative MRI due to structural and visual changes, such as tissue shift, edema, hemorrhage, and the resection region. To address these challenges, we propose a novel post-operative MRI generation method that generates post-operative MRI from preoperative MRI using tumor-aware visual in-context learning. Specifically, we provide explicit visual instruction for generating post-op erative MRI from pre-operative MRI, improving the capture of structural changes. To consider tumor-specific post-operative outcomes, we propose tumorguided retrieval, which retrieves the tumor case most similar to the query pre-operative MRI, and a tumor-aware prompt adapter that integrates tumor resection and anatomical structure information. Our proposed method achieves superior performance on publicly available dataset and is the first to generate post-operative MRI from pre-operative MRI, introducing a new approach to improving patient prognosis.", "filename": "2025_0051.pdf", "year": 2025, "institution": "Korea Universit y", "country": "South Korea", "authors": ["Bogyeong Kang", "Sang-Jun Park", "Minjoo Lim", "Myeongkyun Kang", "Keun-Soo Heo", "Ji-Hye Oh", "Hyun Jung Lee", "Tae-Eui Kam"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0052", "x": 0.983, "y": 6.185, "title": "RadioFormer: Integrating Radiologist Inductive Bias for Tumor Classification on Multi-Sequence MR Images", "abstract": "Multi-sequence magnetic resonance imaging (MRI) plays a critical role in tumor diagnosis but relies heavily on manual interpretation, which is both labor-intensive and dependent on expert knowledge. While deep learning-based diagnostic methods show significant potential, they typically require large datasets for effective training. However, the high cost of data collection and annotation often limits the available dataset size. This highlights the need for models that can effectively train on small datasets, mitigate overfitting, and achieve reliable performance. To address these challenges, we propose RadioFormer, a novel model that incorporates radiologist inductive bias to facilitate efficient learning on small MRI datasets. Unlike traditional 2D or 3D architectures, RadioFormer emulates the radiologist's diagnostic process by explicitly parsing MRI data into three hierarchical levels: (1) singlesequence slice feature extraction, (2) multi-sequence slice information aggregation, and (3) inter-slice information (volume) aggregation. Each level builds upon the previous one, ensuring smooth information flow and a hierarchical understanding of lesion characteristics. By integrating expert knowledge into its design, RadioFormer effectively leverages inductive bias to enhance model generalization on small datasets. We evaluated RadioFormer on three public datasets for brain, breast, and liver tumor classification, where it achieved state-of-the-art performance across all tasks. The code and pre-processed data for RadioFormer are available at https://github.com/aa1234241/RadioFormer/tree/master.", "filename": "2025_0052.pdf", "year": 2025, "institution": "Polytechnical University in Shenzhen", "country": "China", "authors": ["Xiaoyu Bai", "Yong Xia"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0053", "x": 0.802, "y": 6.226, "title": "RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features", "abstract": "Medical image retrieval is a valuable field for supporting clinical decision-making, yet current methods primarily support 2D images and require fully annotated queries, limiting clinical flexibility. To address this, we propose RadiomicsRetrieval , a 3D contentbased retrieval framework bridging handcrafted radiomics descriptors with deep learning-based embeddings at the tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits volumetric data to leverage richer spatial context in medical images. We employ a promptable segmentation model (e.g., SAM) to derive tumor-specific image embeddings, which are aligned with radiomics features extracted from the same tumor via contrastive learning. These representations are further enriched by anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables flexible querying based on shape, location, or partial feature sets. Extensive experiments on both lung CT and brain MRI public datasets demonstrate that radiomics features significantly enhance retrieval specificity, while APE provides global anatomical context essential for location-based searches. Notably, our framework requires only minimal user prompts (e.g., a single point), minimizing segmentation overhead and supporting diverse clinical scenarios. The capability to query using either image embeddings or selected radiomics attributes highlights its adaptability, potentially benefiting diagnosis, treatment planning, and research on large-scale medical imaging repositories. Our code is available at https://github.com/nainye/ RadiomicsRetrieval.", "filename": "2025_0053.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "South Korea", "authors": ["Inye Na", "Nejung Rue", "Jiwon Chung", "Hyunjin Park"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0054", "x": 2.632, "y": 5.246, "title": "SAMASK-CLTR: A Spatial-Aware Mask Guided Learning Model for Benign and Malignant Tumor Classification in ABUS", "abstract": "Automated Breast Ultrasound (ABUS) provides three dimensional volumetric imaging that improves breast lesion detection without radiation exposure and reduces operator dependency. However, the resulting high data volume poses significant challenges for radiologists in localizing lesions accurately and distinguishing benign from malignant cases-challenges that can directly impact early diagnosis and treatment outcomes. To tackle these critical issues, we propose SAMASK-CLTR (Spatial-Aware Mask Prompting with Convolutional Transformer Architecture), a hybrid framework that combines the feature extraction power of CNNs with the global modeling capability of Transformers. In our approach, ResNet-50 extracts hierarchical, multi-scale features that are refined by a Transformer encoder-decoder to capture global context. Crucially, during decoding, a mask prompt enhanced with 3D positional encoding guides the network to focus on key tumor regions, directly addressing the challenges of precise localization and classification. Experiments on 7,073 ABUS images-including 6,973 clinical cases from Internal Datasets and 100 cases from the public ABUS Challenge Cup-demonstrate that SAMASK-CLTR achieves AUCs of 88.45% and 70.46% on internal and external datasets, respectively. These results highlight the potential of our framework to significantly enhance breast cancer diagnosis by improving the accuracy and reliability of lesion classification. Code available at: https://github.com/SAMASK-CLTR/Code.", "filename": "2025_0054.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Peirong Xu", "Luoqian Zhu", "Jingkun Chen", "Xin Qian", "Yue Sun", "Lingyun Bao", "Tao Tan"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0055", "x": 1.659, "y": 2.869, "title": "Self-supervised Multiview Xray Matching", "abstract": "Accurate interpretation of multi-view radiographs is crucial for diagnosing fractures, muscular injuries, and other anomalies. While significant advances have been made in AI-based analysis of single images, current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations. In this work, we present a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views. This is achieved using digitally reconstructed radiographs (DRR), which are automatically derived from unannotated CT volumes. Our approach incorporates a transformer-based training phase to accurately predict c orrespondences across two or more X-ray views. Furthermore, we demonstrate that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data. Extensive evaluations on both synthetic and real X-ray datasets show that incorporating correspondences improves performance in multi-view fracture classification.", "filename": "2025_0055.pdf", "year": 2025, "institution": "Telecom Paris", "country": "France", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0056", "x": 3.862, "y": 5.521, "title": "Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification", "abstract": "The diagnosis of medical diseases faces challenges such as the misdiagnosis of small lesions. Deep learning, particularly multimodal approaches, has shown great potential in the field of medical disease diagnosis. However, the differences in dimensionality between medical imaging and electronic health record data present challenges for effective alignment and fusion. To address these issues, we propose the Multimodal Multiscale Cross-Attention Fusion Network (MMCAF-Net). This model employs a feature pyramid structure combined with an efficient 3D multi-scale convolutional attention module to extract lesion-specific features from 3D medical images. To further enhance m ultimodal data integration, MMCAF-Net incorporates a multi-scale cross-attention module, which resolves dimensional inconsistencies, enabling more effective feature fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results showed a significant improvement in diagnostic accuracy, surpassing current state-of-the-art methods. The code is available at https://github.com/yjx1234/MMCAF-Net", "filename": "2025_0056.pdf", "year": 2025, "institution": "Xidian University", "country": "China", "authors": ["Jianxun Yu", "Ruiquan Ge", "Zhipeng Wang", "Cheng Yang", "Chenyu Lin", "Xianjun Fu", "Jikui Liu", "Ahmed Elazab", "Changmiao Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0057", "x": 5.597, "y": 3.144, "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation", "abstract": "Attention-based methods have demonstrated exceptional performance in modelling long-range dependencies on spherical cortical surfaces, surpassing traditional Geometric Deep Learning (GDL) models. However, their extensive inference time and high memory demands pose challenges for application to large datasets with limited computing resources. Inspired by the state space model in computer vision, we introduce the attention-free Vision Mamba (Vim) to spherical surfaces, presenting a domain-agnostic architecture for analyzing data on spherical manifolds. Our method achieves surface patching by representing spherical data as a sequence of triangular patches derived from a subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on multiple neurodevelopmental phenotype regression tasks using cortical surface metrics from neonatal brains. Experimental results demonstrate that SiM outperforms both attention-and GDL-based methods, delivering 4.8 times faster inference and achieving 91.7% lower memory consumption compared to the Surface Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity analysis further underscores the potential of SiM to identify subtle cognitive developmental patterns. The code is available at https://github.com/Rongzhao-He/ surface-vision-mamba.", "filename": "2025_0057.pdf", "year": 2025, "institution": "Lanzhou University", "country": "China", "authors": ["Rongzhao He", "Weihao Zheng", "Leilei Zhao", "Ying Wang", "Dalin Zhu", "Dan Wu", "Bin Hu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0058", "x": -0.238, "y": 4.542, "title": "TemSAM: Temporal-Aware Segment Anything Model for Cerebrovascular Segmentation in Digital Subtraction Angiography Sequences", "abstract": "Digital Subtraction Angiography (DSA) is the gold standard in vascular disease imaging but it poses challenges due to its dynamic frame changes. Early frames often lack detail in small vessels, while late frames may obscure vessels visible in earlier phases, necessitating timeconsuming expert interpretation. Existing methods primarily focus on single-frame analysis or basic temporal integration, treating all frames uniformly and failing to exploit complementary inter-frame information. Furthermore, existing pre-trained models like the Segment Anything Model (SAM), while effective for general medical video segmentation, fall short in handling the unique dynamics of DSA sequences driven by contrast agents. To overcome these limitations, we introduce Tem-SAM, a novel temporal-aware segment anything model for cerebrovascular segmentation in DSA sequences. TemSAM integrates two main components: (1) a multi-level Minimum Intensity Projection (MIP) global prompt that enhances temporal representation through a MIP-guided Global Attention (MGA) module, utilizing global information provided by MIP, and (2) a complementary information fusion module, which includes a frame selection module and a Masked Cross-Temporal Attention Module, enabling additional foreground information extraction from complementary frame. Our experimental results demonstrate that Tem-SAM significantly outperforms existing methods. Our code is available at https://github.com/zhang-liang-hust/TemSAM.", "filename": "2025_0058.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Liang Zhang", "Xixi Jiang", "Xiaohuan Ding", "Zihang Huang", "Tianyu Zhao", "Xin Yang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0059", "x": 3.278, "y": 3.96, "title": "tHPM-LDM: Integrating Individual Historical Record with Population Memory in Latent Diffusion-Based Glaucoma Forecasting", "abstract": "Longitudinal medical records offer crucial insights into disease progression, including structural changes and dynamic evolution, essential for clinicians in treatment planning. However, existing disease forecasting methods are hindered by irregular data collection intervals, negligence in inter-patient relationships, and a lack of case-reference capabilities. We introduce tHPM-LDM, a glaucoma forecasting framework leveraging continuous-time attention within a historical condition module to capture disease progression from irregularly acquired records. Notably, our approach integrates population memory, enabling personalized forecasting through relevant p opulation patterns. Empirical evaluations on the SIGF glaucoma longitudinal dataset demonstrate the significant improvements of our approach in image prediction and category consistency compared to state-of-the-art methods. Furthermore, our approach provides interpretable individual-population patterns and showcases robust performance despite missing visits.", "filename": "2025_0059.pdf", "year": 2025, "institution": "University of Liverpool", "country": "UK", "authors": ["Yuheng Fan", "Jianyang Xie", "Yimin Luo", "Yanda Meng", "Savita Madhusudhan", "Gregory Y H Lip", "Li Cheng", "Yalin Zheng", "He Zhao"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0060", "x": 0.83, "y": 5.083, "title": "Time-Contrastive Pretraining for In-Context Image and Video Segmentation", "abstract": "In-context learning (ICL) has shown promise for generalizing to new visual tasks using a few examples, but current methods are limited. They typically rely on a rigid gridding strategy that restricts the number and resolution of context images. We propose Temporal, a novel approach that overcomes these limitations by reformulating visual ICL as a video object segmentation (VOS) problem. This VOS-based approach naturally handles a variable number of full-resolution context images. To automatically select the most relevant context for a given query, we introduce a prompt retriever pretrained on videos using a time-contrastive objective. This objective learns from the temporal coherence of video, using adjacent frames as positive examples (i.e., useful context images) and distant frames as negatives. For image segmentation, our retriever builds a pseudo-video by prepending the retrieved context images to the query image, which is then processed by the VOS model. For video segmentation, the retriever identifies keyframes, our ICL pipeline generates their masks, and these masks are propagated through the video. On the MICCAI FLARE 2022 challenge, Temporal significantly outperforms baselines, achieving a Dice score of 90.95% for image segmentation (+10.64%) and 92.45% for video segmentation (+14.88%).", "filename": "2025_0060.pdf", "year": 2025, "institution": "University of A lberta", "country": "Canada", "authors": ["Assefa Wahd", "Jacob Jaremko", "Abhilash Hareendranathan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0061", "x": 4.549, "y": 6.004, "title": "TMSE: Tri-Modal Survival Estimation with Context-Aware Tissue Prototype and Attention-Entropy Interaction", "abstract": "Survival prediction plays a crucial role in clinical decisionmaking, enabling personalized treatments by integrating multi-modal medical data, such as histopathology images, pathology reports, and genomic profiles. However, the heterogeneity across these modalities and the high dimensionality of Whole Slide Images (WSI) make it challenging to capture survival-relevant features and model their interactions. Existing methods, typically focused on single-modal WSI, fail to leverage multimodal information, such as expert-driven pathology reports, and struggle with the computational complexity of WSI. To address these issues, we propose a novel Tri-Modal Survival Estimation framework (TMSE), which includes three components: (1) Pathology report processing pipeline, curated with expert knowledge, with both the pipeline and the processed structured report being publicly available; (2) Context-aware Tissue Prototype (CTP) module, which uses Mamba and Gaussian mixture models to extract compact, survival-relevant features from WSI, reducing redundancy while preserving histological details;(3) Attention-Entropy Interaction (AEI) module, a attention mechanism enhanced with entropy-based optimization to align and fuse three modalities: WSI, pathology reports, and genomic data. Extensive evaluation on three TCGA datasets (BLCA, BRCA, LUAD) shows that our approach achieves superior performance in survival prediction. Data and code are available: https://github.com/RuofanZhang8/TMSE.", "filename": "2025_0061.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Ruofan Zhang", "Mengjie Fang", "Shengyuan Liu", "Zipei Wang", "Jie Tian", "Di Dong"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0062", "x": 3.751, "y": 6.782, "title": "Top-Down Attention-Based Multiple Instance Learning for Whole Slide Image Analysis", "abstract": "Multiple instance learning (MIL) has become the de facto standard approach for whole-slide image analysis in computational pathology (CPath). While instance-wise attention tends to miss correlations between instances, self-attention can capture these interactions, but remains agnostic to the particular task. To address this issue, we introduce Top-Down Attention-based Multiple Instance Learning (TDA-MIL), an architecture that first learns a general representation from the data via self-attention in an initial inference step, then identifies task-relevant instances through a feature selection module, and finally refines these representations by injecting the selected instances back into the attention mechanism for a second inference step. By focusing on task-specific signals, TDA-MIL effectively discerns subtle, yet significant, regions within each slide, leading to more precise classification. Extensive experiments on detecting lymph node metastasis in breast cancer, biomarker screening for microsatellite instability in different organs, and challenging molecular status prediction for HER2 in breast cancer show that TDA-MIL consistently surpasses other MIL baselines, underscoring the effectiveness of our proposed task-relevant refocusing and its broad applicability across CPath tasks. Our implementation is released at https://github.com/agentdr1/TDA_MIL.", "filename": "2025_0062.pdf", "year": 2025, "institution": "University of Regensburg", "country": "Germany", "authors": ["Daniel Reisenbüchler", "Ruining Deng", "Christian Matek", "Friedrich Feuerhake", "Dorit Merhof"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0063", "x": 3.462, "y": 1.704, "title": "Wavelet-Driven Decoupling and Physics-Informed Mapping Network for Accelerated Multi-parametric MR Imaging", "abstract": "Multi-parametric magnetic resonance imaging (MRI) is an advanced MRI technique that can provide multiple quantitative maps simultaneously based on acquired multi-echo images. However, the lengthy scan time often limits its application. Accelerated multiparametric MRI using deep learning is of great interest. The existing studies have two limitations: 1) inefficient use of the multi-echo information; 2) lack of physical prior for parametric mapping. To address these issues, in this work, we propose a novel decoupling-driven and physicsinformed reconstruction network for accelerated multi-parametric MRI. Specifically, to better align and integrate multi-echo information, we propose a novel decoupling technique consisting of wavelet-driven decoupling module, contrastive and echo-dependent decoupling losses, such that the multi-echo features can be effectively decoupled into echo-dependent and echo-independent components. Only the echo-independent features are fused across multiple echoes. Besides, Bloch equations are incorporated as physical priors to guide the parametric mapping network. Experimental results on our in-house data (12-echo sequence) show that our method outperforms the state-of-the-art methods by 1.54% in average SSIM and 1.70 dB in average PSNR for 4× acceleration, which significantly advances the performance limitation for multi-parametric MRI. Our code is available at https://github.com/IDEARL23/WDPM-Net.", "filename": "2025_0063.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Ruilong Dan", "Kaicong Sun", "Yichen Zhou", "Minqiang Jia", "Yuxuan Liu", "Han Zhang", "Xiaopeng Zong", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0064", "x": 0.734, "y": 3.7, "title": "XFMamba: Cross-Fusion Mamba for Multi-view Medical Image Classification", "abstract": "Compared to single-view medical image classification, using multiple views can significantly enhance predictive accuracy as it can account for the complementarity of each view while leveraging correlations between views. Existing multi-view approaches typically employ separate convolutional or transformer branches combined with simplistic feature fusion strategies. However, these approaches inadvertently disregard essential cross-view correlations, leading to suboptimal classification performance, and suffer from challenges with limited receptive field (CNNs) or quadratic computational complexity (transformers). Inspired by state space sequence models, we propose XFMamba, a pure Mamba-based cross-fusion architecture to address the challenge of multiview medical image classification. XFMamba introduces a novel twostage fusion strategy, facilitating the learning of single-view features and their cross-view disparity. This mechanism captures spatially long-range dependencies in each view while enhancing seamless information transfer between views. Results on three public datasets, MURA, CheXpert, and DDSM, illustrate the effectiveness of our approach across diverse multiview medical image classification tasks, showing that it outperforms existing convolution-based and transformer-based multi-view methods. Code is available at https://github.com/XZheng0427/XFMamba.", "filename": "2025_0064.pdf", "year": 2025, "institution": "Mary Univ ersity of London", "country": "UK", "authors": ["Xiaoyu Zheng", "Xu Chen", "Shaogang Gong", "Xavier Griffin", "Greg Slabaugh"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0065", "x": 4.492, "y": 3.184, "title": "You Can Detect It: Fetal Biometric Estimation Using Ellipse Detection", "abstract": "The cardiothoracic diameter ratio (CTR) biometric in fourchamber ultrasound plane is often measured for diagnosing congenital heart disease. However, due to the commonly existing artifacts like acoustic shadowing, manual measurement can be time-consuming and labor-intensive task, and may result in high measurements variability. Presently, one of the most popular approaches is segmentation-based methods, which utilize deep learning networks to segment the cardiac and thoracic regions. Then, the metric is calculated through an ellipse fitting scheme. This is inefficient, and requires additional post-processing. To tackle the above problems, in this paper, we therefore present a onestage ellipse detection network, namely EllipseDet, which detects the cardiac and thoracic regions in ellipses, and then automatically calculates the CTR biometric in four-chamber view. In particular, we formulate the network that detects the center of each object as points and regresses the ellipses' parameters simultaneously. Besides, we propose a novel ellipse feature alignment module and Ellipse-IoU loss to further regulate the regression procedure. We have evaluated EllipseDet on a clinical echocardiogram dataset and the experimental results show that our proposed framework outperforms several state-of-the-art methods. As an open science, source code, images dataset and pre-trained weights are available at https://github.com/szuboy/FOCUS-dataset.", "filename": "2025_0065.pdf", "year": 2025, "institution": "Shenzhen University Medical School", "country": "China", "authors": ["Hongyuan Zhang", "Haoyu Xie", "Tingting Ye", "Songxiong Wu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0066", "x": 3.716, "y": 2.379, "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging", "abstract": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the trade-off between spatial and temporal resolution requires prolonged scan time that can compromise temporal fidelity-especially during rapid, large-amplitude motion. Traditional approaches typically rely on registration-based interpolation to generate intermediate frames. However, these methods struggle with large deformations, resulting in misregistration, artifacts, and diminished spatial consistency. To address these challenges, we propose TSSC-Net, a novel framework that generates intermediate frames while preserving spatial consistency. To improve temporal fidelity under fast motion, our diffusion-based temporal superresolution network generates intermediate frames using the start and end frames as key references, achieving 6× temporal super-resolution in a single inference step. Additionally, we introduce a novel tri-directional Mamba-based module that lev erages long-range contextual information to effectively resolve spatial inconsistencies arising from cross-slice misalignment, thereby enhancing volumetric coherence and correcting cross-slice errors. Extensive experiments were performed on the public ACDC cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results demonstrate that TSSC-Net can generate highresolution dynamic MRI from fast-motion data while preserving structural fidelity and spatial consistency. The code will be available at https://github.com/Joker-ZXR/TSSC-Net.", "filename": "2025_0066.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Xuanru Zhou", "Jiarun Liu", "Shoujun Yu", "Hao Yang", "Cheng Li", "Tao Tan", "Shanshan Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0067", "x": 4.582, "y": 4.81, "title": "A Novel ED Triage Framework Using Conditional Imputation, Multi-scale Semantic Learning, and Cross-Modal Fusion", "abstract": "In emergency departments (ED), efficient triage is essential for timely patient care, but challenges like missing and sparse data often hinder the prediction performance of severity level and department. To address these issues, we propose a novel intelligent triage method that incorporates a Conditional Gaussian Mixture Imputation (CGMI) and a Feature Densification Module (FDM). The CGMI handles missing data through conditional probability modeling, while the FDM obtains correlations between variables by calculating the Manhattan distance between non-zero values in a one-hot coded feature. In addition, we design a multi-scale Feature Extraction Module (mFEM) to capture multi-level semantic information from patient complaints. Subsequently, two feature fusion strategies were introduced: early fusion and late fusion. The early fusion combines Principal Component Analysis (PCA)-processed features with another modality. The late fusion with enhancement introduces reverse features of another modality and applies an attention mechanism to obtain salient features. Experimental results show that our method outperforms existing approaches, achieving 84.83% sensitivity, 85.11% specificity, and 61.42% Cohen's Kappa for severity prediction and 90.89% sensitivity, 91.04% specificity, and 85.87% Cohen's Kappa for department prediction. Our method significantly improves the sensitivity, specificity, and robustness of ED triage, demonstrating superior performance and reliability in handling missing and sparse clinical data. The code is available at https://git hub.com/xiaoyiseu/CGMI.", "filename": "2025_0067.pdf", "year": 2025, "institution": "Southeast University", "country": "China", "authors": ["Yi Xiao", "Jun Zhang", "Cheng Chi", "Chunyu Wang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0068", "x": 0.374, "y": 2.809, "title": "A Two-Stage Method for Specular Highlight Detection and Removal in Medical Images", "abstract": "In minimally invasive surgeries, such as endoscopic and ophthalmic procedures, specular highlights on tissue and instrument surfaces can obscure critical details, compromising surgical safety and precision. Traditional methods rely on color segmentation and filtering optimization but are highly sensitive to lighting variations and produce suboptimal restoration. While deep learning enhances detection robustness, its effectiveness is constrained by the scarcity of annotated medical data and unnatural boundary transitions in restored regions. To address these challenges, this paper proposes a two-stage hierarchical network framework. First, a Hierarchical Feature Attention Network (HFA-Net) is designed, integrating spatial-shift segmented attention (S 2 MLP), dualflow attention (DFA), multi-scale feature fusion (SFF), and partial mask convolution (PMConv) to achieve precise detection and removal of specular highlights. Second, a large-mask inpainting model (LaMa) is introduced, utilizing dilated mask expansion to enhance contextual awareness and improve texture consistency in the restored regions. To address the scarcity of medical highlight datasets, we construct four specialized datasets covering various surgical scenarios, including ophthalmic injections and instrument reflections, while also incorporating publicly available data to enhance model generalization. Experimental results demonstrate that the proposed method outperforms existing approaches across six datasets in terms of detection accuracy and restoration quality, particularly excelling in complex textures and natural boundary transitions. Our code is available at https://github.com/tkllndxn/highlight- removal.", "filename": "2025_0068.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Zefeng Li", "Mingyue Cui", "Daosong Hu", "Jin Gong", "Jingchong Weng", "Zeyu Zhang", "Lele Tian", "Mengran Li", "Kai Huang"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0069", "x": 0.308, "y": 3.948, "title": "Accurate Boundary Alignment and Realism Enhancement for Colonoscopic Polyp Image-Mask Pair Generation", "abstract": "Polyp segmentation is the foundation of colonoscopic lesion screening, diagnosis, and therapy. However, the data size of images and annotations is limited. The latent diffusion model (LDM) has emerged as a powerful tool in synthesizing high-quality medical images with low computational costs. However, the challenges of boundary-aligned imagemask pairs and image realism remain unresolved, showing that (i) the spatial relationship between the boundaries is easily distorted in the latent space; (ii) the diversity of colors, shapes, and textures, along with low boundary contrast and textures similar to surrounding tissue, makes boundary distinction of the polyps difficult. This paper proposes Polyp-LDM that encodes polyps and masks into the same latent space via a unified variational autoencoder (VAE) to align their boundaries. Furthermore, Polyp-LDM refines texture and ligh ting while preserving the structure by fine-tuning the VAE decoder with data augmentation and applying the style cloning module to enhance image realism. Quantitative evaluations and user preference study demonstrate that our method outperforms existing methods in image-mask pair generation. Moreover, segmentation models trained with augmented data generated by polyp-LDM achieve the best performance on three public polyp datasets. The code is available at https://github.com/16rq/Polyp-LDM.", "filename": "2025_0069.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Riyu Qiu", "Kun Xia", "Feng Gao", "Shuting Yang", "Du Cai", "Jiacheng Wang", "Yinran Chen", "Liansheng Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0070", "x": 1.795, "y": 4.822, "title": "ADA: An Adaptive Augmentation Framework for Single-Source Domain Generalization in Medical Image Segmentation", "abstract": "In medical image analysis, significant challenges arise from domain shifts. Models trained on one dataset often struggle to generalize to unseen domains, limiting their clinical utility. To overcome this challenge, recent advancements have tried to increase the diversity of training data with data augmentation, in which the augmentation rules are pre-set before training commences and remain unchanged throughout the training process. Previous methods do not augment according to the unique characteristics of individual samples. As a result, they fail to cover the full diversity of unseen domains. To tackle this problem, we propose a learnable framework, the Adaptive Augmentation Framework (ADA), which can adaptively augment data catering to each individual sample. It has three operators for different purposes: 1) the Learnable Bezier Remap operator dynamically adjusts parameters to do the augmentation according t o its content features. 2) the Channel Shift Control operator dynamically tunes shift and scale parameters for each color channel. By capturing fine-grained variations and improving spectral detail representation. 3) The Gradient-guided Feature Weaken operator dynamically reduces the influence of high-impact features to improve the model's ability to generalize. Extensive experiments conducted on seven medical segmentation datasets demonstrate that adaptive augmentation is more likely to cover large diversity in the unseen domain.", "filename": "2025_0070.pdf", "year": 2025, "institution": "Beijing Normal-Hong Kong Baptist University", "country": "China", "authors": ["Runlin Huang", "Hongmin Cai", "Weipeng Zhuo", "Shangyan Cai", "Haowei Lin", "Wentao Fan", "Weifeng Su"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0071", "x": 1.888, "y": 4.027, "title": "Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis", "abstract": "Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose Adaptively Distilled ControlNet, a taskagnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves 2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at https://github.com/Qiukunpeng/ADC.", "filename": "2025_0071.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Kunpeng Qiu", "Zhiying Zhou", "Yongxin Guo"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0072", "x": 2.121, "y": 4.212, "title": "Anatomy-Based Self-supervised Pre-training for Scale-Robust Hierarchical Representations in Chest X-Rays", "abstract": "In self-supervised pre-training, learning consistent and hierarchical representations that capture relationships among anatomical semantics holds promise for enhancing the performance and interpretability of downstream tasks. However, the representations learned by existing methods are vulnerable to scale variations, which manifests as inconsistency on some scales and misjudgments of hierarchy. Therefore, we propose a scale-robust anatomical representation learning framework with self-supervision, which incorporates contrastive learning with our newly proposed pretext tasks: location-scale prediction(LSP) and decomposition prediction(DP). Our method addresses the vulnerability from three aspects: 1) It uses multi-scale patches as inputs to embrace diverse anatomical semantics in pre-training. 2) LSP promotes consistency at multi-scales by enhancing the model's sensitivity to scale and resolving representation conflicts caused by multi-scale inputs. 3) DP eliminates hierarchy misjudgments by producing hierarchical representations for anatomies and their constituent parts that better balance the similarity and discriminability. Evaluations across six chest X-ray datasets demonstrate that the representations learned by our method are consistent and hierarchical at multi-scales and have great transferring ability to various downstream tasks. The code is publicly available at https://github.com/SurongChu/SRHRS.", "filename": "2025_0072.pdf", "year": 2025, "institution": "Taiyuan University of Technology", "country": "China", "authors": ["Surong Chu", "Yan Qiang", "Guohua Ji", "Xueting Ren", "Lijing Zhang", "Baoping Jia", "Yangyang Wei", "Juanjuan Zhao", "Shuo Li"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0073", "x": 0.989, "y": 2.371, "title": "BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via Bezier Curve Refinement", "abstract": "Laparoscopic liver surgery, while minimally invasive, poses significant challenges in accurately identifying critical anatomical structures. Augmented reality (AR) systems, integrating MRI/CT with laparoscopic images based on 2D-3D registration, offer a promising solution for enhancing surgical navigation. A vital aspect of the registration progress is the precise detection of curvilinear anatomical landmarks in laparoscopic images. In this paper, we propose BCRNet (Bezier Curve Refinement Network), a novel framework that significantly enhances landmark detection in laparoscopic liver surgery primarily via the Bezier curve refinement strategy. The framework starts with a Multi-modal Feature Extraction (MFE) module designed to robustly capture semantic features. Then we propose Adaptive Curve Proposal Initialization (ACPI) to generate pixel-aligned Bezier curves and confidence scores for reliable initial proposals. Additionally, we design the Hierarchical Curve Refinement (HCR) mechanism to enhance these proposals iteratively through a multi-stage process, capturing fine-grained contextual details from multi-scale pixel-level features for precise Bezier curve adjustment. Extensive evaluations on the L3D and P2ILF datasets demonstrate that BCRNet outperforms state-of-the-art methods, achieving significant performance improvements. Our code is available at https://github.com/ jinlab-imvr/BCRNet.", "filename": "2025_0073.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Qian Li", "Feng Liu", "Shuojue Yang", "Daiyun Shen", "Yueming Jin"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0074", "x": 1.538, "y": 7.11, "title": "BioD2C: A Dual-Level Semantic Consistency Constraint Framework for Biomedical VQA", "abstract": "Biomedical visual question answering (VQA) has been widely studied and has demonstrated significant application value and potential in fields such as assistive medical diagnosis. Despite their success, current biomedical VQA models perform multimodal information interaction only at the model level within large language models (LLMs), leading to suboptimal multimodal semantic alignment when dealing with complex tasks. To address this issue, we propose BioD2C: a novel Dual-level Semantic Consistency Constraint Framework for Biomedical VQA, which achieves dual-level semantic interaction alignment at both the model and feature levels, enabling the model to adaptively learn visual features based on the question. Specifically, we firstly integrate textual features into visual features via an image-text fusion mechanism as feature-level semantic interaction, obtaining visual features conditioned on the given text; and then introduce a text-queue-based cross-modal soft semantic loss function to further align the image semantics with the question semantics. Specifically, in t his work, we establish a new dataset, BioVGQ, to address inherent biases in prior datasets by filtering manually-altered images and aligning question-answer pairs with multimodal context, and train our model on this dataset. Extensive experimental results demonstrate that BioD2C achieves state-of-the-art (SOTA) performance across multiple downstream datasets, showcasing its robustness, generalizability, and potential to advance biomedical VQA research. The source code of this work and the BioVGQ dataset can be accessed through code and dataset.", "filename": "2025_0074.pdf", "year": 2025, "institution": "Shandong University", "country": "China", "authors": ["Zhengyang Ji", "Shang Gao", "Li Liu", "Yifan Jia", "Yutao Yue"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0075", "x": 0.211, "y": 4.039, "title": "CD-PolypNet: Cross-Domain Polyp Segmentation Network with Internal Feature Distillation and Dual-Stream Boundary Focus via Large Vision Model", "abstract": "Leveraging large vision models (LVMs), such as the Segment Anything Model (SAM), in medical image analysis presents significant potential to enhance diagnostic efficiency. Existing SAM-based medical segmentation methods inadequately address two critical challenges: rapidly adapting LVMs to medical tasks through few-shot fine-tuning, and the inherent difficulty in distinguishing lesions from anatomically similar background regions in medical images. To overcome these limitations, we propose CD-PolypNet, a novel framework integrating a Semantic Supervision via Feature Distillation (SSFD) and an Edge-Guided Feature Branch (EFB). The SSFD module leverages feature distillation to transfer knowledge from SAM's strongly supervised features into early-stage feature learning, enabling efficient domain adaptation of large vision models under data scarcity. Concurrently, EFB enhances boundary discrimination in lightweight decoder through a hybrid strategy combining the Canny operator and Edge-Frequency Gated Convolution (EFGConv), thereby prioritizing edge-aware feature extraction. Extensive experiments across five challenging medical imaging datasets demonstrate that our method not only surpasses state-of-the-art approaches in accuracy and robustness but also establishes a new paradigm for crossdomain adaptation of large vision models in specialized medical applications. The codes are available at https://github.com/ChangpengYue/ CD-PolypNet.", "filename": "2025_0075.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Changpeng Yue", "Jianxiang Zhao", "Chao Wang", "Xinglun Zhao", "Axiu Mao", "Jia Hou", "Chenggang Yan", "Kai Zhao", "Shuai Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0076", "x": 3.781, "y": 5.512, "title": "Clinical Data-Driven Retrieval-Augmented Model for Lung Nodule Malignancy Prediction", "abstract": "Deep learning techniques have been widely applied to lung nodule malignancy prediction tasks. Recently, the emergence of Vision-Language Models (VLMs) has enabled the use of textual information, further improving diagnostic accuracy. Nevertheless, two key limitations persist: (1) the insufficient utilization of clinical data to enhance computer-aided diagnosis, and (2) the limited ability of existing frameworks to leverage similar cases in the diagnostic process. To address these issues, we propose a clinical data-driven, retrieval-augmented VLM framework for lung nodule malignancy prediction. The proposed framework comprises a multimodal encoder, a retrieval-augmented module, and a text encoder. Lesion classification is achieved by evaluating the similarities between the combined visual and clinical data features and the text features of predefined categories, thereby establishing a robust mechanism for malignancy prediction. Moreover, the retrieval-augmented module further refines the prediction process by incorporating similar cases retrieved using clinical data as a query, thus facilitating more informed and accurate decisions. Overall, this framework comprehensively utilizes clinical data by integrating it into CT image features and enabling crossinteraction in the retrieval-augmented module to support diagnosis with similar cases. Experimental results on the publicly available LIDC-IDRI dataset demonstrate that the proposed framework achieves significant improvements in lung nodule malignancy prediction, with an approximate 3% increase in accuracy. Our code is released on Github: https:// github.com/chenn-clear/ClinicalRA.", "filename": "2025_0076.pdf", "year": 2025, "institution": "Ritsumeikan University", "country": "Japan", "authors": ["Ruibo Hou", "Shurong Chai", "Rahul Kumar Jain", "Yinhao Li", "Jiaqing Liu", "Shiyu Teng", "Xiaoyu Shi", "Lanfen Lin", "Yen-Wei Chen"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0077", "x": 2.813, "y": 4.371, "title": "Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging", "abstract": "Accurate, real-time monitoring of tissue ischemia is crucial to understand tissue health and guide surgery. Spectral imaging shows great potential for contactless and intraoperative monitoring of tissue oxygenation. Due to the difficulty of obtaining direct reference oxygenation values, conventional methods are based on linear unmixing techniques. These are prone to assumptions and these linear relations may not always hold in practice. In this work, we present deep learning approaches for real-time tissue oxygenation estimation using Monte-Carlo simulated spectra. We train a fully connected neural network (FCN) and a convolutional neural network (CNN) for this task and propose a domainadversarial training approach to bridge the gap between simulated and real clinical spectral data. Results demonstrate that these deep learning models achieve a higher correlation with capillary lactate measurements, a well-known marker of hypoxia, obtained during spectral imaging in surgery, compared to traditional linear unmixing. Notably, domainadversarial training effectively reduces the domain gap, optimizing performance in real clinical settings. Keywords: multispectral imaging • tissue oxygenation • domain adaptation • deep learning • simulations • real-time imaging", "filename": "2025_0077.pdf", "year": 2025, "institution": "Ghent University/imec", "country": "Belgium", "authors": ["Jens De Winne", "Siri Willems", "Siri Luthman", "Danilo Babin", "Hiep Luong", "Wim Ceelen"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0078", "x": -0.133, "y": 2.03, "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data", "abstract": "The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average .36.635 PSNR rendering quality with .156 FPS and . 16.1MB model size, outperforming state-of-the-art methods in rendering quality and efficiency. Project is available at: https://med-air.github.io/ClipGS.", "filename": "2025_0078.pdf", "year": 2025, "institution": "The Chinese University of Hong", "country": "China", "authors": ["Chengkun Li", "Yuqi Tong", "Kai Chen", "Zhenya Yang", "Ruiyang Li", "Shi Qiu", "Jason Ying-Kuen Chan", "Pheng-Ann Heng", "Qi Dou"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0079", "x": 2.448, "y": 4.837, "title": "DINO Adapted to X-Ray (DAX): Foundation Models for Intraoperative X-Ray Imaging", "abstract": "Intraoperative X-ray imaging represents a key technology for guiding orthopedic interventions. Recent advancements in deep learning have enabled automated image analysis in this field, thereby streamlining clinical workflows and enhancing patient outcomes. However, many existing approaches depend on task-specific models and are constrained by the limited availability of annotated data. In contrast, self-supervised foundation models have exhibited remarkable potential to learn robust feature representations without label annotations. In this paper, we introduce DINO Adapted to X-ray (DAX), a novel framework that adapts DINO for training foundational feature extraction backbones tailored to intraoperative X-ray imaging. Our approach involves pre-training on a novel dataset comprising over 632,000 image samples, which surpasses other publicly available datasets in both size and feature diversity. To validate the successful incorporation of relevant domain knowledge into our DAX models, we conduct an extensive evaluation of all backbones on three distinct downstream tasks and demonstrate that small head networks can be trained on top of our frozen foundation models to successfully solve applications regarding (1) body region classification, (2) metal implant segmentation, and (3) screw object detection. The results of our study underscore the potential of the DAX framework to facilitate the development of robust, scalable, and clinically impactful deep learning solutions for intraoperative X-ray image analysis. Source code and model checkpoints are available at https://github. com/JoshuaScheuplein/DAX.", "filename": "2025_0079.pdf", "year": 2025, "institution": "Friedrich-Alexander-Universität", "country": "Germany", "authors": ["Joshua Scheuplein", "Maximilian Rohleder", "Andreas Maier", "Björn Kreher"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0080", "x": 1.556, "y": 5.196, "title": "Dual-Branch Dynamic Coupling Weakly Supervised Learning for Class-Incremental Histopathological Region Segmentation", "abstract": "Histopathological region segmentation faces two main challenges: catastrophic forgetting and the high cost of pixel-level annotations. Recent studies have focused on incremental learning of new categories using low-cost image-level labels. However, the limitations of multiple instance learning (MIL) in modeling instance relationships hinder further improvement in segmentation performance. To address these challenges, we propose the Dual-branch Dynamic Coupling (DDCWISS) network for weakly supervised class-incremental learning in histopathological region segmentation. Our architecture overcomes the limitations of isolated local feature computation in traditional MIL by enabling complementary feature extraction through parallel local representation and global modeling branches. Additionally, we propose a learnable coupling module to ensure effective multi-scale feature fusion, while the dual-path supervision mec hanism simultaneously enhances segmentation accuracy. Experiments on the CPATH dataset demonstrate that our method significantly reduces reliance on costly pixel-level annotations for histopathological region segmentation, while effectively alleviating the catastrophic forgetting problem during incremental learning. These results highlight the potential of DDCWISS as a scalable, weakly supervised Class-Incremental paradigm for medical image analysis. The source code is publicly available at: https://github.com/XiaoyanHong24/DDCWISS.", "filename": "2025_0080.pdf", "year": 2025, "institution": "Jiangnan U niversity", "country": "China", "authors": ["Xiaoyan Hong", "Jiansong Fan", "Zhaohong Deng", "Xiang Pan"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0081", "x": 0.823, "y": 2.162, "title": "EG-Net: An Edge-Guided Network for Rigid Registration of Laparoscopic Low-Overlap Point Clouds", "abstract": "Laparoscopic augmented reality (LAR) enables real-time visualization of internal organ anatomy, effectively reducing surgical risks. Rigid point cloud registration aligns the spatial position of the preoperative image point cloud with the intraoperative laparoscopic video point cloud, playing a pivotal role in the virtual-real fusion visualization for LAR. However, the limited field of view in laparoscopic surgery results in only partial visibility of the organ. This leads to an incomplete video point cloud that exhibits low overlap with the image point cloud, rendering registration highly susceptible to local optima. Moreover, the smooth and texture-deficient organ surface makes popular superpoint matching methods based on feature similarity ineffective. Inspired by the highly consistent morphology of the video and image point clouds at organ bottom edges, we propose an edge guidance (EG) mechanism to address the challenge of sparse surface features in laparoscopic scenes. The EG mechanism identifies edge points by calculating the standard deviation of correlations among neighboring points, prioritizes edge alignment, and subsequently guides the matching of other points. We leverage this mechanism to develop an edge-guided rigid point cloud registration network, EG-Net. Compared with the state-of-the-art method PARE-Net, EG-Net achieves at least a 7% improvement in accuracy and an 11% increase in speed across three laparoscopic datasets: the public DePoLL dataset, a pig liver surgery dataset, and a human liver surgery dataset. With its high accuracy, fast speed, and strong generalization, EG-Net holds significant potential for clinical applications in laparoscopic surgery. The code is available at: https://github.com/FDC-WuWeb/EG-Net.", "filename": "2025_0081.pdf", "year": 2025, "institution": "University of Science and Tec hnology of China", "country": "People's Republic of China", "authors": ["Wenbin Wu", "Yifan Gao", "Yixiu Wang", "Jiayi Zhang", "Yiming Zhao", "Xin Gao"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0082", "x": -0.123, "y": 2.759, "title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation", "abstract": "Endoscopic video generation is crucial for advancing medical imaging and enhancing diagnostic capabilities. However, prior efforts in this field have either focused on static images, lacking the dynamic context required for practical applications, or have relied on unconditional generation that fails to provide meaningful references for clinicians. Therefore, in this paper, we propose the first conditional endoscopic video generation framework, namely EndoGen. Specifically, we build an autoregressive model with a tailored Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the learning of generating multiple frames as a grid-based image generation pattern, which effectively capitalizes the inherent global dependency modeling capabilities of autoregressive architectures. Furthermore, we propose a Semantic-Aware Token Masking (SAT) mechanism, whic h enhances the model's ability to produce rich and diverse content by selectively focusing on semantically meaningful regions during the generation process. Through extensive experiments, we demonstrate the effectiveness of our framework in generating high-quality, conditionally guided endoscopic content, and improves the performance of downstream task of polyp segmentation. Code released at https://www.github.com/CUHK-AIM-Group/ EndoGen", "filename": "2025_0082.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": null, "authors": ["Xinyu Liu", "Hengyu Liu", "Cheng Wang", "Tianming Liu", "Yixuan Yuan"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0083", "x": -0.053, "y": 2.328, "title": "EndoMetric: Near-Light Monocular Metric Scale Estimation in Endoscopy", "abstract": "Geometric reconstruction and SLAM with endoscopic images have advanced significantly in recent years. In most medical fields, monocular endoscopes are employed, and the algorithms used are typically adaptations of those designed for external environments, resulting in 3D reconstructions with an unknown scale factor.For the first time, we propose a method to estimate the real metric scale of a 3D reconstruction from standard monocular endoscopic images, under unknown varying albedo, without relying on application-specific learned priors. Our fully model-based approach leverages the near-light sources embedded in endoscopes, positioned at a small but nonzero baseline from the camera, in combination with the inverse-square law of light attenuation, to accurately recover the metric scale from scratch. This enables the transformation of any endoscope into a metric device, which is crucial for applications such as measuring polyps, stenosis, or assessing the extent of diseased tissue.", "filename": "2025_0083.pdf", "year": 2025, "institution": "Universidad de Zaragoza", "country": "Spain", "authors": ["Raúl Iranzo", "Víctor M Batlle", "Juan D Tardós", "José M M Montiel"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0084", "x": 0.331, "y": 2.874, "title": "Endoscopic Artifact Inpainting for Improved Endoscopic Image Segmentation", "abstract": "Endoscopic imaging plays a crucial role in modern diagnostics and minimally invasive procedures. However, artifacts caused by specular and diffuse reflections present significant challenges, particularly in tasks such as endoscopic image segmentation. Existing methods tackling endoscopic artifacts typically address only one type of reflection, failing to fully account for the non-Lambertian reflectance of endoscopic tissue structures. Therefore, inspired by the simplified Phong model for endoscopy, we propose a two-stage artifact inpainting framework. The first stage suppresses specular artifacts, while the second stage focuses on inpainting diffuse artifacts. Additionally, we introduce a weight map to control the handling of diffuse artifacts, ensuring a more precise enhancement. To evaluate its effectiveness, we focus on its impact on endoscopic image segmen tation tasks. Extensive experiments on multiple colonoscopy and dental endoscopy datasets demonstrate that our framework can robustly improve the segmentation performance of endoscopic images, offering better enhancement than existing state-of-the-art methods. Particularly, for zero-shot SAM segmentation of teeth, a significant performance boost is observed after inpainting, with mDice and mIoU increasing from 51.5%/39.3% to 96.1%/93.0%. Code is available at GitHub.", "filename": "2025_0084.pdf", "year": 2025, "institution": "Beijing University of Posts and Telecommunications (BUPT)", "country": "China", "authors": ["Zhangyuan Yu", "Chenlin Du", "Hongrui Liang", "Xiuqi Zheng", "Zeyao Ma", "Mingjun Wu", "Mingwu Ao", "Qicheng Lao"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0085", "x": 1.097, "y": 5.39, "title": "Enjoying Information Dividend: Gaze Track-Based Medical Weakly Supervised Segmentation", "abstract": "Weakly supervised semantic segmentation (WSSS) in medical imaging struggles with effectively using sparse annotations. One promising direction for WSSS leverages gaze annotations, captured via eye trackers that record regions of interest during diagnostic procedures. However, existing gaze-based methods, such as GazeMedSeg, do not fully exploit the rich information embedded in gaze data. In this paper, we propose GradTrack, a framework that utilizes physicians' gaze track, including fixation points, durations, and temporal order, to enhance WSSS performance. GradTrack comprises two key components: (1) the Gaze Track Map Generation module for creating hierarchical attention maps, and (2) the Track Attention module for integrating attention features, which collaboratively enable progressive feature refinement through multi-level gaze supervision during the decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets demonstrate that our GradTrack consistently outperforms existing gaze-based methods, achieving Dice score improvements of 3.21% and 2.61%, respectively. Moreover, GradTrack significantly narrows the performance gap with fully supervised models, such as nnUNet.", "filename": "2025_0085.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Zhisong Wang", "Yiwen Ye", "Ziyang Chen", "Yong Xia"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0086", "x": 1.06, "y": 1.794, "title": "Facial Appearance Prediction with Conditional Multi-scale Autoregressive Modeling for Orthognathic Surgical Planning", "abstract": "Craniomaxillofacial deformities often necessitate orthognathic surgery to correct jaw positions and improve both function and aesthetics. The existing patient-specific optimal face prediction for soft-tissue-driven planning struggles to accurately capture fine facial details and maintain harmonious alignment among key facial features. In this paper, we propose a novel Conditional Autoregressive Modeling for Orthognathic Surgery (CAMOS) framework that directly predicts patients' optimal 3D face from their preoperative appearance. Our approach employs a hierarchical, coarse-to-fine next-scale prediction strategy, beginning with large-scale pretraining on 44,602 control faces to construct a robust generative model that captures diverse demographic features. Subsequently, the model is fine-tuned on an in-house dataset of 86 orthognathic surgery patients, establishing a conditional path that integrates patient-specific information to form a conditional generative model. Evaluation on both public and in-house datasets demonstrates that CAMOS successfully generates patient-specific optimal face with high quality, effectively addressing the limitations of prior single-step approaches. Source code is available at https://github.com/RPIDIAL/ CAMOS.", "filename": "2025_0086.pdf", "year": 2025, "institution": "Rensselaer Polytec hnic Institute", "country": "USA", "authors": ["Jungwook Lee", "Xuanang Xu", "Daeseung Kim", "Tianshu Kuang", "Hannah H Deng", "Xinrui Song", "Yasmine Soubra", "Rohan Dharia", "Michael A K Liebschner", "Jaime Gateno", "Pingkun Yan"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0087", "x": 0.302, "y": 2.629, "title": "Guided Augmentation for Monocular Depth Estimation in Cell Microscopy", "abstract": "Monocular Depth Estimation (MDE) in cell microscopy provides critical insights into cellular structures, with applications spanning cancer diagnostics, hematological analysis, and tumor margin assessment. However, it presents unique challenges such as sparse z-stacks with limited focal planes, optical aberrations degrading depth precision, and the inherently ill-posed nature of inferring depth from single 2D images. Existing MDE methods often rely on semantic priors, geometric modeling, or self-supervised learning. While effective in macroscopic applications, these approaches struggle with microscopy-specific challenges involving domain-specific feature distributions.To address these limitations, we propose a novel deep learning-based physics-guided augmentation strategy leveraging Extended Depth of Field (EDOF) images to enhance MDE performance. To demonstrate the effectiveness of our approach, we employ a regression model trained to predict z-stack levels from individual cell images and a UNet-based model to synthesize blurred cell images at intermediate z-levels by modeling the point spread function (PSF) of the imaging process. Experiments on Giemsa-stained peripheral blood smear data demonstrate significant improvements in MDE over training without augmentation and simple augmentation strategies. Ablation studies validate the robustness of our approach, providing a promising framework for advancing medical microscopy-related applications.", "filename": "2025_0087.pdf", "year": 2025, "institution": "Indian Institute of Technology Madras", "country": "India", "authors": ["Abhishek Viswanathan", "A N Rajagopalan", "Nikhil Yelamarthy", "Ankit Rai", "Pradeep Ramachandran"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0088", "x": 1.52, "y": 6.836, "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "abstract": "Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination-the tendency to generate descriptions that are inconsistent with the visual content-remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of B. Khanal, S. Pokhrel and S. Bhandari-These authors contributed equally to this work. B. Khanal was asso ciated with Multimodal Learning Lab, UoA, while working on this paper.", "filename": "2025_0088.pdf", "year": 2025, "institution": "Rochester Institute of Technology", "country": "USA", "authors": ["Bidur Khanal", "Sandesh Pokhrel", "Sanjay Bhandari", "Ramesh Rana", "Nikesh Shrestha", "Ram B Gurung", "Cristian Linte", "Angus Watson", "Yash R Shrestha", "Binod Bhattarai"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0089", "x": 4.362, "y": 5.675, "title": "HARM3-Fusion: Hierarchical Attentional Representation Learning of Multi-modal, Multi-temporal, and Multi-sequence Fusion for Pathological Complete Response Prediction of Head and Neck Squamous Cell Carcinoma", "abstract": "The precise prediction of Pathological Complete Response (pCR) following Neoadjuvant Chemo-ImmunoTherapy (NCIT) in Head and Neck Squamous Cell Carcinoma (HNSCC) is crucial for optimizing therapeutic strategies and prognostic evaluation. Current methods exhibit limitations in simultaneously modeling multi-temporal treatment dynamics, multi-sequence magnetic resonance imaging (MRI) correlations, and multi-modal feature interactions. To address this challenge, we present a novel multi-modal representation and fusion framework, HARM 3 -Fusion, which innovatively processes multi-temporal, multi-sequence MRI data and hierarchically fuses it with whole slide image (WSI) to enhance the accuracy of pCR prediction. Specifically, our method comprises three key modules: a multi-temporal MRI fusion module based on Loss-enhanced Dual-stream Convolutional Variational Auto-Encod er (LD-VAE), designed to decouple features from pretreatment and post-treatment MRI scans; a multi-sequence MRI fusion module based on self-attention for integrating MRI features from T1 and T2 weighted sequences; and a multi-modal MRI-WSI fusion module based on cross-attention to fuse complementary information between MRI and WSI. To evaluate the efficacy of HARM 3 -Fusion, we establish", "filename": "2025_0089.pdf", "year": 2025, "institution": "Shenzhen MSU-BIT University", "country": "China", "authors": ["Jianye Wang", "Xinyue Liu", "Zhiying Gong", "Lingjie Yang", "Hanwen Zhang", "Yu Long", "Yimeng Fan", "Yuncheng Jiang", "Xiaohui Duan", "Weibing Zhao"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0090", "x": 0.924, "y": 2.75, "title": "Hierarchical Feature Learning for Medical Point Clouds via State Space Model", "abstract": "Deep learning-based point cloud modeling has been widely investigated as an indispensable component of general shape analysis. Recently, transformer and state space model (SSM) have shown promising capacities in point cloud learning. However, limited research has been conducted on medical point clouds, which have great potential in disease diagnosis and treatment. This paper presents an SSM-based hierarchical feature learning framework for medical point cloud understanding. Specifically, we down-sample the input into multiple levels through the farthest point sampling. At each level, we perform a series of k-nearest neighbor (KNN) queries to aggregate multi-scale structural information. To assist SSM in processing point clouds, we introduce coordinate-order and inside-out scanning strategies for efficient serialization of irregular points. Point features are calculated progressively from short neighbor sequences and long point sequences through vanilla and group Point SSM blocks, to capture both local patterns and longrange dependencies. To evaluate the proposed method, we build a largescale medical point cloud dataset named MedPointS for anatomy classification, completion, and segmentation. Extensive experiments conducted on MedPointS demonstrate that our method achieves superior performance across all tasks. The dataset is available at https://flemme- docs.readthedocs.io/en/latest/medpoints.html. Code is merged into a public medical imaging platform: https://github.com/wlsdzyzl/flemme.", "filename": "2025_0090.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Guoqing Zhang", "Jingyun Yang", "Yang Li"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0091", "x": 3.224, "y": 3.538, "title": "HRVVS: A High-Resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors", "abstract": "The segmentation of the hepatic vasculature in surgical videos holds substantial clinical significance in the context of hepatectomy procedures. However, owing to the dearth of an appropriate dataset and the inherently complex task characteristics, few researches have been reported in this domain. To address this issue, we first introduce a high quality frame-by-frame annotated hepatic vasculature dataset containing 35 long hepatectomy videos and 11442 high-resolution frames. On this basis, we propose a novel high-resolution video vasculature segmentation network, dubbed as HRVVS. We innovatively embed a pretrained visual autoregressive modeling (VAR) model into different layers of the hierarchical encoder as prior information to reduce the information degradation generated during the do wnsampling process. In addition, we designed a dynamic memory decoder on a multi-view segmentation network to minimize the transmission of redundant information while preserving more details between frames. Extensive experiments on surgical video datasets demonstrate that our proposed HRVVS significantly outperforms the state-of-the-art methods. The source code and dataset will be publicly available at https://github.com/scott-yjyang/HRVVS.", "filename": "2025_0091.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Xincheng Yao", "Yijun Yang", "Kangwei Guo", "Ruiqiang Xiao", "Haipeng Zhou", "Haisu Tao", "Jian Yang", "Lei Zhu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0092", "x": 0.17, "y": 3.858, "title": "Hybrid Graph Mamba: Unlocking Non-Euclidean Potential for Accurate Polyp Segmentation", "abstract": "Colorectal polyp segmentation can assist doctors in screening colonoscopy images, which is crucial for the prevention of colorectal cancer. Although deep learning has significantly advanced polyp segmentation, three issues remain: (1) Most polyp segmentation methods only extract Euclidean features such as shape and texture, while neglecting non-Euclidean features, such as the geometric topology between the polyp and its surrounding tissue; (2) Non-Euclidean features vary across different regions, but most feature fusion methods overlook both the non-Euclidean topological structures and the differences between internal, edge, and background regions. (3) Low-level features are not fully exploited, and the differences between low-and high-level features are not effectively addressed. To resolve these issues, we propose Hybrid Graph Mamba (HGM) based on Mamba and Graph Convolutional Network (GCN). Our model first uses the pyramid vision transformer to extract features at different levels. Next, we propose hybrid graph Mamba modules to process low-level features from multiple directions using quaddirectional Mamba a nd extract non-Euclidean features with GCN. A boundary discrimination fusion module is also designed to handle highlevel features, extracting semantic information for the interior, edges, and background to improve the fusion of low-and high-level features. Finally, a bidirectional Mamba decoder combines bidirectional Mamba and dilated convolutions to aggregate multi-scale features, minimizing information loss and producing the final prediction. Extensive experiments on five benchmark datasets demonstrate that HGM significantly outperforms eight state-of-the-art models. Our code is publicly available at https://github.com/YueyueZhu/HGM.", "filename": "2025_0092.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Yueyue Zhu", "Haolin Lv", "Geng Chen", "Zhonghao Zhang", "Haotian Jiang", "Yong Xia"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0093", "x": 0.778, "y": 6.117, "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication", "abstract": "The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic con trol, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot i nteraction while improving patient experience and acceptance of robotic ultrasound. The code is available at https://github.com/stytim/IVS.", "filename": "2025_0093.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Tianyu Song", "Feng Li", "Yuan Bi", "Angelos Karlas", "Amir Yousefi", "Daniela Branzan", "Zhongliang Jiang", "Ulrich Eck", "Nassir Navab"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0094", "x": 0.746, "y": 3.861, "title": "KMUNet: A Novel Medical Image Segmentation Model Based on KAN and Mamba", "abstract": "Medical image segmentation is essential for identifying lesion regions and diagnosing disease. Convolutional neural networks (CNNs) and transformerbased models often struggle to effectively capture both local details and global contextual features in medical images, leading to a decline in segmentation performance. To address this problem, a novel medical image segmentation model, KMUNet, is proposed by integrating Kolmogorov-Arnold networks (KAN) and Mamba based on the traditional U-shape architecture. This model employs a CNNbased encoder to extract local features and integrates a State Space Model-based Mamba module in the decoder to capture long-range dependencies. Initially, a global downsampling module, called KAN-PatchEmbed is presented. This module differs from traditional convolutional operations in utilizing an interval sampling strategy to alleviate the loss of feature information and KAN to reduce computational complexity, respectively. Furthermore, the Kolmogorov-Arnold Spatial-Channel Attention module is designed for skip connections, where KAN is employed to allocate the weight of the current channel by aggregating features across all stages. Finally, the proposed model was evaluated on three publicly available datasets. Experimental results reveal that KMUNet outperforms other models in segmentation tasks and produces more visually appealing segmentation results. Our code is available at https://github.com/zhang-hongsheng/KMUNet.", "filename": "2025_0094.pdf", "year": 2025, "institution": "Xiangtan University", "country": "China", "authors": ["Hongsheng Zhang", "Yuting Duan", "Ting Liu", "Weifeng Zhang", "Hongzhong Tang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0095", "x": 0.578, "y": 2.226, "title": "LiteTracker: Leveraging Temporal Causality for Accurate Low-Latency Tissue Tracking", "abstract": "Tissue tracking plays a critical role in various surgical navigation and extended reality (XR) applications. While current methods trained on large synthetic datasets achieve high tracking accuracy and generalize well to endoscopic scenes, their runtime performances fail to meet the low-latency requirements necessary for real-time surgical applications. To address this limitation, we propose LiteTracker, a low-latency method for tissue tracking in endoscopic video streams. Lite-Tracker builds on a state-of-the-art long-term point tracking method, and introduces a s et of training-free runtime optimizations. These optimizations enable online, frame-by-frame tracking by leveraging a temporal memory buffer for efficient feature re-use and utilizing prior motion for accurate track initialization. LiteTracker demonstrates significant runtime improvements being around .7× faster than its predecessor and . 2× than the state-of-the-art. Beyond its primary focus on efficiency, Lite-Tracker delivers high-accuracy tracking and occlusion prediction, performing competitively on both the STIR and SuPer datasets. We believe LiteTracker is an important step toward low-latency tissue track ing for real-time surgical applications in the operating room. Our code is publicly available at https://github.com/ImFusionGmbH/lite-tracker.", "filename": "2025_0095.pdf", "year": 2025, "institution": "ImFusion GmbH", "country": "Germany", "authors": ["Mert Asim Karaoglu", "Wenbo Ji", "Ahmed Abbas", "Nassir Navab", "Benjamin Busam", "Alexander Ladikos"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0096", "x": 4.574, "y": 6.04, "title": "Memory-Augmented Incomplete Multimodal Survival Prediction via Cross-Slide and Gene-Attentive Hypergraph Learning", "abstract": "Multimodal pathology-genomic analysis is critical for cancer survival prediction. However, existing approaches predominantly integrate formalin-fixed paraffin-embedded (FFPE) slides with genomic data, while neglecting the availability of other preservation slides, such as Fresh Froze (FF) slides. Moreover, as the high-resolution spatial nature of pathology data tends to dominate the cross-modality fusion process, it hinders effective multimodal fusion and leads to modality imbalance challenges between pathology and genomics. These methods also typically require complete data modalities, limiting their clinical applicability with incomplete modalities, such as missing either pathology or genomic data. In this paper, we propose a multimodal survival prediction framework that leverages hypergraph learning to effectively integrate multi-WSI information and cross-modality interactions between pathology slides and genomics d ata while addressing modality imbalance. In addition, we introduce a memory mechanism that stores previously learned paired pathology-genomic features and dynamically compensates for incomplete modalities. Experiments on five TCGA datasets demonstrate that our model outperforms advanced methods by over 2.3% in C-Index. Under incomplete modality scenarios, our approach surpasses pathology-only (3.3%) and gene-only models (7.9%). Code: https://github.com/MCPathology/M2Surv.", "filename": "2025_0096.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0097", "x": -0.374, "y": 4.423, "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "abstract": "Surgical video segmentation is a critical task in computerassisted surgery, essential for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has demonstrated remarkable advancements in both image and video segmentation. However, the inherent limitations of SAM2's greedy selection memory design are amplified by the unique properties of surgical videos-rapid instrument movement, frequent occlusion, and complex instrument-tissue interaction-resulting in diminished performance in the segmentation of complex, long videos. To address these challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video object segmentation strategy, featuring novel context-aware and occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against occlusions and interactions arising from complex instrument movements while maintaining accuracy in segmenting objects throughout videos. Employing a multi-target, single-lo op, one-prompt inference further enhances the efficiency of the tracking process in multi-instrument videos. Without introducing any additional parameters or requiring further training, MA-SAM2 achieved performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and EndoVis2018 datasets, respectively, demonstrating its potential for practical surgical applications. The code is available at https://github.com/Fawke108/MA- SAM2.", "filename": "2025_0097.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0098", "x": 4.292, "y": 2.408, "title": "Multi-linear 3D Craniofacial Infant Shape Model", "abstract": "After birth, the cranium and facial skeleton undergo rapid growth. Routine postnatal assessment is crucial for the early identification of craniofacial deformities, often characterized by asymmetric growth patterns. However, a comprehensive 3D shape model capturing both soft tissue and bony structures during early craniofacial development does not yet exist. We introduce the first integrated 3D shape model of the infant head and skull, constructed from a large dataset of photogrammetric scans complemented by a smaller set of computed tomography scans. Our INfant CRANial (INCRAN) model captures detailed facial expressions and overall cranial shape variations, incorporating the most advanced representation of cranial sutures on the underlying skull to date. By mapping cranial measurements to the model's latent space, we further distinguish v arious craniofacial deformities from normal shape variations, enabling automated diagnosis and correction proposals. Additionally, we propose a novel method for constructing a multi-linear model from an uncontrolled expression space by projecting an autoencoder back into PCA space, thus enhancing model interpretability. INCRAN supports growth monitoring and holds potential for improving infant healthcare and craniofacial treatment strategies.", "filename": "2025_0098.pdf", "year": 2025, "institution": "ETH Zuric h", "country": "Switzerland", "authors": ["Till N Schnabel", "Yoriko Lill", "Benito K Benitez", "Gaspard Krief", "Sebastián Tapia Corón", "Friederike Prüfer", "Philipp Metzler", "Andreas A Mueller", "Markus Gross", "Barbara Solenthaler"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0099", "x": 2.092, "y": 6.401, "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-Level Relevance in Gastrointestinal Imaging", "abstract": "Ensuring reliability is paramount in deep learning, particularly within the domain of medical imaging, where diagnostic decisions often hinge on model outputs. The capacity to separate out-ofdistribution (OOD) samples has proven to be a valuable indicator of a model's reliability in research. In medical imaging, this is especially critical, as identifying OOD inputs can help flag potential anomalies that might otherwise go undetected. While many OOD detection methods rely on feature or logit space representations, recent works suggest these approaches may not fully capture OOD diversity. To address this, we propose a novel OOD scoring mechanism, called NERO, that leverages neuron-level relevance at the feature layer. Specifically, we cluster neuron-level relevance for each in-distribution (ID) class to form representative centroids and introduce a relevance distance metric to quantify a n ew sample's deviation from these centroids, enhancing OOD separability. Additionally, we refine performance by incorporating scaled relevance in the bias term and combining feature norms. Our framework also enables explainable OOD detection. We validate its effectiveness across multiple deep learning architectures on the gastrointestinal imaging benchmarks Kvasir and GastroVision, achieving improvements over state-of-the-art OOD detection methods. Code Available: https://github.com/bhattarailab/NERO.", "filename": "2025_0099.pdf", "year": 2025, "institution": "Informatics Institute for Research", "country": "Nepal", "authors": ["Anju Chhetri", "Jari Korhonen", "Prashnna Gyawali", "Binod Bhattarai"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0100", "x": 3.066, "y": 2.102, "title": "Noise-Controllable Complex-Valued Diffusion Model for k-Space Data of Hyperpolarized $$^{129}$$Xe Lung MRI Generation", "abstract": "Hyperpolarized 129 Xe lung magnetic resonance imaging (MRI) offers a method for visualizing human lung function. However, its long imaging time limits widespread research and clinical adoption. Deep learning has shown significant potential in addressing undersampled MRI reconstruction challenges. Yet, the clinical novelty of hyperpolarized 129 Xe lung MRI results in a particular lack of raw k-space data. To address this, we propose a Noise-Controllable Complex-Valued Diffusion Model (NC-CDM) to augment the available data from limited clinical training set. Specifically, complex-valued convolutional kernels replace traditional ones, enhancing feature extraction and data utilization efficiency by learning rich representations from k-space. In addition, a noise-controllable module is introduced to mitigate estimation biases caused by thermal noise during MRI acquisition in t he training phases. Experiments compare the proposed NC-CDM with other state-of-the-art models. Fréchet Inception Distance (FID) and Inception Score (IS) metrics show that our method obtains higher image quality. The generated data, mixed with real data, are subsequently applied to downstream MRI reconstruction task using two deep learning-based MRI reconstruction methods: CasNet and KIKI-net. The results show that reconstruction networks trained on our generated data exhibit superior reconstruction performance.", "filename": "2025_0100.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Linxuan Han", "Sa Xiao", "Muhong Li", "Jinghan Liu", "Xin Zhou"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0101", "x": -0.18, "y": 2.141, "title": "Occlusion-Free 4D Gaussians for Open Surgery Videos Using Multi-camera Shadowless Lamps", "abstract": "Video recording of open surgery is in great demand for education and research purposes but is challenging due to the busy and dynamic environment. The state-of-the-art system uses multi-view cameras installed in shadowless lamps (McSL) and implements an automatic camera switching algorithm to avoid disturbances. However, this algorithm leads to missing pixels and distorted projection due to mathematical image warping and does not always provide the best perspective. We propose using 4D Gaussian Splatting (4DGS) to create editable 3D videos a nd remove Gaussians occluding surgical fields from a perspective. We enable occlusion-free 3D videos by addressing two occlusion removal approaches via (1) occlusion masking and (2) density-based Gaussian filtering. We create a real-surgery dataset and demonstrate that our method outperforms the state-of-the-art auto view-switching approach.", "filename": "2025_0101.pdf", "year": 2025, "institution": "Keio University", "country": "Japan", "authors": ["Yuna Kato", "Shohei Mori", "Hideo Saito", "Yoshifumi Takatsume", "Hiroki Kajita", "Mariko Isogawa"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0102", "x": 3.602, "y": 1.513, "title": "Physics-Informed Neural Operators for Tissue Elasticity Reconstruction", "abstract": "Magnetic Resonance Elastography (MRE) is a non-invasive imaging technique that estimates tissue elasticity using MagneticResonance Imaging. The conventional approach for elasticity reconstruction in MRE involves solving an inverse problem through numerical methods such as Helmholtz inversion and the finite element method. However, these techniques suffer from noise sensitivity and high computational costs due to iterative optimization. Recently, Physics-Informed Neural Networks (PINNs) have been studied for tissue elasticity reconstruction, integrating physical constraints into deep learning models. While PINNs improve noise resistance, they require a separate network to be trained for each instance, resulting in a computationally inefficient training. In this study, we introduce an operator learning-based approach to tissue elasticity reconstruction, which learns a generalized mapping from input measurements to tissue elasticity. This method enables simultaneous learning across multiple instances, significantly improving computational efficiency. Experimental results using box and abdomen simulation data show that our approach achieves superior reconstruction performance and robustness to noise.", "filename": "2025_0102.pdf", "year": 2025, "institution": "Chung-Ang University", "country": "Republic of Korea", "authors": ["Youjin Kim", "Jae Yong Lee", "Junseok Kwon"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0103", "x": -0.076, "y": 2.21, "title": "PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy", "abstract": "Endoluminal endoscopic procedures are essential for diagnosing colorectal cancer and other severe conditions in the digestive tract, urogenital system, and airways. 3D reconstruction and novel-view synthesis from endoscopic images are promising tools for enhancing diagnosis. Moreover, integrating physiological deformations and interaction with the endoscope enables the development of simulation tools from real video data. However, constrained camera trajectories and viewdependent lighting create artifacts, leading to inaccurate or overfitted reconstructions. We present PR-ENDO, a novel 3D reconstruction framework leveraging the unique property of endoscopic imaging, where a single light source is closely aligned with the camera. Our method separates light effects from tissue properties. PR-ENDO enhances 3D Gaussian Splatting with a physically based relightable model. We boost the traditional ligh t transport formulation with a specialized MLP capturing complex light-related effects while ensuring reduced artifacts and better generalization across novel views. PR-ENDO achieves superior reconstruction quality compared to baseline methods on both public and in-house datasets. Unlike existing approaches, PR-ENDO enables tissue modifications while preserving a physically accurate response to light, making it closer to real-world clinical use. Repository: https://github. com/SanoScience/PR-ENDO.", "filename": "2025_0103.pdf", "year": 2025, "institution": "Warsaw University of Technology", "country": "Poland", "authors": ["Joanna Kaleta", "Weronika Smolak-Dyżewska", "Dawid Malarz", "Diego Dall’alba", "Przemysław Korzeniowski", "Przemysław Spurek"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0104", "x": 0.533, "y": 5.355, "title": "Pre-trained LLM is a Semantic-Aware and Generalizable Segmentation Booster", "abstract": "With the advancement of Large Language Model (LLM) for natural language processing, this paper presents an intriguing finding: a frozen pre-trained LLM layer can process visual tokens for medical image segmentation tasks. Specifically, we propose a simple hybrid structure that integrates a pre-trained, frozen LLM layer within the CNN encoderdecoder segmentation framework (LLM4Seg). Surprisingly, this design improves segmentation performance with a minimal increase in trainable parameters across various modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our in-depth analysis reveals the p otential of transferring LLM's semantic awareness to enhance segmentation tasks, offering both improved global understanding and better local modeling capabilities. The improvement proves robust across different LLMs, validated using LLaMA and DeepSeek. Code is available at: https://github. com/FengheTan9/LLM4Seg.", "filename": "2025_0104.pdf", "year": 2025, "institution": "University of Science and T echnology of China", "country": "P.R. China", "authors": ["Fenghe Tang", "Wenxin Ma", "Zhiyang He", "Xiaodong Tao", "Zihang Jiang", "Shaohua Kevin Zhou"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0105", "x": 1.113, "y": 6.902, "title": "Prolog-Driven Rule-Based Diagnostics with Large Language Models for Precise Clinical Decision Support", "abstract": "Recently, large language models (LLMs) have been increasingly utilized for decision support across various domains. However, due to their probabilistic nature and diverse learning influences, LLMs can sometimes generate inaccurate or fabricated information, a phenomenon known as \"hallucination\". This issue is particularly problematic in fields like medical diagnosis, where accuracy is crucial and the margin for error is minimal. The risk of hallucination is exacerbated when patient data are incomplete or vary across different clinical departments. Consequently, using LLMs directly for clinical decision support presents significant challenges. In this paper, we introduce ProCDS, a system that integrates Prolog-based rule diagnostics with LLMs to enhance the precision of clinical decision support. ProCDS begins by converting medical protocols into a set of rules and patient information into facts. Then, we design an update cycle to extract and update related facts and rules due to possible discrepancies and missing patient information. After that, we perform a logical inference using the Prolog engine a nd acquire the response. If the Prolog engine cannot produce certain results, ProCDS would perform another iteration of facts and rules update to fix the potential mismatch and perform logical inference again. Through this iterative neuro-symbolic integrated process, ProCDS can perform transparent and accurate clinical decision support. We evaluated ProCDS in Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) real-world clinical scenarios and other logical reasoning benchmarks, achieving high accuracy and reliability in our results. Our project page is available at: https://github.com/testlbin/procds.", "filename": "2025_0105.pdf", "year": 2025, "institution": "Shanghai University of Engineering Science", "country": "China", "authors": ["Xiaoyu Tan", "Bin Li", "Weidi Xu", "Chao Qu", "Wei Chu", "Yinghui Xu", "Yuan Qi", "Xihe Qiu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0106", "x": 0.019, "y": 2.338, "title": "Reconstructing 3D Hand-Instrument Interaction from a Single 2D Image in Medical Scenes", "abstract": "Capturing the hand movements of physicians and their interactions with medical instruments plays a critical role in behavior analysis and surgical skill assessment. However, hand-instrument interaction in medical contexts is far more challenging than in general tasks. The weak texture and reflective properties of surgical instruments frequently result in failures in pose estimation. Moreover, the long and thin shape characteristics of the instruments and the sparse points of the reconstructed hand lead to difficulties in accurately grasping the instrument or may result in spatial penetration during interaction. To address failures in pose estimation, we build 3D models of medical instruments as priors to optimize instrument pose estimation. To resolve the issues of inaccurate grasping and minimize spatial penetration, we propose a contact-po intcentered interaction module by refining the surface details of the fingers to optimize the hand-instrument relationship computation. Experiments on medical scenario data demonstrate that our method achieves stateof-the-art performance across multiple evaluation metrics. Additionally, the 3D models developed in this work encompass a wide range of surgical instruments, based on real medical devices, and we will release them at https://github.com/xumiao66/MedIns-3D to support and promote further research.", "filename": "2025_0106.pdf", "year": 2025, "institution": "Chinese Academy of Sciences", "country": "China", "authors": ["Miao Xu", "Xiangyu Zhu", "Jinlin Wu", "Ming Feng", "Zelin Zang", "Hongbin Liu", "Zhen Lei"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0107", "x": -0.353, "y": 4.233, "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-Term Tracking", "abstract": "Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex realworld surgical scenarios. In this paper, we introduce ReSurgSAM2, a twostage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. U pon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets are available at https://github.com/jinlab-imvr/ReSurgSAM2.", "filename": "2025_0107.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Haofeng Liu", "Mingqi Gao", "Xuxiao Luo", "Ziyue Wang", "Guanyi Qin", "Junde Wu", "Yueming Jin"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0108", "x": -0.076, "y": 3.186, "title": "RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches", "abstract": "Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation/reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual/CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporallyconsistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable tempo ral parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP).", "filename": "2025_0108.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Shawn Mathew", "Saad Nadeem", "Arie Kaufman"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0109", "x": 3.123, "y": 4.827, "title": "SABPI-Net: A Novel Structure-Aware Network for Accurate and Domain-Invariant Retinopathy of Prematurity Diagnosis", "abstract": "Delayed treatment of retinopathy of prematurity (ROP) can diminish therapeutic efficacy and may lead to severe, potentially irreversible damage. Automated diagnosis of ROP presents significant challenges, including the detection of subtle early lesions, the variability of clinical phenotypes, and inconsistencies in imaging quality. To address these, which cannot be well addressed by existing general foundation models, we propose structure-aware proxy interaction network (SABPI-Net) within a universal learning framewrok. SABPI-Net incorporates a high-frequency mapping branch, and introduces a proxy interaction attention module to enable effective interaction between its trunk feature encoding branch and the high-frequency mapping branch. This enhances the model's ability to perceive fine retinal detail structures. Domainagnostic embedding space self-matching, guided by a memory-bank lowfrequency component replacement strategy, facilitates domain-invariant learning and ensures consistent model performance across diverse image styles. In this study, classification task for ROP is conducted on the largest clinical color fundus photography dataset to date, achieving an accuracy of 95.32%. Extensive experiments further validate the effectiveness and superiority of SABPI-Net in diagnosing ROP diseases.", "filename": "2025_0109.pdf", "year": 2025, "institution": "Macao Polytechnic U niversity", "country": "China", "authors": ["Shaobin Chen", "Xinyu Zhao", "Huazhu Fu", "Tao Tan", "Jiaju Huang", "Xiangyu Xiong", "Zhenquan Wu", "Behdad Dashtbozorg", "Baiying Lei", "Guoming Zhang", "Yue Sun"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0110", "x": 2.582, "y": 2.57, "title": "Self-supervised Axial Super-Resolution for Volume Microscopy via Diffusion-Guided Structure Distillation", "abstract": "Anisotropic resolution remains a fundamental challenge in 3D microscopy, where axial resolution is significantly lower than lateral resolution due to physical limitations. To address this, we propose a self-supervised volume super-resolution (VSR) framework named Diffusion to Resolution (D2R), which leverages 2D diffusion priors to enhance axial resolution without requiring high-resolution (HR) volume as supervision. D2R consists of three stages: (1) learning biological priors via a 2D diffusion model trained on high-resolution XY slices, (2) generating pseudo-HR lateral (XZ/YZ) volumes through cross-plane fusion, and (3) performing stable structure distillation to train a 3D VSR network. To further improve VSR quality, we introduce Axial Enhancement Network (AENet), a 3D VSR model incorporating lightweight channel attention to enhance fine details while main taining inter-slice continuity. Extensive experiments on FIB-SEM datasets demonstrate that D2R-AENet outperforms state-of-the-art self-supervised methods in both image similarity and membrane segmentation accuracy, achieving performance close to supervised approaches. These results validate the effectiveness of our framework in high-fidelity volumetric reconstruction under practical conditions where HR references are unavailable. Codes are available at https://github.com/hmzawz2/D2R-models.", "filename": "2025_0110.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Bohao Chen", "Yanchao Zhang", "Yanan Lv", "Hua Han", "Xi Chen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0111", "x": -0.527, "y": 3.1, "title": "SemiVT-Surge: Semi-supervised Video Transformer for Surgical Phase Recognition", "abstract": "Accurate surgical phase recognition is crucial for computerassisted interventions and surgical video analysis. Annotating long surgical videos is labor-intensive, driving research toward leveraging unlabeled data for strong performance with minimal annotations. Although self-supervised learning has gained popularity by enabling large-scale pretraining followed by fine-tuning on small labeled subsets, semisupervised approaches remain largely underexplored in the surgical domain. In this work, we propose a video transformer-based model with a robust pseudo-labeling framework. Our method incorporates temporal consistency regularization for unlabeled data and contrastive learning with class prototypes, which leverages both labeled data and pseudolabels to refine the feature space. Through extensive experiments on the private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and the public Cholec80 dataset, we demonstrate the effectiveness of our approach. By incorporating unlabeled data, we achieve state-of-the-art performance on RAMIE with a 4.9% accuracy increase and obtain comparable results to full supervision while using only 1/4 of the labeled data on Cholec80. Our findings establish a strong benchmark for semi-supervised surgical phase recognition, paving the way for future research in this domain. Code is available at", "filename": "2025_0111.pdf", "year": 2025, "institution": "Eindhoven University of T echnology", "country": "The Netherlands", "authors": ["Yiping Li", "Ronald De Jong", "Sahar Nasirihaghighi", "Tim Jaspers", "Romy Van Jaarsveld", "Gino Kuiper", "Richard Van Hillegersberg", "Fons Van Der Sommen", "Jelle Ruurda", "Marcel Breeuwer", "Yasmina Al Khalil"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0112", "x": 1.499, "y": 2.398, "title": "SFPFR: Self-supervised Facial Paralysis Face Reconstruction Under Few Views", "abstract": "3D face reconstruction methods exhibit significant limitations when applied to pathological cases such as facial paralysis, due to inherent challenges including asymmetric motion and non-linear muscle dynamics. To address these gaps, we propose SFPFR, a self-supervised framework for facial paralysis 3D face reconstruction leveraging 1-3 viewpoints. We first propose a self-supervised learning paradigm integrating reconstruction loss, multi-view consistency loss, and a Mamba-based temporal loss to reconstruct 3D face without ground-truth; then, a partitioned dynamic fusion module that adaptively weights multi-view features ensuring precise geometric reconstruction and pathological detail preservation; last, we propose FPD-100, the first multi-view video dataset for facial paralysis, comprising 30,000 frames from 100 patients of 3 views. Extensive experiments validate SFPFR's superiority, achieving state-ofthe-art PSNR (27.74) and FID (37.13). It enables clinical applications in severity assessment, rehabilitation monitoring, and treatment planning, while the dataset and code will be open-sourced to catalyze research in pathological facial analysis.", "filename": "2025_0112.pdf", "year": 2025, "institution": "Qingdao University of Science and Technology", "country": "China", "authors": ["Yaru Qiu", "Xinru Wang", "Jianfang Zhang", "Bo Liu", "Peng Bai", "Yuanyuan Sun"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0113", "x": 1.485, "y": 4.833, "title": "Source-Free Active Domain Adaptation for Efficient Medical Video Polyp Segmentation", "abstract": "Deep learning models have shown remarkable performance in medical video object segmentation. However, addressing the cross-center domain issue is crucial for achieving consistent performance across different medical facilities. Emerging Source-Free Active Domain Adaptation (SFADA) techniques can enhance the performance of target domain segmentation models, ensuring data privacy and security. While current approaches primarily focus on image-level tasks and mainly emphasize intra-frame pixel correlations, they overlook temporal correlations, which restricts their performance in video frame recommendation. Consequently, this paper proposes the first video-level SFADA method and evaluates it on video polyp segmentation across different data centers. Specifically, the Spatial-Temporal Active Recommendation (STAR) strategy is devised to recommend a few highly valuable frames for annotation by comprehensively evaluating the object spatial correlation and temporal movement density across different video frames, along with a Passive Phase Correction (PPC) module is proposed to suppress the noisy source disruptions of the remaining unlabeled data during the finetuning stage. Experimental results demonstrate that with a tiny quantity of annotation, our method significantly improves performance over the lower bound and achieves better performance than existing SOTA methods, which is valuable for practical clinical employment (link).", "filename": "2025_0113.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Jialu Li", "Hongqiu Wang", "Weiming Wang", "Jing Qin", "Qiong Wang", "Lei Zhu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0114", "x": -0.28, "y": 4.823, "title": "SR-SAM: Subspace Regularization for Domain Generalization of Segment Anything Model", "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have been widely used to adapt foundation models like the Segment Anything Model (SAM) for better generalization in unseen domains. Despite their widespread use, PEFT often suffers from overfitting to the source training domain, which limits their generalization performance. To address this limitation, we propose a novel subspace regularization (SR) method for robust fine-tuning. Our approach iteratively removes the knowledge of task-specific directions, as identified by LoRA parameters learned from the source domain, from the subspace of pre-trained weights. This strategy effectively encourages the LoRA parameters to acquire a more diverse range of knowledge. In addition, we introduce an exponential moving average (EMA) LoRA module that aggregates historical updates of the LoRA parameters throughout the fine-tuning process. This aggregation enhances stabilit y and the generalizability of the learned features by smoothing the trajectory of parameter updates. Our enhanced framework, SR-SAM, incorporates both subspace regularization and the EMA LoRA module to fine-tune the popular SAM model effectively. Experimental results on two widely used domain generalization benchmarks demonstrate that SR-SAM outperforms existing state-of-the-art methods, underscoring the effectiveness of our method. The source code is available at https://github.com/xjiangmed/SR-SAM.", "filename": "2025_0114.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Xixi Jiang", "Chen Yang", "Liang Zhang", "Tim Kwang-Ting Cheng", "Xin Yang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0115", "x": -0.416, "y": 2.781, "title": "SurgSora: Object-Aware Diffusion Model for Controllable Surgical Video Generation", "abstract": "Surgical video generation can enhance medical education and research, but existing methods lack fine-grained motion control and realism. We introduce SurgSora, a framework that generates high-fidelity, motion-controllable surgical videos from a single input frame and userspecified motion cues. Unlike prior approaches that treat objects indiscriminately or rely on ground-truth segmentation masks, SurgSora leverages self-predicted object features and depth information to refine RGB appearance and optical flow for precise video synthesis. It consists of three key modules: (1) the Dual Semantic Injector, which extracts objectspecific RGB-D features and segmentation cues to enhance spatial representations; (2) the Decoupled Flow Mapper, which fuses multi-scale optical flow with semantic features for realistic motion dynamics; and(3) the Trajectory Controller, which estimates sparse optical flow and enables user-guided object movement . By conditioning these enriched features within the Stable Video Diffusion, SurgSora achieves stateof-the-art visual authenticity and controllability in advancing surgical video synthesis, as demonstrated by extensive quantitative and qualitative comparisons. Our human evaluation in collaboration with expert surgeons further demonstrates the high realism of SurgSora-generated videos, highlighting the potential of our method for surgical training and education. Our project is available at surgsora.github.io.", "filename": "2025_0115.pdf", "year": 2025, "institution": "The University of S ydney", "country": "Australia", "authors": ["Tong Chen", "Shuya Yang", "Junyi Wang", "Long Bai", "Hongliang Ren", "Luping Zhou"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0116", "x": -0.606, "y": 3.05, "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "abstract": "Surgical phase recognition plays a crucial role in surgical workflow analysis, enabling various applications such as surgical monitoring, skill assessment, and workflow optimization. Despite significant advancements in deep learningbased surgical phase recognition, these models remain inherently opaque, making it difficult to understand how they make decisions. This lack of interpretability hinders trust and makes it challenging to debug the model. To address this challenge, we propose SurgX, a novel concept-based explanation framework that enhances the interpretability of surgical phase recognition models by associating neurons with relevant concepts. In this paper, we introduce the process of selecting representative example sequences for neurons, constructing a concept set tailored to the surgical video dataset, associating neurons with concepts and identifying neurons crucial for predictions. Through extensive experiments on two surgical phase recognition models, we validate our method and analyze the explanation for prediction. This highlights the potential of our method in explaining surgical phase recognition. The code is available at https://github.com/ailab- kyunghee/SurgX.", "filename": "2025_0116.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Ka Young Kim", "Hyeon Bae Kim", "Seong Tae Kim"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0117", "x": 0.09, "y": 3.989, "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy", "abstract": "Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intrapolyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive reassociations betw een visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at github.com/lparolari/temporally-aware-polyp-counting.", "filename": "2025_0117.pdf", "year": 2025, "institution": "University of Padova", "country": "Italy", "authors": ["Luca Parolari", "Andrea Cherubini", "Lamberto Ballan", "Carlo Biffi"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0118", "x": 0.758, "y": 3.665, "title": "Tetra-Orientated Mamba with T2-FLAIR Mismatch Features for Glioma Segmentation, IDH Genotyping, and Grading", "abstract": "Tumor grading and Isocitrate Dehydrogenase (IDH) status are key prognostic biomarkers. Transformer-based methods are widely applied in glioma segmentation and diagnosis, but challenges still exist due to the tumor's heterogeneity and the computational burden of Transformers. We propose a multi-task network called MTamba for glioma segmentation, IDH genotyping, and grading. We design Tetra-oriented Mamba to perform global information interaction from different orientations in MRIs for segmentation. We design a T2-FLAIR mismatch feature extraction module to explore the mismatch features between T2 and FLAIR images at different depths to enhance diagnosis. We propose a channel-space Siamese Mamba fusion module to fuse T2-FLAIR mismatch features with multimodal MRI features from the segmen tation encoder for diagnosis. Finally, we apply an uncertainty loss optimization method to jointly optimize glioma segmentation, IDH genotyping, and grading. We validate MTamba on the publicly available UCSF-PDGM and BraTS2020 datasets, and experimental results show that MTamba outperforms existing multi-task learning methods. The code for MTamba is available at https://github.com/xhwv/MTamba.", "filename": "2025_0118.pdf", "year": 2025, "institution": "Cent ral South University", "country": "China", "authors": ["Xinyu Li", "Jin Liu", "Hulin Kuang", "Yuanzhuo Wang", "Jianxin Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0119", "x": -0.209, "y": 4.937, "title": "TGSAM-2: Text-Guided Medical Image Segmentation Using Segment Anything Model 2", "abstract": "The Segment Anything Model 2 (SAM-2) has shown impressive capabilities for promptable segmentation in images and videos. However, SAM-2 primarily operates on visual prompts including points, boxes, and masks, which does not natively support text prompts. This limitation is particularly noticeable in medical imaging, where domainspecific textual descriptions are often beneficial for annotating subtle abnormalities and identifying regions of interest. In this paper, we introduce Text-Guided SAM-2 (TGSAM-2), a medical image segmentation model tailored to leverage text prompts as contextual guidance. We propose a text-conditioned visual perception module that conditions visual features on textual descriptions, and refine the memory encoder to track target objects using medical text prompts. We evaluate our method on four medical image datasets with video-like characteristics, including 2D image sequences (e.g. Endoscopy, Ultrasound) and 3D volumes (e.g. CT, MRI). Experimental results demonstrate that our method outperforms state-of-the-art models, including both image-only and text-guided medical image segmentation methods.", "filename": "2025_0119.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Runtian Yuan", "Ling Zhou", "Jilan Xu", "Qingqiu Li", "Mohan Chen", "Yuejie Zhang", "Rui Feng", "Tao Zhang", "Shang Gao"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0120", "x": 4.49, "y": 3.436, "title": "Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines", "abstract": "Left ventricular (LV) indicator measurements following clinical echocardiography guidelines are important for diagnosing cardiovascular disease. Although existing algorithms have explored automated LV quantification, they can struggle to capture generic visual representations due to the normally small training datasets. Therefore, it is necessary to introduce vision foundational models (VFM) with abundant knowledge. However, VFMs represented by the segment anything model (SAM) are usually suitable for segmentation but incapable of identifying key anatomical points, which are critical in LV indicator measurements. In this paper, we propose a novel framework named AutoSAME, combining the powerful visual understanding of SAM with segmentation and landmark localization tasks simultaneously. Consequently, the framework mimics the operation of cardiac sonographers, achieving LV indicator measurements consistent with clinical guidelines. We further present filtered cross-branch attention (FCBA) in AutoSAME, which leverages relatively comprehensive features in the segmentation to enhance the heatmap regression (HR) of key points from the frequency domain perspective, optimizing the visual representation learned by the latter. Moreover, we propose spatial-guided prompt alignment (SGPA) to automatically generate prompt embeddings guided by spatial properties of LV, thereby improving the accuracy of dense predictions by prior spatial knowledge. The extensive experiments on an echocardiography dataset demonstrate the efficiency of each design and the superiority of our AutoSAME in LV segmentation, landmark localization, and indicator measurements. The code will be available at https://github. com/QC-LIU-1997/AutoSAME.Tuo Liu and Qinghan Yang-Equal Contribution.", "filename": "2025_0120.pdf", "year": 2025, "institution": "Southeast University", "country": "China", "authors": ["Tuo Liu", "Qinghan Yang", "Yu Zhang", "Rongjun Ge", "Yang Chen", "Guangquan Zhou"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0121", "x": 1.044, "y": 2.57, "title": "Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection", "abstract": "Liver landmarks provide crucial anatomical guidance to the surgeon during laparoscopic liver surgery to minimize surgical risk. However, the tubular structural properties of landmarks and dynamic intraoperative deformations pose significant challenges for automatic landmark detection. In this study, we introduce TopoNet, a novel topologyconstrained learning framework for laparoscopic liver landmark detection. Our framework adopts a snake-CNN dual-path encoder to simultaneously capture detailed RGB texture information and depth-informed topological structures. Meanwhile, we propose a boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D features to enhance edge perception while preserving global topology. Additionally, a topological constraint loss function is embedded, which contains a centerline c onstraint loss and a topological persistence loss to ensure homotopy equivalence between predictions and labels. Extensive experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves outstanding accuracy and computational complexity, highlighting the potential for clinical applications in laparoscopic liver surgery. Our code is available at https://github.com/cuiruize/TopoNet", "filename": "2025_0121.pdf", "year": 2025, "institution": "The Hong Kong Polytechnic University", "country": "China", "authors": ["Ruize Cui", "Jiaan Zhang", "Jialun Pei", "Kai Wang", "Pheng-Ann Heng", "Jing Qin"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0122", "x": 3.589, "y": 2.988, "title": "Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning", "abstract": "A generative model for the mesh geometry of intracranial aneurysms (IA) is crucial for training networks to predict blood flow forces in real time, which is a key factor affecting disease progression. This need is necessitated by the absence of a large IA image datasets. Existing shape generation methods struggle to capture realistic IA features and ignore the relationship between IA pouches and parent vessels, limiting physiological realism and their generation cannot be controlled to have specific morphological measurements. We propose AneuG, a twostage Variational Autoencoder (VAE)-based IA mesh generator. In the first stage, AneuG generates low-dimensional Graph Harmonic Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes. GHD enables more accurate shape encoding than alternatives. In the second stage, AneuG generates parent vessels conditioned on GHD tokens, by generating vascular centerline and propagating the cross-section. IA shape generation can be conditioned on specific clinically relevant shape measurements, enabling controlled studies on how morphological variations impact flow behaviors. Additional, our novel Morphing Energy Alignment constraint and Morphological Marker Calculator improve generation fidelity and controllability. Source code and implementation details are available at https://github.com/anonymousaneug/AneuG.", "filename": "2025_0122.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Wenhao Ding", "Kangjun Ji", "Simão Castro", "Yihao Luo", "Dylan Roi", "Choon Hwai Yap"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0123", "x": -0.137, "y": 4.127, "title": "Unsupervised Quality Control and Enhancement of Polyp Segmentation in Colonoscopy Videos Using Spatiotemporal Consistency", "abstract": "Reliable polyp segmentation in colonoscopy videos is crucial for early detection and prevention of colorectal cancer. While deep learning-based segmentation models show promise, their performance can be inconsistent, and robust methods for assessing segmentation quality without ground-truth annotations are lacking. This paper presents a novel quality control framework for polyp segmentation that leverages the temporal consistency inherent in colonoscopy videos. Our framework utilizes the Segment Anything Model 2 (SAM2), a powerful video segmentation foundation model, to propagate segmentation predictions between adjacent frames. By evaluating the consistency between these propagated segmentations and the original model predictions, we obtain an unsupervised Segmentation Quality Assessment (SQA) score for each frame. Furthermore, we introduce a re-segmentation module that refines low-quality segmentations by leveraging information from high-quality frames, identified based on their SQA scores. Experiments on the SUN-SEG and PolypGen datasets demonstrate a moderate to strong correlation between the SQA scores produced by our framework and the groundtruth segmentation quality. The re-segmentation module also improves overall segmentation performance without requiring model retraining or fine-tuning. This work suggests a step towards building more reliable and trustworthy AI-assisted colonoscopy systems. The code is available at https://github.com/LYJ-NJUST/Seg-Quality-Control.", "filename": "2025_0123.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Yujia Li", "Tao Zhou", "Ruixuan Wang", "Shuo Wang", "Yizhe Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0124", "x": 1.073, "y": 3.803, "title": "UWT-Net: Mining Low-Frequency Feature Information for Medical Image Segmentation", "abstract": "Medical image segmentation is the core technology of precision medicine, which can improve diagnostic accuracy, optimize treatment plans, and enhance research efficiency. U-Net is a classical and fundamental model in this field. Because of its excellent architecture, Transformer and MLP have been fused on top of it in subsequent work, all with good results. Each of these methods has advantages, but none further explores the image's low-frequency feature information. The lowfrequency feature information reflects the overall structure and contour of the image and provides key background and boundary information for image segmentation. To address this problem, we explore the potential of Wavelet Convolutions for medical segmentation tasks by proposing a novel feature extraction block: the Image Multi-frequency Feature Information Extraction (IMFIE) block. The IMFIE block can effectively extract both high-frequency and low-frequency feature information from images b y combining Wavelet Convolutions. This approach takes full advantage of their excellent ability to mine and utilize low-frequency information in images while expanding the receptive field at a low cost. We propose a novel model, UWT-Net, which leverages the IMFIE block and reconstructs the classical U-Net. Experiments on three public pathology image datasets show that the proposed method outperforms the state-of-the-art baseline U-KAN. Code is available at https://github. com/zpc2002zpc/UWT-Net.git.", "filename": "2025_0124.pdf", "year": 2025, "institution": "Sichuan Agricultural Universit y", "country": "China", "authors": ["Pengcheng Zhang", "Xiaocao Ouyang", "Ran Peng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0125", "x": 0.387, "y": 3.749, "title": "WDNet: A Novel Wavelet-Guided Hierarchical Diffusion Network for Multi-target Segmentation in Colonoscopy Images", "abstract": "Semantic segmentation in colonoscopy images is pivotal in aiding healthcare professionals to interpret images and enhance diagnostic precision. Nonetheless, the detection of polyps and instruments is challenged by the difficulty in capturing the textures and edges of tiny lesions, and these challenges are exacerbated by low contrast, inconsistent illumination, and noise. To address these challenges, we introduce WDNet, a network adopting a multi-tiered feature extraction and fusion approach, with each encoder layer amalgamating local and global information to construct expressive high-level representations. The input of the network is derived from wavelet transform to dissect images into low-and high-frequency sub-bands, utilizing learnable soft-thresholding to diminish noise while maintaining essential features. High-frequency data are adept at capturing details and edges, whereas low-frequency data furnish a global context. Moreover, WDNet harnesses a diffusionbased decoding mechanism with adaptive step sizes to amplify target region features and mitigate background interference, achieving meticulous segmentation. Comprehensive experiments conducted on a new surgical dataset, along with public benchmarks underscore its remarkable performance. WDNet not only exhibits state-of-the-art performance of semantic segmentation in colonoscopy images with remarkable detail and boundary accuracy but also stands out in processing speed, facilitating the swift handling of extensive datasets. The dataset and source code are available at https://github.com/hedongdong6060/WDNet.", "filename": "2025_0125.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Dongdong He", "Fang Ma", "Ziteng Liu", "Xunhai Yin", "Hao Liu", "Wenpeng Gao", "Chenghong Zhang", "Yili Fu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0126", "x": 2.723, "y": 1.393, "title": "A New Paradigm for Low-Dose PET/CT Reconstruction with Mamba-Powered Progressive Network and Physics-Informed Consistency", "abstract": "Positron Emission Tomography (PET) is a powerful imaging technique but involves radiation exposure due to the use of radioactive tracers. A promising solution to mitigate this risk is reconstructing standard-dose PET (SPET) from low-dose PET (LPET). Previous studies have primarily focused on attenuation-corrected PET data; however, the attenuation correction process can amplify noise and artifacts, especially in low-dose scenarios. Additionally, PET scans are often paired with CT scans for attenuation correction, further contributing to radiation exposure. To address these challenges, we propose a new paradigm that reconstructs Attenuation-Corrected SPET (AC SPET) and standard-dose CT (SCT) images from the original Non-Attenuation-Corrected LPET (NAC LPET)) and low-dose CT (LCT) data through a collaborative reconstruction framework. Key components of our proposed method include: (1) a coarse-to-fine learning strategy, wherein specialized reconstruction basis is initially built by processing each modality individually, follo wed by Domain Adapters to facilitate cross-modal feature correlation; (2) a hybrid Mamba-powered Expert Network that effectively captures long-range dependencies between different regions of whole-body PET/CT images; and (3) a Physics-informed Mutual Loss function to enforce consistency between the PET and CT domains, ensuring robust and reliable reconstruction results. Extensive experiments on the collected dataset demonstrate that our model achieves diagnosticquality reconstruction while significantly reducing radiation exposure.", "filename": "2025_0126.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Zixin Tang", "Caiwen Jiang", "Zhiming Cui", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0127", "x": 0.367, "y": 2.925, "title": "A Prior-Driven Lightweight Network for Endoscopic Exposure Correction", "abstract": "Against this endoscopic exposure correction task, although some past studies have yielded promising results, these methods do not fully explore the task-specific priors, and they generally require a large number of parameters thus compromising their applications on resourceconstrained devices. In this paper, we carefully explore that regardless of the exposure level degradation, the illumination information is usually contained in the low frequency part, and the relative smoothness of structures in captured endoscopic images generally lead to the sparse high-frequency representation. Motivated by such prior understandings, we specifically construct a lightweight wavelet transform-based hierarchical network structure for this correction task, called WTNet, which utilizes the inherent frequency decomposition characteristics of wavelet transform and makes the core of network learning focus on the modelling of low-frequency information. Based on four datasets and three different tasks, including exposure correction, low-light enhancement, and downstream segmentation, we comprehensively substantiate the superiority of our proposed WTNet. With only 1.41M model parameters, our WTNet achieves a better balance between performance and cost, and demonstrates favorable clinical application potential. The code will be available at https://github.com/charonf/WTNet.", "filename": "2025_0127.pdf", "year": 2025, "institution": "East China Normal University", "country": "China", "authors": ["Zhijian Wu", "Hong Wang", "Yuxuan Shi", "Dingjiang Huang", "Yefeng Zheng"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0128", "x": -0.391, "y": 3.39, "title": "Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision", "abstract": "Surgical AI often involves multiple tasks within a single procedure, like phase recognition or assessing the Critical View of Safety in laparoscopic cholecystectomy. Traditional models, built for one task at a time, lack flexibility, requiring a separate model for each. To address this, we introduce MML-SurgAdapt, a unified multi-task framework with Vision-Language Models (VLMs), specifically CLIP, to handle diverse surgical tasks through natural language supervision. A key challenge in multi-task learning is the presence of partial annotations when integrating different tasks. To overcome this, we employ Single Positive Multi-Label (SPML) learning, which traditionally reduces annotation burden by training models with only one positive label per instance. Our framework extends this approach to integrate data from multiple surgical tasks within a single procedure, enabling effective learning despite incomplete or noisy annotations. We demonstrate the effectiveness of our model on a combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50, utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt p erforms comparably to task-specific benchmarks, with the added advantage of handling noisy annotations. It also outperforms the existing SPML frameworks for the task. By reducing the required labels by 23%, our approach proposes a more scalable and efficient labeling process, significantly easing the annotation burden on clinicians. To our knowledge, this is the first application of SPML to integrate data from multiple surgical tasks, presenting a novel and generalizable solution for multi-task learning in surgical computer vision. Implementation is available at: https://github.com/CAMMA-public/MML- SurgAdapt.", "filename": "2025_0128.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Soham Walimbe", "Britty Baby", "Vinkle Srivastav", "Nicolas Padoy"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0129", "x": -0.108, "y": 3.094, "title": "Automated Auditing of Upper Endoscopy Procedure Times: A Temporal Multiclass Analysis", "abstract": "Upper endoscopy is the preferred method for detecting earlystage gastrointestinal diseases and plays a crucial role in managing gastric cancer. Quality assessment has been a recurring concern in clinical research, particularly regarding the time specialists spend examining different anatomical sites. While current guidelines emphasize thorough inspection and documentation to minimize blind spots, adherence remains low due to the lack of second readers. State-of-the-art automatic approaches audit single-frame or fixed temporal windows, with limited performance in real applications. This paper introduces the Multi-Scale Sequence Informative (MSSI) module, a Transformerbased attention mechanism that audits video sequences across multiple temporal scales. The proposed approach estimates the time spent exploring different organs and regions of the stomach. The method processes 15 to 196 tokens (1 to 13 s) by a sliding window, building up a mosaic of sampled frames. Each frame is encoded with a pretrained endoscopy embedding which feeds a Vision Transformer to capture short-, mid-, and long-range dependencies. The approach is evaluated with 233 endoscopic procedures (∼1.6 million frames), demonstrating a close alignment between estimated procedural times and expert-validated standards. It achieved 92.03% macro precision in organ classification and 89.34% in distinguishing 23 specific views of different stomach sites, a total of 27 classes to audit, showing real potential to be applicable in real clinical scenarios. Our code is available at https://github.com/Cimalab-unal/EndoAudit.git.", "filename": "2025_0129.pdf", "year": 2025, "institution": "Universidad Nacional de Colombia", "country": "Colombia", "authors": ["Diego Bravo", "Josué Ruano", "Martín Gómez", "Fabio A Gónzalez", "Eduardo Romero"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0130", "x": -0.003, "y": 2.078, "title": "BridgeSplat: Bidirectionally Coupled CT and Non-rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "abstract": "We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated bac k to update the CT. We demonstrate BridgeSplat's effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at https://maxfehrentz.github.io/ct-informed-splatting/.", "filename": "2025_0130.pdf", "year": 2025, "institution": "TU Munich", "country": "Germany", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0131", "x": 2.809, "y": 4.608, "title": "Clinically-Guided Data Synthesis for Laryngeal Lesion Detection", "abstract": "Although computer-aided diagnosis (CADx) and detection (CADe) systems have made significant progress in various medical domains, their application is still limited in specialized fields such as otorhinolaryngology. In the latter, current assessment methods heavily depend on operator expertise, and the high heterogeneity of lesions complicates diagnosis, with biopsy persisting as the gold standard despite its substantial costs and risks. A critical bottleneck for specialized endoscopic CADx/e systems is the lack of well-annotated datasets with sufficient variability for real-world generalization. This study introduces a novel approach that exploits a Latent Diffusion Model (LDM) coupled with a ControlNet adapter to generate laryngeal endoscopic imageannotation pairs, guided by clinical observations. The method addresses data scarcity by conditioning the diffusion process to produce realistic, high-quality, and clinically relevant image features that capture diverse anatomical conditions. The proposed approach can be leveraged to expand training datasets for CADx/e models, empowering the assessment process in laryngology. Indeed, during a downstream task of detection, the addition of only 10% synthetic data improved the detection rate of laryngeal lesions by 9% when the model was internally tested and 22.1% on out-of-domain external data. Additionally, the realism of the generated images was evaluated by asking 5 expert otorhinolaryngologists with varying expertise to rate their confidence in distinguishing synthetic from real images. This work has the potential to accelerate the development of automated tools for laryngeal disease diagnosis, offering a solution to data scarcity and demonstrating the applicability of synthetic data in real-world scenarios (We publicly share our code at https://github.com/ChiaraBaldini/endoLDMC.git).", "filename": "2025_0131.pdf", "year": 2025, "institution": "Istituto Italiano di Tecnologia", "country": "Italy", "authors": ["Chiara Baldini", "Kaisar Kushibar", "Richard Osuala", "Simone Balocco", "Oliver Diaz", "Karim Lekadir", "Leonardo S Mattos"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0132", "x": 5.649, "y": 3.885, "title": "Cross-Modal Contrastive Learning for Emotion Recognition: Aligning ECG with EEG-Derived Features", "abstract": "Emotion recognition plays a vital role in affective computing and mental health monitoring within intelligent healthcare systems. While EEG captures rich emotional patterns, its clinical applicability is limited by cumbersome acquisition and susceptibility to motion artifacts. In contrast, electrocardiogram (ECG) signals are more accessible and less prone to artifacts, but lack direct semantic representation of emotions categories. To address this challenge, we introduce a cross-modal alignment approach using contrastive learning. First, we extract emotional features from EEG signals using a pre-trained encoder. Then, we align the ECG encoder to these EEG-derived features through a contrastive learning framework, using sequence and patch level semantic alignment based on a temporal patch shuffle strategy. This method effectively combines the strengths of both modalities. Experiments o n the DREAMER and AMIGOS datasets show that our method outperforms other baseline methods in emotion recognition tasks. Additional ablation studies and visualizations further reveal the contribution of core components. From a practical application perspective, our approach facilitates accurate emotion recognition in scenarios where EEG acquisition is impractical, providing a more accessible alternative for real-world affective computing applications. The code is available at https://github.com/pokking/ ECG EEG alignmen t.", "filename": "2025_0132.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Yi Wu", "Yuhang Chen", "Jiahao Cui", "Jiaji Liu", "Lin Liang", "Shuai Li"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0133", "x": 2.726, "y": 4.405, "title": "DiDGen: Diffusion-Based Dual-Task Synthesis for Dermoscopic Data Generation", "abstract": "Computer-aided diagnosis (CAD) systems for skin lesion analysis reduce costs and workload associated with the manual inspection of skin diseases. Nevertheless, the performance of deep learning (DL)-based CAD systems is constrained by the limited availability of labeled data, necessitating advanced dataset augmentation techniques. To address this limitation, we propose DiDGen, a novel method that employs Diffusion models (DMs) for Dermoscopic image Generation and lesion-mask pair synthesis. Specifically, we introduce DermPrompt, a new type of structured text prompt rich with clinical details annotated by large language models (LLMs), which facilitates DMs' learning of fine-grained visual representations. Additionally, we propose a new paradigm for lesion-mask pair synthesis by incorporating a region-aware attention loss during finetuning to facilitate the build of semantic connections between text and visual representations, and then integrating test-time layout guidance with attention-based annotation to synthesize diverse and accurate lesion-mask pairs in a training-free manner. Extensive experiments demonstrate that our method improves the quality and diagnostic utility of generated dermoscopic images, thereby enhancing DL model performance in skin lesion classification and segmentation tasks. Our code is available at https://github.com/junjie-shentu/ DiDGen.", "filename": "2025_0133.pdf", "year": 2025, "institution": "Durham University", "country": "UK", "authors": ["Junjie Shentu", "Matthew Watson", "Noura Al Moubayed"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0134", "x": 1.947, "y": 5.51, "title": "DIY Challenge Blueprint: From Organization to Technical Realization in Biomedical Image Analysis", "abstract": "Biomedical image analysis challenges have become the de facto standard for publishing new datasets and benchmarking different state-of-the-art algorithms. Most challenges use commercial cloud-based platforms, which can limit custom options and involve disadvantages such as reduced data control and increased costs for extended functionalities. In contrast, Do-It-Yourself (DIY) approaches have the capability to emphasize reliability, compliance, and custom features, providing a solid basis for low-cost, custom designs in self-hosted systems. Our approach emphasizes cost efficiency, improved data sovereignty, and strong compliance with regulatory frameworks, such as the GDPR. This paper presents a blueprint for DIY biomedical imaging challenges, designed to provide institutions with greater autonomy over their challenge infrastructure. Our approach comprehensively addresses both organizational and technical dimensions, including key user roles, data management strategies, and secure, efficient workflows. Key technical contributions include a modular, containerized infrastructure based on Docker, integration of open-source identity management, and automated solution evaluation workflows. Practical deployment guidelines are provided to facilitate implementation and operational stability. The feasibility and adaptability of the proposed framework are demonstrated through the MICCAI 2024 PhaKIR challenge with multiple international teams submitting and validating their solutions through our self-hosted platform. This work can be used as a baseline for future self-hosted DIY implementations and our results encourage further studies in the area of biomedical image analysis challenges.", "filename": "2025_0134.pdf", "year": 2025, "institution": "Regensburg Medical Image Computing (ReMIC)", "country": "Germany", "authors": ["Leonard Klausmann", "Tobias Rueckert", "David Rauber", "Raphaela Maerkl", "Suemeyye R Yildiran", "Max Gutbrod", "Christoph Palm"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0135", "x": 0.958, "y": 4.125, "title": "DyMAS-Net: Dynamic Multi-scale Adaptive Sampling Network for Efficient Medical Image Segmentation", "abstract": "Achieving high-precision medical image segmentation while maintaining computational efficiency remains a critical challenge for clinical applications. Existing methods often struggle to balance multi-scale feature fusion, lightweight design and contextual modeling, particularly for complex medical scenes with ambiguous boundaries. To address these limitations, We propose DyMAS-Net, a lightweight framework integrating multi-scale convolution, adaptive dynamic sampling, and dual attention mechanisms. Key innovations include: (i) Hierarchical Multi-Scale Convolution Block (HMCB) combining grouped depthwise convolutions with hybrid attention to capture cross-scale dependencies; (ii) Adaptive Dynamic Sampling Module (ADSM) that dynamically adjusts receptive fields through learnable position offsets and scope prediction, enabling context-aware upsampling with minimal computational overhead; (iii) Dual Attention Fusion Unit (DAFU) integrating channel-spatial attention for global context modeling and depthwise separable gating for local feature refinement. Extensive evaluations across 7 medical image segmentation tasks (breast cancer, thyroid nodules, skin lesions) show DyMAS-Net achieves state-of-the-art performance with an average Dice score of 87.19%, outperforming TransUnet and SwinUnet by 3.02% and 2.77%, respectively. Remarkably, it attains this with only 6.24M parameters and 8.87G FLOPs, 93. 3% fewer parameters than TransUnet. The framework's efficiency-accuracy balance enables practical deployment in resource-constrained environments, thus promoting health equity.", "filename": "2025_0135.pdf", "year": 2025, "institution": "Nankai Univ ersity", "country": "China", "authors": ["Siqi Wang", "Qingxue Zhao", "Di Wu", "Jiakang Gao", "Jun Tian"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0136", "x": 1.538, "y": 6.196, "title": "Endo-CLIP: Progressive Self-supervised Pre-training on Raw Colonoscopy Records", "abstract": "Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP's three-stage framework-cleansing, attunement, and unificationaddresses these challenges by: (1) removing background frames, (2) leveraging large language models (LLMs) to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level crossattention to resolve mu lti-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis. Code will be made publicly available on https://github. com/chrlott/EndoCLIP.", "filename": "2025_0136.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Yili He", "Yan Zhu", "Peiyao Fu", "Ruijie Yang", "Tianyi Chen", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0137", "x": 0.019, "y": 2.539, "title": "Endo-FASt3r: Endoscopic Foundation Model Adaptation for Structure from Motion", "abstract": "Accurate depth and camera pose estimation is essential for achieving high-quality 3D visualisations in robotic-assisted surgery. Despite recent advancements in foundation model adaptation to monocular depth estimation of endoscopic scenes via self-supervised learning (SSL), no prior work has explored their use for pose estimation. These methods rely on low rank-based adaptation approaches, which constrain model updates to a low-rank space. We propose Endo-FASt3r, the first monocular SSL depth and pose estimation framework that uses foundation models for both tasks. We extend the Relo c3r relative pose estimation foundation model by designing Reloc3rX, introducing modifications necessary for convergence in SSL. We also present DoMoRA, a novel adaptation technique that enables higher-rank updates and faster convergence. Experiments on the SCARED dataset show that Endo-FASt3r achieves a substantial .10% improvement in pose estimation and a . 2% improvement in depth estimation over prior work. Similar performance gains on the Hamlyn and StereoMIS datasets reinforce the generalisability of Endo-FASt3r across differen t datasets. Our code is available at: https://github.com/Mona-ShZeinoddin/Endo_FASt3r.git.", "filename": "2025_0137.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Mona Sheikh Zeinoddin", "Mobarak I Hoque", "Zafer Tandogdu", "Greg L Shaw", "Matthew J Clarkson", "Evangelos B Mazomenos", "Danail Stoyanov"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0138", "x": -0.09, "y": 2.115, "title": "EndoPlanar: Deformable Planar-Based Gaussian Splatting for Surgical Scene Reconstruction", "abstract": "Accurate reconstruction of deformable soft tissues from endoscopic stereo videos is essential to improve surgical navigation and automation in robot-assisted image-guided procedures. While recent Gaussian splatting techniques achieve real-time rendering with impressive results on endoscopic datasets, conventional 3D Gaussian splatting methods suffer from volumetric biases, leading to inaccuracies in 3D geometry and depth estimation. To overcome these limitations, we propose EndoPlanar, a novel deformable planar-based Gaussian splatting approach. By flattening volumetric Gaussians to a 2D plane, our method enables unbiased depth computation and normal map estimation, which are difficult to achieve with traditional ellipsoidal Gaussians. Furthermore, we introduce a regularization strategy for smooth planar-derived normal maps to refine surface quality. Additionally, we enhance model initialization using Gaussian mixture-based background segmentation, improving the representation of unseen objects and accelerating convergence. We evaluate EndoPlanar on two standard benchmarks, EndoN-eRF and StereoMIS, demonstrating promising performance by outperforming all baselines in reconstruction quality with PSNR of 34.51 dB while maintaining real-time inference speeds of 307.5 FPS.", "filename": "2025_0138.pdf", "year": 2025, "institution": "Chulalongkorn Univ ersity", "country": "Thailand", "authors": ["Thatphum Paonim", "Chayapon Sasnarukkit", "Natawut Nupairoj", "Peerapon Vateekul"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0139", "x": 0.303, "y": 2.808, "title": "Endoscopic Depth-of-Field Expansion via Cascaded Network with Two-Streamed Multi-scale Fusion", "abstract": "The application of ultrahigh definition endoscopy systems in minimally invasive surgeries has become increasingly widespread. However, their high resolution results in a reduced depth of field (DOF), making it difficult to achieve clear imaging across the entire frame. Unlike improvements in optical structures, we address this issue using a deep learning-based multi-focus image fusion (MFIF) approach. Traditional MFIF methods are less effective in endoscopic scenarios due to their inadequate design for extracting information from complex organ structures. To address these limitations, this work proposes a two-streamed cascaded encoder-decoder network that incorporates multi-scale feature extraction and fusion mechanisms validated in medical image segmentation. The network includes novel multi-scale fusion module with cross-axial attention that hierarchically integrates features using attention-guided weights and hybrid operations, effectively preserving intra-domain textures while modeling cross-domain dependencies. The framework is rigorously validated using novel real-world endoscopic datasets collected from imaging experimental platform. The experimental results demonstrate that the proposed method outperforms traditional approaches in benchmark tests. Code available at: https://github.com/luoyu5023/CTMFusion.", "filename": "2025_0139.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Xiang Deng", "Xing Liu", "Tian Xu", "Xiaoyue Liu", "Tianyuan Gan", "Chen Lu", "Congcong Zhou", "Peng Wang", "Yong Lei", "Xuesong Ye"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0140", "x": 1.201, "y": 2.472, "title": "Endplate3D-QCT: A High-Resolution Dataset and Benchmark for Automated 3D Segmentation of Lumbar Vertebral Endplates in QCT", "abstract": "Accurate segmentation of lumbar vertebral endplates is essential for assessing bone density and biomechanical properties in spinal disorders. While quantitative computed tomography (QCT) provides detailed bone density measurements, existing segmentation approaches primarily focus on vertebral bodies and intervertebral discs, often neglecting the precise delineation of endplates. Current deep learning methods perform well in healthy spines but struggle with pathological cases due to the thin and morphologically complex nature of endplates, particularly in the presence of osteophytes and degenerative changes. To address these challenges, we introduce the first publicly available dataset, Endplate3D-QCT, which contains pixel-level annotations of lumbar endplates in clinical QCT scans. Our dataset includes high-precision 3D segmentation masks targeting cortical endplates and subchondral bone, along with an automated evaluation framework for model assessment. We benchmark multiple deep learning models, including EfficientUNet, UNet, VNet, UNETR and SwinUNETR, using nnUNet as the training framework. While these models achieve Dice scores around 0.9, they exhibit inconsistencies in endplate identification, leading to false positives and false negatives. These findings highlight the need for further advancements in endplate segmentation techniques. Our dataset and benchmarks provide a valuable foundation for improving spinal implant design, bone density mapping, and computational modeling of vertebral load distribution. The dataset and the evaluation code are available at https:// github.com/yin876705249/Endplate3D-QCT.", "filename": "2025_0140.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Zixun Yin", "Da Zou", "Yi Zhao", "Chenbin Zhang", "Weishi Li", "Minghui Wu", "Kun Yan", "Ping Wang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0141", "x": 0.156, "y": 4.013, "title": "ESPNet: Edge-Aware Feature Shrinkage Pyramid for Polyp Segmentation", "abstract": "Despite numerous techniques developed for polyp segmentation, the issue of generalizability to new centers and populations persists. To address these issues, we compile a multicenter train set consisting of 4,000 polyp frames and propose a novel approach toward generalizing to different data centers, difficult polyp morphologies (e.g., flat or small), and inflammatory conditions such as inflammatory bowel disease (IBD). In this regard, we propose a transformer-based polyp segmentation model to leverage global contextual information, and enhancement of local feature interactions through a novel feature decoding and fusion method, and polyp edge features. This combines the vision transformers' strong contextual understanding with enhanced locality modeling through graph-based relational understanding and multiscale feature aggregation. We compare our model with eight recent state-of-the-art methods under five widely used metrics on the following benchmark datasets: Kvasir-Sessile, SUN-SEG-Easy (Seen), ETIS-LaribPolypDB, CVC-ColonDB, PolypGen-C6, and our in-house IBD dataset. Extensive experiments show that our model outperforms stateof-the-art methods on out-of-distribution datasets with mIoU improvements of 2.84% on ETIS-LaribPolypDB, 1.26% on CVC-ColonDB, 1.90% on PolypGen-C6, and 3.52% on the in-house IBD polyp dataset compared to the most accurate recent method. The code is available at https:// github.com/Raneem-MT/ESPNet.", "filename": "2025_0141.pdf", "year": 2025, "institution": "Univ ersity of Leeds", "country": "UK", "authors": ["Raneem Toman", "Venkataraman Subramanian", "Sharib Ali"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0142", "x": 1.657, "y": 2.511, "title": "Estimating Bone Mineral Density and Muscle Mass from EOS Low Dose X-Ray Imaging System", "abstract": "The EOS imaging system is a low-dose, biplanar X-ray modality offering high-fidelity anatomical visualization in standing and seated positions, benefiting total hip arthroplasty (THA) by providing accurate skeletal alignment and implant positioning pre-and postoperatively. Evaluating bone mineral density (BMD) and muscle mass before surgery is useful for predicting outcomes and tailoring rehabilitation. Although CT and DXA can assess these metrics effectively, they increase cost and radiation exposure. Recent advances in deep learning have enabled BMD and muscle mass estimation from plain radiographs, among which one promising approach with potentially high generalizability to new modality utilized 2D-3D registration with CT of the same patient in training data preparation. However, limited EOS availability constrains large data collection. We devised and validated a deep learning framework to predict BMD and muscle mass from EOS images by fine-tuning a model trained on plain radiographs. Our dataset comprised 77 pairs of pre-and postoperative EOS images and CT scans, then underwent 2D-3D registration to create paired training data. Our contribution is two-fold: 1) we achieved reliable BMD and muscle mass estimation in THA cases with minimal training data, and 2) we experimentally demonstrated that only 40 paired EOS-CT images were sufficient to reach high accuracy, supporting feasibility in resource-limited settings. Future work will extend this approach to broader patient populations and anatomical sites while performing external validation to assess potential domain shifts across different facilities.", "filename": "2025_0142.pdf", "year": 2025, "institution": "Nara Institute of Science and Technology", "country": "Japan", "authors": ["Kazuki Suehara", "Yi Gu", "Yoshito Otake", "Keisuke Uemura", "Masashi Okamoto", "Kunihiko Tokunaga", "Hugues Talbot", "Yoshinobu Sato"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0143", "x": 0.036, "y": 3.123, "title": "Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery", "abstract": "Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can lead to severe postoperative complications if undetected. However, their rarity results in highly imbalanced datasets, posing challenges for AI-based detection and severity quantification. We propose BetaMixer, a novel deep learning model that addresses these challenges through a Beta distribution-based mixing approach, converting discrete IAE severity scores into continuous values for precise severity regression (0-5 scale). BetaMixer employs Beta distribution-based sampling to enhance underrepresented classes and regularizes intermediate embeddings to maintain a structured feature space. A generative approach aligns the feature space with sampled IAE severity, enabling robust classification and severity regression via a transformer. Evaluated on the MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84, demonstrating strong performance on imbalanced data. By integrating Beta distribution-based sampling, feature mixing, and generative modeling, BetaMixer offers a robust solution for IAE detection and quantification in clinical settings.", "filename": "2025_0143.pdf", "year": 2025, "institution": "University of Strasb ourg", "country": "France", "authors": ["Rupak Bose", "Chinedu Innocent Nwoye", "Jorge F Lazo", "Joël L Lavanchy", "Nicolas Padoy"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0144", "x": 4.463, "y": 2.652, "title": "Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose", "abstract": "Analyzing fetal body motion and shape is paramount in prenatal diagnostics and monitoring. Existing methods for fetal MRI analysis mainly rely on anatomical keypoints or volumetric body segmentations. Keypoints simplify body structure to facilitate motion analysis, but may ignore important details of full-body shape. Body segmentations capture complete shape information but complicate temporal analysis due to large non-local fetal movements. To address these limitations, we construct a 3D articulated statistical fetal body model based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm iteratively estimates body pose in the image space and body shape in the canonical pose space. This approach improves robustness to MRI motion artifacts and intensity distortions, and reduces the impact of incomplete surface observations due to challenging fetal poses. We train our model on segmentations and keypoints derived from 19, 816 MRI volumes across 53 subjects. Our model captures body shap e and motion across time series and provides intuitive visualization. Furthermore, it enables automated anthropometric measurements traditionally difficult to obtain from segmentations and keypoints. When tested on unseen fetal body shapes, our method yields a surface alignment error of 3.2 mm for 3 mm MRI voxel size. To our knowledge, this represents the first 3D articulated statistical fetal body model, paving the way for enhanced fetal motion and shape analysis in prenatal diagnostics. The code is available at https://github. com/MedicalVisionGroup/fetal-smpl.", "filename": "2025_0144.pdf", "year": 2025, "institution": "Computer Science", "country": "USA", "authors": ["Yingcheng Liu", "Peiqi Wang", "Sebastian Diaz", "Esra Abaci Turk", "Benjamin Billot", "P Ellen Grant", "Polina Golland"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0145", "x": 0.193, "y": 3.863, "title": "FPN-in-FPN: A Nested Multi-scale Aggregation Network for Polyp Segmentation", "abstract": "Colorectal cancer is a leading cause of cancer-related deaths worldwide, and precise polyp segmentation plays a crucial role in its early detection. U-shaped architectures are widely used for polyp segmentation due to their ability to capture multi-scale contextual information effectively. However, it is suboptimal to solely use top-down or bottomup fusion flow in traditional U-shaped architectures. Additionally, most existing methods only focus on improving the feature fusion module, often introducing more computational costs. In this work, we propose a novel and efficient nested multi-scale feature aggregation network that integrates high-level semantic information with low-level boundary details within skip connections, effectively handling the diverse shapes and sizes of polyp regions. Specifically, we introduce a bidirectional FPNin-FPN m odule that fuses features across stages through both bottom-up and top-down pathways. This module adds only 0.12M extra parameters with minimal computational overhead while significantly enhancing segmentation performance in small networks. Extensive experiments on polyp segmentation datasets demonstrate that our network outperforms existing methods in both accuracy and efficiency. Code is available at https://github.com/Yejin0111/FPN-in-FPN.", "filename": "2025_0145.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Jin Ye", "Yanzhou Su", "Yicheng Wu", "Junjun He", "Bohan Zhuang", "Zhaolin Chen", "Jianfei Cai"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0146", "x": 0.414, "y": 2.218, "title": "From Sight to Skill: A Surgeon-Centered Augmented Reality System for Ureteroscopy Training", "abstract": "The number of kidney stone cases in the U.S. has tripled since 1980. Unfortunately, 23% of kidney stone removal surgeries require repeat procedures within 20 months. The high repeat surgery rate is often due to surgeons missing stones in the initial treatment. Effective training can reduce the need for re-operation, yet learning opportunities in the operating room (OR) are limited, as patient care must take priority. Augmented reality (AR) can improve the effectiveness of training in the OR by providing real-time visual feedback, but the interface must be carefully designed not to interfere with clinical workflow. Building on prior design guidelines and AR training tools, we design and evaluate the effectiveness of enhancing training through three AR gaze markers. Our AR training system tracks the expert's eye gaze and projects the marker onto the trainee's head-mounted display to provide visual guidance. Eight trainees performed a simulated ureteroscopy task of identifying kidney stones in h igh-fidelity kidney phantoms while guided by an expert. We record the number of stones they found, time, and eye-gaze metrics. At the end of each trial, trainees provide subjective feedback on task load and performance through the NASA-TLX questionnaire. Results show that while some gaze markers increased perceived mental demand, they enhanced engagement and performance. Gaze metrics revealed that marker shape affects cognitive load, as measured by the fixation-to-saccades ratio. By translating prior design principles into an AR-based guidance system, this work supports intraoperative training and highlights AR's potential in surgical education.", "filename": "2025_0146.pdf", "year": 2025, "institution": "Vanderbilt University", "country": "USA", "authors": ["Jumanh Atoum", "Fangjie Li", "Ayberk Acar", "Nicholas L Kavoussi", "Jie Ying Wu"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0147", "x": -0.535, "y": 3.021, "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video", "abstract": "Object-centric slot attention is an emerging paradigm for unsupervised learning of structured, interpretable object-centric representations (slots). This enables effective reasoning about objects and events at a low computational cost and is thus applicable to critical healthcare applications, such as real-time interpretation of surgical video. The heterogeneous scenes in real-world applications like surgery are, however, difficult to parse into a meaningful set of slots. Current approaches with an adaptive slot count perform well on images, but their performance on surgical videos is low. To address this challenge, we propose a dynamic temporal slot transformer (DTST) module that is trained both for temporal reasoning and for predicting the optimal future slot initialization. The model achieves state-of-the-art performance on multiple surgical databases, demonstrating that unsupervised object-centric methods can be applied to real-world data and become part of the common arsenal in healthcare application (Code and models are publicly available at: https://github.com/PCASOlab/Xslot.).", "filename": "2025_0147.pdf", "year": 2025, "institution": "University of Pennsylvania", "country": "USA", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Marcel Hussing", "Edward Zhang", "Eric Eaton", "Daniel A Hashimoto"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0148", "x": 4.553, "y": 5.839, "title": "FViM: Frequency Vision Mamba for Label-Free Cell Death Pathway Prediction in Lung Cancer Chemotherapy", "abstract": "Chemotherapy is the standard first-line treatment for lung cancer, and cellular death is an inevitable consequence of the process. However, current methods lack high-throughput, label-free approaches for accurately assessing cell death, and existing techniques struggle to capture cellular heterogeneity, complicating the prediction of lung cancer prognosis. Therefore, we propose frequency vision Mamba (FViM) for label-free cell death pathway prediction in lung cancer chemotherapy. Specifically, we introduce multi-dimensional optical time-stretch imaging flow cytometry (OTS-IFC) to capture high-throughput, multidimensional cell images under various cell death states. To effectively extract key features that are highly indicative of cellular heterogeneity, we propose FViM that integrates modeling remote dependencies of Mamba alongside frequency domain analysis of Fourier Transform. FViM first employs the frequency guided enhancement (FGE) module to enhance cellular detail features in the high-frequency domain, while reinforcing global contextual features in the low-frequency domain. The enhanced features are then processed through the Mamba-based visual state space block, which models the intricate relationships between different visual states, achieving a holistic prediction of cell death states. Experimental results demonstrate that FViM outperforms existing state-of-the-art (SOTA) methods. Notably, FViM successfully predicts cell death pathways in response to increasing cisplatin concentrations, demonstrating its potential for effective and promising applications in lung cancer chemotherapy. Our code is available at https://github. com/yzygit1230/FViM.", "filename": "2025_0148.pdf", "year": 2025, "institution": "Wuhan Univ ersity", "country": "China", "authors": ["Zhaoyi Ye", "Shubin Wei", "Liye Mei", "Yueyun Weng", "Qing Geng", "Du Wang", "Cheng Lei"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0149", "x": 4.36, "y": 6.675, "title": "GE2Hist: Generating Histology Images from Single-Cell Gene Expression via Cross-Modal Generative Network", "abstract": "Histological images are essential in biomedical research and diagnosis, extending beyond detailed cell and tissue morphology to provide an intuitive view of the cellular microenvironment and spatial relationships. While single-cell gene expression data reveal molecular distinctions in cell states, their complexity obscures cellular interactions and spatial organization. To overcome this, reconstructing histological images from large-scale single-cell data is essential for intuitively visualizing spatial architecture. This paper proposes a single-cell-level histological image generation method that derives cell state representations from gene expression data using a single-cell foundation model. A conditional diffusion model is leveraged to generate histological images, accurately reconstructing the cellular microenvironment and spatial cell type distribution. By decoupling cellular state into two components, cell type and microenvironment, we propose two complementary approaches for generating pathology images, one conditioned on scRNA-seq data and the other on cell type. Our approach successfully generates high-quality histological images of human breast and colon cancer tissues, capturing key spatial features such as cell density, compositional distribution, and cell spacing within tissues.", "filename": "2025_0149.pdf", "year": 2025, "institution": "South China University of Technology", "country": "China", "authors": ["Hongmin Cai", "Boan Ji", "Shangyan Cai", "Yi Liao", "Jiazhou Chen", "Weitian Huang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0150", "x": 0.21, "y": 3.945, "title": "Holistic White-Light Polyp Classification via Alignment-Free Dense Distillation of Auxiliary Optical Chromoendoscopy", "abstract": "White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main colonoscopic modalities for polyp classification. While NBI, as optical chromoendoscopy, offers valuable vascular details, WLI remains the most common and often the only available modality in resource-limited settings. However, WLI-based methods typically underperform, limiting their clinical applicability. Existing approaches transfer knowledge from NBI to WLI through global feature alignment but often rely on cropped lesion regions, which are susceptible to detection errors and neglect contextual and subtle diagnostic cues. To address this, this paper proposes a novel holistic classification framework that leverages full-image diagnosis without requiring polyp localization. The key innovation lies in the Alignment-free Dense Distillation (ADD) module, which enables fine-grained cross-domain knowledge distillation regardless of misalignment between WLI and NBI images. Without resorting to explicit image alignment, ADD learns pixel-wise cross-domain affinities to establish correspondences between feature maps, guiding the distillation along the most relevant pixel connections. To further enhance distillation reliability, ADD incorporates Class Activation Mapping (CAM) to filter cross-domain affinities, ensuring the distillation path connects only those semantically consistent regions with equal contributions to polyp diagnosis. Extensive results on public and in-house datasets show that our method achieves state-of-the-art performance, relatively outperforming the other approaches by at least 2.5% and 16.2% in AUC, respectively. Code is available at: https://github.com/Huster-Hq/ADD.", "filename": "2025_0150.pdf", "year": 2025, "institution": "Huazhong University of S cience and Technology", "country": "China", "authors": ["Qiang Hu", "Qimei Wang", "Jia Chen", "Xuantao Ji", "Mei Liu", "Qiang Li", "Zhiwei Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0151", "x": 0.708, "y": 2.328, "title": "HoloPointNet: A Deep Learning Framework for Efficient 3D Point Cloud Holography", "abstract": "HoloPointNet presents a novel deep-learning framework for 3D point cloud holography. Generally, computer-generated holography (CGH) methods typically rely on stacked 2D slices and suffer from inefficiencies. These 2D slices often contain empty regions in natural 3D scenes or are intentionally sparse in applications like holographic optogenetics. This results in excessive memory consumption and increased processing latency. In contrast, HoloPointNet directly processes 3D point cloud data using a concatenation-based feature extractor, followed by hierarchical upsampling and wavefront reconstruction modules, eliminating redundant spatial regions and improving efficiency. This design allows for the direct mapping of point cloud data to phase modulations for spatial light modulators (SLMs). By employing a structured convolutional feature transformation pipeline, HoloPointNet enables hierarchical refinement of spatial embeddings, enhancing feature encoding accuracy. HoloPoint-Net offers the capability to generate multiplane holograms, effectively addressing the complexities of 3D volumetric data. This capability, combined with fast inference times, enables real-time holography for applications such as optogenetics. The code is available at https://github.com/ AnkitAmrutkar/HoloPointNet.git.", "filename": "2025_0151.pdf", "year": 2025, "institution": "RWTH Aachen U niversity", "country": "Germany", "authors": ["Ankit Amrutkar", "Ahmet Nazlioglu", "Björn Kampa", "Volkmar Schulz", "Johannes Stegmaier", "Markus Rothermel", "Dorit Merhof"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0152", "x": 1.026, "y": 3.847, "title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation", "abstract": "Multimodal medical image segmentation faces significant challenges in the context of gastric cancer lesion analysis. This clinical context is defined by the scarcity of independent multimodal datasets and the imperative to amalgamate inherently misaligned modalities. As a result, algorithms are constrained to train on approximate data and depend on application migration, leading to substantial resource expenditure and a potential decline in analysis accuracy. To address those challenges, we have made two major contributions: First, we publicly disseminate the GCM 2025 dataset, which serves as the first large-scale, open-source collection of gastric cancer multimodal MRI scans, featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500 patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework that employs an original HWA block with learnable window aggregation layers to establish dynamic feature correspondences between different modalities' anatomical structures, and leverages the innovative tri-orientated fusion mamba mechanism for context modeling and capturing long-range spatial dependencies. Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021 dataset validate the performance of our framework, demonstrating that the new approach surpasses existing methods by up to 1.68% in the Dice score while maintaining solid robustness. The dataset and code are public via this URL.", "filename": "2025_0152.pdf", "year": 2025, "institution": "South China University of Technology", "country": "China", "authors": ["Jiaming Liang", "Lihuan Dai", "Xiaoqi Sheng", "Xiangguang Chen", "Chun Yao", "Guihua Tao", "Qibin Leng", "Hongmin Cai", "Xi Zhong"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0153", "x": 1.593, "y": 4.173, "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "abstract": "Existing methods for multimodal MRI segmentation with missing modalities typically assume that all MRI modalities are available during training. However, in clinical practice, some modalities may be missing due to the sequential nature of MRI acquisition, leading to performance degradation. Furthermore, retraining models to accommodate newly available modalities can be inefficient and may cause overfitting, potentially compromising previously learned knowledge. To address these challenges, we propose Replay-based Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to enable the segmentation model to learn from newly acquired MRI modalities without forgetting previously learned information. To enhance segmentation performance across diverse patient scenarios, we introduce the Cross-Patient Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture high-order associations between patients. Additionally, we incorporate Tversky-Aware Contrastive (TAC) loss to effectively mitigate information imbalance both across and within different modalities. Extensive experiments on the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art methods, achieving an improvement of over 2% in the Dice Similarity Coefficient across various tumor regions. Our code is available at ReHyDIL.", "filename": "2025_0153.pdf", "year": 2025, "institution": "Northeast Forestry University", "country": "China", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0154", "x": 2.763, "y": 7.626, "title": "Hypergraph-Guided Federated Distillation Learning for Efficient and Robust Multi-center fMRI Data Analysis", "abstract": "Multi-center fMRI data analysis faces significant challenges such as data privacy concerns and data integration issues. Federated learning, as an innovative distributed machine learning approach, enables cross-center collaboration by sharing model parameters instead of raw data. However, existing methods often struggle with improving the robustness and inference efficiency of multi-center fMRI data processing. To address these challenges, we propose a novel hypergraph-guided federated distillation framework(HGFD) for multi-center fMRI data analysis. HGFD utilizes a hypergraph structure to model the spatiotemporal features of brain activity, capturing high-order correlations across brain regions. Furthermore, a hypergraph-based knowledge distillation approach is utilized to transfer high-order structural representations into shallow neural networks, thereby preserving their ability for complex relational inference and significantly enhancing computational efficiency. In the federated learning process, participating centers only need to share the parameters of their shallow neural networks to a central server. Through parameter aggregation, each center's shallow network can learn the high-order structural information of other centers. Experiments on multi-center fMRI dataset demonstrate that the proposed method not only improves the robustness and consistency of fMRI-based prediction tasks but also achieves efficient and accurate predictions while ensuring data privacy.", "filename": "2025_0154.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Tao Jin", "Yidan Xu", "Yuhan Gao", "Xichun Sheng", "Chenggang Yan", "Yaoqi Sun", "Xiangmin Han", "Yue Gao"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0155", "x": 0.993, "y": 4.137, "title": "Implicit U-KAN2.0: Dynamic, Efficient and Interpretable Medical Image Segmentation", "abstract": "Image segmentation is a fundamental task in both image analysis and medical applications. State-of-the-art methods predominantly rely on encoder-decoder architectures with a U-shaped design, commonly referred to as U-Net. Recent advancements integrating transformers and MLPs improve performance but still face key limitations, such as poor interpretability, difficulty handling intrinsic noise, and constrained expressiveness due to discrete layer structures, often lacking a solid theoretical foundation. In this work, we introduce Implicit U-KAN 2.0, a novel U-Net variant that adopts a two-phase encoder-decoder structure. In the SONO phase, we use a second-order neural ordinary differential equation (NODEs), called the SONO block, for a more efficient, expressive, and theoretically grounded modeling approach. In the SONO-MultiKAN phase, we integrate the second-order NODEs and MultiKAN layer as the core computational block to enhance interpretability and representation power. Our contributions are threefold. First, U-KAN 2.0 is an implicit deep neural network incorporating MultiKAN and second order NODEs, improving interpretability and performance while reducing computational costs. Second, we provide a theoretical analysis demonstrating that the approximation ability of the MultiKAN block is independent of the input dimension. Third, we conduct extensive experiments on a variety of 2D and a single 3D dataset, demonstrating that our model consistently outperforms existing segmentation networks. Project Website: https://math-ml-x.github.io/IUKAN2/.", "filename": "2025_0155.pdf", "year": 2025, "institution": "University of Cambridge", "country": "UK", "authors": ["Chun-Wun Cheng", "Yining Zhao", "Yanqi Cheng", "Javier A Montoya-Zegarra", "Carola-Bibiane Schönlieb", "Angelica I Aviles-Rivero"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0156", "x": 2.502, "y": 5.05, "title": "Improved Tumor Segmentation Using Selective Synthetic Augmentation for Enhanced Surgical Planning in Breast MRI", "abstract": "Breast-conserving surgery (BCS) is the preferred treatment for early-stage breast cancer, offering survival rates comparable to mastectomy while preserving breast aesthetics. Accurate tumor segmentation is essential for surgical planning, yet segmentation models often exhibit biases toward specific tumor sizes, particularly underperforming on smaller tumors. To address this, we propose a novel approach that uses generative models to improve segmentation across tumor sizes. Specifically, we adapt the Stable Diffusion model and apply a Denoising Diffusion Probabilistic Model (DDPM) inversion approach to generate synthetic tumors of controlled sizes within real breast MRIs, h elping to balance tumor size distribution in the training data. By augmenting the dataset with 10-20% synthetic tumor images, our method significantly improves segmentation accuracy for small tumors without compromising performance for larger tumors. This enhancement allows for more precise tumor assessment, leading to better-informed surgical decisions and potentially reducing unnecessary mastectomies.", "filename": "2025_0156.pdf", "year": 2025, "institution": "BeamWorks Inc", "country": "Republic of Korea", "authors": ["Miguel Luna", "John Baek", "Won Hwa Kim", "Wan Gyu Son", "Kwang Min Lee", "Hye Jung Kim", "Jaeil Kim"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0157", "x": 1.537, "y": 7.052, "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "abstract": "In high-stakes medical applications, consistent answering across diverse question phrasings is essential for reliable diagnosis. However, we reveal that current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility in Medical Visual Question Answering, as their answers fluctuate significantly when faced with semantically equivalent rephrasings of medical questions. We attribute this to two limitations: (1) insufficient alignment of medical concepts, leading to divergent reasoning patterns, and (2) hidden biases in training data that prioritize syntactic shortcuts over semantic understanding. To address these challenges, we construct RoMed, a dataset built upon original VQA datasets containing 144k questions with variations spanning word-level, sentence-level, and semantic-level perturbations. When evaluating stateof-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming performance drops (e.g., a 40% decline in Recall) compared to original VQA benchmarks, exposing critical robustness gaps. To bridge this gap, we propose Consistency and Contrastive Learning (CCL), which integrates two key components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with medical knowledge rather than shallow feature patterns, and (2) bias-aware contrastive learning, mitigating dataspecific priors through discriminative representation refinement. CCL achieves SOTA performance on three popular VQA benchmarks and notably improves answer consistency by 50% on the challenging RoMed test set, demonstrating significantly enhanced robustness. Code will be released.", "filename": "2025_0157.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0158", "x": 6.693, "y": 3.869, "title": "Learning 3D Medical Image Models from Brain Functional Connectivity Network Supervision for Mental Disorder Diagnosis", "abstract": "In MRI-based mental disorder diagnosis, most previous studies focus on functional connectivity network (FCN) derived from functional MRI (fMRI). However, the small size of annotated fMRI datasets restricts its wide application. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w) MRI, which are commonly used and readily accessible in clinical settings, are often overlooked. To integrate the complementary information from both function and structure for improved diagnostic accuracy, we propose CINP (Contrastive Image-Network Pretraining), a framework that employs contrastive learning between sMRI and FCN. During pre-training, we incorporate masked image modeling and network-image matching to enhance visual representation learning and modality alignment. Since the CINP facilitates knowledge transfer from FCN to sMRI, we introduce network prompting. It utilizes only sMRI from suspected patients and a small amount of FCNs from different patient classes for diagnosing mental disorders, which is practical in real-world clinical scenario. The competitive performance on three mental disorder diagnosis tasks demonstrate the effectiveness of the CINP in integrating multimodal MRI information, as well as the potential of incorporating sMRI into clinical diagnosis using network prompting.", "filename": "2025_0158.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Xingcan Hu", "Wei Wang", "Li Xiao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0159", "x": 6.368, "y": 4.61, "title": "Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder", "abstract": "While imaging-genetics holds great promise for unraveling the complex interplay between brain structure and genetic variation in neurological disorders, traditional methods are limited to simplistic linear models or to black-box techniques that lack interpretability. In this paper, we present NeuroPathX, an explainable deep learning framework that uses an early fusion strategy powered by cross-attention mechanisms to capture meaningful interactions between structural variations in the brain derived from MRI and established biological pathways derived from genetics data. To enhance interpretability and robustness, we introduce two loss functions over the attention matrix -a sparsity loss that focuses on the most salient i nteractions and a pathway similarity loss that enforces consistent representations across the cohort. We validate NeuroPathX on both autism spectrum disorder and Alzheimer's disease. Our results demonstrate that NeuroPathX outperforms competing baseline approaches and reveals biologically plausible associations linked to the disorder. These findings underscore the potential of NeuroPathX to advance our understanding of complex brain disorders.", "filename": "2025_0159.pdf", "year": 2025, "institution": "Boston University", "country": "USA", "authors": ["Jueqi Wang", "Zachary Jacokes", "John Darrell Van Horn", "Michael C Schatz", "Kevin A Pelphrey", "Archana Venkataraman"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0160", "x": 1.328, "y": 5.117, "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment", "abstract": "Learning from sparse labels is a challenge commonplace in the medical domain. This is due to numerous factors, such as annotation cost, and is especially true for newly introduced tasks. When dense pixel-level annotations are needed, this becomes even more unfeasible. However, being able to learn from just a few annotations at the pixellevel, while extremely difficult and underutilized, can drive progress in studies where perfect annotations are not immediately available. This work tackles the challenge of learning the dense prediction task of keypoint localization from a few point annotations in the context of 2d carcinosis keypoint localization from laparoscopic video frames for diagnostic planning of advanced ovarian cancer patients. To enable this, we formulate the problem as a sparse heatmap regression from a few point annotations per image and p ropose a new loss function, called Crag and Tail loss, for efficient learning. Our proposed loss function effectively leverages positive sparse labels while minimizing the impact of false negatives or missed annotations. Through an extensive ablation study, we demonstrate the effectiveness of our approach in achieving accurate dense localization of carcinosis keypoints, highlighting its potential to advance research in scenarios where dense annotations are challenging to obtain.", "filename": "2025_0160.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Farahdiba Zarin", "Riccardo Oliva", "Vinkle Srivastav", "Armine Vardazaryan", "Andrea Rosati", "Alice Zampolini Faustini", "Giovanni Scambia", "Anna Fagotti", "Pietro Mascagni", "Nicolas Padoy"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0161", "x": 3.019, "y": 1.784, "title": "LIP-CAR: A Learned Inverse Problem Approach for Medical Imaging with Contrast Agent Reduction", "abstract": "The adoption of contrast agents in medical imaging is essential for accurate diagnosis. While highly effective and characterized by an excellent safety profile, the use of contrast agents has its limitation, including rare risk of allergic reactions, potential environmental impact and economic burdens on patients and healthcare systems. This work addresses the contrast agent reduction (CAR) problem, aiming to minimize the administered dosage while preserving image quality. Unlike existing deep learning methods that simulate high-dose images from lowdose inputs via end-to-end models, we p ropose a learned inverse problem (LIP) approach. By learning an operator that maps high-dose to lowdose images, we reformulate CAR as an inverse problem, solved through regularized optimization to enhance data consistency. Numerical experiments on pre-clinical images demonstrate improved accuracy compared to traditional methods.", "filename": "2025_0161.pdf", "year": 2025, "institution": "University o f Bologna", "country": "Italy", "authors": ["Davide Evangelista", "Elena Morotti", "Sonia Colombo Serra", "Pengpeng Luo", "Giovanni Valbusa", "Davide Bianchi"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0162", "x": 0.244, "y": 3.826, "title": "Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation", "abstract": "Polyp segmentation in colonoscopy images is crucial for early detection and diagnosis of colorectal cancer. However, this task remains a significant challenge due to the substantial variations in polyp shape, size, and color, as well as the high similarity between polyps and surrounding tissues, often compounded by indistinct boundaries. While existing encoder-decoder CNN and transformer-based approaches have shown promising results, they struggle with stable segmentation performance on polyps with weak or blurry boundaries. These methods exhibit limited abilities to distinguish between polyps and non-polyps and capture essential boundary cues. Moreover, their generalizability still falls short of meeting the demands of real-time clinical applications. To address these limitations, we propose SAM-MaGuP, a groundbreaking approach for robust polyp segmentation. By incorporating a boundary distillation module and a 1D-2D Mamba a dapter within the Segment Anything Model (SAM), SAM-MaGuP excels at resolving weak boundary challenges and amplifies feature learning through enriched global contextual interactions. Extensive evaluations across five diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods, achieving unmatched segmentation accuracy and robustness. Our key innovations-a Mamba-guided boundary prior and a 1D-2D Mamba block-set a new benchmark in the field, pushing the boundaries of polyp segmentation to new heights.", "filename": "2025_0162.pdf", "year": 2025, "institution": "University of Surrey", "country": "UK", "authors": ["Tapas K Dutta", "Snehashis Majhi", "Deepak Ranjan Nayak", "Debesh Jha"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0163", "x": 1.328, "y": 6.853, "title": "Medical Contrastive Learning of Positive and Negative Mentions", "abstract": "Contrastive learning techniques have achieved significant success and have been widely applied in both general and medical domains. However, there is a data difference between the general domain and the medical domain about negative mentions, which almost never appear in general domain but almost always in medical domain. We find that most existing medical contrastive learning methods do not effectively utilize or even overlook the numerous negative mentions present in the data during training, resulting in deficient multimodal feature alignment capabilities. To address this issue, we propose the Visual Entailment Based Contrastive Learning (VECL) method. By introducing a ternary v isual entailment contrast relationship of entailment, neutral, and contradiction, our method effectively utilizes both positive and negative mentions for modeling fine-grained sample relationships, enhancing the model's multimodal feature alignment capabilities. The experiment results show that we achieves SOTA performance on classification, grounding and report generation tasks. Resources are maintained at https://github.com/WVeLong/VECL.", "filename": "2025_0163.pdf", "year": 2025, "institution": "Tsinghua Univ ersity", "country": "China", "authors": ["Weilong Wu", "Jingzhi Yang", "Xun Zhu", "Xiao Zhang", "Ziyu Liu", "Miao Li", "Ji Wu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0164", "x": 4.512, "y": 4.975, "title": "MedICL: In-Context Learning for Semantically Enhanced AKI Prediction in Cardiac Surgery", "abstract": "Cardiac surgery is associated with the risk of acute kidney injury (AKI), which can lead to prolonged hospital stays and increased mortality. Accurate prediction of AKI before its onset could significantly improve patient outcomes. However, existing AKI prediction models primarily focus on numerical features such as laboratory values and vital signs, while overlooking textual features, including preoperative diagnoses and surgical procedures. To address this limitation, we propose MedICL, which applies in-context learning (ICL) to the cardiac surgery domain. By leveraging the powerful comprehension and reasoning capabilities of large language models, MedICL enables the integration of textual and numerical features for AKI prediction. Nevertheless, the performance of ICL is highly sensitive to the quality of the provided examples, potentially limiting its effectiveness. To overcome this challenge, we introduce a Semantic Matching Unit (SMU), which selects semantically relevant examples for each sample, thereby significantly enhancing the model's performance. Furthermore, we observed that ICL-based AKI predictions often suffer from instability and exhibit suboptimal performance on downstream tasks. To address these issues, we developed the Task Adaptability Enhancer (TAE), which calibrates the prediction probabilities generated by ICL on the validation set. This approach not only stabilizes the model's outputs but also enhances its adaptability to specific task scenarios. A series of experiments on the datasets collected from West China Hospital (WCH) demonstrated that MedICL achieved state-of-the-art performance. These results highlight the indispensable role of medical text data in AKI prediction for cardiac surgery scenarios, showcasing its potential to improve clinical practice. C. Su and Y. Wang-Con tributed equally.", "filename": "2025_0164.pdf", "year": 2025, "institution": "City University of Hong Kong", "country": "Hong Kong SAR", "authors": ["Chenyang Su", "Yishun Wang", "Boqiang Xu", "Rong Feng", "Lei Du", "Hongbin Liu", "Gaofeng Meng"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0165", "x": -0.343, "y": 3.035, "title": "Mission Balance: Generating Under-Represented Class Samples Using Video Diffusion Models", "abstract": "Computer-assisted interventions can improve intraoperative guidance, particularly through deep learning methods that harness the spatiotemporal information in surgical videos. However, the severe data imbalance often found in surgical video datasets hinders the development of high-performing models. In this work, we aim to overcome the data imbalance by synthesizing surgical videos. We propose a unique twostage, text-conditioned diffusion-based method to generate high-fidelity surgical videos for under-represented classes. Our approach conditions the generation process on text prompts and decouples spatial and temporal modeling by utilizing a 2D latent diffusion model to capture spatial content and then integrating temporal attention layers to ensure temporal consistency. Furthermore, we introduce a rejection sampling strategy to select the most suitable synthetic samples, effectively augmenting existing datasets to address class imbalance. We evaluate our method on two downstream tasks-surgical action recognition and intraoperative event prediction-demonstrating that incorporating synthetic videos from our approach substantially enhances model performance.", "filename": "2025_0165.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Danush Kumar Venkatesh", "Isabel Funke", "Micha Pfeiffer", "Fiona Kolbinger", "Hanna Maria Schmeiser", "Marius Distler", "Jürgen Weitz", "Stefanie Speidel"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0166", "x": -0.372, "y": 3.474, "title": "Multi-modal Representations for Fine-Grained Multi-Label Critical View of Safety Recognition", "abstract": "The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zeroshot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-A daptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multimodal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/ CVS-AdaptNet.", "filename": "2025_0166.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Britty Baby", "Vinkle Srivastav", "Pooja P Jain", "Kun Yuan", "Pietro Mascagni", "Nicolas Padoy"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0167", "x": 0.442, "y": 2.265, "title": "NAVIUS: Navigated Augmented Reality Visualization for Ureteroscopic Surgery", "abstract": "Ureteroscopy is the standard of care for diagnosing and treating kidney stones and tumors. However, current ureteroscopes have a limited field of view, requiring significant experience to adequately navigate the renal collecting system. This is evidenced by the fact that inexperienced surgeons have higher rates of missed stones. %22 of patients with residual stones require re-operation within 20 months. In order to aid surgeons to fully explore the kidney, this study presents the Navigated Augmented Reality Visualization for Ureteroscopic Surgery (NAVIUS) system. NAVIUS assists surgeons by providing 3D maps of the target anatomy, real-time scope positions, and preoperative imaging overlays. To enable real-time navigation and visualization, we integrate an electromagnetic tracker-based navigation pipeline with augmented reality (AR) visualizations. NAVIUS connects to 3D Slicer and Unity with OpenIGTLink, and u ses HoloLens 2 as a holographic interface. We evaluate NAVIUS through a user study where surgeons conducted ureteroscopy on kidney phantoms with and without visual guidance. We observed that surgeons explored more areas within the collecting system with NAVIUS (average 23.73% increase), and experienced lower task load, as measured by NASA-TLX (up to 27.27% reduction). NAVIUS is a step towards intraoperative AR guidance for better surgical outcomes and surgeons' experience. The codebase for the system is available at: https://github.com/vu-maple-lab/NAVIUS.", "filename": "2025_0167.pdf", "year": 2025, "institution": "Vanderbilt University", "country": "USA", "authors": ["Ayberk Acar", "Jumanh Atoum", "Peter S Connor", "Clifford Pierre", "Carisa N Lynch", "Nicholas L Kavoussi", "Jie Ying Wu"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0168", "x": 4.068, "y": 3.085, "title": "NIMOSEF: Neural Implicit Motion and Segmentation Functions", "abstract": "We present NIMOSEF, a novel unified framework that leverages neural implicit functions for joint segmentation, reconstruction, and displacement field estimation in cardiac magnetic resonance imaging (CMRI). By leveraging on a shared implicit representation for joint segmentation and motion estimation our approach improves spatio-temporal consistency with respect to conventional grid-based convolutional neural networks and implicit segmentation functions. NIMOSEF employs an auto-decoder architecture to learn subject-specific latent representations from unstructured point clouds derived from image intensities and reference segmentations. These latent codes, when combined with 4D space-time coordinates, enable the generation of high-resolution segmentation outputs and smooth, temporally coherent motion estimates. Experimental evaluation on a subset of 700 random patients from the UK Biobank demonstrates that our method achieves competitive segmentation accuracy-attaining Dice scores of up to 0.93 for the LV, 0.90 for the RV and 0.83 for the LV myocardium, with improved spatiotemporal consistency, predicting a smaller number of disconnected components. Simultaneously, it achieves an average registration error of the whole heart boundary of 3.08 ± 1.23 mm measured by the Chamfer distance, and 8.57 ± 4.74 mm according to the 95th percentile Hausdorff distance. Additionally, feature importance analysis reveals that the learnt implicit representation encodes physiologically relevant information. These results suggest that NIMOSEF offers a promising alternative for high-resolution, temporally consistent cardiac segmentation and motion estimation, with promising potential for advancing clinical assessment of cardiac function.", "filename": "2025_0168.pdf", "year": 2025, "institution": "Lausanne University Hospital", "country": "Switzerland", "authors": ["Jaume Banus", "Antoine Delaloye", "Pedro M. Gordaliza", "Costa Georgantas", "Ruud B Van Heeswijk", "Jonas Richiardi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0169", "x": 0.177, "y": 3.754, "title": "PolyMamba: Spatial-Prior Guided Mamba for Polyp Segmentation with High-Frequency Enhancement", "abstract": "Accurate polyp segmentation during colonoscopy is crucial for the early detection and timely intervention of colorectal cancer. Recently, Mamba, a State Space Model, has gained significant attention in polyp segmentation due to its remarkable ability to model long-range dependencies with linear computational complexity. However, Mambabased methods face two key challenges: (1) their fixed scanning pattern limits the capture of dynamic spatial context, impairing the precise localization of irregular polyps; (2) during the calculation process, the highfrequency information that is crucial to local details is weakened, and the blurred mid-frequency information becomes dominant, thereby reducing the boundary accuracy. To overcome these limitations, we propose Poly-Mamba, a novel framework that integrates spatial priors while enhancing high-frequency information for more accurate polyp segmentation. Specifically, our framework introduces a Spatial-Prior Guided module, which leverages explicit spatial priors extracted from Transformer-based methods to counteract the local perception bias caused by Mamba's fi xed scanning pattern. Additionally, we design a Dual-Gate Frequency Enhancement module, which applies two Gaussian filters to generate spectra with different high-frequency thresholds, and uses the difference between them as an attention map to selectively enhance high-frequency features, thereby refining the polyp boundaries. Comprehensive experiments on five widely used polyp segmentation datasets demonstrate that PolyMamba not only surpasses existing state-of-the-art techniques but also provides a novel frequency-domain perspective, offering new insights into improving segmentation performance.", "filename": "2025_0169.pdf", "year": 2025, "institution": "China University of Geosciences", "country": "China", "authors": ["Renyu Fu", "Shurui Hu", "Xiao Zheng", "Chang Tang", "Xinwang Liu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0170", "x": 0.049, "y": 3.954, "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis", "abstract": "Early detection, accurate segmentation, classification and tracking of polyps during colonoscopy are critical for preventing colorectal cancer. Many existing deep-learning-based methods for analyzing colonoscopic videos either require task-specific fine-tuning, lack tracking capabilities, or rely on domain-specific pre-training. In this paper, we introduce PolypSegTrack, a novel foundation model that jointly addresses polyp detection, segmentation, classification and unsupervised tracking in colonoscopic videos. Our approach leverages a novel conditional mask loss, enabling flexible training across datasets with either pixel-level segmentation masks or bounding box annotations, allowing us to bypass task-specific fine-tuning. Our unsupervised tracking module reliably associates polyp instances across frames using ob ject queries, without relying on any heuristics. We leverage a robust vision foundation model backbone that is pre-trained unsupervisedly on natural images, thereby removing the need for domain-specific pre-training. Extensive experiments on multiple polyp benchmarks demonstrate that our method significantly outperforms existing state-of-the-art approaches in detection, segmentation, classification, and tracking.", "filename": "2025_0170.pdf", "year": 2025, "institution": "United Imaging Intelligence", "country": "USA", "authors": ["Anwesa Choudhuri", "Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Terrence Chen", "Ziyan Wu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0171", "x": 4.131, "y": 2.117, "title": "Predicting Longitudinal Brain Development via Implicit Neural Representations", "abstract": "Predicting individualized perinatal brain development is crucial for understanding personalized neurodevelopmental trajectories, however, remains challenging due to limited longitudinal data. While population based atlases model generic trends, they fail to capture subject-specific growth patterns. In this work, we propose a novel approach leveraging Implicit Neural Representations (INRs) to predict individualized brain growth over multiple weeks. Our method learns from a limited dataset of less than 100 paired fetal and neonatal subjects, sampled from the developing Human Connectome Project. The trained model demonstrates accurate personalized future and past trajectory predictions from a single calibration scan. By incorporating conditional external factors such as birth age or birth weight, our mo del further allows the simulation of neurodevelopment under varying conditions. We evaluate our method against established perinatal brain atlases, demonstrating higher prediction accuracy and fidelity up to 20 weeks. Finally, we explore the method's ability to reveal subject-specific cortical folding patterns under varying factors like birth weight, further advocating its potential for personalized neurodevelopmental analysis.", "filename": "2025_0171.pdf", "year": 2025, "institution": "Technical University Munich", "country": "Germany", "authors": ["Maik Dannecker", "Daniel Rueckert"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0172", "x": 0.033, "y": 2.009, "title": "Real-Time SLAM-Based Correction and 3D Visualization for Fluorescence Lifetime Imaging", "abstract": "Fluorescence lifetime (FLT) imaging has been shown to distinguish tumors from normal tissue with high accuracy. However, the practical utility of FLT imaging is hindered by slow acquisition speeds and depth-dependent inaccuracies. To address these challenges, we introduce FLT-SLAM, a novel algorithm that combines rapid FLT imaging with simultaneous localization and mapping (SLAM) for real-time 3D surface reconstruction and depth-corrected FLT estimation. Using a stereo laparoscope, our approach extracts real-time depth information to improve accuracy, while achieving acquisition speeds exceeding 5 Hz. FLT maps are overlaid onto large-scale 3D surface models generated by SLAM, improving visualization and spatial awareness. We validate FLT-SLAM through phantom and ex-vivo tissue measurements, and show that it reduces FLT estimation errors by nearly 20%, thereby demonstrating its potential to enhance real-time, depth-corrected FLT imaging for surgical applications.", "filename": "2025_0172.pdf", "year": 2025, "institution": "Harvard Medical School", "country": "USA", "authors": ["Murali Krishnamoorthy", "Haoyin Zhou", "Katherine Frazee", "Rahul Pal", "Jayender Jagadeesan", "Anand T N Kumar"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0173", "x": 1.858, "y": 6.03, "title": "RefineNet: Elevating Medical Foundation Models Through Quality-Centric Data Curation by MLLM-Annotated Proxy Distillation", "abstract": "The rapid advancement of medical foundation models creates unprecedented demand for large-scale training data, yet existing medical repositories remain contaminated by heterogeneous mixtures of high-and low-quality image-text pairs-a severe data pollution problem that significantly bottlenecks model performance and optimization. While manual curation could theoretically ensure quality, it is impractical for managing large-scale datasets effectively.To address this critical challenge, we introduce RefineNet-a scalable framework that systematically refines data quality by distilling multimodal large language model (MLLM) insights into an offline reward model.RefineNet innovatively decouples human decision-making for quality assessment into two key dimensions: image-text fidelity and semantic consistency. By strategically filtering and curating datasets, RefineNet d emonstrates remarkable performance improvements across diagnostic tasks. Specifically, our method selects 50% high-quality data subsets that outperform full-data baselines by 9.15% in Recall@10 (retrieval), 85.59 AUC (classification), and 72.59% accuracy (visual question answering). Moreover, RefineNet achieves notable agreement with human expert judgments (Pearson's r = 0.67), providing clinicians an auditable bridge between automated curation and validation.", "filename": "2025_0173.pdf", "year": 2025, "institution": "Macao Polytechnic U niversity", "country": "China", "authors": ["Ningyi Zhang", "Yuan Gao", "Xin Wang", "Ka-Hou Chan", "Jian Wu", "Chan-Tong Lam", "Shanshan Wang", "Yue Sun", "Sio-Kei Im", "Tao Tan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0174", "x": -0.344, "y": 4.874, "title": "SAMUSA: Segment Anything Model 2 for UltraSound Annotation", "abstract": "Interactive segmentation tools, such as SAM2, have shown strong performance in reducing annotation effort in natural images. However, unlike natural images, ultrasound images and videos often lack well-defined structure boundaries, which significantly degrade the performance of region-based point prompts in SAM models. To address these limitations, we introduce the Segment Anything Model 2 for UltraSound Annotation (SAMUSA). SAMUSA is based on SAM2 and introduces a new prompt strategy with boundary and temporal points, along with a novel boundary loss function, enabling the model to more efficiently segment structures with poorly defined boundaries, such as liver masses. We integrated SAMUSA as a 3D Slicer plugin, where it can be used for US videos and 3D US volumes segmentation. We present a prospective user study involving 6 participants (3 surgeons and 3 radiographers), which showed an average 34.1% annotation time reduction for image liver mass segmentation.", "filename": "2025_0174.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Baptiste Podvin", "Toby Collins", "Güinther Saibro", "Chiara Innocenzi", "Yuchuan Yang", "Flavio Milana", "Yvonne Keeza", "Grace Ufitinema", "Florien Ujemurwego", "Guido Torzilli", "Jacques Marescaux", "Daniel George", "Alexandre Hostettler"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0175", "x": 2.232, "y": 3.669, "title": "Semantic Interpolative Diffusion Model: Bridging the Interpolation to Masks and Colonoscopy Image Synthesis for Robust Generalization", "abstract": "Polyp segmentation is a representative task in computeraided clinical diagnosis in colonoscopy analysis. However, strict regulations limit the availability of large, high-quality image-mask paired datasets for segmentation. As a result, recent studies have focused on models that generate images conditioned on masks. However, due to rigid annotation constraints and a high reliance on fixed masks, the synthesized images often exhibit limited variation, leading to a lack of generalization in downstream tasks. This study introduces the Semantic Interpolative Diffusion Model (SIDM), which applies interpolation to both the given masks and the colonoscopy images to generate pairs of interpolated masks and images. First, a background semantic label was devised by labeling background regions based on the colonoscopy imaging environment. Both the masks and the background semantic labels are applied as multi-conditions to the diffusion model for colonoscopy image generation. After training, interpolation on both the masks and background semantic labels is performed at a chosen ratio. Applying the interpolated masks and labels to the model generates an i ntermediate perspective of colonoscopy images that partially incorporates features from each condition. By augmenting the dataset with these pairs of interpolated masks and generated images with interpolated conditions, segmentation models can extend the coverage of possible colonoscopy scenarios and mitigate the limitations of fixed masks, leading to robust generalization. Experimental comparisons against existing generative models, using the same test data across different segmentation models and different test datasets with the same model, demonstrate the effective generalization of the proposed model. The code is available at https://github.com/DSLab-MJU/ SIDM.", "filename": "2025_0175.pdf", "year": 2025, "institution": "Myongji Universit y", "country": "South Korea", "authors": ["Chanyeong Heo", "Jaehee Jung"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0176", "x": 0.029, "y": 4.795, "title": "Sparsely Annotated Medical Image Segmentation via Cross-SAM of 3D and 2D Networks", "abstract": "Medical image segmentation typically relies on large, accurately annotated datasets. However, acquiring pixel-level annotations is a labor-intensive process that demands substantial effort from domain experts, posing significant challenges in obtaining such annotations in real-world clinical settings. To tackle this challenge, we present the SA-Net framework, which leverages cross-supervision from segment anything models (SAM) and 2D segmentation networks to learn from sparse annotations. Specifically, we design an interactive graph learning segmentation network, which employs a bilateral graph convolution (BGC) module to capture more detailed features from multiple perspectives, facilitating the generation of high-quality pseudo-labels, which can serve as direct supervision for semantic segmentation networks and SAM, enabling the synthesis of additional annotations to enhance the training process. The m ulti-scale attention (MSA) module facilitates cross-layer interaction by partitioning channel label groups and capturing global information across layers, while the recovery module (RM) utilizes deep features and lowlevel features to fuse global context information and reconstruct lesion boundary regions. Our experimental results on LUNA16, AbdomenCT-1K, and self-collected datasets demonstrate the effectiveness of SA-Net. Our code is available at https://github.com/CTSegPilot/SA-Net.git.", "filename": "2025_0176.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Huaqiang Su", "Zaiyi Liu", "Lisha Yao", "Sunyun Li", "Hun Lin", "Guoliang Chen", "Xin Chen", "Haijun Lei", "Baiying Lei"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0177", "x": 2.548, "y": 1.693, "title": "STMDiff: Spatiotemporal Matching Diffusion Model for Dual-Time-Point Total-Body PET/CT Imaging via Contrastive Learning", "abstract": "Total body PET/CT systems, which enable unprecedented image quality and ultrahigh sensitivity, are widely utilized for diagnosing and treating diseases like tumors. Unlike regular protocols, dual-timepoint imaging (DTPI)-where patients undergo a dual PET/CT scan to enhance lesion contrast -exposes them to higher radiation doses due to an additional CT scan for PET attenuation correction and anatomical localization. To mitigate radiation exposure, we introduce STMDiff, a spatiotemporal matching diffusion model, which reuse CT images from first scanning time point for PET attenuation correction at second scanning time point. Spatiotemporal matching strategy implemented with contrastive learning aims to find the k-best-matched CT images, which enriches the multimodal features of STMdiff and bypasses the crossmodal registration, facilitating the generation of attenuation-corrected (AC) PET images alleviating alignment e rrors. Both qualitative and quantitative results illustrate that the AC PET images from STMDiff not only obtain the best quantitative scores (PSNR: 37.72 ± 6.85 dB; SSIM: 0.96 ± 0.03; RMSE: 2.35 ± 1.03), but also preserve metabolic information. Moreover, clinical assessment results show that the standardized uptake value (SUV) distribution of our method is more consistent with that of real AC PET images (Our code is available at https://github. com/LEE12365/STMDiff).", "filename": "2025_0177.pdf", "year": 2025, "institution": "Research Center for Medical AI", "country": null, "authors": ["Wenbo Li", "Zhenxing Huang", "Lianghua Li", "Chunyan Yang", "Yihan Wang", "Wenjian Qin", "Na Zhang", "Hairong Zheng", "Dong Liang", "Jianjun Liu", "Zhanli Hu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0178", "x": 1.02, "y": 2.216, "title": "Stronger Together: Registering Preoperative Imagery, LUS, and MIS Liver Images", "abstract": "This study addresses the critical challenges of accurate tumor localization in minimally invasive surgery (MIS) of the liver, where limited visibility and the absence of tactile feedback complicate surgery. The study focuses on integrating all three standard modalities: preoperative 3D models, laparoscopic ultrasound (LUS), and MIS images. Unlike previous approaches, our method exploits the interrelationships among all these modalities, without relying on markers or external sensors, to maximize applicability. It uses an advanced geometric model to integrate the existing registration constraints between pairs of modalities, such as the anatomical landmarks, with new spatial constraints, including the contact of the LUS transducer with the liver and the agreement of the LUS and the preoperative tumor profiles. Experimental validation on phantoms and patient data shows that the method boosts accuracy.", "filename": "2025_0178.pdf", "year": 2025, "institution": "Clermont Auvergne INP", "country": "France", "authors": ["Mohammad Mahdi Kalantari", "Erol Ozgur", "Mohammad Alkhatib", "Navid Rabbani", "Yamid Espinel", "Richard Modrzejewski", "Bertrand Le roy", "Emmanuel Buc", "Youcef Mezouar", "Adrien Bartoli"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0179", "x": 0.48, "y": 5.393, "title": "Structure-Aware Cross-Modal Prompt Tuning for Autonomous Bronchoscopic Navigation", "abstract": "Autonomous bronchoscopic navigation is vital for pulmonary disease diagnosis and treatment but still suffers from subtle anatomical variations and open-set bronchial variants. Current vision-language foundation models enable open-set recognition but get trapped in capturing fine-grained spatial features and disentangling class-specific attributes. We propose a structure-aware cross-modal prompt tuning framework that combines the contrastive language-image pre-training (CLIP) model and the efficient segment anything model (EfficientSAM) to address these limitations. Specifically, EfficientSAM extracts structure-aware features for learnable textual prompts via cross-modal attention to enrich visual embeddings in CLIP, while a base-unknown decoupled head disentangles shared anatomical knowledge and class-specific features i n the latent space, enhancing separability for both base and open-set classes. Moreover, unified optimization aligns multi-modal distributions using imagetext matching loss and base-unknown decoupled loss. We evaluate our method on clinical bronchoscopic data, with the experimental results showing that our method outperforms state-of-the-art approaches and improves recognition and open-set identification (88.94%, 87.00%).", "filename": "2025_0179.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Hao Fang", "Zhuo Zeng", "Jianwei Yang", "Wenkang Fan", "Xiongbiao Luo"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0180", "x": -0.131, "y": 2.141, "title": "SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction", "abstract": "Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting methods have shown promise in achieving highquality reconstructions with fast rendering speeds, their use of inverse depth loss functions compresses depth variations. This can lead to a loss of fine geometric details, limiting their ability to capture precise 3D geometry and effectiveness in intraoperative applications. To address the limitations of existing methods, we developed SurgicalGS, a dynamic 3D Gaussian Splatting framework specifically designed for improved geometric accuracy in surgical scene reconstruction. Our approach integrates a temporally coherent multi-frame depth fusion and an adaptive motion mask for Gaussian initialisation. Besides, we represent dynamic scenes using the Flexible Deformation Model and introduce a novel normalized depth regularization loss and an unsupervised depth smoothness constraint to ensure high geometric accuracy in the reconstruction. Extensive experiments on two real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art reconstruction quality, especially in precise geometry, advancing the usability of 3D Gaussian Splatting in robotic-assisted surgery. Our code is available at https://github.com/ neneyork/SurgicalGS.", "filename": "2025_0180.pdf", "year": 2025, "institution": "The Hamlyn Centre for Robotic Surgery", "country": "UK", "authors": ["Jialei Chen", "Xin Zhang", "Mobarak I Hoque", "Francisco Vasconcelos", "Danail Stoyanov", "Daniel S Elson", "Baoru Huang"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0181", "x": 1.4, "y": 5.199, "title": "Synchronous Inhibition and Activation for Weakly Supervised Semantic Segmentation of Pathology Images", "abstract": "Tissue-level semantic segmentation is crucial in digital pathology workflow. However, since dense pixel-level annotation of gigapixel pathology images is expensive and time-consuming, Weakly Supervised Semantic Segmentation (WSSS) methods have gradually attracted attention. The WSSS methods using image-level labels usually rely on Class Activation Map to generate pseudo labels, which have difficulty capturing complete object regions and may incorrectly activate regions with weak semantic relevance of pathology images. In this work, we propose SIA-WSSS, a weakly supervised semantic segmentation model for pathology images that synchronous inhibition and activation. Specifically, we first extract pathology images class and patch tokens using a VisionTransformer (ViT) and construct a Regularized Focus Mechanism (RFM). The RFM implicitly regularizes class-patch interactions through graph learning, ensuring that class tokens can dynamically compress patch information and inhibit irrelevant backgrounds. Next, we in troduce a Discriminative Activation Module to contrast the class tokens of fine-grained regions and global objects to capture the unique features of each class and activate the foreground region. Moreover, we design a Regional Self-modulation Module synchronizing each region's activation and inhibition information to generate segmentation results with finer structures. Experimental results on the LUAD-HistoSeg and BCSS-WSSS datasets demonstrate that the proposed SIA-WSSS significantly outperforms state-of-the-art WSSS methods. The code is available at https://github.com/Jsf826/SIA-WSSS.", "filename": "2025_0181.pdf", "year": 2025, "institution": "Jiangnan U niversity", "country": "China", "authors": ["Jiansong Fan", "Yicheng Di", "Jiayu Bao", "Lihua Li", "Xiang Pan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0182", "x": 0.347, "y": 3.984, "title": "Targeted False Positive Synthesis via Detector-Guided Adversarial Diffusion Attacker for Robust Polyp Detection", "abstract": "Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, d etectorconfusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at https://github. com/Huster-Hq/DADA.", "filename": "2025_0182.pdf", "year": 2025, "institution": "Wuhan University of Technology", "country": "China", "authors": ["Quan Zhou", "Gan Luo", "Qiang Hu", "Qingyong Zhang", "Jinhua Zhang", "Yinjiao Tian", "Qiang Li", "Zhiwei Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0183", "x": 4.46, "y": 6.419, "title": "Unsupervised Discovery of Spatiotypes and Context-Aware Graph Neural Networks for Modeling Clinical Endpoints", "abstract": "Human tissue samples exhibit remarkable cellular and structural diversity, where alterations in the spatial arrangement of cells can signal the onset or progression of disease. Therefore, characterizing these spatial cellular interactions and linking them to clinical endpoints is critical to advance our understanding of disease biology and improve patient care. In this work, we introduce a band descriptor that quantifies the local neighborhood of each cell by computing the relative abundance of neighboring cell types using concentric bands. We demonstrate the efficacy of our approach by highlighting two key benefits: it enables the unsupervised discovery of spatiotypes (substructures defined by local cellular configurations), and it provides an explicit encoding of spatial context in cell-level graphs-capturing long-range cell interactions across tissue. Our experiments in a lung tissue cohort reveal distinct spatial patterns of cellular arrangement that differentiate control from disease samples and may also reflect disease progression (unaffected, less affected, or more affected). Furthermore, by explicitly modeling spatial context, our band descriptor enhances node-level representations, enabling an endto-end Graph Neural Network (GNN) to achieve high accuracy in a clinical prediction task with fewer layers. This reduction in network depth decreases over-smoothing and improves interpretability, underscoring our approach's potential for broad adoption in tissue-based studies and clinical applications. Code is available on GitHub (https://github. com/imuhdawood/BandDescriptor).", "filename": "2025_0183.pdf", "year": 2025, "institution": "University of Oxford", "country": "University", "authors": ["Muhammad Dawood", "Emily Thomas", "Rosalin Cooper", "Carlo Pescia", "Anna Sozanska", "Hosuk Ryou", "Daniel Royston", "Jens Rittscher"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0184", "x": 1.051, "y": 4.023, "title": "U-RWKV: Lightweight Medical Image Segmentation with Direction-Adaptive RWKV", "abstract": "Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and main taining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at https://github.com/hbyecoding/U-RWKV.", "filename": "2025_0184.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "China", "authors": ["Hongbo Ye", "Fenghe Tang", "Peiang Zhao", "Zhen Huang", "Dexin Zhao", "Minghao Bian", "Shaohua Kevin Zhou"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0185", "x": 1.766, "y": 6.38, "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "abstract": "As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chainof-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "filename": "2025_0185.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0186", "x": 6.781, "y": 3.752, "title": "A Model Order-Free Method for Stable States Extraction in Dynamic Functional Connectivity", "abstract": "Dynamic functional connectivity (dFC) analysis has revealed that functional connectivity fluctuates over short timescales, reflecting the intrinsic transitions of brain among multiple states. However, dFC data typically exhibit the characteristics of high dimensionality and noise, making it difficult to extract stable and accurate states. Furthermore, accurately identifying model order (i.e., number of states) is challenging due to lack of prior knowledge. To address the above issues, we propose a model order-free method for extracting stable states.Our method can simultaneously capture multi-scale state information and improve the stability of the state. Furthermore, our method estimates the number of states adaptively based on data-driven methods. Based on synthetic data, we evaluated the effectiveness of our method. The results showed that, compared to traditional methods, our method not only accurately estimated the number of states but also extracted states with greater robustness and precision. Additionally, we evaluated the effectiveness and stability of the method using fMRI data from 602 healthy controls and 519 schizophrenia patients. Results demonstrated that our method exhibited significant consistency among the states extracted by multiple runs. Moreover, we identified reliable biomarkers for schizophrenia. In conclusion, we propose a novel state extraction method that does not rely on predefined state numbers, while accurately and stably identifying states.", "filename": "2025_0186.pdf", "year": 2025, "institution": "Shanxi University", "country": "China", "authors": ["Songke Fang", "Vince D Calhoun", "Godfrey Pearlson", "Peter Kochunov", "Theo G M Van Erp", "Yuhui Du"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0187", "x": 6.065, "y": 3.753, "title": "A Novel Fourier Adjacency Transformer for Advanced EEG Emotion Recognition", "abstract": "EEG emotion recognition faces significant hurdles due to noise interference, signal nonstationarity, and the inherent complexity of brain activity which make accurately emotion classification. In this study, we present the Fourier Adjacency Transformer, a novel framework that seamlessly integrates Fourier-based periodic analysis with graph-driven structural modeling. Our method first leverages novel Fourier-inspired modules to extract periodic features from embedded EEG signals, effectively decoupling them from aperiodic components. Subsequently, we employ an adjacency attention scheme to reinforce universal interchannel correlation patterns, coupling these patterns with their samplebased counterparts. Empirical evaluations on SEED and DEAP datasets demonstrate that our method surpasses existing state-of-the-art techniques, achieving an improvement of approximately 6.5% in recognition accuracy. By unifying periodicity and structural insights, this framework offers a promising direction for future research in EEG emotion analysis. The code can be found at: https://github.com/YanhaoHuang23/FAT.", "filename": "2025_0187.pdf", "year": 2025, "institution": "Kunming University of Science and Technology", "country": "China", "authors": ["Jinfeng Wang", "Yanhao Huang", "Sifan Song", "Boqian Wang", "Jionglong Su", "Jiaman Ding"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0188", "x": 1.844, "y": 1.992, "title": "A Novel Streamline-Based Diffusion MRI Tractography Registration Method with Probabilistic Keypoint Detection", "abstract": "Registration of diffusion MRI tractography is an essential step for analyzing group similarities and variations in the brain's white matter (WM). Streamline-based registration approaches can leverage the 3D geometric information of fiber pathways to enable spatial alignment after registration. Existing methods usually rely on the optimization of the spatial distances to identify the optimal transformation. However, such methods overlook point connectivity patterns within the streamline itself, limiting their ability to identify anatomical correspondences across tractography datasets. In this work, we propose a novel unsupervised approach using deep learning to perform streamline-based dMRI tractography registration. The overall idea is to identify corresponding keypoint pairs across subjects for spatial alignment of tractography datasets. We model tractography as point clouds to leverage the graph connectivity along streamlines. We propose a novel keypoint detection method for streamlines, framed as a probabilistic classification task to identify anatomically consistent correspondences across unstructured streamline sets. In the experiments, we compare several existing methods and show highly effective and efficient tractography registration performance.", "filename": "2025_0188.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Junyi Wang", "Mubai Du", "Ye Wu", "Yijie Li", "William M Wells Iii", "Lauren J O’donnell", "Fan Zhang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0189", "x": 6.772, "y": 3.9, "title": "Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification", "abstract": "Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks by tracking BOLD signals across brain regions. However, existing models largely neglect the multi-frequency nature of neuronal oscillations, treating BOLD signals as monolithic time series. This overlooks the crucial fact that neurological disorders often manifest as disruptions within specific frequency bands, limiting diagnostic sensitivity and specificity. While some methods have attempted to incorporate frequency information, they often rely on predefined frequency bands, which may not be optimal for capturing individual variability or disease-specific alterations. To address this, we propose a novel framework featuring Adaptive Cascade Decomposition to learn task-relevant frequency sub-bands for each brain region and Frequency-Coupled Connectivity Learning to capture both intra-and nuanced cross-band interactions in a unified functional network. This unified network informs a novel message-passing mechanism within our Unified-GCN, generating refined node representations for diagnostic prediction. Experimental results on the ADNI and ABIDE datasets demonstrate superior performance over existing methods. The code is available at https://github.com/XXYY20221234/Ada-FCN.", "filename": "2025_0189.pdf", "year": 2025, "institution": "Hong Kong Polytechnic University", "country": "China", "authors": ["Yue Xun", "Jiaxing Xu", "Wenbo Gao", "Chen Yang", "Shujun Wang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0190", "x": 6.925, "y": 3.818, "title": "Adaptive Embedding for Long-Range High-Order Dependencies via Time-Varying Transformer on fMRI", "abstract": "Dynamic functional brain network analysis using rs-fMRI has emerged as a powerful approach to understanding brain disorders. However, current methods predominantly focus on pairwise brain region interactions, neglecting critical high-order dependencies and timevarying communication mechanisms. To address these limitations, we propose the Long-Range High-Order Dependency Transformer (LHD-Former), a neurophysiologically-inspired framework that integrates multiscale long-range dependencies with time-varying connectivity patterns. Specifically, we present a biased random walk sampling strategy with NeuroWalk kernel-guided transfer probabilities that dynamically simulate multi-step information loss through a k-walk neuroadaptive factor, modeling brain neurobiological principles such as distancedependent information loss and state-dependent pathway modulation. This enables the adaptive capture of the multi-scale short-range couplings and long-range high-order dependencies corresponding to different steps across evolving connectivity patterns. Complementing this, the time-varying transformer co-embeds local spatial configurations via topology-aware attention and global temporal dynamics through crosswindow token guidance, overcoming the single-domain bias of conventional graph/transformer methods. Extensive experiments on ABIDE and ADNI datasets demonstrate that LHDFormer outperforms state-ofthe-art methods in brain disease diagnosis. Crucially, the model identifies interpretable high-order connectivity signatures, revealing disrupted long-range integration patterns in patients that align with known neuropathological mechanisms.", "filename": "2025_0190.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Rundong Xue", "Xiangmin Han", "Hao Hu", "Zeyu Zhang", "Shaoyi Du", "Yue Gao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0191", "x": 6.785, "y": 3.904, "title": "Adaptive Graph Learning with Multi-graph Convolutions for Brain Disorder Classification", "abstract": "Functional Magnetic Resonance Imaging (fMRI) provides crucial insights into brain activity but presents challenges due to its high-dimensional, dynamic, and noisy nature. Traditional graph-based approaches for fMRI analysis often rely on predefined correlation structures, which may not accurately reflect the true underlying functional connectivity. To address this limitation, we propose a graph learning framework that dynamically constructs brain graphs and leverages Spline Convolutional Neural Networks (SplineCNN) for localized spatial feature extraction. Our model introduces a Learner Graph module, which infers graph structures in a data-driven manner, mitigating the reliance on predefined connectivity measures. The SplineCNN and Multi-Graph Convolution modules capture fine-grained spatial dependencies, offering improved adaptability to the heterogeneous nature of fMRI data. Additionally, we incorporate contrastive learning to align learned representations with domain-specific priors to improve generalization. Experimental results demonstrate that our approach outperforms traditional correlation-based methods in neurological disorder classification. The proposed framework provides a principled, adaptive solution for learning graph representations from fMRI, enhancing generalizability and robustness in brain network analysis.", "filename": "2025_0191.pdf", "year": 2025, "institution": "Monash University Malaysia", "country": "Malaysia", "authors": ["Fuad Noman", "Null- W Phan", "Hernando Ombao", "Chee-Ming Ting"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0192", "x": 6.965, "y": 4.565, "title": "Anatomy-Guided Multimodal Graph Networks for Alzheimer’s Disease: Integrative Analysis of Cross-Modal Brain Connectivity Signatures", "abstract": "Multimodal neuroimaging grounded in standardized brain atlases enables precise decoding of Alzheimer's progression by capturing both structural atrophy and functional decline across neural circuits. Current methods compromise anatomical fidelity in whole-brain modeling while generating biologically inconsistent cross-modal interactions. To address these dual challenges, we develop a graph learning framework that integrates three synergistic components: anatomically constrained feature extraction preserving region-specific biomarkers through spatial priors, channel-wise attention mechanisms for discriminative pattern refinement, and bidirectional cross-modal adaptation governed by alternating attention to enforce neuropathological consistency. This unified architecture processes sMRI and PET data through sequential stages of anatomical feature preservation, noise-robust feature enhancement, and dynamic modality fusion, ultimately mapping neurodegeneration patterns across scales. Evaluated on ADNI, our framework achieves superior classification accuracy while graph topology analysis reveals clinically significant hub reorganization within the default mode network, directly correlating with progressive connectivity deterioration. The method's capacity to reconcile localized biomarker specificity with systemic network dynamics establishes new standards for computational neuropathology.", "filename": "2025_0192.pdf", "year": 2025, "institution": "Shenzhen University Medical School", "country": "China", "authors": ["Wenzheng Hu", "Zhenghua Guan", "Peng Yang", "Jiaqiang Li", "Yi Liu", "Shushen Gan", "Tuo Cai", "Ao Zhang", "Tengda Zhang", "Junlong Qu", "Shaolong Wang", "Gege Cai", "Xiang Dong", "Tianfu Wang", "Baiying Lei"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0193", "x": 6.986, "y": 4.483, "title": "A$$\\upbeta $$-PET Pattern Prediction via Graph Reconstruction-Aware Fusion (GRAF) of Functional and Structural Networks", "abstract": "Alzheimer's disease (AD) is characterized by abnormal amyloid-β (Aβ) deposition, which causes neural damage and cognitive decline. Aβ positron emission tomography (PET) serves as the gold standard for preclinical diagnosis of AD. However, practical limitations, including high costs, radiation exposure, and constrained accessibility, have motivated recent studies to indirectly predict Aβ deposition patterns from MRI data. Unfortunately, existing methods have not fully leveraged the coupled pathological information from both functional and structural brain networks. To address this gap, we propose Graph Reconstruction-Aware Fusion (GRAF), a novel framework designed to predict regional Aβ-PET patterns by integrating functional and structural pathological information. GRAF employs a graph-masked autoencoder to learn integrated network topology embeddings by reconstructing masked edges from both functional and structural networks, effectively utilizing node and edge features. Subsequently, the well-trained encoders are fine-tuned to predict regional Aβ patterns. Extensive experimental results demonstrate that our proposed GRAF framework outperforms six state-of-the-art methods. Our code and representative case examples are publicly available at https://github.com/ninicassiel/GRAF", "filename": "2025_0193.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Haoyue Yuan", "Yuxiao Liu", "Feihong Liu", "Dinggang Shen"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0194", "x": 2.177, "y": 3.469, "title": "Bayesian Transformers and Higher-Order Graph Matching for Cell Tracking in Serial Tissue Sections", "abstract": "Reliable 3D reconstruction of tissue architecture from sequential 2D multiplex images is challenging due to the noise and distortions introduced by ultrathin (50 nm) slicing and complex alignment procedures. Conventional cell tracking methods often fail under such conditions, resulting in inaccurate linkage of cells across sections. To bridge this gap, we propose a Bayesian Transformer framework that incorporates uncertainty-aware feature embeddings and higher-order graph matching with belief propagation. By tracking cells across consecutive sections, our method facilitates the 3D reconstruction of volumetric tissue organization, even in highly noise-prone scenarios. The methodology begins with a standard segmentation step, followed by feature extraction that computes morphological, shape, and texture descriptors, as well as deep CNN embeddings. These rich, uncertainty-sensitive representations reduce errors caused by both registration artifacts and morphological variability. We validate the effectiveness of the proposed approach on a private multiplex dataset of fixed tissue sections and further demonstrate its generalizability on public time-lapse microscopy videos, showcasing adaptability to diverse datasets. Experimental comparisons reveal that our method outperforms baseline tracking techniques, achieving higher accuracy and more consistent cell linkages across multiple serial sections. The code used in this research with sample dataset are publicly available at https://github.com/NabaviLab/bayesian-transformer-cell-tracking", "filename": "2025_0194.pdf", "year": 2025, "institution": "University of Connecticut", "country": "USA", "authors": ["Mostafa Karami", "Sahand Hamzehei", "David Arce", "Gianna Raimondi", "Linnaea Ostroff", "Sheida Nabavi"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0195", "x": 4.637, "y": 5.955, "title": "Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction", "abstract": "Accurately predicting the survival of cancer patients is crucial for personalized treatment. However, existing studies focus solely on the relationships between samples with known survival risks, without fully leveraging the value of censored samples. Furthermore, these studies may suffer performance degradation in modality-missing scenarios and even struggle during the inference process. In this study, we propose a bipartite patient-modality graph learning with event-conditional modelling of censoring for cancer survival prediction (CenSurv). Specifically, we first use graph structure to model multimodal data and obtain representation. Then, to alleviate performance degradation in modalitymissing scenarios, we design a bipartite graph to simulate the patientmodality relationship in various modality-missing scenarios and leverage a complete-incomplete alignment strategy to explore modality-agnostic features. Finally, we design a plug-and-play event-conditional modeling of censoring (ECMC) that selects reliable censored data using dynamic momentum accumulation confidences, assigns more accurate survival times to these censored data, and incorporates them as uncensored data into training. Comprehensive evaluations on 5 publicly cancer datasets showcase the superiority of CenSurv over the best state-of-the-art by 3.1% in terms of the mean C-index, while also exhibiting excellent robustness under various modality-missing scenarios. In addition, using the plug-and-play ECMC module, the mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of CenSurv is available at https://github. com/yuehailin/CenSurv.", "filename": "2025_0195.pdf", "year": 2025, "institution": "Central South University", "country": "China", "authors": ["Hailin Yue", "Hulin Kuang", "Jin Liu", "Junjian Li", "Lanlan Wang", "Mengshen He", "Jianxin Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0196", "x": 6.962, "y": 3.699, "title": "BiSCoT: Behavior-Informed Subgroup-Consistent Connectome Template for Interpretable Brain Network Analysis", "abstract": "We propose a graph information compression framework, called Behavior-Informed Subgroup-consistent Connectome Template (BISCoT), that learns interpretable functional subnetworks from restingstate fMRI (rs-fMRI) connectivity, which simultaneously capture the heterogeneity of a diverse patient cohort. BISCoT uses multidimensional behavioral profiles to guide the decomposition of a rs-fMRI connectivity matrices into sparse yet representative subnetworks that are consistent within behavioral sub-groups. In particular, our framework adopts a graph convolution network to capture local connectivity features and applies a subgroup-informed pooling process to extract the final subnetworks. We evaluate BISCoT on an in-house dataset of individuals with autism spectrum disorder and demonstrate that the learned subnetworks improve the performance of multiple downstream prediction tasks. In addition, BISCoT extracts consistent connectivity \"templates\" at the subgroup level, which allows for interpretable biomarker identification (Code available at https://github.com/zijianch/biscot).", "filename": "2025_0196.pdf", "year": 2025, "institution": "Boston University", "country": "USA", "authors": ["Zijian Chen", "Stefen Beeler-Duden", "Sophie Lawson", "Zachary Jacokes", "John Darrell Van Horn", "Kevin A Pelphrey", "Archana Venkataraman"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0197", "x": 6.803, "y": 3.559, "title": "Brain Activation Mapping Based on Regional Synchronization of fMRI Signals Embedded in Graph Eigenmodes", "abstract": "Functional magnetic resonance imaging (fMRI) analysis models the detected temporal signals as a superposition of linear hemodynamic responses (HDR) to task-related stimuli, yielding spatial maps of brain function. However, recent studies have demonstrated that neural responses exhibit significant nonlinearity, challenging the validity of such linear models. In this work, we propose a novel mathematical framework, Regional Synchronization based on Graph Eigenmodes (RS-GEm), to analyze fMRI data and localize brain activation without relying on the linear assumptions of traditional models. Using Laplacian Eigenmaps (LEM), we capture the graph structure of the brain and derive its eigenmodes. These eigenmodes characterize possible spatial organizations of neural activity across different hierarchical levels of the human brain. By computing the regional synchronization of fMRI signals embedded in the eigenmode space and employing clustering metrics, we extract task-relevant eigenmodes to identify task-evoked activation regions. Validations on the Human Connectome Project (HCP) dataset demonstrate that our method can map task-evoked brain activations without the linear assumptions. The proposed approach offers a novel methodological framework for elucidating understudied aspects of brain function featured with nonlinear HDRs, thereby facilitating a more complete understanding of brain dynamics.", "filename": "2025_0197.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Zhenyu Tang", "Yu Zhao", "Yang Yang", "Xiaoyu Liu", "Jingyong Su"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0198", "x": 7.092, "y": 4.014, "title": "Brain Wiring Knowledge Graph Reasoning: A Region Embedding Approach for Logical Neuronal Relation Inference", "abstract": "Brain Wiring Knowledge Graph is a high-level abstraction from a physical neuronal wiring diagram with semantic information, helping us better understand brain functions. However, there is currently no approach that simultaneously learns both the physical connectivity and the conceptual semantic connectivity patterns within the connectome. In this paper, we propose using knowledge graphs to integrate physical connectivity and semantic connectivity. We construct knowledge graphs from the connectomes of Drosophila and a partial human cortex. Then, we further propose a brain wiring knowledge graph reasoning framework based on Lie Group Embedding for logical neuronal relation inference. By integrating multi-dimensional neuronal data, including synaptic connectivity, spatial localization, functional activity, cellular properties, and morphological characteristics, we construct a heterogeneous brain wiring knowledge graph to capture the intricate relationships between neurons. Link prediction and neuron classification tasks reveal the connection patterns of neurons in brain functions and the distribution patterns of functional regions. Experimental results demonstrate that the proposed method excels in logical reasoning tasks. The learned embeddings of neurons can reveal the taxonomy of complex neuronal functions. Our code is available at https://github.com/zzy2018730/reasoning.", "filename": "2025_0198.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Zhengyun Zhou", "Guojia Wan", "Fei Liao", "Wenbin Hu", "Minghui Liao", "Junchao Qiu", "Xinyuan Li", "Bo Du"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0199", "x": 6.558, "y": 3.665, "title": "Brain-Environment Cross-Attention (BECA) Meta-matching: A New Perspective of Brain Connectome Zero-Shot Learning", "abstract": "Zero-shot learning (ZSL) is critical for deep learning models being deployed in unseen downstream applications. Given that fMRI studies of the human connectome with respect to cognitive disorders are boutique and lack sufficient labeled samples, a reliable and interpretable ZSL technology is necessary to empower the brain foundation model for clinical applications. Although self-supervised learning and transfer learning on data reconstruction and semantic information, respectively, have achieved success in ZSL performance for language and vision, little attention has been paid to the recognition of brain disordering. In contrast to stereotypical language or vision data, the human brain is a dynamically wired system where distributed regions communicate through functional connectivity and spontaneously respond to stimuli from environmental exposures. Thus, functional neuroimages are often associated with phenotypic traits underlying brain-environment interactions (BEIs), such as cognitive states and clinical outcomes. By capitalizing on large-scale functional neuroimages as well as a rich collection of BEI data, we break the frame of self-supervised and transfer learning by using logical regression as the pre-training objective for brain connectome. We formulate ZSL on unseen classes by identifying a reliable matching across environmental variables, which is derived from a decoder-only model for BEI prediction from functional connectivity. Together, we present a novel learning schema of brain-environment crossattention (BECA) meta-matching, which is a new horizon of ZSL for brain connectome. In experiments, all fMRI data in HCP-young adult and HCP-aging datasets are utilized for pre-training, and BECA is evaluated on disease early diagnosis of Autism, Parkinson's disease, and Schizophrenia, where promising results indicate the great potential to facilitate current neuroimaging applications in clinical routines.", "filename": "2025_0199.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Ziquan Wei", "Tingting Dan", "Guorong Wu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0200", "x": 6.342, "y": 3.507, "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "abstract": "Recent advances in deep learning have made it possible to predict phenotypic measures directly from functional magnetic resonance imaging (fMRI) brain volumes, sparking significant interest in the neuroimaging community. However, existing approaches, primarily based on convolutional neural networks or transformer architectures, often struggle to model the complex relationships inherent in fMRI data, limited by their inability to capture long-range spatial and temporal dependencies. To overcome these shortcomings, we introduce BrainMT, a novel hybrid framework designed to efficiently learn and integrate long-range spatiotemporal attributes in fMRI data. Our framework operates in two stages: (1) a bidirectional Mamba block with a temporal-first scanning mechanism to capture global temporal interactions in a computationally efficient manner; and (2) a transformer block leveraging self-attention to model global spatial relationships across the deep features processed by the Mamba block. Extensive experiments on two large-scale public datasets, UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves state-of-the-art performance on both classification (sex prediction) and regression (cognitive intelligence prediction) tasks, outperforming existing methods by a significant margin. Our code and implementation details are available at link.", "filename": "2025_0200.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Arunkumar Kannan", "Martin A Lindquist", "Brian Caffo"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0201", "x": 6.571, "y": 4.24, "title": "BrainPrompt: Domain Adaptation with Prompt Learning for Multi-site Brain Network Analysis", "abstract": "It is challenging to discriminate autism spectrum disorder (ASD) from a highly heterogeneous database, because there is a great deal of uncontrollable variability in the data from different sites. Recently, prompt learning has received considerable attention in domain adaptation as a promising solution. However, its application to graph data like multi-site brain networks has not been fully studied. It faces two major challenges: (1) complex graph structure; and (2) inter-individual variability. To overcome the issues, we propose a novel prompt-tuning paradigm for multi-site brain network analysis (BrainPrompt) using functional magnetic resonance imaging (fMRI). Specifically, we introduce two tunable soft prompts: (1) a mask prompt to prune noisy edges while preserving important connections, and distill it to reduce domainspecific biases; (2) sample prompts to capture inter-individual variations. Our model outperforms other models on the ABIDE dataset, especially at sites with limited samples (e.g., the Stanford site, which has only 39 samples). BrainPrompt achieves a 35.88% improvement in accuracy compared to the state-of-the-art method, highlighting its superiority in small sites. Furthermore, our results demonstrate the interpretability and generalization of the proposed method. Our code is available at https:// github.com/zliuzeng/BrainPrompt.", "filename": "2025_0201.pdf", "year": 2025, "institution": "Northeastern University", "country": "China", "authors": ["Liuzeng Zhang", "Lanting Li", "Peng Cao", "Jinzhu Yang", "Osmar R Zaiane"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0202", "x": 6.755, "y": 4.294, "title": "BrainPrompt: Multi-level Brain Prompt Enhancement for Neurological Condition Identification", "abstract": "Neurological conditions, such as Alzheimer's Disease, are challenging to diagnose, particularly in the early stages where symptoms closely resemble healthy controls. Existing brain network analysis methods primarily focus on graph-based models that rely solely on imaging data, which may overlook important non-imaging factors and limit the model's predictive power and interpretability. In this paper, we present BrainPrompt, an innovative framework that enhances Graph Neural Networks (GNNs) by integrating Large Language Models (LLMs) with knowledge-driven prompts, enabling more effective capture of complex, non-imaging information and external knowledge for neurological disease identification. BrainPrompt integrates three types of knowledge-driven prompts: (1) ROI-level prompts to encode the identity and function of each brain region, (2) subject-level prompts that incorporate demographic information, and (3) disease-level prompts to capture the temporal progression of disease. By leveraging these multi-level prompts, BrainPrompt effectively harnesses knowledge-enhanced multimodal information from LLMs, enhancing the model's capability to predict neurological disease stages and meanwhile offers more interpretable results. We evaluate BrainPrompt on two resting-state functional Magnetic Resonance Imaging (fMRI) datasets from neurological disorders, showing its superiority over state-of-the-art methods. Additionally, a biomarker study demonstrates the framework's ability to extract valuable and interpretable information aligned with domain knowledge in neuroscience.", "filename": "2025_0202.pdf", "year": 2025, "institution": "Nanyang Technological University", "country": "Singapore", "authors": ["Jiaxing Xu", "Kai He", "Yue Tang", "Wei Li", "Mengcheng Lan", "Xia Dong", "Yiping Ke", "Mengling Feng"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0203", "x": 2.309, "y": 3.931, "title": "CAPE: Connectivity-Aware Path Enforcement Loss for Curvilinear Structure Delineation", "abstract": "Promoting the connectivity of curvilinear structures, such as neuronal processes in biomedical scans and blood vessels in CT images, remains a key challenge in semantic segmentation. Traditional pixel-wise loss functions, including cross-entropy and Dice losses, often fail to capture high-level topological connectivity, resulting in topological mistakes in graphs obtained from prediction maps. In this paper, we propose CAPE (Connectivity-Aware Path Enforcement), a novel loss function designed to enforce connectivity in graphs obtained from segmentation maps by optimizing a graph connectivity metric. CAPE uses the graph representation of the ground truth to select node pairs and determine their corresponding paths within the predicted segmentation through a shortest-path algorithm. Using this, we penalize both disconnections and false positive connections, effectively promoting the model to preserve topological correctness. Experiments on 2D and 3D datasets, including neuron and blood vessel tracing demonstrate that CAPE significantly improves topology-aware metrics and outperforms state-of-the-art methods.", "filename": "2025_0203.pdf", "year": 2025, "institution": "Bilkent University", "country": "Turkey", "authors": ["Elyar Esmaeilzadeh", "Ehsan Garaaghaji", "Farzad Hallaji Azad", "Doruk Oner"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0204", "x": 3.551, "y": 4.224, "title": "Cerebrovascular Diseases Screening from Color Fundus Photography via Cross-View Fusion and Graph-Based Discrimination", "abstract": "Cerebrovascular  diseases  can  occur  suddenly  and  unpredictably,  making  it  crucial  to  identify  high-risk  individuals  through screening  to  prevent  or  mitigate  its  impact.  However,  digital  subtraction angiography (DSA), the current gold-standard, is difficult to apply to  large-scale  screening  or  primary  healthcare  settings  due  to  its  high cost, complex operation, and invasive nature. In contrast, Color Fundus Photography (CFP) can reflect related cerebrovascular diseases through retinal  microvascular  changes  while  maintaining  low-cost  and  risk-free advantages.  Nevertheless,  current  CFP  image-based  methods  for  predicting  cerebrovascular  disease  mostly  focus  on  pixel-level  image  features  only,  ignoring  the  correlation  between  arteriovenous  morphology, optic  disc  structure  and  disease  risk.  To  address  this  gap,  we  propose  CVGB-Net,  a  method  that  integrates  a  cross-view  encoder  to  fuse high-level  semantic  features,  primarily  capturing  vascular  abnormalities  in  the  retinal  vasculature  caused  by  cerebrovascular  diseases,  with low-level  pixel  features  extracted  by  the  foundation  model,  RetFound, designed  for  ocular  tasks.  The  fused  cross-view  features  for  each  sample  are  then  processed  through  a  graph-based  discriminator,  which  utilizes  a  graph  adapter  to  link  disease-related  features  across  the  entire dataset.  This  approach  further  enhances  the  model's  ability  to  differentiate  between  diseased  and  healthy  cases.  To  validate  our  approach, we  present  a  tailored  CFP-Cerebrovascular  diseases  Screening  (CCS) dataset with 2,338 expert-diagnosed cases. Experimental results demonstrate the effectiveness of our approach, highlighting its potential for cost-effective  large-scale  cerebrovascular  diseases  screening. https://github.com/glodxy/CVGB_net. ", "filename": "2025_0204.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Congyu Tian", "Shihao Zou", "Xiangyun Liao", "Cheng Chen", "Chubin Ou", "Jianping Lv", "Shanshan Wang", "Weixin Si"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0205", "x": 6.896, "y": 4.704, "title": "ClinGRAD: Clinically-Guided Genomics and Radiomics Interpretable GNN for Dementia Diagnosis", "abstract": "Alzheimer's Disease (AD) remains a major diagnostic challenge due to the complex interplay of genomic, radiomic, and structural factors in disease progression. While deep learning methods can classify AD, current approaches fail to effectively combine multimodal data with clinical knowledge, compromising both accuracy and interpretability. We present ClinGRAD, a clinically-guided heterogeneous graph neural network that combines genomic and radiomic data using connections based on diffusion-weighted imaging (DWI) maps and gene co-expression networks. ClinGRAD's contributions include: (1) a multimodal fusion architecture that integrates validated structural and genetic connectivity patterns for consistent biological feature analysis; (2) a multi-scale graph framework capturing both local brain structure and global genomic pathway relationships; (3) an attention mechanism that provides clinically relevant explanations of gene-structure interactions; and (4) pathway-based gene clustering that reveals underlying biological mechanisms and their clinical implications. ClinGRAD outperforms existing models, achieving an accuracy of 93.15%, distinguishing AD from control, mild cognitive impaired, and vascular dementia patients while maintaining biological coherence through its clinical guidance framework. The code is available at https://github.com/BioMedIA-MBZUAI/ClinGRAD", "filename": "2025_0205.pdf", "year": 2025, "institution": "University of Artificial Intelligence", "country": "UAE", "authors": ["Salma Hassan", "Mostafa Salem", "Vijay Ram Kumar Papineni", "Ayman Elsayed", "Mohammad Yaqub"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0206", "x": 2.611, "y": 6.217, "title": "Concept-Induced Graph Perception Model for Interpretable Diagnosis", "abstract": "Due to the high stakes in medical decision-making, there is a compelling demand for interpretable deep learning methods in medical image analysis. Concept-based interpretable models, which predict human-understandable concepts (e.g., plaque or telangiectasia in skin images) prior to making the final prediction (e.g., skin disease type), provide valuable insights into the decision-making processes of the model.However, existing concept-based models often overlook the intricate relationships between image sub-regions and treat concepts in isolation, leading to unreliable diagnostic decisions.To overcome these limitations, we propose a Concept-induced Graph Perception (CGP) Model for interpretable diagnosis. CGP probes concept-specific visual features from various image sub-regions and learns the interdependencies between these concepts through neighborhood structural learning and global contextual reasoning, ultimately generating diagnostic predictions based on the weighted importance of different concepts. Experimental results on three public medical datasets demonstrate that CGP mitigates the trade-off between task accuracy and interpretability, while maintaining robustness to real-world concept distortions.", "filename": "2025_0206.pdf", "year": 2025, "institution": "Hunan University", "country": "China", "authors": ["Lei Zhao", "Changjian Chen", "Bin Pu", "Xiaoming Qi", "Fengfeng Peng", "Chunlian Wang", "Kenli Li", "Guanghua Tan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0207", "x": 7.105, "y": 4.274, "title": "Conditional Graph Diffusion with Topological Constraints for Brain Network Generation", "abstract": "The acquisition of structural brain network data is inherently challenging due to high costs of Diffusion Tensor Imaging (DTI) and the complexity of data processing such as tractography. Moreover, medical datasets often exhibit severe class imbalance where the sample size of healthy subjects highly exceeds that of diseased. While recent graph generation models offer a potential solution, its application to brain networks is understudied as they often underestimate preserving topological feature which is an essential biomarker. To address these limitations, we propose a conditional graph diffusion model that ensures high-fidelity graph generation by leveraging persistent homology. Specifically, we introduce a Conditional Graph Diffusion (ConGD) method that utilizes Condition Infused Attention (CIA) module with class and structure conditioning, to enable the targeted synthesis of brain networks, and Topology Aligning (TA) regularization to enforce topological consistency. Experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that our approach provides high-fidelity synthetic brain networks under label conditions, which are further validated for improving predictive performance through downstream graph classification tasks.", "filename": "2025_0207.pdf", "year": 2025, "institution": "Pohang University of Science and Technology", "country": "South Korea", "authors": ["Joonhyuk Park", "Donghyun Lee", "Guorong Wu", "Won Hwa Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0208", "x": 6.73, "y": 3.633, "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification", "abstract": "Understanding the organization of human brain networks has become a central focus in neuroscience, particularly in the study of functional connectivity, which plays a crucial role in diagnosing neurological disorders. Advances in functional magnetic resonance imaging and machine learning techniques have significantly improved brain network analysis. However, traditional machine learning approaches struggle to capture the complex relationships between brain regions, while deep learning methods, particularly Transformer-based models, face computational challenges due to their quadratic complexity in long-sequence modeling. To address these limitations, we propose a Core-Periphery State-Space Model (CP-SSM), an innovative framework for functional connectome classification. Specifically, we introduce Mamba, a selective state-space model with linear complexity, to effectively capture longrange dependencies in functional brain networks. Furthermore, inspired by the core-periphery (CP) organization, a fundamental characteristic of brain networks that enhances efficient information transmission, we design CP-MoE, a CP-guided Mixture-of-Experts that improves the representation learning of brain connectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE and ADNI. Experimental results demonstrate that CP-SSM surpasses Transformer-based models in classification performance while significantly reducing computational complexity. These findings highlight the effectiveness and efficiency of CP-SSM in modeling brain functional connectivity, offering a promising direction for neuroimaging-based neurological disease diagnosis. Our code is available at https://github.com/m1nhengChen/cpssm.", "filename": "2025_0208.pdf", "year": 2025, "institution": "University of Texas at Arlington", "country": "USA", "authors": ["Minheng Chen", "Xiaowei Yu", "Jing Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0209", "x": 6.885, "y": 4.105, "title": "Cross-Modal Brain Graph Transformer via Function-Structure Connectivity Network for Brain Disease Diagnosis", "abstract": "Multi-modal brain networks represent the complex connectivity between different brain regions from both functional and structural perspectives, which is of great significance for brain disease diagnosis. However, existing methods are limited to information fusion in the feature dimension, failing to fully exploit the complementary information between functional and structural connectivity networks. To address these issues, this paper proposes a cross-modal brain graph transformer (CBGT) method for brain disease diagnosis, which also provides an in-depth analysis of coupled function-structure connectivity networks. Specifically, CBGT consists of two main modules: the cross-modal Transformer module enhances the attention mechanism by utilizing structural connectivity features extracted through machine learning methods, capturing long-range dependencies in the cross-modal brain network. The cross-modal topK pooling module combines information from both functional and structural connectivity networks to select significant regions of interest (ROIs) during the reconstruction of the pooled graph, aiming to retain as much effective information as possible. Experiments conducted on the ABIDE and ADNI datasets demonstrate that the proposed method outperforms state-of-the-art approaches. Interpretation analysis reveals that the proposed method can identify multi-modal biomarkers associated with brain diseases.", "filename": "2025_0209.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Jingxi Feng", "Heming Xu", "Junhao Cai", "Yujie Chang", "Dong Zhang", "Shaoyi Du", "Juan Wang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0210", "x": 3.178, "y": 3.637, "title": "DEFUSE-MS: Deformation Field-Guided Spatiotemporal Graph-Based Framework for Multiple Sclerosis New Lesion Detection", "abstract": "Longitudinal magnetic resonance imaging (MRI) is essential for diagnosing and monitoring multiple sclerosis (MS), a chronic central nervous system disorder. Tracking brain lesion evolution over time is essential for predicting MS progression, yet this process is timeconsuming and subject to intra-and interobserver variability. While deep learning models such as convolutional neural networks (CNNs) and vision transformers (ViTs) have been applied to lesion detection, they often struggle to fully capture spatial, structural and temporal relationships. Vision graph neural networks (ViGs) present a novel approach with the potential to improve performance in these tasks by effectively capturing relational and structural information. We introduce DEFUSE-MS, a Deformation Field-Guided Spatiotemporal ViG-Based Framework for detecting MS new T2-weighted lesions. The framework features a Heterogeneous Spatiotemporal Graph Module (HSTGM), which functions as both an encoder and decoder. Evaluated on the MSSEG-II dataset, DEFUSE-MS achieves state-of-the-art performance with a lesion detection F1 score of 0.65, sensitivity (SensL) of 0.74, positive predictive value (PPVL) of 0.65, and a mean segmentation Dice score of 0.55, outperforming the state-of-the-art methods. These results highlight DEFUSE-MS's efficacy in MS new lesion detection. The code is available at https:// github.com/BioMedIA-MBZUAI/DEFUSE-MS.", "filename": "2025_0210.pdf", "year": 2025, "institution": "University of Artificial Intelligence", "country": "UAE", "authors": ["Mostafa Salem", "Salma Hassan", "Vijay Ram Kumar Papineni", "Ayman Elsayed", "Mohammad Yaqub"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0211", "x": 6.845, "y": 3.912, "title": "DHGFormer: Dynamic Hierarchical Graph Transformer for Disorder Brain Disease Diagnosis", "abstract": "The functional brain network exhibits a hierarchical characterized organization, balancing localized specialization with global integration through multi-scale hierarchical connectivity. While graphbased methods have advanced brain network analysis, conventional graph neural networks (GNNs) face interpretational limitations when modeling functional connectivity (FC) that encodes excitatory/inhibitory distinctions, often resorting to oversimplified edge weight transformations. Existing methods usually inadequately represent the brain's hierarchical organization, potentially missing critical information about multi-scale feature interactions. To address these limitations, we propose a novel brain network generation and analysis approach-Dynamic Hierarchical Graph Transformer (DHGFormer). Specifically, our method introduces an FC-inspired dynamic attention mechanism that adaptively encodes brain excitatory/inhibitory connectivity patterns into transformer-based representations, enabling dynamic adjustment of the functional brain network. Furthermore, we design hierarchical GNNs that consider prior functional subnetwork knowledge to capture intra-subnetwork homogeneity and inter-subnetwork heterogeneity, thereby enhancing GNN performance in brain disease diagnosis tasks. Extensive experiments on the ABIDE and ADNI datasets demonstrate that DHGFormer consistently outperforms state-of-the-art methods in diagnosing neurological disorders. The code is available at https://github.com/iMoonLab/ DHGFormer.", "filename": "2025_0211.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Rundong Xue", "Hao Hu", "Zeyu Zhang", "Xiangmin Han", "Juan Wang", "Yue Gao", "Shaoyi Du"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0212", "x": 6.885, "y": 3.609, "title": "Does Connectome Harmonic Analysis Pass the Spin Test?", "abstract": "Connectome harmonic analysis has been proposed as a multimodal approach for studying brain dynamics by decomposing functional MRI signals in a Fourier basis informed by the structural connectome derived from diffusion MRI. In this work we pose the following question: is the propensity of the connectomic Fourier basis to reconstruct resting state fMRI signals truly contingent upon anatomical priors? We present evidence that it is not, by demonstrating that when fewer than n = 100 modes are considered the connectomic eigenbasis obtained through state-of-the-art methodology performs similarly to geometrically transformed versions of that same basis. The main theoretical contribution of this paper is the construction of a regular planar embedding of the left hemisphere's cortical surface, which we use to compute a smoothly parametrised family of cortical transformations which form the basis for an improved Spin Test.", "filename": "2025_0212.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Raphaël Vock", "Antoine Grigis", "Benoît Dufumier", "Edouard Duchesnay"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0213", "x": 6.761, "y": 3.926, "title": "Dual-Stream Multi-band Fusion Network for Dynamic Functional Connectivity Analysis in Brain Disorder Classification", "abstract": "Dynamic functional connectivity (dFC) derived from fMRI captures the temporal dynamics of brain networks, where cross-frequency features provide complementary characterizations for brain disorder classification. Although existing multi-band approaches incorporate sub-band decomposition, they primarily rely on simplistic averaging or fixed-weight strategies, failing to adaptively fuse information across multiple frequency bands. To handle this limitation, we propose a dual-stream multi-band fusion network (DSMFN): 1) The frequency-domain stream employs a sub-band graph encoding-interaction module, where local graph convolution networks (GCNs) extract band-specific topological features, and lightweight convolutions replace computationally intensive attention mechanisms for data-driven band contribution allocation, followed by a global GCN to aggregate cross-band information; 2) The time-domain stream preserves local dynamic properties via residual multi-layer perceptron networks; 3) A feature-temporal dual-dimension cross-attention mechanism jointly models temporal evolution and cross-domain complementarity to adaptively integrate multi-band features with time-varying characteristics. Experiments on two distinct brain disease datasets demonstrate the effectiveness of DSMFN, achieving accuracies of 91.40% for MCI and 70.18% for ASD classification. This study provides an efficient fusion framework for multi-band dynamic brain network analysis, advancing precise diagnosis of brain disorders. Our code is available at https://git hub.com/WuLingBNU/DSMFN.", "filename": "2025_0213.pdf", "year": 2025, "institution": "Beijing Normal University", "country": "China", "authors": ["Ling Wu", "Hexi Li", "Zhengyuan Lyu", "Zhiwei Song", "Hu Yu", "Xiaojuan Guo"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0214", "x": 6.898, "y": 3.841, "title": "Dynamic Function-Structure Connectivity Coupling for Predicting Progression Trajectories in Neurocognitive Decline", "abstract": "Function-structure connectivity (FSC) coupling helps reveal alterations in the interplay between brain functional connectivity (FC) and structural connectivity (SC) caused by neurocognitive decline. Existing studies on FSC coupling typically focus on modeling interactions between static FC and SC features, ignoring temporal dynamics conveyed in functional MRI (fMRI) time series. Additionally, conventional strategies often compute global whole-brain FSC correlation or assess local region-specific FSC correspondences, without capturing complex inter-region dependencies between FC and SC patterns. To this end, we propose a dynamic function-structure connectivity coupling (DFSC) framework to predict progression trajectories in neurocognitive decline with fMRI and diffusion tensor imaging (DTI) data. In DFSC, we first construct static SC and dynamic FC graphs and use graph neural networks (GNNs) for feature learning, yielding new SC and FC embeddings. Based on these embeddings, we construct dynamic local-to-global FSC coupling graphs to capture both region-specific and inter-region dependencies between FC and SC, followed by GNNs to generate dynamic FSC coupling embeddings. These multi-view embeddings are finally fed into a squeeze-excitation readout module and a Transformer for feature fusion and prediction. Experimental results on two datasets with paired fMRI and DTI data from a total of 231 subjects demonstrate that our DFSC outperforms several state-of-the-art methods. With the DFSC, one can identify both discriminative brain regions and between-group FSC coupling difference, facilitating objective quantification of structural and functional brain changes associated with neurocognitive decline.", "filename": "2025_0214.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Qianqian Wang", "Wei Wang", "Hong-Jun Li", "Weili Lin", "Mingxia Liu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0215", "x": 6.524, "y": 3.955, "title": "Edge-Aware Hierarchical Graph Transformer to Decode Brain Arterial Network", "abstract": "Many biomedical data exhibit intrinsic graph-like properties, making graph neural networks (GNNs) widely adopted modeling tools. The brain arterial network (BAN) represents the most complex arterial network in humans, where conventional GNNs struggle to capture critical long-range relationships. Recent graph transformers have enabled modeling of these long-range dependencies through attention mechanisms; however, they face challenges in incorporating hierarchical information, especially when strong anatomical priors exist within the graph structure. While some approaches have attempted to integrate hierarchical information into graph transformers, they primarily focus on node feature aggregation, despite BAN's most clinically significant features residing in edges rather than nodes. To address these limitations, we propose a hierarchical graph transformer (HGT) with edge-aware structural encoding that better incorporates anatomical and multi-scale structural information. Our approach achieves state-of-the-art performance across all 11 tasks. This work lays the foundation for individualized risk assessment that complements traditional systemic risk evaluation methods.", "filename": "2025_0215.pdf", "year": 2025, "institution": "University of Washington", "country": "USA", "authors": ["Kaiyu Zhang", "Li Chen", "Wenjin Liu", "Taewon Kim", "Xin Wang", "Yin Guo", "Zhiwei Tan", "Zhensen Chen", "Angie Tang", "Xihai Zhao", "Thomas S Hatsukami", "Mahmud Mossa-Basha", "Niranjan Balu", "Chun Yuan"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0216", "x": 3.74, "y": 6.759, "title": "Explainable Classifier for Malignant Lymphoma Subtyping via Cell Graph and Image Fusion", "abstract": "Lymphoma subtype classification has a direct impact on treatment and outcomes, necessitating models that are both accurate and explainable. This study proposes a novel explainable Multi-Instance Learning (MIL) framework that identifies subtype-specific Regions of Interest (ROIs) from Whole Slide Images (WSIs) while integrating features of cell distribution and image. Our framework simultaneously addresses three objectives: (1) indicating appropriate ROIs for each subtype, (2) explaining the frequency and spatial distribution of characteristic cell types, and (3) reaching accurate subtyping using both cell distribution and image modalities. Our method fuses cell graph and image features extracted for each patch in a WSI by a Mixture-of-Experts-based approach and classifies subtypes within an MIL framework. Experiments on a dataset of 1,233 WSIs demonstrate that our approach achieves stateof-the-art accuracy compared to ten other methods and provides regionand cell-level explanations that align with a pathologist's perspective.", "filename": "2025_0216.pdf", "year": 2025, "institution": "Institute of Science Tokyo", "country": "Japan", "authors": ["Daiki Nishiyama", "Hiroaki Miyoshi", "Noriaki Hashimoto", "Koichi Ohshima", "Hidekata Hontani", "Ichiro Takeuchi", "Jun Sakuma"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0217", "x": 4.819, "y": 4.084, "title": "Explainable Integrative Bipartite Graph Convolutional Neural Network for Predicting Ejection Fraction in Echocardiography", "abstract": "Ejection fraction (EF) estimation in echocardiography is a key indicator to examine cardiac functions and to determine the optimal treatments for patients prone to heart dysfunctions, such as heart failure. Recently, machine learning has shown promising predictive performance, as diagnostic tools, to estimate EF using echocardiograms. However, most state-of-the-art models have overlooked diversity of phenotypes in echocardiography, derived from patient's demography (e.g., sex and age). In this study, we propose a novel integrative bipartite graph neural network (IBi-GNN) that integrates demographic variables of patients with echocardiograms to improve the EF predictive performance and model interpretability in precision medicine. In the experiments, IBi-GNN significantly reduced the estimation errors compared to the benchmark models, and the significant improvement was statistically assessed. We also show that IBi-GNN is interpretable to identify interaction between the multi-modalities. The interpretation provides comprehensive understanding of the relationships between demographic factors and cardiac structures. The open-source codes are publicly available at https://github.com/datax-lab/IBi-GNN.", "filename": "2025_0217.pdf", "year": 2025, "institution": "Hanyang University", "country": "South Korea", "authors": ["Seungeun Lee", "Jaeyoung Kim", "Kyungtae Kang", "Mingon Kang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0218", "x": 7.155, "y": 4.595, "title": "GPU Accelerated Modeling of Cortical Radial and Tangential Connectivity Changes in Neurodegeneration", "abstract": "The diffusion MRI signals in the human cerebral cortex are strongly associated with neurodegenerative diseases. Although models like NODDI have been extensively used to characterize cortical microstructure degeneration, they fall short in capturing detailed, orientation-specific connectivity changes within the cortex. In this study, we introduce a method to decompose cortical tissue diffusion signal to radial and tangential components. Our approach uses data from multishell diffusion imaging and combines it with anatomical information from brain surfaces. By applying a GPU accelerated probabilistic optimization framework, we can accurately and efficiently estimate these diffusion components while keeping the results smooth and consistent with the cortical anatomy. We test our method on data from HCP subjects and a clinical dataset of patients with autosomal dominant Alzheimer's Disease (ADAD) subjects. Our results demonstrate that the proposed method can more effectively reveal cortical gray matter connectivity changes related to tau pathology than metrics from the NODDI model.", "filename": "2025_0218.pdf", "year": 2025, "institution": "Stevens Neuroimaging and Informatics Institute", "country": "USA", "authors": ["Hongbo Zhang", "Xinyu Nie", "Jiaxin Yue", "Yuan Li", "John Ringman", "Yonggang Shi"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0219", "x": 6.919, "y": 4.103, "title": "Graph Disentanglement Learning for fMRI Analysis: Decoupling Disease, Covariates, and Individual Variability", "abstract": "Functional magnetic resonance imaging (fMRI) is a powerful tool for diagnosing neurological disorders. However, accurately distinguishing disease-related features from confounding covariates (e.g., age, gender, site) and individual variability remains a challenge. To tackle this problem, we propose a novel graph disentanglement learning (GDL) framework that decomposes the latent features from fMRI images into 3 components: disease-related features, covariate-related features, and individual variations. The covariate-related features are learned by aligning 2 subject similarity matrices between the features and the true covariates. The disease-related features are guided by a classification loss. We validate our method on 3 fMRI datasets: ADHD-200, schizophrenia (SCZ), and Presbycusis. The method outperforms existing approaches by an average of 0.5%, 1.7%, and 2.1% in accuracy on the 3 datasets respectively. Ablation studies confirm that our model is robust to hyperparameter selection. The disease-associated regions identified by our model align with established clinical findings. These results suggest that GDL is a promising tool for fMRI-based disease diagnosis and biomarker discovery. The code is publicly available at https://github.com/perpetualmachine/ GDL MICCAI.", "filename": "2025_0219.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University School of Medicine", "country": "China", "authors": ["Shengjie Zhang", "Zhuangzhuang Jiang", "Xin Shen", "Ziqi Yu", "Xiang Chen", "Xiao-Yong Zhang", "Yuan Zhou"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0220", "x": 3.7, "y": 6.531, "title": "Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading", "abstract": "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency. The code is available: https://github.com/NabaviLab/IRM- GLAT.", "filename": "2025_0220.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Masum Shah Junayed", "John Derek Van Vessem", "Qian Wan", "Gahie Nam", "Sheida Nabavi"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0221", "x": 3.117, "y": 3.791, "title": "Graph-PAVNet: A Graph-Based Learning Framework for Pulmonary Artery and Vein Separation Using Multimodal Feature Sampling", "abstract": "Pulmonary artery-vein separation is critical for clinical diagnosis and treatment planning. However, existing pixel-or voxel-based methods often produce fragmented predictions, significantly reducing clinical confidence. To address above problems, we propose Graph-PAVNet, a graph structure learning framework designed for PA/PV separation. First, our Light Vessel Structured Modelling (LVSM) module constructs a topology-aware vascular graph by leveraging the inherent structural and semantic relationships within the vascular network. LVSM shifts from traditional voxel-level predictions to topology-based branchlevel inference, effectively resolving prediction discontinuity. However, it is challenging for a single graph to do the separation task. Due to this issue, we propose the Modal Feature Sampling (MFS) module. MFS enriches node features by constructing a hybrid Real-Virtual (RV) feature matrix that integrates multi-source information. It also employs a dynamic feature weighting mechanism to achieve cross-modal complementarity, overcoming the challenges posed by modal discrepancies. For hierarchical inference, the Hierarchical Graph Attention Network (HGAT) stratifies nodes by vascular generation order (main to peripheral branches) and employs hierarchical masking to enforce structured inter-layer propagation. At last, we introduce a novel metric: Branch Misprediction Coefficient (BMC) to better evaluate the clinical relevance and branch inconsistency. Experimental results show that our method outperforms existing approaches in both quantitative accuracy and clinical interpretability, offering a new paradigm for pulmonary artery-vein separation.", "filename": "2025_0221.pdf", "year": 2025, "institution": "Northeastern University", "country": "China", "authors": ["Qingya Li", "Ye Yuan", "Lu Liu", "Nan Bao", "Lisheng Xu", "Wenjun Tan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0222", "x": 6.857, "y": 3.956, "title": "Heterogeneous Masked Attention-Guided Path Convolution for Functional Brain Network Analysis", "abstract": "Brain functional connectivity analysis plays a crucial role in the computer-aided diagnosis of brain disorders. The brain organization is a heterogeneous structure with distinct functional divisions. However, current heterogeneous algorithms often introduce excessive parameters while characterizing heterogeneous relationships, leading to redundancy and overfitting. To address these issues, we propose the Heterogeneous Masked Attention-Guided Path Convolution (HM-AGPC) for functional brain network analysis. The HM-AGPC introduces a heterogeneous masked attention generation mechanism that preserves valuable heterogeneous relationships while minimizing redundant interactions and highlighting crucial functional connections. Moreover, the framework incorporates an attention-guided path convolution strategy, which leverages attention weights to guide the convolution kernel in focusing on the most salient features and pathways. This approach improves model performance without directly introducing extra parameters, thereby enhancing feature learning efficiency. We evaluate HM-AGPC on the ABIDE dataset using ten-fold cross-validation, where it demonstrates superior performance in the disease diagnosis task compared to state-of-the-art methods. Additionally, the framework demonstrates high interpretability, making it a promising tool for computer-aided diagnosis and the identification of potential biomarkers.", "filename": "2025_0222.pdf", "year": 2025, "institution": "South China University of Technology", "country": "China", "authors": ["Jiakun Xu", "Xin Zhang", "Tong Xiong", "Shengxian Chen", "Xiaofen Xing", "Jindou Hao", "Xiangmin Xu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0223", "x": 6.618, "y": 3.472, "title": "Hierarchical Characterization of Brain Dynamics via State Space-Based Vector Quantization", "abstract": "Understanding brain dynamics through functional Magnetic Resonance Imaging (fMRI) remains a fundamental challenge in neuroscience, particularly in capturing how the brain transitions between various functional states. Recently, metastability, which refers to temporarily stable brain states, has offered a promising paradigm to quantify complex brain signals into interpretable, discretized representations. In particular, compared to cluster-based machine learning approaches, tokenization approaches leveraging vector quantization have shown promise in representation learning with powerful reconstruction and predictive capabilities. However, most existing methods ignore brain transition dependencies and lack a quantification of brain dynamics into representative and stable embeddings. In this study, we propose a Hierarchical State space-based Tokenization network, termed HST, which quantizes brain states and transitions in a hierarchical structure based on a state spacebased model. We introduce a refined clustered Vector-Quantization Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback and clustering to improve quantization performance while facilitating metastability with representative and stable token representations. We validate our HST on two public fMRI datasets, demonstrating its effectiveness in quantifying the hierarchical dynamics of the brain and its potential in disease diagnosis and reconstruction performance. Our method offers a promising framework for the characterization of brain dynamics, facilitating the analysis of metastability.", "filename": "2025_0223.pdf", "year": 2025, "institution": "University Hospital Tübingen", "country": "Germany", "authors": ["Yanwu Yang", "Thomas Wolfers"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0224", "x": 7.002, "y": 3.816, "title": "Hyperbolic Kernel GCN with Structure-Function Connectivity Coupling for Neurocognitive Impairment Analysis", "abstract": "Diffusion tensor imaging (DTI) and functional MRI (fMRI) provide complementary views of the brain by revealing the physical structure connectivity (SC) between brain regions and functional connectivity (FC) between those regions during neural processing. Previous evidence has shown that fusing the two modalities facilitates the identification of abnormal connectivity associated with neurocognitive disorders. However, existing fusion approaches are generally performed in Euclidean space and thus cannot effectively capture the intrinsic hierarchical organization of structural/functional brain networks. To this end, we propose a novel hyperbolic kernel graph convolutional network with SC-FC Coupling (HKC) for neurocognitive impairment analysis. The HKC consists of a hyperbolic kernel graph convolutional network for extracting localto-global features from DTI and fMRI, an SC-FC coupling module that models global SC-FC interactions based on encoded DTI and fMRI features, and a hyperbolic neural network predictor for classification. Our HKC captures both local and global dependencies among structurally and functionally connected brain regions while preserving the hierarchical organization of brain networks. We evaluate HKC on paired DTI and fMRI data from 68 individuals with HIV-associated asymptomatic neurocognitive impairment and 69 healthy controls, with experimental results suggesting its superiority over state-of-the-art methods. Additionally, HKC identifies key SC-FC patterns in ANI, highlighting the visual network and fronto-cerebellar connections as critical biomarkers.", "filename": "2025_0224.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Meimei Yang", "Yongheng Sun", "Qianqian Wang", "Wei Wang", "Hong-Jun Li", "Mingxia Liu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0225", "x": 6.682, "y": 4.084, "title": "Integrating Meta-analysis in Multi-modal Brain Studies with Graph-Based Attention Transformer", "abstract": "Multi-modal neuroimaging studies are essential for exploring various brain disorders; however, they are typically limited in sample size owing to the cost of image acquisition. Meta-analysis is an underutilized method that integrates the findings from multiple studies derived from large samples to assist individual studies. Neuroimaging studies are increasingly adopting transformer architecture for network analysis; however, they tend to overlook local brain networks. To address these gaps, we propose the Meta-analysis Enhanced Graph Attention TransFormer (MEGATF), a novel method for performing multimodal brain analysis built on a graph transformer framework aided with metaanalysis information derived from NeuroSynth. Our method adapts a graph neural network with a transformer attention mechanism that favors local networks and multimodal interactions using PET or cortical thickness. Our method achieved a state-of-the-art classification performance on mild cognitive impairment and attention-deficit/hyperactivity disorder datasets, distinguishing individuals with brain disorders from controls. Furthermore, it identified disease-affected brain regions and associated cognitive decoding that aligned with existing findings, thereby enhancing its interpretability. Our code is at https://github.com/gudt ls17/MEGATF.", "filename": "2025_0225.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "South Korea", "authors": ["Hyoungshin Choi", "Sunghun Kim", "Jong-Eun Lee", "Bo-Yong Park", "Hyunjin Park"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0226", "x": 4.681, "y": 5.144, "title": "Knowledge-Enhanced Complementary Information Fusion with Temporal Heterogeneous Graph Learning for Disease Prediction", "abstract": "Disease prediction based on multimodal data is a critical yet challenging task in healthcare, especially in intensive care units (ICUs) where patients present complex clinical trajectories with multiple admissions and comorbidities. Current multimodal learning approaches lack effective modeling of cross-modal complementary information, which leads to suboptimal feature interactions. Besides, traditional methods that incorporate external knowledge graphs (KGs) often introduce noise and computational complexity, due to the use of all one-hop neighbors within the KGs. To address these challenges, we propose Knowledge-Enhanced Complementary Information Fusion with temporal heterogeneous graph learning (KCIF) for patient modeling. KCIF introduces a temporal heterogeneous admission graph (THAG) that integrates KGs to capture semantic and temporal dependencies across admissions. It further employs a complementary information fusion mechanism to leverage mutual enhancement between lab tests and medical events. Extensive experiments on the MIMIC-III/IV benchmarks demonstrate that KCIF consistently outperforms baselines, achieving improvements of over 2.5%-6.0% in w-F1 score and 1.7%-4.5% in R@20 across multiple ICU disease prediction. The code is available at https://github.com/Boaz- SCUT/KCIF.", "filename": "2025_0226.pdf", "year": 2025, "institution": "Chinese Academy of Sciences", "country": "China", "authors": ["Zongbao Yang", "Shihuan He", "Zhichen Chen", "Hao Zhang", "Ruxin Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0227", "x": 3.678, "y": 6.784, "title": "Knowledge-Guided Multi-scale Graph Mamba for Whole Slide Image Classification", "abstract": "Whole Slide Images (WSIs) are crucial for cancer diagnosis in digital pathology. WSI classification typically relies on Multiple Instance Learning (MIL). Existing MIL methods use attention mechanisms to highlight key instances but struggle to capture instance interactions. Although Transformers, State Space Models (SSMs), and Graph Neural Networks (GNNs) have made progress in solving this problem, they still face two main issues: (1) insufficient guidance from class-related information in modeling instance relationships, and (2) inadequate interaction between slides at different magnifications. To address these issues, we propose Knowledge-guided Multi-scale Graph Mamba (KMG-Mamba), which incorporates a Knowledge-guided Graph Representation (KGR) method for class-related guidance and Cross-scale Knowledge Interaction Mamba (CKIM) to facilitate effective cross-magnification information exchange. Experimental results on three public datasets show KMG-Mamba outperforms current MIL methods in WSI classification.", "filename": "2025_0227.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Minghong Duan", "Zhiwei Yang", "Yingfan Ma", "Manning Wang", "Zhijian Song"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0228", "x": 7.138, "y": 4.409, "title": "LG-DBGL: Lateralization-Guided Dissociative Brain Graph Learning for Alzheimer’s Disease Identification", "abstract": "Thanks to advances in neuroimaging, graph neural networks (GNNs) have emerged as a powerful tool for learning brain graph representations to identify Alzheimer's Disease (AD). However, existing methods often overlook the brain's hemispherical lateralization, enforcing homogeneous information propagation between hemispheres, which limits their learning capabilities. In this study, we propose a novel dissociative brain graph learning framework (LG-DBGL) guided by brain lateralization to enhance AD identification. Specifically, the Lateralized Decoupling (LD) module partitions brain networks into left/right hemispheric and cross-hemispheric sub-networks. The Dissociative Graph Encoder (DGE) module then independently learns representations for each subnetwork, preserving lateralized functional features and avoiding feature confusion. Finally, the Multi-Source Fusion Mechanism (MSFM) dynamically quantifies the contribution of each sub-network to AD-related pathological features, enabling lateralization-guided multi-source feature fusion. Comprehensive experiments conducted on a real-world dataset demonstrate the effectiveness of our LG-DBGL. Our code is publicly available at https://github.com/ilove-gh/LG-DBGL.", "filename": "2025_0228.pdf", "year": 2025, "institution": "Inner Mongolia University", "country": "China", "authors": ["Jiazhen Ye", "Manman Yuan", "Junlin Li", "Weiming Jia", "Jiacheng Wang", "Jiapei Li"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0229", "x": 6.851, "y": 4.192, "title": "Meta-analysis Guided Multi-task Graph Transformer Network for Diagnosis of Neurological Disease and Cognitive Deficits", "abstract": "Neurological diseases, such as schizophrenia and attention deficit hyperactivity disorder (ADHD), alter functional connectivity (FC) and are often accompanied by cognitive deficits. Leveraging shared neural mechanisms underlying both neurological disease and cognitive deficits can enhance diagnostic accuracy. However, due to the complex neural mechanisms of these conditions, diagnosing them based on FC alone still presents challenges in terms of accuracy and biomarker reliability. To address these challenges, we designed a meta-analysis guided multi-task graph transformer network to simultaneously predict neurological disease and cognitive deficits and examine alterations in brain FC associated with these conditions. The framework employs a graph transformer method as the encoder and integrates a joint attention mechanism to capture shared disease-cognition features while utilizing saliency pooling to extract saliency weights for each task. To enhance the reliability of saliency weights, we incorporate meta-analysis guidance that aggregates data from 470 functional studies in the BrainMap database. Then, we establish reference probability maps for brain activations associated with neurological diseases and cognitive deficits using a Naive Bayes classifier. The saliency weights learned from saliency pooling are then constrained to align with these references using Pearson correlation. Experiments on the COBRE and ADHD-200 datasets indicate that our proposed method outperforms state-of-the-art multi-task learning models in classifying schizophrenia and ADHD, as well as in predicting their related cognitive deficits. Moreover, the biomarkers extracted from our models exhibit biologically meaningful patterns.", "filename": "2025_0229.pdf", "year": 2025, "institution": "Nanyang Technological University", "country": "Singapore", "authors": ["Jing Xia", "Yi Hao Chan", "Jagath C Rajapakse"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0230", "x": 6.298, "y": 3.629, "title": "MindLink: Subject-Agnostic Cross-Subject Brain Decoding Framework", "abstract": "Brain decoding is a pivotal topic in neuroscience, aiming to reconstruct stimuli (e.g., image) from brain activity (e.g., fMRI). However, existing methods rely on subject-specific modules and flatten 3D voxel grids, limiting generalization and discarding spatial information. To address these issues, we propose MindLink, a scalable cross-subject brain decoding framework designed to link multiple subjects into a single model by extracting subject-invariant features while preserving the spatial structure of 3D fMRI data. This is achieved by parcellating 3D fMRI into standardized cubic patches processed by a 3D Vision Transformer for informative representations. Domain adversarial training enhances crosssubject generalizability by extracting subject-agnostic features within a single model structure. We also introduce a two-level alignment strategy that effectively bridges fMRI and stimuli image embeddings through instance-level consistency and flexible token-level matching. MindLink achieves comparable or even better performance over state-of-the-art methods on the NSD dataset with a constant parameter size across subjects and demonstrates strong adaptability to new subject.", "filename": "2025_0230.pdf", "year": 2025, "institution": "Pohang University of Science and Technology (POSTECH)", "country": "South Korea", "authors": ["Sungyoon Jung", "Donghyun Lee", "Won Hwa Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0231", "x": 7.017, "y": 4.305, "title": "MMBNA: Masked Multiview Brain Network Analysis via Disentangling for Alzheimer’s Early Diagnosis with fMRI", "abstract": "Alzheimer's disease (AD), as a progressive neurodegenerative disorder, poses a growing global health threat, making early diagnosis imperative. Multiview brain network (BN) analysis from restingstate functional MRI (rs-fMRI) has emerged as a promising approach, where brain regions and their interactions are modeled as nodes and edges across complementary views. However, existing methods have limitations. First, they rely on single-measure BNs with fixed nodes and edges, potentially insufficient for capturing complex brain interactions. Second, they lack effective separation of view-consistent and viewspecific representations, leading to redundancy and reduced generalizability. To address these challenges, we propose a novel Masked Multiview Brain Network Analysis (MMBNA) framework, integrating multimeasure BNs construction, random masking, and disentangled representation learning. Specifically, we first construct multiview BNs via multi-measure connectivity (capturing full/partial/nonlinear correlations) and multi-granularity masking (at node/edge/feature levels), enriching spatio-temp-oral-topological diversity while preserving semantic similarity. Subsequently, we perform the view-consistent representation learning via cross-view masking, and then a disentangling mechanism is introduced to learn a purer view-specific representation to filter out the redundancy from view-consistent representations, resulting in more compact multiview brain representations. Experiments on the ADNI2 subset of the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, demonstrate the effectiveness of the proposed method, achieving significant improvements in diagnostic accuracy and interpretability compared to state-of-the-art approaches.", "filename": "2025_0231.pdf", "year": 2025, "institution": "Shandong Jianzhu University", "country": "China", "authors": ["Dequan Meng", "Jie Guo", "Junze Wang", "Xiaoming Xi", "Lishan Qiao", "Limei Zhang", "Mingxia Liu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0232", "x": 4.438, "y": 6.584, "title": "MoST-IG: Morphology-Guided Spatial Transcriptomics Integration via Visual-Genomic Graph Optimal Transport", "abstract": "Spatial transcriptomics (ST) is crucial for understanding cellular heterogeneity and tissue organization. However, integrating spatial transcriptomics across multiple slices remains challenging for downstream analyses, as ST slices may exhibit significant batch effects. Current methods mostly depend on forced integration via contrastive learning, which may ignore the inherent biological heterogeneity, thus impacting the performance of downstream analyses. To address these challenges, we introduce MoST-IG, a multimodal framework for morphology-guided multi-slice ST integration. MoST-IG comprises two key components: (1) Cross-modal alignment between histology prior and ST. We integrate histological patterns derived from the pathological foundation model with ST using our proposed Visual-Genomic Graph Optimal Transport (VG-GOT) module. This visual-genomic alignment preserves biological heterogeneity through morphology-guided regularization while enriching the spatial context of ST data with morphological features to provide a more discriminative representation and enhance downstream performance. (2) Integration of Multi ST-Slices. A multi ST-slices Contrastive Learning (mST-CL) module is designed via two complementary triplet losses-one for both inter-slice and intra-slice cell type mapping. Experiments show that MoST-IG outperforms leading methods in both cancer grading and detection, as well as tissue structure clustering, while better preserving tissue landmarks in multi-slice ST integration. The code is available at https://github.com/HKU-MedAI/MoST-IG.", "filename": "2025_0232.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Liting Yu", "Tao Ma", "Weiqin Zhao", "Zhuo Liang", "Lequan Yu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0233", "x": 5.527, "y": 4.552, "title": "Multi-modal Graph-Based Machine Learning for Predicting Surgical Outcome in Epilepsy Patients", "abstract": "Reliable prediction of seizure outcomes after surgical intervention before ablative surgery could play a critical role for tailoring epilepsy treatment. However, for diverse patient populations, accurate and personalized predictions remain challenging with traditional methods. Current methods rely heavily on clinical expertise and experience, and data driven tools may help in supporting clinicians to make more informed surgical decisions. This study presents a novel deep learning-based spatio-temporal graph neural network (ST-GNN) model to predict reduction in seizure frequency utilizing high-quality stereo electroencephalography (sEEG) and structural magnetic resonance imaging (MRI) data. sEEG and MRI data are curated from patients with pharma-coresistant refractory epilepsy and suspected wide/complex seizure networks or multifocal epilepsy. A total of 10 pediatric patients with sEEG contacts in the thalamus were considered, where data from multiple ictal events was used to train the model. Our ST-GNN model integrates local and global connectivity using graph convolutions with multi-scale attention mechanisms to capture patterns between difficult-to-study regions such as the thalamus and cortical/subcortical regions, both from MRI and sEEG. The model achieved an accuracy of 90.4%, and 75.4% in predicting seizure outcomes for seizure-wise and patient-wise prediction respectively. Edge-level connectivity analysis highlighted the thalamus and mid insula regions as key regions. Our findings underscore the potential of new connectivity-based deep learning models leveraging multimodal data for enhancing the prediction of seizure outcomes and tailoring treatment planning for epilepsy. Our multi-modal approach can help inform AI-assisted personalized epilepsy treatment planning. Code is available on our GitHub.", "filename": "2025_0233.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Artur Arturi Aharonyan", "Syeda Abeera Amir", "Nunthasiri Wittayanakorn", "Marius George Linguraru", "Chima Oluigbo", "Syed Muhammad Anwar"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0234", "x": 4.493, "y": 5.989, "title": "Multimodal Hypergraph Guide Learning for Non-invasive CcRCC Survival Prediction", "abstract": "Multimodal medical imaging provides critical data for the early diagnosis and clinical management of clear cell renal cell carcinoma (ccRCC). However, early prediction primarily relies on computed tomography (CT), while whole-slide images (WSI) are often unavailable. Consequently, developing a model that can be trained on multimodal data and make predictions using single-modality data is essential. In this paper, we propose a multimodal hypergraph guide learning framework for non-invasive ccRCC survival prediction. First, we propose a patch-aware global hypergraph computation (PAGHC) module, including a hypergraph diffusion step for capturing correlational structure information and a control step to generate stable WSI semantic embeddings. These WSI semantic embeddings are then used to guide a cross-view fusion method, forming the hypergraph WSI-guided cross-view fusion (HWCVF) to generate CT semantic embeddings, improving single-modality performance in inference. We validate our proposed method on three ccRCC datasets, and quantitative results demonstrate a significant improvement in Cindex, outperforming state-of-the-art methods. The source code is available in https://github.com/iMoonLab/PAGHC.", "filename": "2025_0234.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Jielong Yan", "Xiangmin Han", "Jieyi Zhao", "Yue Gao"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0235", "x": 6.823, "y": 3.617, "title": "Multi-sensory Cognitive Computing for Learning Population-Level Brain Connectivity", "abstract": "The generation of connectional brain templates (CBTs) has recently garnered significant attention for its potential to identify unique connectivity patterns shared across individuals. However, existing methods for CBT learning such as conventional machine learning and graph neural networks (GNNs) are hindered by several limitations. These include: (i) poor interpretability due to their black-box nature, (ii) high computational cost, and (iii) an exclusive focus on structure and topology, overlooking the cognitive capacity of the generated CBT. To address these challenges, we introduce mCOCO (multi-sensory COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC) to learn population-level functional CBT from BOLD (Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow for tracking state changes over time, enhancing interpretability and enabling the modeling of brainlike dynamics, as demonstrated in prior literature. By integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO captures not only structure and topology but also how brain regions process information and adapt to cognitive tasks such as sensory processing, all in a computationally efficient manner. Our mCOCO framework consists of two phases: (1) mapping BOLD signals into the reservoir to derive individual functional connectomes, which are then aggregated into a group-level CBT-an approach, to the best of our knowledge, not previously explored in functional connectivity studies -and (2) incorporating multisensory inputs through a cognitive reservoir, endowing the CBT with cognitive traits. Extensive evaluations show that our mCOCO-based template significantly outperforms GNN-based CBT in terms of centeredness, discriminativeness, topological soundness, and multi-sensory memory retention. Our source code is available at https://github.com/basiralab/mCOCO.", "filename": "2025_0235.pdf", "year": 2025, "institution": "University of Sousse", "country": "Tunisia", "authors": ["Mayssa Soussia", "Mohamed Ali Mahjoub", "Islem Rekik"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0236", "x": 6.981, "y": 4.166, "title": "Multi-view Graph Contrastive Learning with Dynamic Self-aware and Cross-Sample Topology Augmentation for Brain Disorder Diagnosis", "abstract": "Resting-state functional MRI (rs-fMRI) has been increasingly employed to aid in brain disorder diagnosis and reveal the pathological mechanisms underlying neurological diseases. However, clinical applications of current automated diagnosing techniques remain constrained by the complexity of brain topology structures and the high costs associated with expert-derived biomarkers. Recent advancements in research have shown that Graph Contrastive Learning (GCL) holds substantial potential for overcoming these challenges and improving diagnosis accuracy. Nevertheless, existing GCL-based methods predominantly generate a static augmented brain network during graph augmentation and primarily focus on the semantic differences between the original and augmented views. To address above issues, we introduce MGCL-DA (Multiview Graph Contrastive Learning with Dynamic Self-aware and Crosssample Topology Augmentation), a novel framework aimed at generating two complementary augmentations of brain networks that account for both individual-specific and inter-subject functional heterogeneity, as well as dynamically regulating the update of augmented views to optimize the transmission of discriminative features. Furthermore, we incorporate multi-view graph contrastive learning with min-max constraints, applying distinct contrastive constraints based on specific augmentation semantics to enable pairwise comparisons between the original network and its two augmented views. Extensive experiments on the MDD dataset demonstrate the superior classification performance of MGCL-DA over several state-of-the-arts.", "filename": "2025_0236.pdf", "year": 2025, "institution": "Nanjing Forestry University", "country": "China", "authors": ["Hao Zhang", "Xiaoyun Liu", "Shuo Huang", "Yonggui Yuan", "Daoqiang Zhang", "Li Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0237", "x": 6.426, "y": 3.831, "title": "PMIL: Prompt Enhanced Multimodal Integrative Analysis of fMRI Combining Functional Connectivity and Temporal Latency", "abstract": "Functional connectivity (FC) analysis is the primary approach for studying functional magnetic resonance imaging (fMRI) data, focusing on the spatial patterns of brain activity. However, this method often neglects the temporal dynamics inherent in the timeseries nature of fMRI data, such as latency structure and intrinsic neural timescales (INT). These temporal features provide complementary insights into brain signals, capturing signal propagation and neural persistence information that FC alone cannot reveal. To address this limitation, we introduce Prompt enhanced multimodal integrative analysis (PMIL), a multimodal framework built on a transformer architecture that integrates latency structure and INT with conventional FC, enabling a more comprehensive analysis of fMRI data. Additionally, PMIL leverages text prompts within a state-of-the-art vision-language model to enhance the integration of INT with latency structure and FC. Our framework achieves state-of-the-art performance on an autism dataset, effectively distinguishing autistic patients from neurotypical individuals. Furthermore, PMIL identified disease-affected brain regions that align with findings from existing research, thereby enhancing its interpretability. The code for PMIL is publicly available at https://github.com/gudtls17/ PMIL.", "filename": "2025_0237.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "South Korea", "authors": ["Hyoungshin Choi", "Jonghun Kim", "Jiwon Chung", "Bo-Yong Park", "Hyunjin Park"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0238", "x": 7.065, "y": 4.334, "title": "PRGNN: Pyramidal Region Graph Neural Network for Region-Based Brain PET Classification", "abstract": "Brain positron emission tomography (PET) has been widely used for the diagnosis of various neurodegenerative diseases. To assist physicians, convolutional neural networks (CNNs) and transformers have been explored for prediction of diseases based on brain PET images. While these models show promising performance, they are designed to process the entire image, which facilitates shortcut learning by extracting irrelevant features. To alleviate shortcut learning, we observe that brain images share the same structure, and regions of interest (ROIs) can be defined for relevant regions. In this regard, we propose Pyramidal Region Graph Neural Network (PRGNN), which employs a 3D convolutional backbone to learn multi-level feature representations and constructs nodes that correspond to anatomical ROIs. Using ROI-based node embeddings, PRGNN extracts metabolic patterns in functionally relevant regions and performs explicit inter-regional reasoning. We evaluate PRGNN on classifying 18F-fluorodeoxyglucose (FDG) and amyloid PET, outperforming models based on CNN, transformer, and GNN. Moreover, interpretability analyses highlight disease-relevant regions that align with clinical observations, demonstrating PRGNN's potential for improving diagnostic performance and reliability. Code is available at https://github.com/Treeboy2762/PRGNN.", "filename": "2025_0238.pdf", "year": 2025, "institution": "Yonsei University", "country": "Republic of Korea", "authors": ["Daesung Kim", "Seungbeom Seo", "Boosung Kim", "Kyobin Choo", "Youngjun Jun", "Mijin Yun"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0239", "x": 4.316, "y": 6.281, "title": "Probabilistic Integration of Renal Cancer Radiology and Pathology Using Graph Neural Networks", "abstract": "Kidney tumors can be highly heterogeneous from the microscopic to the macroscopic scale. To address this, we propose a sparsityinformed probabilistic integration of radiomics and pathomics for kidney cancer analysis. We construct radiology and pathology graphs to model spatial correlations, then use a probabilistic method and graph neural networks to identify biomarkers and aggregate spatial features. Our validation shows that this integrated approach significantly outperforms traditional methods in kidney survival analysis, achieving a notable improvement of 0.084 in the concordance index, enabling better prognostic assessments for kidney cancer patients. The source code has been released by https://github.com/shangqigao/RadioPath.", "filename": "2025_0239.pdf", "year": 2025, "institution": "University of Cambridge", "country": "UK", "authors": ["Shangqi Gao", "Shangde Gao", "Ines Machado", "Mireia Crispin-Ortuzar"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0240", "x": 6.197, "y": 3.476, "title": "pyOpenNFT: An Open-Source Python Framework for ML-Based Real-Time fMRI and EEG-fMRI Neurofeedback", "abstract": "Real-time functional magnetic resonance imaging (rt-fMRI) is a powerful neuroimaging tool for monitoring brain activity and neurofeedback (NF) applications with promising therapeutic potential in psychiatric and neurological disorders. However, technical implementation of NF using acquired real-time fMRI and/or predicted real-time fMRI signals based on electroencephalographic (EEG) records remains restrictive and often lacks reproducibility. Here, a fully Python-based pyOpenNFT framework was designed for greater flexibility, modularity, and real-time processing efficiency. Its functionality was also extended with a ML-based prediction server for the fMRI NF signal using processed EEG records. The framework streamlines fMRI data acquisition and/or EEG-based prediction, NF signal estimation, and quality assessment (rtQA) without necessarily requiring a GUI. The FastAPI-based implementation for an EEG-based predictor integrates a Lab Streaming Layer (LSL) interface for processed EEG records and delivers real-time predictions of fMRI time-series for target brain regions. The system supports the visualization of additional NF sources by querying a RESTful interface, facilitating interoperability with external applications. Efficient real-time processing is achieved through parallelized workflows, optimized data handling, and shared memory buffers for seamless exchange of brain volumes, time-series data, and rtQA metrics. With open-source code available on GitHub, pyOpenNFT advances multimodal real-time neuroimaging and extends the platform for scientific, clinical and educational applications.", "filename": "2025_0240.pdf", "year": 2025, "institution": "Skolkovo Institute of Science and Technology", "country": "Russia", "authors": ["Ekaterina Antipushina", "Nikita Davydov", "Riccardo De Feo", "Evgeny Prilepin", "Artem Nikonorov", "Yury Koush"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0241", "x": 7.139, "y": 4.606, "title": "Robust Topographical Representation for Longitudinal Propagation of Tau Pathology", "abstract": "Tau pathology is a hallmark of Alzheimer's disease (AD), and longitudinal tau positron emission tomography (PET) provides valuable insights into disease progression. However, the integration of tau PET data into computational models remains limited by challenges in encoding topographical information and ensuring longitudinal consistency. Existing biomarker-based representations often lack spatial flexibility and fail to account for covariance between brain regions. Additionally, traditional approaches often treat longitudinal scans as independent observations, neglecting temporal coherence. To address these limitations, we propose a novel Multiresolutional Reeb Graph representation that encodes the spatiotemporal propagation of tau topographical information. Our method constructs Reeb graphs to capture tau topography at a static time point and extends them into a multiresolutional framework to model disease evolution. We introduce a topology-based measurement for quantifying pathology spatial distribution similarity, and a severity interleaving distance for robust longitudinal staging. The efficiency of the proposed representation is validated in two downstream tasks: an integrated subtyping and staging system, and the longitudinal pathology prediction. The promising results compared with the current methods demonstrate the great potential of the proposed representation to enhancing the application of longitudinal tau PET data, and offering a reliable approach for studying AD progression.", "filename": "2025_0241.pdf", "year": 2025, "institution": "University of Southern California (USC)", "country": "USA", "authors": ["Jiaxin Yue", "Jianwei Zhang", "Xinkai Wang", "Yonggang Shi"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0242", "x": 2.841, "y": 3.161, "title": "Semantically Consistent Discrete Diffusion for 3D Biological Graph Modeling", "abstract": "3D spatial graphs play a crucial role in biological and clinical research by modeling anatomical networks such as blood vessels, neurons, and airways. However, generating 3D biological graphs while maintaining anatomical validity remains challenging, a key limitation of existing diffusion-based methods. In this work, we propose a novel 3D biological graph generation method that adheres to structural and semantic plausibility conditions. We achieve this by using a novel projection operator during sampling that stochastically fixes inconsistencies. Further, we adopt a superior edge-deletion-based noising procedure suitable for sparse biological graphs. Our method demonstrates superior performance on two real-world datasets, human circle of Willis and lung airways, compared to previous approaches. Importantly, we demonstrate that the generated samples significantly enhance downstream graph labeling performance. Furthermore, we show that our generative model is a reasonable out-of-the-box link predictior.", "filename": "2025_0242.pdf", "year": 2025, "institution": "University of Zurich", "country": "Switzerland", "authors": ["Chinmay Prabhakar", "Suprosanna Shit", "Tamaz Amiranashvili", "Hongwei Bran Li", "Bjoern Menze"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0243", "x": 5.872, "y": 3.832, "title": "ShareLink: Neuro-Inspired EEG-Based Cross-Subject Emotion Recognition via Shared Bi-hemisphere", "abstract": "Emotion recognition plays a pivotal role in human-computer interaction by enabling machines to perceive and adapt to human affective states. While neuroimaging studies [ 15,20] reveal significant functional lateralization between the left and right cerebral hemispheres during emotional processing, existing EEG-based emotion recognition methods face two critical challenges: (1) difficulty in aligning crosshemispheric semantic features, and (2) limited generalizability across subjects and scenarios. To address these issues, we propose ShareLink, a novel EEG-based framework with Shared Cross-Hemispheric Structures. Our approach introduces three key innovative modules: (1) the Dynamic Shared Hemispheric Structure (DSHS) enforces non-Euclidean hemispheric structure constraints by sharing learnable adjacency matrix parameters across the bi-hemispheres, thereby effectively aligning semantic representations and extracting more discriminative hemispheric asymmetry features; (2) the Cross-Hemisphere Attention (CHA) shares similarity matrix between the hemispheres to establish dynamic interhemispheric links, enhancing the model's ability to capture interaction information while reducing parameters and mitigating overfitting risks;(3) the Shared Hemispheres Mixture-of-Experts (SHMoE) leverages multiple expert modules to abstract representations into a finite set of characteristics and employs a shared expert set to map bi-hemispheres features into a unified space, ensuring consistent and generalizable leftright hemisphere representations. Evaluated on SEED and SEED-IV datasets under cross-subject paradigms, ShareLink achieves accuracies of 80.61%. ±6.16% and 63.33%. ±8.29%, demonstrating superior crossdomain generalization. This work provides new insights into neurophysiologically inspired computational models for emotion recognition. The codes are available at: https://github.com/Huangzx1023/ShareLink.", "filename": "2025_0243.pdf", "year": 2025, "institution": "Chinese Academy of Sciences", "country": "China", "authors": ["Zixuan Huang", "Lingyao Kong", "Licheng Ao", "Shiyi Yao", "An Xiang", "Fen Miao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0244", "x": 6.917, "y": 4.352, "title": "SPromptGL: Semantic Prompt Guided Graph Learning for Multi-modal Brain Disease", "abstract": "Multi-modal brain disease diagnosis provides a more robust and comprehensive prediction of diverse diseases by integrating medical data from different modalities. However, recent methods generally fail to account for the modality-specific discriminant regions in semantic information, which causes models to focus on non-lesion areas while neglecting the actual lesion regions. To address this issue, we propose Semantic Prompt-guided Graph Learning (SPromptGL), a novel approach for multi-modal disease prediction that captures the discriminative regions of different modalities while enhancing their interaction and fusion. Firstly, to explore the relationship between subjects of different modalities, we propose constructing an interactively multi-relation graph for multi-modal data. It is dynamically learned by designing graph learning loss terms. The multi-layer graph convolutional neural network is utilized to learn context-enriched representations for each subject. Then, to better capture the significant region representations of different modalities, we propose a semantic prompt-guided learning network to excavate the modality-specific lesion regions of related diseases. Specifically, a set of semantic prompts of related brain diseases is first guided to capture fine-grained local details to enhance patch representation. And then we couple with a relation-aware embedding strategy to refine discriminative features. Compared with state-of-the-art methods, our approach achieves superior performance on different benchmark datasets. Code is available at https://github.com/wanxixi11/SPromptGL.", "filename": "2025_0244.pdf", "year": 2025, "institution": "Anhui University", "country": "China", "authors": ["Xixi Wan", "Bo Jiang", "Shihao Li", "Aihua Zheng"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0245", "x": 6.8, "y": 3.843, "title": "T&F-DFC FusionNet: Time&Frequency-Dynamic Functional Connectivity Fusion Network for ADHD Diagnosis in Children Based on fNIRS", "abstract": "Early diagnosis of attention deficit hyperactivity disorder (ADHD) in children and its underlying neurobiological mechanisms have become a focal point of research. Existing AI-based diagnostic methods show promise but struggle to fully capture dynamic correlations between brain regions, limiting their clinical effectiveness. In this study, we proposed a time&frequency-dynamic functional connectivity fusion network (T&F-DFC FusionNet) based on functional near-infrared spectroscopy (fNIRS) to assist in the objective diagnosis of children with ADHD in clinical practice. The T&F-DFC FusionNet can extract the time and frequency domain features of spatial dynamic functional connectivity across channels of fNIRS data, and improve the diagnostic results by effectively fusing multi-domain features. Meanwhile, T&F-DFC FusionNet used a leave-one-ROI-out method to study specific functional brain regions with abnormal connectivity in children with ADHD to identify clinically significant biomarkers. Through a series of experiments based on clinical data, the results show that T&F-DFC FusionNet is effective in diagnosing ADHD in children, and its performance is significantly better than that of the comparison model. In addition, notably, our findings suggest that connectivity abnormalities in the right dorsolateral prefrontal cortex and the BA 8 may be key brain regions involved in the pathogenesis of ADHD in children. In summary, this study provides new insights and methods for clinical auxiliary diagnosis and mechanism exploration of ADHD.Keywords: children with ADHD • fNIRS • dynamic functional connectivity • time domain • frequency domain M. Chu and Y. Ma-Co-first Authors.", "filename": "2025_0245.pdf", "year": 2025, "institution": "Northwest University", "country": "China", "authors": ["Mengxiang Chu", "Yunxiang Ma", "Xiaowei He", "Xiao Li", "Jiaojiao Ren", "Zhengyu Zhong", "Jingjing Yu", "Hongbo Guo"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0246", "x": 6.681, "y": 3.884, "title": "Task-Aligned fMRI Generation Model for Brain Disorder Diagnosis", "abstract": "Functional magnetic resonance imaging (fMRI) is essential for understanding and diagnosing brain disorders. However, the challenge of small sample sizes, due to high acquisition costs and low annotation efficiency, hinders deeper exploration of the mechanisms underlying brain diseases. Recently, generative diffusion models have shown great potential for time series data generation, but directly using them for fMRI generation still has some issues. Firstly, most of them are designed for single time series, ignoring the significant dependency information between multiple time series when applied to fMRI. Since fMRI time series from different brain regions exhibit correlations, it is necessary to consider this characteristic when generating fMRI. Secondly, the generation process often lacks the involvement of label information, which limits their applicability in facilitating classification tasks. Thirdly, the alignment between the generated data and the target tasks is often insufficient, limiting its effectiveness for brain disorder diagnosis. To address these issues, we propose a novel task-aligned fMRI generation method based on the diffusion model. Specifically, a functional brain network (FBN) is incorporated into the diffusion model as prior knowledge to guide and constrain the data generation process, ensuring that the generated fMRI respects the functional connectivity characteristics observed in actual fMRI. To effectively and flexibly generate class-specific fMRI, a representative class-wise FBN is utilized as the prior FBN. Meanwhile, Y. Li and X. Wu-Contributed equally to this work.", "filename": "2025_0246.pdf", "year": 2025, "institution": "Shenzhen Campus of Sun Yat-sen University", "country": "China", "authors": ["Yifan Li", "Xiaotong Wu", "Xiaocai Zhang", "Haiteng Jiang", "Weiwen Wu", "Dinggang Shen", "Jianjia Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0247", "x": 7.094, "y": 4.054, "title": "The Refining of Brain Connectivity Features on Residual Posterior Patterns", "abstract": "In conjunction with graph neural networks (GNNs), functional connectivity analysis based on fMRI data can provide insights into the interaction and communication patterns in brain network, which has gained increasing attention in the diagnosis of neuropsychiatric disorders. However, traditional GNN based models focus primarily on brain regions, with limited attention given to changes in brain connectivity induced by diseases, and often lack specific methods to address noise and outliers. To accurately preserve and analyze connections in brain networks and retain the structure information in the original graph over message passing, we propose a Residual-Posterior Line Graph Network (RP-LGN). RP-LGN innovatively re-models each edge as a node to highlight functional connectivity information. Subsequently, it integrates residual blocks and a single-pass, low-variance Bayesian variational inference method to approximate the true posterior distribution. Bayesian variational posterior facilitates the quantification of uncertainty in model predictions and enhances model robustness in the presence of noise and anomalous data, ultimately promoting more accurate clinical decisionmaking. Compared with other models, the performance of RP-LGN was validated on the ABIDE dataset and ADHD-200 dataset, with significant accuracy improvements, and revealed significant site-specific differences and unique connection patterns associated with diseases. Our code is available at: https://github.com/YeDbae/RP-LGN.", "filename": "2025_0247.pdf", "year": 2025, "institution": "Southwest Jiaotong University", "country": "China", "authors": ["Xinbei Zha", "Jiaming Zhang", "Jin Gu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0248", "x": 6.722, "y": 3.538, "title": "TReND: Transformer Derived Features and Regularized NMF for Neonatal Functional Network Delineation", "abstract": "Precise parcellation of functional networks (FNs) of early developing human brain is the fundamental basis for identifying biomarker of developmental disorders and understanding functional development. Resting-state fMRI (rs-fMRI) enables in vivo exploration of functional changes, but adult FN parcellations cannot be directly applied to the neonates due to incomplete network maturation. No standardized neonatal functional atlas is currently available. To solve this fundamental issue, we propose TReND, a novel and fully automated self-supervised transformer-autoencoder framework that integrates regularized nonnegative matrix factorization (RNMF) to unveil the FNs in neonates. TReND effectively disentangles spatiotemporal features in voxel-wise rs-fMRI data. The framework integrates confidence-adaptive masks into transformer self-attention layers to mitigate noise influence. A self-supervised decoder acts as a regulator to refine the encoder's latent embeddings, which serve as reliable temporal features. For spatial coherence, we incorporate brain surface-based geodesic distances as spatial encodings along with functional connectivity from temporal features. The TReND clustering approach processes these features under sparsity and smoothness constraints, producing robust and biologically plausible parcellations. We extensively validated our TReND framework on three different rs-fMRI datasets: simulated, dHCP and HCP-YA against comparable traditional feature extraction and clustering techniques. Our results demonstrated the superiority of the TReND framework in the delineation of neonate FNs with significantly better spatial contiguity and functional homogeneity. Collectively, we established TReND, a novel and robust framework, for neonatal FN delineation. TReND-derived neonatal FNs could serve as a neonatal functional atlas for perinatal populations in health and disease.", "filename": "2025_0248.pdf", "year": 2025, "institution": "University of Pennsylvania", "country": "USA", "authors": ["Sovesh Mohapatra", "Minhui Ouyang", "Shufang Tan", "Jianlin Guo", "Lianglong Sun", "Yong He", "Hao Huang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0249", "x": 3.257, "y": 5.74, "title": "Unsupervised Anomaly Detection on Preclinical Liver H &E Whole Slide Images Using Graph Based Feature Distillation", "abstract": "Toxicity assessment of candidate compounds is an essential part of safety evaluation in the preclinical stage of drug development. Traditionally, drug safety evaluations depend on manual histopathological examinations of tissue sections from animal subjects, often leading to significant effort in evaluating normal tissues. Moreover, the collection of abnormality samples poses significant challenges due to the rarity and diversity of various types of abnormalities. This makes it impractical to develop a comprehensive training dataset that encompasses all potential anomalies, particularly those that are underrepresented. Consequently, traditional supervised learning methods may face difficulties, leading to a growing interest in unsupervised approaches for anomaly detection. In this study, we present GraphTox, a multi-resolution graph-based anomaly detector designed to assess hepatotoxicity in Rattus norvegicus liver tissues. GraphTox is built upon a novel resolution-aware foundation model pre-trained on 2.7 million liver tissue patches. Additionally, GraphTox employs graph-based feature distillation on normal liver whole slide images (WSIs) to identify hepatotoxicity. Our results demonstrate that GraphTox achieves an 11.1% improvement in area under the receiver operating characteristic curve (AUC) on an independent testing set compared to the best-performing non-graph-based anomaly detection models, and an 8.1% improvement over a graph-based model derived from a resolution-agnostic foundation model UNIv2. These findings highlight that GraphTox effectively leverages the resolution-aware digital pathology foundation model to capture multi-scale tissue characteristics within the local tissue graphs, thereby enhancing anomaly detection across various scales 1 Our code is available at https://linlilamb. github.io/GraphTox-project-page/.", "filename": "2025_0249.pdf", "year": 2025, "institution": "RaDS IT, Merck and Co., Inc", "country": "USA", "authors": ["Lin Li", "Lillie Shelton", "Thomas Forest", "Kyathanahalli Janardhan", "Tiffany Jenkins", "Michael J Napolitano", "Roujia Wang", "David Leigh", "Tosha Shah", "Grady Earl Carlson", "Rajath Soans", "Antong Chen"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0250", "x": 6.068, "y": 3.841, "title": "Various Attention Mechanism Graph Convolutional Network with Multi-source Domain Adaptation for Cross-Subject EEG Emotion Recognition", "abstract": "EEG-based emotion recognition is vital for patients who are unable to express emotions normally through physical or verbal means. It can provide essential support for their emotional expression and rehabilitation. EEG signals are highly non-stationary, and there is significant variability in emotional expression among individuals. The Graph Convolutional Network (GCN) has shown excellent performance in EEG signal feature extraction, but their accuracy in cross-subject scenarios remains unsatisfactory. In this paper, we propose a Various Attention Mechanism Graph Convolutional Network with Multi-source Domain Adaptation (VAG-MSDA) model for cross-subject EEG emotion recognition. VAG extracts features through the GCN with various attention mechanism to capture the emotional cognitive attributes of the graph structure in spectral, local, and global spatial domains, ensuring the richness and stability of feature information while reducing redundancy. Additionally, MSDA is used to align the feature distributions and classifiers among different individuals, further enhancing the model's generalization ability. Experiments were conducted on the SEED and SEED-IV datasets. The results demonstrate that the proposed VAG-MSDA model achieves significant performance improvements and reaches state-of-the-art performance levels on the SEED-IV dataset. Our code is open-sourced at https://github.com/e6ut/vag-msda.", "filename": "2025_0250.pdf", "year": 2025, "institution": "Hebei University of Technology", "country": "China", "authors": ["Shuo Shi", "Xulei Zheng", "Taiyi Wu", "Xiaoke Hao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0251", "x": 2.762, "y": 4.622, "title": "A Frequency-Aware Self-supervised Learning for Ultra-Wide-Field Image Enhancement", "abstract": "Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware selfsupervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high-and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.", "filename": "2025_0251.pdf", "year": 2025, "institution": "Zhejiang University of Technology", "country": "China", "authors": ["Weicheng Liao", "Zan Chen", "Jianyang Xie", "Yalin Zheng", "Yuhui Ma", "Yitian Zhao"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0252", "x": 3.433, "y": 4.76, "title": "A Hybrid Contrastive Ordinal Regression Method for Advancing Disease Severity Assessment in Imbalanced Medical Datasets", "abstract": "Accurate disease grading is critical for early diagnosis and effective treatment planning. However, class imbalance and subtle interclass variations in real-world disease grading datasets make it challenging for traditional classification models to differentiate between neighboring disease stages and preserve ordinal label relationships. Existing approaches emphasize inter-class ordinal relationships but fail to distinguish closely related categories effectively. To address these limitations, we consider disease grading as an ordinal regression problem and adopt a supervised contrastive learning approach to design a hybrid supervised contrastive ordinal learning framework. Our framework consists of three basic modules: 1) prototype-based contrastive ordinal learning, 2) weighted sample-based contrastive learning and 3) disease stage grading using regression. To deal with class imbalance while enhancing intra-class consistency and inter-class separation, we design a distance-based prototype contrastive ordinal loss, which pushes the samples closer to their class centers while maintaining their ordinality. This approach captures subtle differences within closely related disease stages and results in a separable ordinal latent space. Additionally, a per-sample class weighting strategy is integrated into weighted supervised contrastive ordinal learning to prevent class collapse, ensuring balanced gradient contributions and robust inter-class separation. Our approach effectively captures both large-scale and fine-grained variations, enabling precise ordinal classification for disease grading. We validate the framework on diabetic retinopathy and breast cancer datasets, demonstrating its adaptability across medical conditions and potential to enhance diagnostic accuracy in medical imaging applications.", "filename": "2025_0252.pdf", "year": 2025, "institution": "Edith Cowan University", "country": "Australia", "authors": ["Afsah Saleem", "Joshua R Lewis", "Syed Zulqarnain Gilani"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0253", "x": 1.536, "y": 6.823, "title": "Advancing Medical Representation Learning Through High-Quality Data", "abstract": "Despite the growing scale of medical Vision-Language datasets, the impact of dataset quality on model performance remains under-explored. We introduce Open-PMC, a high-quality medical dataset from PubMed Central, containing 2.2 million image-text pairs, enriched with image modality annotations, subfigures, and summarized in-text references. Notably, the in-text references provide richer medical context, extending beyond the abstract information typically found in captions. Through extensive experiments, we benchmark Open-PMC against larger datasets across retrieval and zero-shot classification tasks. Our results show that dataset quality-not just size-drives significant performance gains. We complement our benchmark with an in-depth analysis of feature representation. Our findings highlight the crucial role of data curation quality in advancing multimodal medical AI. We release Open-PMC, along with the trained models and our codebase.", "filename": "2025_0253.pdf", "year": 2025, "institution": "York University", "country": "Canada", "authors": ["Negin Baghbanzadeh", "Adibvafa Fallahpour", "Yasaman Parhizkar", "Franklin Ogidi", "Shuvendu Roy", "Sajad Ashkezari", "Vahid Reza Khazaie", "Michael Colacci", "Ali Etemad", "Arash Afkanpour", "Elham Dolatabadi"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0254", "x": 3.698, "y": 6.035, "title": "Background-Invariant Independence-Guided Multi-head Attention Network for Skin Lesion Classification", "abstract": "Biomedical image classification faces several adversarial challenges, including occlusions from artifacts, variations in tissue pigmentation, and class imbalance, which hinder model generalization. Existing attention mechanisms enhance region localization but often introduce redundant dependencies across attention heads, limiting feature diversity. We propose the Background-Invariant Independence-Guided Multi-head Attention Network (BIIGMA-Net) to address these issues. BIIGMA-Net employs Multi-head Independence-Guided Channel Attention (MICA), where each head independently learns feature importance while enforcing neuron-wise independence using the Hilbert-Schmidt Independence Criterion (HSIC) to enhance feature diversity. Additionally, a saliency-driven mechanism suppresses background activations by selectively shuffling non-salient vectors, preventing the model from relying on static background cues. By integrating these strategies, BIIGMA-Net improves robustness against spurious background noise while ensuring complementary feature extraction. Extensive experiments on popular skin cancer datasets (ISIC-17, ISIC-18 and ISIC-19) demonstrate the framework's effectiveness and robustness. Our code is available at: https://github.com/shb2908/BIIGMA-Net.", "filename": "2025_0254.pdf", "year": 2025, "institution": "Jadavpur University", "country": "India", "authors": ["Debasmit Roy", "Srinjoy Dutta", "Soham Bose", "Friedhelm Schwenker", "Ram Sarkar"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0255", "x": 1.277, "y": 3.23, "title": "BayeSMM: Robust Deep Combined Computing Tackling Heavy-Tailed Distribution in Medical Images", "abstract": "Abnormal structures in multi-modality medical images often lead to heterogeneous heavy-tailed distributions. However, traditional models, especially those relying on Gaussian distributions, struggle to effectively capture these outliers. To address this, we propose BayeSMM, a novel framework that leverages Student's t distribution mixture models (SMM) to simultaneously perform registration and segmentation for misaligned multi-modality medical images. Specifically, we construct a Bayesian Student's t mixture model incorporating the heavy-tailed nature of the Student's t distribution and develop variational inference to optimize the model. Guided by variational inference, we design a novel deep learning architecture that performs registration and segmentation jointly. We demonstrate the effectiveness of BayeSMM with experiments on the MS-CMR dataset, where the results show superior performance compared to existing combined computing methods, and yield enhanced robustness under the simulated heavy-tailed setting. The code is available at https://github.com/HenryLau7/BayeSMM.", "filename": "2025_0255.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Yuanye Liu", "Ruoxuan Zhen", "Shangqi Gao", "Xinzhe Luo", "Xin Gao", "Qingchao Chen", "Xiahai Zhuang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0256", "x": 1.979, "y": 3.169, "title": "Beyond Shadows: Learning Physics-Inspired Ultrasound Confidence Maps from Sparse Annotations", "abstract": "This paper introduces a novel user-centered approach for generating confidence maps in ultrasound imaging. Existing methods, relying on simplified models, often fail to account for the full range of ultrasound artifacts and are limited by arbitrary boundary conditions, making frame-to-frame comparisons challenging. Our approach integrates sparse binary annotations into a physics-inspired probabilistic graphical model that can estimate the likelihood of confidence maps. We propose to train convolutional neural networks to predict the most likely confidence map. This results in an approach that is fast, capable of dealing with various artifacts, temporally stable, and allows users to directly influence the algorithm's behavior using annotations. We demonstrate our method's ability to cope with a variety of challenging artifacts and evaluate it quantitatively on two downstream tasks, bone shadow segmentation and multi-modal image registration, with superior performance than the state-of-art. We make our training code public.", "filename": "2025_0256.pdf", "year": 2025, "institution": "ImFusion GmbH", "country": "Germany", "authors": ["Matteo Ronchetti", "Rüdiger Göbl", "Bugra Yesilkaynak", "Oliver Zettinig", "Nassir Navab"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0257", "x": 2.392, "y": 1.949, "title": "BME$$^2$$: A Plug-and-Play Bridge-Based Module for Misalignment Estimation and Elimination in Multi-scan Image Restoration", "abstract": "The multi-scan imaging procedure, involving both multimodal and multi-timepoint scans, captures temporal changes and complementary cross-modality information, playing a key role in clinical diagnosis. Multi-scan image restoration (IR), which leverages highquality reference scans to aid in restoring degraded current scans, holds significant potential for reducing the cost of the multi-scan procedure. However, misalignment between scans, arising from patient physiological or posture changes, impacts the ability of networks to exploit cross-scan correlations and leads to declined restoration performance. To this end, we propose a plug-and-play Bridge-Based Module for Misalignment Estimation and Elimination (BME 2 ), which adopts a coarse-to-fine strategy to estimate cross-scan misalignment. Specifically, a lightweight misalignment estimation (ME) network first predicts the initial deformation fields, which are then iteratively refined via a latent Schrödinger bridge-based model to obtain the final estimation. Notably, BME 2 can be added to arbitrary backbones and only introduces mild computational costs. Validated on brain MRI and abdominal CT datasets, BME 2 universally enhances four baselines, achieving average PSNR gains of 0.54 and 0.65 dB on brain and abdominal data, respectively. The codes are available at: https://github.com/ChenWenxuan2021/BME2.", "filename": "2025_0257.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Wenxuan Chen", "Caiwen Jiang", "Xiaolei Song", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0258", "x": 3.757, "y": 3.263, "title": "CardioInterp: Generative Modeling for Cardiovascular OCT Interpolation with Anatomical Continuity and Fidelity", "abstract": "Cardiovascular Optical Coherence Tomography (OCT) is hindered by the brief imaging window provided by contrast agents, making it challenging to capture high-resolution images of multiple plaques over long vessel sections. Rapid catheter pullback and coarse spatial resolution increase the likelihood of missing subtle pathologies and critical plaque microstructures, compromising diagnostic accuracy. To address this, we introduce CardioInterp, the first generative interpolation model for cardiovascular OCT, designed to synthesize high-fidelity intermediate B-slices, enhancing structural continuity and spatial resolution. Our architecture integrates a latent diffusion framework with a novel Dual-Path Fusion Decoder designed to ensure inter-slice structural continuity while preserving microanatomical fidelity. Experiments on cardiovascular OCT datasets demonstrate that CardioInterp achieves superior interpolation quality (PSNR = 28.59, SSIM = 51.80%) at 6 times upscaling of B-slices and spatial resolution, surpassing traditional medical image interpolation methods and setting a new benchmark. This innovative computational approach enables high-resolution imaging of long vessel sections within a limited temporal window in cardiovascular OCT. The code is available at: https://github.com/Lee728243228/CardioInterp.", "filename": "2025_0258.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "Hong Kong", "authors": ["Linyuan Li", "Bing Yang", "Minqing Zhang", "Mengxian He", "Wu Yuan"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0259", "x": 3.196, "y": 5.864, "title": "Controllable Image Synthesis Workflow for Enhancing Cervical Cell Detection", "abstract": "Cervical cancer is the only cancer that can be eliminated, yet it causes over 300,000 deaths annually. Early detection of its precancerous lesions can significantly reduce both incidence and mortality rates, while the process is labor-intensive and demands highly trained professionals. The application of artificial intelligence for cervical cell detection shows great promise but frequently encounters challenges such as limited data scale and class imbalance, stemming from the difficulties associated with expert annotation and the diverse types of cervical cells. To address this, current studies tend to design advanced detection models, while little attention is given to the potential improvements of data augmentation. In this work, we innovatively present the first controllable image synthesis workflow with adaptive cell segmentation and style transfer to synthesize realistic cervical cell images with bounding box annotations. Specifically, an adaptive cell segmentation method was introduced to cut target cells of varying sizes and morphologies from real images. These cells are then controllably pasted onto blank backgrounds to synthesize coarse images, which were further refined to realistic ones through the style transfer approach. The extensive experiment on a private long-tailed dataset demonstrated that our proposed workflow can generate realistic cervical cell images, thereby enhancing model training and improving the performance of cervical cell detection, generally and categorically. The code is available at https://github.com/huyihuang/ImageSynthesisForCCD.", "filename": "2025_0259.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Yihuang Hu", "Qi Chen", "Linbo Liao", "Weiping Lin", "Huisi Wu", "Liansheng Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0260", "x": 1.052, "y": 5.243, "title": "Cost-Effective Active Learning for Nucleus Detection Using Crowdsourced Annotations with Dynamic Weighting Adjustment", "abstract": "Accurate nucleus detection in pathology images is crucial for disease diagnosis. Deep learning based methods require extensive annotations of nuclei, which are time-consuming for pathologists. Active learning (AL) provides an attractive paradigm for reducing annotation efforts by iteratively selecting the most valuable samples for annotation. However, most AL methods do not consider utilizing crowdsourced annotations from multiple workers with varying expertise levels and labeling costs, limiting their practical applicability. Recent approaches design AL strategies that adaptively select the most cost-effective worker for each sample, but these methods solely focus on the classification task, overlooking the development of an AL framework with crowdsourced annotations for the detection task. Additionally, they struggle to adapt to the changes in model performance during AL iterations, resulting in inefficiencies in sample selection and cost management. Based on the above considerations, we propose C2AL, a novel cost-effective AL framework using crowdsourced annotations for nucleus detection in pathology images. Specifically, we design a new criterion in the form of score function and a dynamic weighting adjustment strategy to iteratively select the most cost-effective sample-worker pairs from the crowdsourced data. Then, based on the selected sample-worker pairs, the labeled pool is updated and the detection model is trained for performance evaluation. To the best of our knowledge, this is the first AL framework for detecting nuclei in the crowdsourced environment, and the experimental results on one real-world and two simulated crowdsourced datasets demonstrate that C2AL achieves higher detection accuracy at lower annotation costs compared to existing methods.", "filename": "2025_0260.pdf", "year": 2025, "institution": "Nanjing University of Aeronautics and Astronautics", "country": "China", "authors": ["Jiao Tang", "Yuankun Zu", "Qi Zhu", "Peng Wan", "Daoqiang Zhang", "Wei Shao"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0261", "x": 2.385, "y": 3.469, "title": "D 2 MAE: Diffusional Deblurring MAE for Ultrasound Image Pre-training", "abstract": "Recent advances in generative self-supervised learning, particularly Masked Autoencoders (MAE), have shown significant promise in medical image pre-training. However, ultrasound poses unique challenges due to its intrinsic low signal-to-noise ratio. While previous studies have enhanced MAE with deblurring for improved performance, their static deblurring strategy fails to consider domain discrepancies arising from variations in ultrasound imaging. To overcome these limitations, we propose D 2 MAE-a Diffusional Deblurring-enhanced MAE framework that seamlessly integrates a diffusional deblurring objective into MAE pre-training, simultaneously optimizing both deblurring and masked image reconstruction within a unified framework. Furthermore, we introduce an optimal blurriness-aware fine-tuning strategy that dynamically adjusts blurriness through an optimal blurriness search procedure, effectively accommodating the inherent domain discrepancies in ultrasound images. Extensive experiments across multiple ultrasound datasets, including thyroid, pancreas, and ovary, demonstrate that D 2 MAE outperforms state-of-the-art methods, significantly enhancing generalizability and diagnostic performance across diverse ultrasound tasks. Our results establish D 2 MAE as a superior approach for ultrasound imaging pre-training, paving the way for improved ultrasound image analysis. The code and pre-trained models are publicly available on GitHub.", "filename": "2025_0261.pdf", "year": 2025, "institution": "West China Hospital", "country": "China", "authors": ["Qingbo Kang", "Jun Gao", "Hongkai Zhao", "Zhu He", "Kang Li", "Qicheng Lao"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0262", "x": 2.136, "y": 4.135, "title": "Difficulty Estimation for Image-Specific Medical Image Segmentation Quality Control", "abstract": "In clinical decisions, trusting erroneous information can be as harmful as discarding crucial data. Without accurate quality assessment of medical image segmentation, both can occur. In current segmentation quality control, any segmentation with a Dice Similarity Coefficient (DSC) above a set threshold would be considered \"good enough\", while segmentations below the threshold would be discarded. However, those global thresholds ignore input-specific factors, increasing the risk of accepting inaccurate segmentations into clinical workflows or discarding valuable information. To address this, we introduce a new paradigm for segmentation quality control: image-specific segmentation quality thresholds, based on inter-observer agreement prediction. We illustrate this on a multi-annotator COVID-19 lesion segmentation dataset. To better understand the factors that contribute to segmentation difficulty, we categorize radiomic features into four distinct groups -imaging, texture, border and geometrical -to identify factors influencing expert disagreement, finding that lesion texture and geometry were most influential. In a simulated clinical setting, our proposed ensemble regressor, using automated segmentations and uncertainty maps, achieved a 5.6% MAE when predicting the mean annotator DSC score, enhancing precision by a factor of two compared to case-invariant global thresholding. By shifting to image-specific segmentation quality levels, our approach not only reduces the likelihood of erroneous segmentations but also increases the chances of including accurate ones in clinical decision-making.", "filename": "2025_0262.pdf", "year": 2025, "institution": "DTU Compute", "country": "Denmark", "authors": ["Joris Fournel", "Axel Bartoli", "Baptiste Marchi", "Arnaud Maurin", "Siavash Arjomand Bigdeli", "Alexis Jacquier", "Aasa Feragen"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0263", "x": 2.34, "y": 3.586, "title": "DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model", "abstract": "Annotation variability remains a substantial challenge in medical image segmentation, stemming from ambiguous imaging boundaries and diverse clinical expertise. Traditional deep learning methods producing single deterministic segmentation predictions often fail to capture these annotator biases. Although recent studies have explored multi-rater segmentation, existing methods typically focus on a single perspective-either generating a probabilistic \"gold standard\" consensus or preserving expert-specific preferences-thus struggling to provide a more omni view. In this study, we propose DiffOSeg, a two-stage diffusion-based framework, which aims to simultaneously achieve both consensus-driven (combining all experts' opinions) and preference-driven (reflecting experts' individual assessments) segmentation. Stage I establishes population consensus through a probabilistic consensus strategy, while Stage II captures expert-specific preference via adaptive prompts. Demonstrated on two public datasets (LIDC-IDRI and NPC-170), our model outperforms existing state-of-the-art methods across all evaluated metrics. Source code is available at https://github.com/string-ellipses/ DiffOSeg.", "filename": "2025_0263.pdf", "year": 2025, "institution": "West China Hospital", "country": "China", "authors": ["Han Zhang", "Xiangde Luo", "Yong Chen", "Kang Li"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0264", "x": 2.608, "y": 3.73, "title": "DiffStain: Conditioned Diffusion-Based Semantic Virtual Staining with Mask Guidance", "abstract": "Fluorescent staining is crucial for studying the morphology and dynamics of subcellular structures in biological and medical research, though being slow, expensive, and causing phototoxicity in live cells. Existing methods use deep generative models for image-to-image translation to generate diverse fluorescent images of subcellular structures. However, the pixel-level image generation approaches struggle to preserve fine structural details during the reconstruction process. In this paper, we introduce DiffStain, a novel approach that leverages maskguided diffusion models for semantic virtual staining. The goal is to generate fluorescent images based on a brightfield input image. Rather than relying on deliberately selected image filters for subcellular structure segmentation, our approach employs an unsupervised deep neural spectral clustering method to combat the noisy and ambiguous structural boundaries. We also integrate mask guidance into the reverse denoising process, which helps highlight the regions of the subcellular structures that require precise representation in the generated fluorescent images. The masks produced by the spectral clustering model provide valuable feedback, enabling iterative refinements of the fluorescent images. Experiments showcase that our DiffStain method achieves state-of-the-art virtual staining performances on public microscopy datasets. Code is available at: https://github.com/StrengthInNumber/DiffStain.", "filename": "2025_0264.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Yikai Han", "Jimao Jiang", "Yuru Pei"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0265", "x": 2.311, "y": 2.221, "title": "Diffusing Boundaries: CBCT-to-CT Translation with Extended Field of View", "abstract": "Cone-beam computed tomography (CBCT) is an essential imaging modality for adaptive radiotherapy, enabling the positioning and real-time verification of anatomical changes. However, CBCT images suffer from artifacts and lack the accurate Hounsfield unit (HU) calibration necessary for dose computation. Additionally, CBCT's limited field of view (FOV) further complicates its direct application for replanning. To address these limitations, we propose a novel framework leveraging diffusion models to synthesize a synthetic CT (sCT) from CBCT while inpainting the extended FOV using the original planning CT (pCT). Our method integrates with any CBCT-to-CT diffusion framework without degrading its performance, ensuring accurate HU values and comprehensive anatomical coverage for dose computation without requiring new CT acquisitions. Quantitative and qualitative evaluations demonstrate that our approach preserves the baseline CBCT-to-CT translation quality while effectively extending the FOV, offering a streamlined and effective solution for adaptive radiotherapy workflows.", "filename": "2025_0265.pdf", "year": 2025, "institution": "TheraPanacea", "country": "France", "authors": ["Quentin Spinat", "Audrey Duran", "Olivier Teboul", "Nikos Paragios", "Nikos Komodakis"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0266", "x": 2.604, "y": 2.316, "title": "Directional Adaptive Shuffle-Based Visual State-Space Models for Medical Image Restoration", "abstract": "Medical image restoration (MedIR) demands precise modeling of anisotropic spatial dependencies, where directional anatomical patterns are frequently degraded by conventional methods. We propose Directional Adaptive Shuffle Mamba (DASMamba), a state-space model architecture that addresses this challenge through two novel components: (1) the Directional Adaptive Shuffle Module (DASM), which captures long-range dependencies via directional adaptive random shuffle and selective scanning, and (2) the Dual-path Feedforward Network (DPFN), enhancing feature representation through multi-scale learning and dynamic channel fusion. By integrating these modules into a hierarchical U-shaped architecture, DASMamba achieves stateof-the-art performance on MRI super-resolution, CT denoising, and PET synthesis tasks while maintaining linear computational complexity. Our framework's ability to preserve diagnostically critical structural details underscores its clinical value. The code is available at https://github.com/cc111mp/DASMamba-MedIR.", "filename": "2025_0266.pdf", "year": 2025, "institution": "Hong Kong University of Science and Technology", "country": "China", "authors": ["Simon C K Chan", "Lulin Shi", "Bingxin Huang", "Terence T W Wong"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0267", "x": 4.474, "y": 3.5, "title": "EchoCardMAE: Video Masked Auto-Encoders Customized for Echocardiography", "abstract": "Echocardiography, a vital cardiac imaging modality, faces challenges due to limited annotated data, impeding the application of deep learning. This paper introduces EchoCardMAE, a customized masked video autoencoder framework designed to leverage unlabeled echocardiography data and enhance performance across diverse cardiac tasks. EchoCardMAE addresses key challenges in echocardiogram analysis through three innovations built upon masked video modeling (MVM): (1) Key Area Masking, which concentrates feature learning on the diagnostically relevant sector of the image; (2) Temporal-Invariant Alignment Loss, promoting feature consistency across different clips of the same echocardiogram; and (3) Reconstruction Denoising, improving robustness to speckle noise inherent in echocardiography. We comprehensively evaluated EchoCardMAE on three public datasets, demonstrating stateof-the-art results in ejection fraction (EF) estimation, Myocardial infarction (MI) prediction, and cardiac segmentation. For example, on the EchoNet-Dynamic dataset, EchoCardMAE achieved an EF estimation MAE of 3.78 and a left ventricular segmentation mDice of 92.96, surpassing existing methods. The code is available at https://github.com/ m1dsolo/EchoCardMAE.", "filename": "2025_0267.pdf", "year": 2025, "institution": "Dalian University of Technology", "country": "China", "authors": ["Xuan Yang", "Rui Xu", "Xinchen Ye", "Zhihui Wang", "Miao Zhang", "Yi Wang", "Xin Fan", "Hongkai Wang", "Qingxiong Yue", "Xiangjian He", "Yen-Wei Chen"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0268", "x": 4.473, "y": 3.544, "title": "EchoViewCLIP: Advancing Video Quality Control through High-performance View Recognition of Echocardiography", "abstract": "Echocardiography is a critical imaging technique for diagnosing cardiac diseases, requiring accurate view recognition to support clinical analysis. Despite advancements in deep learning for automating this task, existing models face two major limitations: they support only a limited number of cardiac views, insufficient for complex cardiac diseases, and they inadequately handle out-of-distribution (OOD) samples, often misclassifying them into generic categories. To address these issues, we present EchoViewCLIP, a novel framework for fine-grained cardiac view recognition and OOD detection. Built on our collected large-scale dataset annotated with 38 standard views and OOD data, EchoView-CLIP integrates a Temporal-informed Multi-Instance Learning (TML) module to preserve temporal information and identify key frames, along with a Negation Semantic-Enhanced (NSE) Detector to effectively reject OOD views. Additionally, we introduce a quality assessment branch to evaluate the quality of detected in-distribution (ID) views, enhancing the reliability of echocardiographic analysis. Our model achieves 96.1% accuracy across 38 view recognition tasks. The code is available at https:// github.com/xmed-lab/EchoViewCLIP.", "filename": "2025_0268.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Shanshan Song", "Yi Qin", "Honglong Yang", "Taoran Huang", "Hongwen Fei", "Xiaomeng Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0269", "x": 2.827, "y": 1.937, "title": "FIND-Net – Fourier-Integrated Network with Dictionary Kernels for Metal Artifact Reduction", "abstract": "Metal artifacts, caused by high-density metallic implants in computed tomography (CT) imaging, severely degrade image quality, complicating diagnosis and treatment planning. While existing deep learning algorithms have achieved notable success in Metal Artifact Reduction (MAR), they often struggle to suppress artifacts while preserving structural details. To address this challenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary Kernels), a novel MAR framework that integrates frequency and spatial domain processing to achieve superior artifact suppression and structural preservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and trainable Gaussian filtering, treating MAR as a hybrid task operating in both spatial and frequency domains. This approach enhances global contextual understanding and frequency selectivity, effectively reducing artifacts while maintaining anatomical structures. Experiments on synthetic datasets show that FIND-Net achieves statistically significant improvements over state-of-the-art MAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR improvement, confirming robustness across varying artifact complexities. Furthermore, evaluations on real-world clinical CT scans confirm FIND-Net's ability to minimize modifications to clean anatomical regions while effectively suppressing metal-induced distortions. These findings highlight FIND-Net's potential for advancing MAR performance, offering superior structural preservation and improved clinical applicability. Code is available at (https:// github.com/Farid-Tasharofi/FIND-Net).", "filename": "2025_0269.pdf", "year": 2025, "institution": "Friedrich-Alexander-Universität Erlangen", "country": "Germany", "authors": ["Farid Tasharofi", "Fuxin Fan", "Melika Qahqaie", "Mareike Thies", "Andreas Maier"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0270", "x": 2.135, "y": 4.148, "title": "Finer Disentanglement of Aleatoric Uncertainty Can Accelerate Chemical Histopathology Imaging", "abstract": "Label-free chemical imaging holds significant promise for improving digital pathology workflows, but data acquisition speed remains a limiting factor. To address this gap, we propose an adaptive strategy initially scan the low information (LI) content of the entire tissue quickly, identify regions with high aleatoric uncertainty (AU), and selectively re-image them at better quality to capture higher information (HI) details. The primary challenge lies in distinguishing between high-AU regions mitigable through HI imaging and those that are not. However, since existing uncertainty frameworks cannot separate such AU subcategories, we propose a fine-grained disentanglement method based on post-hoc latent space analysis to unmix resolvable from irresolvable high-AU regions. We apply our approach to streamline infrared spectroscopic imaging of breast tissues, achieving superior downstream segmentation performance. This marks the first study focused on fine-grained AU disentanglement within dynamic image spaces (LI-to-HI), with novel application to streamline histopathology. Code will be made public.", "filename": "2025_0270.pdf", "year": 2025, "institution": "University of Illinois Urbana-Champaign", "country": "USA", "authors": ["Ji-Hun Oh", "Kianoush Falahkheirkhah", "Rohit Bhargava"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0271", "x": 2.562, "y": 3.317, "title": "From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation", "abstract": "Accurate segmentation of orbital bones in facial computed tomography (CT) images is essential for the creation of customized implants for reconstruction of defected orbital bones, particularly challenging due to the ambiguous boundaries and thin structures such as the orbital medial wall and orbital floor. In these ambiguous regions, existing segmentation approaches often output disconnected or under-segmented results. We propose a novel framework that corrects segmentation results by leveraging consensus from multiple diffusion model outputs. Our approach employs a conditional Bernoulli diffusion model trained on diverse annotation patterns per image to generate multiple plausible segmentations, followed by a consensus-driven correction that incorporates position proximity, consensus level similarity, and gradient direction similarity to correct challenging regions. Experimental results demonstrate that our method outperforms existing methods, significantly improving recall in ambiguous regions while preserving the continuity of thin structures. Furthermore, our method automates the manual process of segmentation result correction and can be applied to image-guided surgical planning and surgery.", "filename": "2025_0271.pdf", "year": 2025, "institution": "Seoul Women's University", "country": "Republic of Korea", "authors": ["Jinseo An", "Min Jin Lee", "Kyu Won Shim", "Helen Hong"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0272", "x": 2.052, "y": 3.563, "title": "GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images", "abstract": "Chest X-Ray (CXR) imaging for pulmonary diagnosis raises significant challenges, primarily because bone structures can obscure critical details necessary for accurate diagnosis. Recent advances in deep learning, particularly with diffusion models, offer significant promise for effectively minimizing the visibility of bone structures in CXR images, thereby improving clarity and diagnostic accuracy. Nevertheless, existing diffusion-based methods for bone suppression in CXR imaging struggle to balance the complete suppression of bones with preserving local texture details. Additionally, their high computational demand and extended processing time hinder their practical use in clinical settings. To address these limitations, we introduce a Global-Local Latent Consistency Model (GL-LCM) architecture. This model combines lung segmentation, dualpath sampling, and global-local fusion, enabling fast high-resolution bone suppression in CXR images. To tackle potential boundary artifacts and detail blurring in local-path sampling, we further propose Local-Enhanced Guidance, which addresses these issues without additional training. Comprehensive experiments on a self-collected dataset SZCH-X-Rays, and the public dataset JSRT, reveal that our GL-LCM delivers superior bone suppression and remarkable computational efficiency, significantly outperforming several competitive methods. Our code is available at https://github.com/diaoquesang/GL-LCM.", "filename": "2025_0272.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Yifei Sun", "Zhanghao Chen", "Hao Zheng", "Yuqing Lu", "Lixin Duan", "Fenglei Fan", "Ahmed Elazab", "Xiang Wan", "Changmiao Wang", "Ruiquan Ge"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0273", "x": 3.158, "y": 4.91, "title": "GRASP-PsONet: Gradient-Based Removal of Spurious Patterns for PsOriasis Severity Classification", "abstract": "Psoriasis (PsO) severity scoring is vital for clinical trials but is hindered by inter-rater variability and the burden of in-person clinical evaluation. Remote imaging utilizing patient-captured mobile photos offers scalability but introduces challenges, such as variations in lighting, background, and device quality that are often imperceptible to humans but may impact model performance. These factors, coupled with inconsistencies in dermatologist annotations, reduce the reliability of automated severity scoring. We propose a framework to automatically flag problematic training images that introduce biases and reinforce spurious correlations which degrade model generalization by using a gradientbased interpretability approach. By tracing the gradients of misclassified validation images, we detect training samples where model errors align with inconsistently rated examples or are affected by subtle, non-clinical artifacts. We apply this method to a ConvNeXT-based weakly supervised model designed to classify PsO severity from phone images. Removing 8.2% of flagged images improves model AUC-ROC by 5% (85% to 90%) on a held-out test set. Commonly, multiple annotators and an adjudication process ensure annotation accuracy, which is expensive and timeconsuming. Our method correctly detects training images with annotation inconsistencies, potentially eliminating the need for manual reviews. When applied to a subset of training images rated by two dermatologists, the method accurately identifies over 90% of cases with inter-rater disagreement by rank-ordering and reviewing only the top 30% of training data. This framework improves automated scoring for remote assessments, ensuring robustness and scalability despite variability in data collection. Our method handles both inconsistencies in image conditions and annotations, making it ideal for applications lacking standardization of controlled clinical environments.", "filename": "2025_0273.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Basudha Pal", "Sharif Amit Kamran", "Brendon Lutnick", "Molly Lucas", "Chaitanya Parmar", "Asha Patel Shah", "David Apfel", "Steven Fakharzadeh", "Lloyd Miller", "Gabriela Cula", "Kristopher Standish"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0274", "x": 3.453, "y": 1.781, "title": "Harnessing Side Information for Highly Accelerated MRI", "abstract": "Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k -spacedata, via an ill-posed or illconditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the side information obtained from sources other than the current acquisition. In this paper, we present the Trust-Guided Variational Network (TGVN), an endto-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily undersampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN.", "filename": "2025_0274.pdf", "year": 2025, "institution": "NYU Center for Data Science", "country": "USA", "authors": ["Arda Atalık", "Sumit Chopra", "Daniel K Sodickson"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0275", "x": 3.721, "y": 4.147, "title": "Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound", "abstract": "Accurate carotid plaque grading (CPG) is vital to assess the risk of cardiovascular and cerebrovascular diseases. Due to the small size and high intra-class variability of plaque, CPG is commonly evaluated using a combination of transverse and longitudinal ultrasound views in clinical practice. However, most existing deep learning-based multiview classification methods focus on feature fusion across different views, neglecting the importance of representation learning and the difference in class features. To address these issues, we propose a novel Corpus-View-Category Refinement Framework (CVC-RF) that processes information from Corpus-, View-, and Category-levels, enhancing model performance. Our contribution is four-fold. First, to the best of our knowledge, we are the foremost deep-learning-based method for CPG according to the latest Carotid Plaque-RADS guidelines. Second, we propose a novel centermemory contrastive loss, which enhances the network's global modeling capability by comparing with representative cluster centers and diverse negative samples at Corpus-level. Third, we design a cascaded downsampling attention module to fuse multi-scale information and achieve implicit feature interaction at View-level. Finally, a parameter-free mixture-of-experts weighting strategy is introduced to leverage class clustering knowledge to weight different experts, enabling feature decoupling at Category-level. Experimental results indicate that CVC-RF effectively models global features via multi-level refinement, achieving state-Z. Zhu, J. Wang and Y. Jiang-Contribute equally to this work.", "filename": "2025_0275.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Zhiyuan Zhu", "Jian Wang", "Yong Jiang", "Tong Han", "Yuhao Huang", "Ang Zhang", "Kaiwen Yang", "Mingyuan Luo", "Zhe Liu", "Yaofei Duan", "Dong Ni", "Tianhong Tang", "Xin Yang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0276", "x": 6.421, "y": 3.514, "title": "High-Res Brain Source Imaging of MEG Using a Vector Bayesian Beamformer with Noise Learning", "abstract": "Magnetoencephalogram (MEG) with high spatio-temporal resolution plays a crucial role in the field of functional imaging. Incorporating vector source modeling enables explicit estimation of triaxial current components, thereby mitigating reconstruction errors caused by orientation bias in scalar leadfield approximations. This directional precision enables accurate identification of epileptogenic zones and oscillatory network hubs, providing neurosurgeons with electrophysiologically validated targets. Vector beamformers, grounded in spatial filtering theory, provide computationally efficient solutions for large-scale sensor data and dynamic high-resolution analyses. However, a vector source requires a vector beamformer whose performance degrades under high noise, limited time samples, or strongly correlated sources due to sample covariance matrix singularity. In this study, we propose a vector Bayesian learning framework to enhance beamformer robustness by addressing covariance matrix singularity. Specifically, we model the vector source linear system with full positive-definite noise covariance structures and employ data-driven Bayesian learning to refine the sample covariance matrix. By leveraging sparsity priors on source distributions and data-driven, our method improves spatial focusing and temporal reconstruction accuracy. We validated the approach using simulated data across varying signalto-noise ratios (SNR) and real 64-channel optically pumped magnetometer (OPM)-MEG datasets under diverse stimulus-evoked paradigms. Comparative evaluations demonstrate that our Bayesian learning-based framework achieves 18. 03% higher AUC compared to conventional beamformers while preserving millimeter-level spatial precision, outperforming existing benchmarks in both spatial localization accuracy and dynamic reconstruction fidelity for neuroscience and clinical applications. Our codes are publicly accessible at: https://github.com/gao815/ VBNLBF.", "filename": "2025_0276.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Tianyu Gao", "Kunye Liu", "Weikai Ma", "Yang Gao", "Xiaolin Ning"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0277", "x": 1.212, "y": 4.355, "title": "HyperSORT: Self-organising Robust Training with Hyper-networks", "abstract": "Medical imaging datasets often contain heterogeneous biases ranging from erroneous labels to inconsistent labeling styles. Such biases can negatively impact deep segmentation networks performance. Yet, the identification and characterization of such biases is a particularly tedious and challenging task. In this paper, we introduce HyperSORT, a framework using a hyper-network predicting UNets' parameters from latent vectors representing both the image and annotation variability. The hyper-network parameters and the latent vector collection corresponding to each data sample from the training set are jointly learned. Hence, instead of optimizing a single neural network to fit a dataset, HyperSORT learns a complex distribution of UNet parameters where low density areas can capture noise-specific patterns while larger modes robustly segment organs in differentiated but meaningful manners. We validate our method on two 3D abdominal CT public datasets: first a synthetically perturbed version of the AMOS dataset, and TotalSegmentator, a large scale dataset containing real unknown biases and errors. Our experiments show that HyperSORT creates a structured mapping of the dataset allowing the identification of relevant systematic biases and erroneous samples. Latent space clusters yield UNet parameters performing the segmentation task in accordance with the underlying \"learned\" systematic bias. The code and our analysis of the TotalSegmentator dataset are made available: https://github.com/ImFusionGmbH/HyperSORT.", "filename": "2025_0277.pdf", "year": 2025, "institution": "ImFusion", "country": "Germany", "authors": ["Samuel Joutard", "Marijn Stollenga", "Marc Balle Sanchez", "Mohammad Farid Azampour", "Raphael Prevost"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0278", "x": 5.943, "y": 3.742, "title": "Improving Motor Imagery EEG Signal Quality with Dynamic Visual Cues: An Innovative Paradigm and Dataset", "abstract": "The electroencephalogram (EEG) acquisition paradigm is fundamental to brain-computer interface (BCI) research as it directly determines the mechanisms of brain activity evoked, significantly influencing the quality of collected EEG signals. Traditional static cueing paradigms often struggle to effectively induce the motor imagery (MI) state, which can lead to inconsistent task execution and degraded EEG signal quality. This study proposes an innovative MI data acquisition paradigm employing dynamic visual cues depicting real human movements to enhance engagement and more effectively induce the MI state. We build the first novel dynamic visual cueing MI dataset, comprising EEG data acquired using both dynamic and static paradigms from five subjects. We analyze our dynamic visual cueing paradigm using questionnaire, qualitative, and quantitative analyses, evaluating it from subjective experience, physiological phenomena, and EEG signal decoding accuracy perspectives. Experiments show that our dynamic cueing paradigm significantly enhances subjects' task understanding and concentration, leading to greater brain activation and, consequently, improved decoding accuracy of brain states in MI-BCI tasks. By eliciting more pronounced brain state activity, our method fundamentally improves the quality of acquired EEG signals, laying the foundation for accurate decoding of brain states, and provides an innovative perspective for the development and improvement of MI-BCI.", "filename": "2025_0278.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Chenxi Yue", "Huawen Hu", "Qilong Yuan", "Enze Shi", "Jiaqi Wang", "Kui Zhao", "Xuhui Wang", "Shu Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0279", "x": 4.459, "y": 6.695, "title": "Inferring Super-Resolved Gene Expression by Integrating Histology Images and Spatial Transcriptomics with HISTEX", "abstract": "The groundbreaking development of spatial transcriptomics (ST) enables researchers to map gene expression across tissues with spatial precision. However, current next-generation sequencing methods, which theoretically cover the entire transcriptome, face limitations in resolving spatial gene expression at high resolution. The recently introduced Visium HD technology offers a balance between sequencing depth and spatial resolution, but its complex sample preparation and high cost limit its widespread adoption. To address these challenges, we introduce HISTEX, a multimodal fusion approach that leverages a bidirectional cross-attention mechanism and a general-purpose foundation model. HISTEX integrates spot-based ST data with histology images to predict super-resolution (SR) spatial gene expression. Experimental evaluations demonstrate that HISTEX outperforms state-of-the-art methods in accurately predicting SR gene expression across diverse datasets from multiple platforms. Moreover, experimental validation underscores HIS-TEX's potential to generate new biological insights. It enhances spatial patterns, enriches biologically significant pathways, and facilitates the SR annotation of tissue structures. These findings highlight HISTEX as a powerful tool for advancing ST research. Our source code is available at: https://github.com/wenwenmin/HISTEX.", "filename": "2025_0279.pdf", "year": 2025, "institution": "Yunnan University", "country": "China", "authors": ["Shuailin Xue", "Changmiao Wang", "Xiaomao Fan", "Wenwen Min"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0280", "x": 3.038, "y": 4.31, "title": "Intra- and Cross-View Enhancement for OCTA Imaging", "abstract": "Optical coherence tomography angiography (OCTA) is an indispensable modality in ophthalmic imaging, providing highresolution visualization of retinal microvasculature. Recently, deep learning approaches have been explored to reconstruct OCTA images; however, significant challenges persist, particularly the reliance on highquality target data for model training, which is often impractical due to limitations in hardware and acquisition protocols. In this work, we present a novel pipeline for deep learning-based OCTA imaging from repeated OCT B-scans, circumventing the need for high-quality training labels. We introduce an Intra-View Enhancement (IVE) module together with a novel loss function Cross-View Matching (CVM) to improve the imaging. The proposed pipeline is evaluated on a local dataset, demonstrating a relative improvement of 4.97% and 27.42% in PSNR and CNR over state-of-the-art learning-based OCTA method respectively. Our results underscore the effectiveness and clinical viability of the proposed approach for OCTA images, highlighting its potential to advance imaging capabilities in challenging clinical environments.", "filename": "2025_0280.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Jingbo Zeng", "Bingyao Tan", "Zaiwang Gu", "Shenghua Gao", "Leopold Schmetterer", "Jun Cheng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0281", "x": 2.076, "y": 2.838, "title": "Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration", "abstract": "Current self-supervised denoising techniques achieve impressive results, yet their real-world application is frequently constrained by substantial computational and memory demands, necessitating a compromise between inference speed and reconstruction quality. In this paper, we present an ultra-lightweight model that addresses this challenge, achieving both fast denoising and high quality image restoration. Built upon the Noise2Noise training framework-which removes the reliance on clean reference images or explicit noise modeling-we introduce an innovative multistage denoising pipeline named Noise2Detail (N2D). During inference, this approach disrupts the spatial correlations of noise patterns to produce intermediate smooth structures, which are subsequently refined to recapture fine details directly from the noisy input. Extensive testing reveals that Noise2Detail surpasses existing dataset-free techniques in performance, while requiring only a fraction of the computational resources. This combination of efficiency, low computational cost, and data-free approach make it a valuable tool for biomedical imaging, overcoming the challenges of scarce clean training data-due to rare and complex imaging modalities-while enabling fast inference for practical use.", "filename": "2025_0281.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Tomáš Chobola", "Julia A Schnabel", "Tingying Peng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0282", "x": 3.27, "y": 1.886, "title": "MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI", "abstract": "Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images suitable for water and fat signal composition estimation. While the two-point VIBE provides rapid water-fat-separated images, the six-point VIBE allows estimation of the effective transversal relaxation rate R2* and the proton density fat fraction (PDFF), which are imaging markers for health and disease. Ambiguity during signal reconstruction can lead to water-fat swaps. This shortcoming challenges the application of VIBE-MRI for automated PDFF analyses of largescale clinical data and population studies. This study develops an automated pipeline to detect and correct water-fat swaps in non-contrastenhanced VIBE images. Our three-step pipeline begins with training a segmentation network to classify volumes as \"fat-like\" or \"water-like\", using synthetic water-fat swaps generated by merging fat and water vol-", "filename": "2025_0282.pdf", "year": 2025, "institution": "TUM University Hospital", "country": "Germany", "authors": ["Robert Graf", "Hendrik Möller", "Sophie Starck", "Matan Atad", "Philipp Braun", "Jonathan Stelter", "Annette Peters", "Lilian Krist", "Stefan N Willich", "Henry Völzke", "Robin Bülow", "Tobias Pischon", "Thoralf Niendorf", "Johannes C Paetzold", "Dimitrios Karampinos", "Daniel Rueckert", "Jan Kirschke"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0283", "x": 1.589, "y": 6.273, "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "abstract": "Rapid advances in medical imaging technology underscore the critical need for precise and automated image quality assessment (IQA) to ensure diagnostic accuracy. Existing medical IQA methods, however, struggle to generalize across diverse modalities and clinical scenarios. In response, we introduce MedIQA, the first comprehensive foundation model for medical IQA, designed to handle variability in image dimensions, modalities, anatomical regions, and types. We developed a large-scale multi-modality dataset with plentiful manually annotated quality scores to support this. Our model integrates a salient slice assessment module to focus on diagnostically relevant regions feature retrieval and employs an automatic prompt strategy that aligns upstream physical parameter pre-training with downstream expert annotation fine-tuning. Extensive experiments demonstrate that MedIQA significantly outperforms baselines in multiple downstream tasks, establishing a scalable framework for medical IQA and advancing diagnostic workflows and clinical decision-making. Our code is available at https://github.com/siyi- xun/MedIQA.", "filename": "2025_0283.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0284", "x": 2.667, "y": 1.726, "title": "Meta-learning-Driven CT Morphology Disentangled Diffusion Model for Multi-region SPECT Attenuation Correction", "abstract": "SPECT imaging faces persistent challenges from soft-tissue attenuation artifacts in clinical practice. While CT-based correction remains the clinical reference standard, associated radiation risks and infrastructure requirements limit its widespread adoption. To address this, we propose a Meta-Learning-Driven CT Morphology Disentangled Diffusion Model (MetaMorph-Diff), which achieves CT-independent attenuation correction. First, we design a Morphological Structure-Attentive Fusion module that explicitly guides the diffusion process using CT-derived anatomical priors. During training, its Morpho-Attentive Alignment submodule establishes voxel-level physical constraints between SPECT features and attenuation distributions by leveraging CT anatomical priors. During inference, its Morpho-Disentangling Gate achieves complete disentangling from CT dependencies through learned morphological embeddings. Crucially, the model uses only SPECT images during inference to achieve accurate attenuation correction without relying on CT data. Second, we propose a multi-region adaptive meta-learning strategy, which enhances cross-anatomical generalization capability by optimizing model initialization parameters, enabling a single model to achieve consistent and accurate correction across diverse anatomical regions. Our method surpasses existing approaches with higher-precision attenuation distribution prediction and stronger multi-region correction adaptability. The code is available at https:// github.com/yhr1020/MetaMorph-Diff.", "filename": "2025_0284.pdf", "year": 2025, "institution": "Jiangnan University", "country": "China", "authors": ["Haoran Yang", "Jiansong Fan", "Lihua Li", "Xiang Pan"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0285", "x": 3.943, "y": 5.932, "title": "Metastatic Lymph Node Station Classification in Esophageal Cancer via Prior-Guided Supervision and Station-Aware Mixture-of-Experts", "abstract": "Assessing lymph node LN metastasis in CT is critical for esophageal cancer treatment planning. While clinical criteria are commonly used, the diagnostic accuracy is low with sensitivities ranging from 39.7% to 67.2% in previous studies. Deep learning would have the potential to improve it by learning from large-scale accurately labeled data. However, from the surgical procedure in LN dissection, pathological report only indicates the number of dissected LNs in each lymph node station (LN-station) with the number of metastatic ones found in the respective LN-station. So, it is difficult to establish one-to-one pairing between LN instances observed in CT and their metastasis status confirmed in the pathological report. In contrast, gold reference labels on LN-station metastasis can be readily retrieved from pathology reports at scale. Hence, instead of distinguishing LN instance metastasis, we directly classify LN-station metastasis using pathology-confirmed station labels. We first segment mediastinal LN-stations automatically to serve as input for classification. Then, to improve classification performance, we automatically segment all visible LN instances in CT and design a new LN prior-guided attention loss to explicitly regularize the network to focus on regions of suspicious LNs. Furthermore, considering the varying appearances and contexts of different LN-station, we propose a station-aware mixture-of-experts module, where the expert is H. Li, Y. Wang and Q. Yu-Equal contribution.", "filename": "2025_0285.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Haoshen Li", "Yirui Wang", "Qinji Yu", "Jie Zhu", "Ke Yan", "Dazhou Guo", "Le Lu", "Bin Dong", "Li Zhang", "Xianghua Ye", "Qifeng Wang", "Dakai Jin"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0286", "x": 3.791, "y": 6.855, "title": "MoMIL: Mixture of Multi-instance Learners for Modeling Multiple Compound Activities in High Content Imaging", "abstract": "High content imaging (HCI) plays a pivotal role in targetdirected drug discovery (TDD) by identifying compound activities across tests (or assays) designed for specific therapeutic targets. However, realworld assays often exhibit extreme label sparsity over large compound libraries, making accurate predictions challenging. Recent studies following multi-label learning (MLL) struggle in such scenarios when optimizing a single objective across multiple assays without assay-specific adaptations. To address this, we propose Mixture of Multi-Instance Learners (MoMIL), a multi-task learning (MTL) framework integrating hard-parameter sharing with assay-specific Multiple Instance Learners (MILs), enabling knowledge sharing and task-specific adaptations. Furthermore, we introduce complementary enhancements: HCI-specific foundation models (FMs), an assay selection algorithm, and a label imputation method to boost Mixture of Multi-Instance Learners (MoMIL)'s learning capabilities. We benchmark MoMIL on two extensive HCI datasets, achieving up to ∼6% and ∼8% improvement over state-of-the-art MLL and MTL methods. Moreover, MoMIL shows strong generalization to unseen assays, outperforming assay-specific single-task learning (STL) methods in 11 out of 12 assays.", "filename": "2025_0286.pdf", "year": 2025, "institution": "Janssen R&D, LLC", "country": null, "authors": ["Pushpak Pati", "Hsiu-Chi Cheng", "Steffen Jaensch", "Walid M Abdelmoula", "Krishna Chaitanya", "Michiel Van Dyck", "Tomé Albuquerque", "Samantha Allen", "Litao Zhang", "Tommaso Mansi", "Rui Liao", "Zhoubing Xu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0287", "x": 2.45, "y": 3.549, "title": "MorphoBoost: Morphology-Driven Boundary Enhancement Model for Accurate Segmentation of Langerhans Cells in Corneal Confocal Microscopy Images", "abstract": "Accurate segmentation of Langerhans cells (LCs) in corneal confocal microscopy (CCM) images is crucial for diagnosing and monitoring various ocular and systemic diseases. However, existing segmentation methods often struggle with the misidentification of activated LCs and inaccurate boundary delineation due to their complex morphological features and background noise. In this paper, we propose a novel segmentation framework, MorphoBoost, which integrates morphologydriven data augmentation and boundary optimization loss to address these challenges. MorphoBoost employs a localization before segmentation strategy, enhancing the diversity of activated LCs via spatial and appearance transformations, and refining segmentation boundaries through pixel-level and image-level optimizations. Our methods achieve state-of-the-art performance in segmenting both LCs types, especially activated ones. It establishes a new benchmark with a 17.10% increase in the Dice coefficient and a 5.71 decrease in modified Hausdorff distance over previous methods. This is bolstered by validation on clinical data.", "filename": "2025_0287.pdf", "year": 2025, "institution": "Ningbo Institute of Materials Technology and Engineering", "country": "China", "authors": ["Hongshuo Li", "Ankai Dong", "Tiande Zhang", "Shijia Zhou", "Yalin Zheng", "Lei Mou", "Yitian Zhao"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0288", "x": 3.117, "y": 1.977, "title": "MRI Motion Artifact Correction via Frequency-Assisted Artifact Disentanglement and Confidence-Guided Knowledge Distillation", "abstract": "Motion artifacts degrade MR image quality affecting clinical diagnoses. Although deep learning-based motion artifact correction (MAC) methods show promise, they are limited by the lack of real paired motion-corrupted and motion-free images. We propose a novel frequencyassisted artifact disentanglement learning framework for MAC of MR images. Our approach integrates a frequency-decomposed motion correction network (FDMC-Net) for content-artifact disentanglement over the real unpaired data, coupled with confidence-guided knowledge distillation using simulated paired data. Specifically, considering that motion artifacts are primarily caused by high-frequency k-space misalignment, FDMC-Net decomposes motion-corrupted MR images into low-frequency and high-frequency components and then employs dedicated encoders to disentangle content and artifact features. FDMC-Net is trained by unsupervised cycle-consistent adversarial loss over realistic unpaired data, and confidence-guided knowledge distillation loss by distilling a teacher model trained on simulated paired data. Experiments demonstrate its state-ofthe-art performance, with ablation studies confirming the effectiveness of frequency-assisted disentanglement and confidence-guided distillation.", "filename": "2025_0288.pdf", "year": 2025, "institution": "Xian Jiaotong University", "country": "China", "authors": ["Jiazhen Wang", "Heran Yang", "Yizhe Yang", "Jian Sun"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0289", "x": 2.68, "y": 1.346, "title": "MS-IQA: A Multi-scale Feature Fusion Network for PET/CT Image Quality Assessment", "abstract": "Positron Emission Tomography/Computed Tomography (PET/CT) plays a critical role in medical imaging, combining functional and anatomical information to aid in accurate diagnosis. However, image quality degradation due to noise, compression and other factors could potentially lead to diagnostic uncertainty and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT image, both low-level features like distortions and high-level features like organ anatomical structures affect the diagnostic value of the image. However, existing medical image quality assessment (IQA) methods are unable to account for both feature types simultaneously. In this work, we propose MS-IQA, a novel multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale features from various intermediate layers of ResNet and Swin Transformer, enhancing its ability of perceiving both local and global information. In addition, a multi-scale feature fusion module is also introduced to effectively combine high-level and low-level information through a dynamically weighted channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset, we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT images with quality scores assigned by radiologists. Experiments on our dataset and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed model has achieved superior performance against existing state-of-the-art methods in various IQA metrics. This work provides an accurate and efficient IQA method for PET/CT. Our code and dataset are available at https://github.com/MS-IQA/MS-IQA/.", "filename": "2025_0289.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Siqiao Li", "Chen Hui", "Wei Zhang", "Rui Liang", "Chenyue Song", "Feng Jiang", "Haiqi Zhu", "Zhixuan Li", "Hong Huang", "Xiang Li"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0290", "x": 2.722, "y": 1.404, "title": "Multi-tracer Uptake Correction for PET-MR via Aligned-Feature Guidance and Multi-scale Pixel-Adaptive Routing", "abstract": "Positron Emission Tomography combined with MagneticResonance (PET-MR) imaging has emerged as a promising modality that offers both soft tissue and biochemical function information, while substantially reducing radiation exposure compared to PET-CT imaging. However, systematic clinical evaluations reveal notable discrepancies in standardized uptake value ratios between PET-MR and PET-CT scans, largely due to the inherent limitations of MR-based PET attenuation correction. To address this issue, we propose a unified uptake correction framework to harmonize PET-MR images with PET-CT scans across different tracers. This framework employs a three-stage training scheme. The first stage learns to represent CT features, aiming to capture condensed anatomical patterns associated with PET imaging. The second stage aligns MR features to the fixed CT features learned in the first stage, thereby enabling the transfer of anatomical prior knowledge from CT to MR features. The third stage integrates aligned MR features to guide PET-MR tracer uptake correction and uses a Multi-scale Pixel Routing module to mitigate interference among different tracers. We conduct comprehensive experiments on 70 patients with three distinct tracers to demonstrate the superiority of our framework over existing methods in PET-MR harmonization with PET-CT images. This work represents the first investigation and solution for multi-tracer quantification discrepancies between PET-MR and standard PET-CT, potentially advancing the clinical standardization of PET-MR imaging. Our code will be available at GitHub.", "filename": "2025_0290.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Aocheng Zhong", "Haolin Huang", "Jing Wang", "Zhenrong Shen", "Haiyu Song", "Junlei Wu", "Yuhua Zhu", "Yang Liu", "Chuantao Zuo", "Qian Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0291", "x": 2.438, "y": 3.426, "title": "Noisy Label Refinement Based on Discrete Diffusion Process in 3D Ossicle Segmentation", "abstract": "Ossicular chain lesions can cause hearing loss, making accurate segmentation of ossicles critical for clinical diagnosis and treatment. Ultra-high-resolution computed tomography (U-HRCT) provides quality images for ossicle segmentation tasks, but the complex structure of the stapes and variations in annotators' experience often lead to noisy labels in 3D annotation within clinical practice. To address this, we propose a novel framework tailored for two types of noisy labels: (1) incompletestructure labels, and (2) complete-structure but inaccurate labels. For the former, we introduce a Dilating&Selecting (D&S) framework, which completes missing structures using a dilating Volumetric Discrete Diffusion Refiner (VDDR) with a novel cover loss and evaluates label completeness via a completeness selection strategy. For the latter, we introduce a noise-based augmentation to better train VDDR. Experimental results demonstrate that D&S framework reduce the time cost of manual annotation by 90.2%, while VDDR outperforms other state-of-the-art methods. To facilitate further research and development, our code and two datasets are publicly available.", "filename": "2025_0291.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Linqian Fan", "Mengshi Zhang", "Yonghao Wang", "Wenkai Lu", "Hongxia Yin"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0292", "x": 1.85, "y": 3.608, "title": "NQNN: Noise-Aware Quantum Neural Networks for Medical Image Classification", "abstract": "Noisy labels in high-dimensional, and multiclass medical image datasets pose a significant challenge for machine learning models. While hybrid quantum-classical architectures, such as quantum neural networks (QNNs), have shown promise in medical imaging, their robustness under noisy label conditions remains largely unexplored. To address this gap, we propose a Noise-aware Quantum Neural Network (NQNN), integrating Fourier Attenuation, Reweight Estimation, and Adaptive Pooling to enhance feature extraction and classification robustness. Fourier Attenuation filters high-frequency noise, Reweight Estimation prioritizes cleaner labels based on uncertainty, and Adaptive Pooling dynamically refines feature aggregation. We evaluate NQNN on six benchmark medical datasets (PathMNIST, BloodMNIST, OrganAM-NIST, OrganCMNIST, OCTMNIST, and DermaMNIST) across noise ratios (10%, 30%, and 50%) and classification configurations (binary, four-class, and full multiclass). Comparative benchmarks against five QNN-based and two deep-learning baselines demonstrate NQNN's superior performance, such as achieving 80.25% accuracy on organCMNIST at 10% noise and maintaining strong performance at higher noise ratios. Our ablation studies validate the effectiveness of each noise-handling mechanism, highlighting their complementary contributions to noise robustness. By bridging quantum advancements with real-world medical diagnostics, NQNN establishes a new benchmark for noise-resilient medical image classification, offering a scalable and adaptive quantumclassical learning framework.", "filename": "2025_0292.pdf", "year": 2025, "institution": "Boise State University", "country": "USA", "authors": ["Maqsudur Rahman", "Jun Zhuang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0293", "x": 6.394, "y": 4.238, "title": "Path Signature Features Revealed SSRI-Induced White Matter Morphological Reorganization in Depressions", "abstract": "Selective serotonin reuptake inhibitors (SSRIs) are recognized as the first-line treatment for major depressive disorder (MDD); however, the characterization of their microstructural effects on white matter (WM) is still limited. This study presents a novel path signature (PS) framework to quantify longitudinal WM plasticity utilizing clinical-grade diffusion magnetic resonance imaging (dMRI) data. This approach overcomes the limitations of conventional diffusion metrics, achieving a sensitivity of 1 mm 3 without requiring high-resolution imaging. By combining rough path theory with super-resolution mapping, significant SSRIinduced reorganization is found in the transverse pontine tract, left anterior limb of the internal capsule, and splenium of the corpus callosum in MDD patients. Changes in PS features in these fiber bundles and the left corticospinal tract correlate positively with reductions in the 17-item Hamilton Depression Rating Scale scores, providing preliminary evidence of a relationship between WM alterations and clinical outcomes. The findings establish PS analysis as a promising tool for detecting macrostructural plasticity in WM due to SSRIs, thereby bridging the critical gap between microstructural diffusion metrics and circuit-level reorganization, and providing a novel insight into comprehensive biomarkers for precision antidepressant therapy.", "filename": "2025_0293.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Jiaolong Qin", "Weihong Dong", "Huangjing Ni", "Zhijian Yao", "Qing Lu", "Ye Wu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0294", "x": 2.767, "y": 6.419, "title": "PathVG: A New Benchmark and Dataset for Pathology Visual Grounding", "abstract": "With the rapid development of computational pathology, many AI-assisted diagnostic tasks have emerged. Cellular nuclei segmentation can segment various types of cells for downstream analysis, but it relies on predefined categories and lacks flexibility. Moreover, pathology visual question answering can perform image-level understanding but lacks region-level detection capability. To address this, we propose a new benchmark called Pathology Visual Grounding (PathVG), which aims to detect regions based on expressions with different attributes. To evaluate PathVG, we create a new dataset named RefPath which contains 27,610 images with 33,500 language-grounded boxes. Compared to visual grounding in other domains, PathVG presents pathological images at multi-scale and contains expressions with pathological knowledge. In the experimental study, we found that the biggest challenge was the implicit information underlying the pathological expressions. Based on this, we proposed Pathology Knowledge-enhanced Network (PKNet) as the baseline model for PathVG. PKNet leverages the knowledge-enhancement capabilities of Large Language Models (LLMs) to convert pathological terms with implicit information into explicit visual features, and fuses knowledge features with expression features through the designed Knowledge Fusion Module (KFM). The proposed method achieves state-of-theart performance on the PathVG benchmark. The source code and dataset have been available at https://github.com/ssecv/PathVG.", "filename": "2025_0294.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Chunlin Zhong", "Shuang Hao", "Junhua Wu", "Xiaona Chang", "Jiwei Jiang", "Xiu Nie", "He Tang", "Xiang Bai"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0295", "x": 1.591, "y": 2.637, "title": "Pose as Clinical Prior: Learning Dual Representations for Scoliosis Screening", "abstract": "Recent AI-based scoliosis screening methods primarily rely on large-scale silhouette datasets, often neglecting clinically relevant postural asymmetries-key indicators in traditional screening. In contrast, pose data provide an intuitive skeletal representation, enhancing clinical interpretability across various medical applications. However, pose-based scoliosis screening remains underexplored due to two main challenges: (1) the scarcity of large-scale, annotated pose datasets; and (2) the discrete and noise-sensitive nature of raw pose coordinates, which hinders the modeling of subtle asymmetries. To address these limitations, we introduce Scoliosis1K-Pose, a 2D human pose annotation set that extends the original Scoliosis1K dataset, comprising 447,900 frames of 2D keypoints from 1,050 adolescents. Building on this dataset, we introduce the Dual Representation Framework (DRF), which integrates a continuous skeleton map to preserve spatial structure with a discrete Postural Asymmetry Vector (PAV) that encodes clinically relevant asymmetry descriptors. A novel PAV-Guided Attention (PGA) module further uses the PAV as clinical prior to direct feature extraction from the skeleton map, focusing on clinically meaningful asymmetries. Extensive experiments demonstrate that DRF achieves state-of-the-art performance. Visualizations further confirm that the model leverages clinical asymmetry cues to guide feature extraction and promote synergy between its dual representations. The dataset and code are publicly available at https://zhouzi180.github.io/Scoliosis1K/.", "filename": "2025_0295.pdf", "year": 2025, "institution": "Southern University of Science and Technology", "country": "China", "authors": ["Zirui Zhou", "Zizhao Peng", "Dongyang Jin", "Chao Fan", "Fengwei An", "Shiqi Yu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0296", "x": 1.057, "y": 1.302, "title": "PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model Development", "abstract": "Deep learning (DL), a pivotal technology in artificial intelligence, has recently gained substantial traction in the domain of dental auxiliary diagnosis. However, its application has predominantly been confined to imaging modalities such as panoramic radiographs and Cone Beam Computed Tomography, with limited focus on auxiliary analysis specifically targeting Periapical Radiographs (PR). PR are the most extensively utilized imaging modality in endodontics and periodontics due to their capability to capture detailed local lesions at a low cost. Nevertheless, challenges such as projection angle and artifacts complicate the annotation and recognition of PR, leading to a scarcity of publicly available, large-scale, high-quality PR analysis datasets. This scarcity has somewhat impeded the advancement of DL applications in PR analysis. In this paper, we present PRAD-10K, a dataset for PR analysis. PRAD-10K comprises 10,000 clinical periapical radiograph images, with pixel-level annotations provided by professional endodontists for nine distinct anatomical structures, lesions, and artificial restorations or medical devices. We also include classification labels for images with typical conditions or lesions. Furthermore, we introduce a DL network named PRNet to establish benchmarks for PR segmentation tasks. Experimental results demonstrate that PRNet surpasses previous state-of-the-art medical image segmentation models on the PRAD-10K dataset. The code and dataset will be released at https://github.com/nkicsl/PRAD.", "filename": "2025_0296.pdf", "year": 2025, "institution": "Nankai University", "country": "China", "authors": ["Zhenhuan Zhou", "Yuchen Zhang", "Ruihong Xu", "Xuansen Zhao", "Tao Li"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0297", "x": 3.708, "y": 5.349, "title": "Pretraining on Chronic Lung Inflammatory Disease Datasets to Enhance Indeterminant Lung Cancer Classification Using Masked Autoencoders", "abstract": "Lung cancer remains the leading cause of cancer-related mortality in the United States, despite the adoption of low-dose computed tomography (LDCT) and updated screening guidelines from the United States Preventive Service Task Force (USPSTF) [19]. Limited infrastructure and financial costs continue to hinder widespread LDCT adoption, while the increasing detection of indeterminate pulmonary nodules (4-20 mm) challenges accurate diagnosis and clinical decision-making. We address these limitations by pretraining masked autoencoders (MAE) on the COPDGene dataset, which captures chronic lung inflammatory disease features. Emphysema and airway disease, two distinct subtypes of COPD, are pathophysiological manifestations of chronic lung inflammation [ 4,15]. Incorporating these features may enhance the model's ability to distinguish between malignant and benign pulmonary nodules. By exploring multiple masking strategies, we optimize network attention on parenchymal and perinodular features, improving the extraction of relevant image biomarkers. Our results demonstrate that pretraining on the COPDGene dataset using random masking (r-masking) achieves superior classification performance, with a sensitivity of 88.79%, specificity of 86.27%, and an AUC of 0.931, when compared to selfpretraining on National Lung Cancer Screening Trial (NLST), and supervised learning on NLST. This highlights the importance of leveraging chronic disease datasets for self-supervised learning and underscores the potential of MAE-based approaches to improve nodule classification in clinical settings. Code available at https://github.com/axemasquelin/ RegionalMAE.", "filename": "2025_0297.pdf", "year": 2025, "institution": "Department of Radiology", "country": "USA", "authors": ["Axel H P Masquelin", "Raúl San José Estépar"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0298", "x": 1.478, "y": 5.748, "title": "Query-Level Alignment for End-to-End Lesion Detection with Human Gaze", "abstract": "Lesion detection for medical image is crucial in computeraided diagnostic systems, enabling early disease identification and enhancing clinical decision-making. Existing lesion detection models primarily rely on bounding boxes for supervision, which overemphasize lesion boundaries while neglecting critical internal features, potentially resulting in misdetections. In contrast, clinicians' gaze, which reflects the visual focus during diagnosis, captures internal semantic patterns of lesions, providing a more informative supervisory signal than conventional annotations. Inspired by this insight, we propose a gaze-driven detection framework for enhancing lesion identification accuracy. Specifically, our framework introduces three key gaze-prioritized innovations: 1) an adaptive gaze kernel that prioritizes diagnostically significant highmagnification regions, 2) a gaze-guided assignment module that establishes query-level gaze-region correspondence, and 3) a query-level consistent loss that aligns detection model attention with clinicians' gaze patterns. By incorporating clinicians' expertise through gaze data, our method improves lesion detection accuracy and clinical interpretability. In addition, our method can be designed as a plug-and-play module, which maintains compatibility with mainstream object detectors. To validate the effectiveness of our method, we employ two public and one private datasets, and extensive experiments demonstrate its superiority over existing approaches. Furthermore, we contribute a pioneering gazetracking dataset with 1,669 precise gaze annotations, establishing a new benchmark for gaze-driven research in object detection. The dataset and code is available at https://github.com/YanKong0408/GAA-DETR.", "filename": "2025_0298.pdf", "year": 2025, "institution": "Nanjing University", "country": "China", "authors": ["Yan Kong", "Zhixiang Peng", "Yuan Yin", "Yonghao Li", "Jiangdong Cai", "Sheng Wang", "Qian Wang", "Yuqi Fang", "Caifeng Shan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0299", "x": 1.493, "y": 5.347, "title": "ReCo-I2P: An Incomplete Supervised Lymph Node Segmentation Framework Based on Orthogonal Partial-Instance Annotation", "abstract": "Quantitative analysis of lymph node volume is instrumental in the diagnosis and treatment of cancer. However, automatic segmentation models for lymph nodes necessitate pixel-level labeling, which is both time-consuming and labor-intensive. The scarcity of pixel-level annotations has thus spurred interest in label-efficient learning as a potential solution. Considering the variance of shapes and locations, and the lowcontrast appearance of lymph nodes in computed tomography scans, we propose a new incomplete annotation strategy called orthogonal partialinstance annotation, in which only two orthogonal slices of a small portion of lymph nodes are annotated. To segment as many lymph nodes as possible from such sparse annotations, we propose a prototype-based label-efficient learning framework with a specifically designed loss. Specifically, we extract intra-batch prototypes from the output features of the encoder and store inter-batch prototypes using a momentum-smoothing approach. To re-inject the extracted information from the two kinds of prototypes, we introduce a feature augmentation module that utilizes the extracted prototypes to enhance features. To further complement the predictions generated from enhanced features with those from original features, we design a reliability-based co-teaching strategy based on feature similarity. Experiments demonstrate that our proposed framework outperforms other methods on two mediastinal lymph node datasets. Our implementation is available at https://github.com/HiLab-git/WCODE- PIA.", "filename": "2025_0299.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Litingyu Wang", "Ping Ye", "Wenjun Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0300", "x": 3.053, "y": 4.835, "title": "RIFNet: Bridging Modalities for Accurate and Detailed Ocular Disease Analysis", "abstract": "Color fundus photography (CFP) is widely used in clinical practice for its convenience and accessibility. However, it faces challenges such as low image quality, limited depth information, susceptibility to artifacts and low contrast, which reduce diagnostic accuracy and hinder the detection of small lesions. Fluorescein angiography (FA), on the other hand, effectively highlights features such as vascular leakage and non-perfusion. However, it also has drawbacks, including health risks and the lack of color information. To address these challenges, we propose a multi-stage retinal image fusion framework, RIFNet, to improve image quality and diagnostic efficacy by integrating multimodal information from CFP and FA. First, to address the problem of missing modalities due to the difficulty of accessing FA as an intrusive inspection, we design a bi-stream generative subnetwork to generate pseudo FA images by pre-training with real CFP images as the generating condition, which effectively supplements the modality information. Subsequently, the color representations of different modalities are unified by color coding, and fed into the multimodal discriminative fusion network to generate the fused color-coded images. Finally, a multiscale reconstruction method is used to generate a high-resolution and high-contrast enhanced image. Experiments demonstrate that this multimodal fusion framework supplements FA information, reduces medical costs, and reveals lesion details unobservable with a single modality, supporting accurate ocular disease diagnosis.Y. Li and Q. Hou contribute equally to this work.", "filename": "2025_0300.pdf", "year": 2025, "institution": "Northeastern University", "country": "China", "authors": ["Yuqing Li", "Qingshan Hou", "Peng Cao", "Jianguo Ju", "Tianqi Wang", "Meng Wang", "Ke Zou", "Yih Chung Tham", "Huazhu Fu", "Osmar R Zaiane"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0301", "x": 3.852, "y": 3.534, "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels", "abstract": "The accurate segmentation of myocardial scars from cardiac MRI is essential for clinical assessment and treatment planning. In this study, we propose a robust deep-learning pipeline for fully automated myocardial scar detection and segmentation by fine-tuning state-of-theart models. The method explicitly addresses challenges of label noise from semi-automatic annotations, data heterogeneity, and class imbalance through the use of Kullback-Leibler loss and extensive data augmentation. We evaluate the model's performance on both acute and chronic cases and demonstrate its ability to produce accurate and smooth segmentations despite noisy labels. In particular, our approach outperforms state-of-the-art models like nnU-Net and shows strong generalizability in an out-of-distribution test set, highlighting its robustness across various imaging conditions and clinical tasks. These results establish a reliable foundation for automated myocardial scar quantification and support the broader clinical adoption of deep learning in cardiac imaging. The codes are available at: https://github.com/Danialmoa/YoloSAM.", "filename": "2025_0301.pdf", "year": 2025, "institution": "University of Leicester", "country": "UK", "authors": ["Aida Moafi", "Danial Moafi", "Evgeny M Mirkes", "Gerry P Mccann", "Abbas S Alatrany", "Jayanth Ranjit Arnold", "Mostafa Mehdipour Ghazi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0302", "x": -0.187, "y": 4.815, "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model", "abstract": "Recent \"segment anything\" efforts show promise by learning from large-scale data, but adapting such models directly to medical images remains challenging due to the complexity of medical data, noisy annotations, and continual learning requirements across diverse modalities and anatomical structures. In this work, we propose SAMed-2, a new foundation model for medical image segmentation built upon the SAM-2 architecture. Specifically, we introduce a temporal adapter into the image encoder to capture image correlations and a confidence-driven memory mechanism to store high-certainty features for later retrieval. This memory-based strategy counters the pervasive noise in large-scale medical datasets and mitigates catastrophic forgetting when encountering new tasks or modalities. To train and evaluate SAMed-2, we curate MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21 medical segmentation tasks. Our experiments on both internal benchmarks and 10 external datasets demonstrate superior performance over state-of-the-art baselines in multi-task scenarios.", "filename": "2025_0302.pdf", "year": 2025, "institution": "Lehigh University", "country": "USA", "authors": ["Zhiling Yan", "Sifan Song", "Dingjie Song", "Yiwei Li", "Rong Zhou", "Weixiang Sun", "Zhennong Chen", "Sekeun Kim", "Hui Ren", "Tianming Liu", "Quanzheng Li", "Xiang Li", "Lifang He", "Lichao Sun"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0303", "x": 3.337, "y": 1.908, "title": "SIMPLE: Simultaneous Multi-plane Self-supervised Learning for Isotropic MRI Restoration from Anisotropic Data", "abstract": "Magnetic resonance imaging (MRI) is vital for diagnosing abdominal and neurological conditions, yet conventional sequential slice acquisitions favor in-plane over through-plane resolution to minimize scan time and motion artifacts, leading to anisotropic data and reduced volumetric accuracy. Existing super-resolution (SR) techniques reconstruct isotropic images from anisotropic scans but often rely on simulated downsampling or limited 3D isotropic data, emphasizing through-plane interpolation rather than preserving full anatomy. We introduce SIM-PLE, a Simultaneous Multi-Plane Self-Supervised Learning approach that directly restores isotropic MRI from real-world multi-plane acquisitions via adversarial training. Testing on OASIS-1 brain (n = 416) and Crohn's disease abdominal (n = 115) MRI datasets demonstrates SIMPLE's superiority in image fidelity and anatomical detail over stateof-the-art methods. Notably, SIMPLE achieved lower averaged Kernel Inception Distance (KID) scores than SMORE4 in both brain MRI (28.709 vs. 29.295) and abdominal MRI (17.435 vs. 20.724), retained higher-frequency details as confirmed by Fourier analysis, and was rated 1.5 points higher in the axial plane by radiologists. By improving volumetric analysis and 3D reconstructions, SIMPLE shows promise for enhancing diagnostic accuracy in pathologies demanding precise structural visualization. Our source code is publicly available at https:// github.com/TechnionComputationalMRILab/SIMPLE.", "filename": "2025_0303.pdf", "year": 2025, "institution": "Technion -Israel Institute of Technology", "country": "Israel", "authors": ["Rotem Benisty", "Yevgenia Shteynman", "Moshe Porat", "Anat Ilivitzki", "Moti Freiman"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0304", "x": 2.977, "y": 1.849, "title": "Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction", "abstract": "MR imaging techniques are of great benefit to disease diagnosis. However, due to the limitation of MR devices, significant intensity inhomogeneity often exists in imaging results, which impedes both qualitative and quantitative medical analysis. Recently, several unsupervised deep learning-based models have been proposed for MR image improvement. However, these models merely concentrate on global appearance learning, and neglect constraints from image structures and smoothness of bias field, leading to distorted corrected results. In this paper, novel structure and smoothness constrained dual networks, named S2DNets, are proposed aiming to self-supervised bias field correction. S2DNets introduce piece-wise structural constraints and smoothness of bias field for network training to effectively remove non-uniform intensity and retain much more structural details. Extensive experiments executed on both clinical and simulated MR datasets show that the proposed model outperforms other conventional and deep learning-based models. In addition to comparison on visual metrics, downstream MR image segmentation tasks are also used to evaluate the impact of the proposed model. The source code is available at:https://github.com/LeongDong/S2DNets.", "filename": "2025_0304.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Dong Liang", "Xingyu Qiu", "Yuzhen Li", "Wei Wang", "Kuanquan Wang", "Suyu Dong", "Gongning Luo"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0305", "x": 2.451, "y": 5.33, "title": "T2WI-BCMIC: Non-Fat Saturated T2-Weighted Imaging Dataset for Bladder Cancer Muscle Invasion Classification", "abstract": "Accurate classification of muscle invasion in bladder cancer using computer-aided diagnosis (CAD) is crucial for timely intervention and improved prognosis. Despite advances in deep learning for medical image analysis, muscle invasion classification remains limited by the scarcity of publicly available annotated datasets. To address this, we introduce T2WI-BCMIC, the first expert-annotated dataset for bladder cancer muscle invasion classification. T2WI-BCMIC contains Nonfat saturated T2-weighted magnetic resonance imaging (MRI) images with five-class annotations, covering various invasion depths. We establish a benchmark using several popular deep learning architectures, providing a solid foundation for future comparisons. However, achieving further performance improvements remains challenging due to the small dataset size. Therefore, we propose a novel search-based data augmentation algorithm that increases data diversity by maximizing the divergence from the class-specific manifold, while preserving the class distribution to maintain class identity. Experimental results on T2WI-BCMIC show that our algorithm outperforms existing methods, achieving significant performance improvements. The T2WI-BCMIC dataset and benchmark are available at: https://github.com/T2-MI/T2WI-BCMIC for further research.", "filename": "2025_0305.pdf", "year": 2025, "institution": "South China University of Technology", "country": "China", "authors": ["Han Huang", "Weiyi Chen", "Qiuxia Wu", "Huanjun Wang", "Qian Cai", "Yan Guo"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0306", "x": 3.485, "y": 1.914, "title": "TESLA: Test-Time Reference-Free Through-Plane Super-Resolution for Multi-Contrast Brain MRI", "abstract": "Through-plane super-resolution (SR) in brain magnetic resonance imaging (MRI) is clinically important during clinical assessments. Most existing multi-contrast SR models mainly focus on enhancing inplane image resolution, relying on functions already integrated into MRI scanners. These methods usually leverage proprietary fusion techniques to integrate multi-contrast images, resulting in diminished interpretability. Furthermore, the requirement for reference images during testing limits their applicability in clinical settings. We propose a TEst time reference-free through-plane Super-resoLution network using disentAngled representation learning in multi-contrast MRI (TESLA) to address these challenges. Our method is developed on the premise that multicontrast images consist of shared content (structure) and independent stylistic (contrast) features. Thus, after progressively reconstructing the target image in the first stage, we divide it into shared and independent elements during the structure enhancement phase. In this stage, we employ a pre-trained ContentNet to effectively disentangle high-quality structural information from the reference image, enabling the shared components of the target image to learn directly from those of the reference image through patch-wise contrastive learning during training. Consequently, the proposed model enhances clinical applicability while ensuring model interpretability. Extensive experimental results demonstrate that the proposed model performs favorably against other state-ofthe-art multi-contrast SR models, especially in restoring structural fine details in the through-plane direction. The code is publicly available at https://github.com/Yonsei-MILab/TESLA.", "filename": "2025_0306.pdf", "year": 2025, "institution": "Yonsei University", "country": "Republic of Korea", "authors": ["Yoonseok Choi", "Sunyoung Jung", "Mohammed A Al-Masni", "Ming-Hsuan Yang", "Dong-Hyun Kim"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0307", "x": 4.348, "y": 3.436, "title": "Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining", "abstract": "Artificial intelligence has recently shown promise in automated embryo selection for In-Vitro Fertilization (IVF). However, current approaches either address partial embryo evaluation lacking holistic quality assessment or target clinical outcomes inevitably confounded by extra-embryonic factors, both limiting clinical utility. To bridge this gap, we propose a new task called Video-Based Embryo Gradingthe first paradigm that directly utilizes full-length time-lapse monitoring (TLM) videos to predict embryologists' overall quality assessments. To support this task, we curate a real-world clinical dataset comprising over 2,500 TLM videos, each annotated with a grading label indicating the overall quality of embryos. Grounded in clinical decision-making principles, we propose a Complementary Spatial-Temporal Pattern Mining (CoSTeM) framework that conceptually replicates embryologists' evaluation process. The CoSTeM comprises two branches: (1) a morphological branch using a Mixture of Cross-Attentive Experts layer and a Temporal Selection Block to select discriminative local structural features, and (2) a morphokinetic branch employing a Temporal Transformer to model global developmental trajectories, synergistically integrating static and dynamic determinants for grading embryos. Extensive experimental results demonstrate the superiority of our design. This work provides a valuable methodological framework for AI-assisted embryo selection. The source code is available at https://github.com/RIL-Lab/CoSTeM.", "filename": "2025_0307.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Yong Sun", "Yipeng Wang", "Junyu Shi", "Zhiyuan Zhang", "Yanmei Xiao", "Lei Zhu", "Manxi Jiang", "Qiang Nie"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0308", "x": 0.896, "y": 1.077, "title": "Towards Automated Pediatric Dental Development Staging: A Dataset and Model", "abstract": "Dental development assessment (DDA) is crucial for orthodontic diagnosis and treatment planning. Recent advances in deep learning have shown promising results in dental image analysis tasks. However, the study of dental development staging, particularly in pediatric dental development, remains underexplored. This is primarily attributed to the scarcity of publicly available datasets. In this paper, we present a pediatric Dental Development Staging Dataset(DentalDS). To the best of our knowledge, this is the first publicly available dataset for pediatric DDA. It comprises 2,583 orthopantomogram (OPG) images, with a total of 18,081 annotated teeth. Furthermore, we propose a dental development staging network (DDSNet) designed to address the classification of tooth development stages. In DDSNet, we propose a Region-Instance Cross-Attention (RICA) block and a Multi-Expert Collaborative Classification (MECC) block to enhance the fine-grained feature fusion and classification accuracy of dental development stages. To evaluate the effectiveness of the proposed DDSNet, we conducted experiments on the DentalDS. Our proposed method achieves the state-ofthe-art accuracy of 76.3% and an F1-score of 77.1%, outperforming the existing approach method by 1.9% in accuracy and 3.8% in F1-score. To facilitate further research in pediatric orthodontic treatment, code and dataset will be available at https://github.com/ybupengwang/DDSNet.", "filename": "2025_0308.pdf", "year": 2025, "institution": "Nankai University", "country": "China", "authors": ["Peng Wang", "Along He", "Anli Wang", "Zhenhuan Zhou", "Xiaohang Guan", "Tao Li"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0309", "x": 2.463, "y": 1.28, "title": "UDPET: Ultra-low Dose PET Imaging Challenge Dataset", "abstract": "Positron emission tomography (PET) is widely recognized as the most sensitive molecular imaging modality, enabling the in vivo visualization of molecular pathways. Despite its exceptional utility, concerns about ionizing radiation exposure have limited its broader application. A recent breakthrough in totalbody PET imaging addresses this limitation by significantly increasing geometric coverage and sensitivity. This innovation reduces radiation exposure to levels comparable to the dose received during a transatlantic flight, achieved through advanced computational techniques. To accelerate progress in this field, we have curated a benchmark dataset specifically designed for developing ultra-low dose PET imaging methodologies. This dataset was pivotal in the Ultra-Low Dose PET Imaging Challenge held in 2022, 2023, and 2024. The challenge aimed to foster innovative computational algorithms capable of recovering high-quality imaging from low-dose scans acquired on total-body PET systems. The dataset includes both standard-dose and simulated low-dose total-body PET images from 1,447 patients. These were acquired using Siemens Biograph Vision Quadra PET/CT and United Imaging uExplorer PET/CT scanners. In addition, we have developed a customized evaluation system to assess the performance of algorithms in recovering image quality from low-dose scans. This paper provides a comprehensive description of the benchmark dataset and evaluation framework, aimed at driving future advancements in ultra-low dose PET imaging. The dataset is available at https://udpet-challenge.github.io, subject to the completion of a signed Data Transfer Agreement.", "filename": "2025_0309.pdf", "year": 2025, "institution": "Bern University Hospital", "country": "Switzerland", "authors": ["Song Xue", "Hanzhong Wang", "Yizhou Chen", "Fanxuan Liu", "Hong Zhu", "Marco Viscione", "Rui Guo", "Axel Rominger", "Biao Li", "Kuangyu Shi"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0310", "x": 3.655, "y": 5.112, "title": "Uncertainty-Aware Multi-expert Knowledge Distillation for Imbalanced Disease Grading", "abstract": "Automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. However, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. To address the problem, we propose a novel Uncertainty-aware Multi-experts Knowledge Distillation (UMKD) framework to transfer knowledge from multiple expert models to a single student model. Specifically, to extract discriminative features, UMKD decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. At the output space, an uncertainty-aware decoupled distillation (UDD) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. Additionally, UMKD also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous KD approaches. Extensive experiments on histology prostate grading (SICAPv2 ) and fundus image grading (APTOS ) demonstrate that UMKD achieves a new state-of-theart in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading. The source code has been released by https://github.com/ZJUMAI/UMKD. S. Tong and S. Gao-Equal contribution.", "filename": "2025_0310.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Shuo Tong", "Shangde Gao", "Ke Liu", "Zihang Huang", "Hongxia Xu", "Haochao Ying", "Jian Wu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0311", "x": 3.021, "y": 1.795, "title": "Unsupervised Learning-Based Susceptibility Artifact Correction for Diffusion-Weighted MRI in Multiple Organs", "abstract": "Diffusion-weighted MRI (DWI) is widely used for assessing tissue microstructure, with echo-planar imaging (EPI) sequences being the preferred acquisition method due to their fast speed. However, EPI-based DWI is highly sensitive to field inhomogeneities, leading to susceptibility-induced distortions that compromise image quality. Traditional correction methods, such as TOPUP, estimate displacement fields from a pair of reversed phase-encoding (reversed-PE) images to mitigate these distortions. While effective, these approaches suffer from high computational cost, limiting their clinical utility. In this study, we propose an unsupervised learning method for susceptibility artifact correction in EPI. A transformer-style convolutional network enhanced with deformable convolutions is developed to estimate the displacement field from a pair of reversed-PE images, followed by image unwarping and intensity modulation to generate the distortion-free images. This approach surpasses the performance of conventional U-Net-based methods in accuracy. Additionally, a spatially weighted smoothness loss is introduced to enhance robustness against noise in the input data so that the predicted displacement fields from a pair of low b-value DWI can be applied to correct other images with different b-values and diffusion directions from the same subject, optimizing acquisition and computational efficiency. A single model was trained and evaluated on large datasets from multiple organs, acquired with diverse imaging sequences and parameters, at both 1.5T and 3T. Our results demonstrate that the proposed approach achieves generalizable high-quality distortion correction while significantly reducing processing time compared to TOPUP, highlighting its potential for clinical translation.", "filename": "2025_0311.pdf", "year": 2025, "institution": "Siemens Healthineers", "country": "USA", "authors": ["Shihan Qiu", "Radu Miron", "Yahang Li", "Cornelius Eichner", "Thorsten Feiweier", "Nirmal Janardhanan", "Bryan Clifford", "Omar Darwish", "Mahmoud Mostapha", "Mariappan S Nadar"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0312", "x": 1.154, "y": 2.387, "title": "Veriserum: A Dual-Plane Fluoroscopic Dataset with Knee Implant Phantoms for Deep Learning in Medical Imaging", "abstract": "Veriserum is an open-source dataset designed to support the training of deep learning registration for dual-plane fluoroscopic analysis. It comprises approximately 110,000 X-ray images of 10 knee implant pair combinations (2 femur and 5 tibia implants) captured during 1,600 trials, incorporating poses associated with daily activities such as level gait and ramp descent. Each image is annotated with an automatically registered ground-truth pose, while 200 images include manually registered poses for benchmarking.Key features of Veriserum include dual-plane images and calibration tools. The dataset aims to support the development of applications such as 2D/3D image registration, image segmentation, X-ray distortion correction, and 3D reconstruction.", "filename": "2025_0312.pdf", "year": 2025, "institution": "ETH Zürich", "country": "Switzerland", "authors": ["Jinhao Wang", "Florian Vogl", "Pascal Schütz", "Saša Ćuković", "William R Taylor"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0313", "x": 2.952, "y": 3.735, "title": "VesselVerse: A Dataset and Collaborative Framework for Vessel Annotation", "abstract": "This paper is not about a novel method. Instead, it introduces VesselVerse, a large-scale annotation dataset and collaborative framework for brain vessel annotation. It addresses the critical challenge of data annotation availability in supervised learning segmentation and provides a valuable resource for the community. VesselVers represents the largest public release of brain vessel annotations to date, comprising 950 annotated images from three public datasets across multiple neurovascular imaging modalities. Its design allows for multi-expert annotations per image, accounting for variations across diverse annotation protocols. Furthermore, the framework facilitates the inclusion of new annotations and refinements to existing ones, making the dataset dynamic. To enhance annotation reliability, VesselVerse integrates tools for consensus generation and version control mechanisms, enabling the reversion of errors introduced during annotation refinement. We demonstrate VesselVerse's usability by assessing inter-rater agreement among four expert evaluators.", "filename": "2025_0313.pdf", "year": 2025, "institution": "EURECOM", "country": "France", "authors": ["Daniele Falcetta", "Vincenzo Marciano", "Kaiyuan Yang", "Jon Cleary", "Loïc Legris", "Domenico Rizzaro", "Ioannis Pitsiorlas", "Hava Chaptoukaev", "Benjamin Lemasson", "Bjoern Menze", "Maria A Zuluaga"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0314", "x": 1.675, "y": 3.462, "title": "VisNet: A Human Visual System Inspired Lightweight Dual-Path Network for Medical Images Denoising", "abstract": "Efficiently and accurately removing noise from medical images is crucial for clinical diagnosis. Nevertheless, most deep learningbased medical images denoising methods are highly complex and inaccurate in preserving the edge and shape of different organs, resulting in suboptimal denoising performance. In our study, we propose a Human Visual System Inspired Lightweight Dual-Path Network for medical images denoising (VisNet), which can efficiently and accurately remove noise from different types of medical images. Specifically, to simulate the mechanism in the visual system where magnocellular and parvocellular pathways capture significant and subtle noise, respectively, we design a dual-path multi-scale perception module. Then, to simulate the function of the primary visual cortex, we propose an edge detection and shape adaptation module to preserve the structural information of the medical images. Finally, inspired by dorsal and ventral pathways, a spatialsemantic information extraction module is designed to enhance the main semantic information in the image through the interactive fusion between the spatial and semantic pathways. Experimental results demonstrate that VisNet achieves superior performance across three medical datasets compared to nine existing baselines, while maintaining minimal computational complexity (Params = 0.15, FLOPs = 16.41). In addition, for brain tumor classification, using denoised images of VisNet as input significantly improves accuracy (87.5% vs 96.7%) and achieves performance comparable to noise-free images. Code of VisNet is available at https:// github.com/yuehailin/VisNet.", "filename": "2025_0314.pdf", "year": 2025, "institution": "Central South University", "country": "China", "authors": ["Hailin Yue", "Hulin Kuang", "Lei Ma", "Jin Liu", "Junjian Li", "Jianhong Cheng", "Jianxin Wang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0315", "x": 2.361, "y": 5.86, "title": "Weighted Stratification in Multi-label Contrastive Learning for Long-Tailed Medical Image Classification", "abstract": "Multi-label classification (MLC) in medical image analysis presents significant challenges due to long-tailed class distribution and disease co-occurrence. While contrastive learning (CL) has emerged as a promising solution, recent studies primarily focus on defining positive samples, overlooking the low gradient problem associated with single-disease representation and the impact of co-occurring diseases. To address these issues, we propose ws-MulSupCon, a novel weighted stratification method in CL for MLC. Our gradient analysis indicates that separating the single-disease cases can amplify their gradient contributions. Accordingly, we stratify training samples into single-and multi-disease cases to enhance the representation learning of each disease. Moreover, we design a weighted loss function based on class frequency and disease comorbidity, mitigating the dominance of prevalent diseases and improving rare disease detection. To further discriminate between the healthy and diseased samples, a dedicated CL for healthy cases is introduced, improving overall classification performance and preventing false positives. Extensive experiments on NIH ChestXRay14 and MIMIC-CXR demonstrate that ws-MulSupCon outperforms SoTA methods across nearly all disease classes, showing its superiority and the effectiveness of learning long-tailed distribution in multi-label medical image classification. The code is available at https://github.com/xup6YJ/ws- MulSupCon.", "filename": "2025_0315.pdf", "year": 2025, "institution": "National Yang Ming Chiao Tung University", "country": "Taiwan", "authors": ["Ying-Chih Lin", "Yong-Sheng Chen"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0316", "x": 4.599, "y": 2.978, "title": "Adaptive Frame Selection for Gestational Age Estimation from Blind Sweep Fetal Ultrasound Videos", "abstract": "The blind sweep ultrasound protocol, coupled with artificial intelligence (AI), offers promising solutions for expanding ultrasound availability in low-resource settings. However, existing AI approaches for gestational age (GA) prediction using bind sweeps face challenges like reliance on manual segmentation, computational inefficiency from high frame volume, and suboptimal sampling strategies that compromise performance, particularly with smaller datasets. We propose SelectGA, a novel framework for automated blind sweep analysis that enables effective fine-tuning of pretrained models through adaptive frame selection for GA prediction. Our approach identifies the most informative and least redundant frames, enhancing both training efficiency and prediction accuracy. Validated on data collected from ultrasound devices in diverse resource environments, SelectGA improves gestational age prediction accuracy by 27% on mean absolute error metrics. These results demonstrate substantially improved generalizability, establishing foundations for sustainable AI adoption in prenatal care across resourceconstrained settings. Code is available at: https://github.com/tanya- akumu/selectGA.", "filename": "2025_0316.pdf", "year": 2025, "institution": "Universitat de Barcelona", "country": "Spain", "authors": ["Tanya Akumu", "Marawan Elbatel", "Victor M Campello", "Richard Osuala", "Carlos Martin-Isla", "Ignacio Valenzuela", "Xiaomeng Li", "Bishesh Khanal", "Karim Lekadir"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0317", "x": 4.426, "y": 4.056, "title": "An Anatomical Significance-Aware Architecture for Explainable Myocardial Infarction Prediction via Multi-task Learning", "abstract": "Myocardial infarction (MI) is a significant health burden globally. Its precise prediction is critical yet complicated by the functional complexities of the heart and heterogeneous clinical presentations. Although learning-based methods that model the 3D heart anatomy have been widely studied, improving cardiac embeddings with localized substructures in a multi-task setting, remains under-explored. In this work, we present a novel deep learning model that produces explainable embeddings with high relevance to cardiac function via multi-task learning. Its transformer-based architecture contains modules for both MI classification and cardiac substructure prediction. By jointly learning these tasks with shared embeddings, the model is able to better capture 3D cardiac geometries and deformation across cardiac phases, enhancing its predictive ability. We evaluate the proposed method on cardiac anatomies captured during end-diastolic and end-systolic phases from the UK Biobank study. Compared to the existing learning-based benchmarks, our method exhibits high predictive performance, achieving an area under the receiver operating characteristic curve for MI prediction of 0.802. We also demonstrate the strong explainability of our model by showing that the latent features generated under the proposed multitask setting have a strong and statistically significant correlation with key clinical markers, such as ejection fraction.", "filename": "2025_0317.pdf", "year": 2025, "institution": "University of Oxford", "country": "UK", "authors": ["Jiachuan Peng", "Marcel Beetz", "Abhirup Banerjee", "Min Chen", "Vicente Grau"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0318", "x": 2.353, "y": 5.915, "title": "Bias and Generalizability of Foundation Models Across Datasets in Breast Mammography", "abstract": "Over the past decades, computer-aided diagnosis tools for breast cancer have been developed to enhance screening procedures, yet their clinical adoption remains challenged by data variability and inherent biases. Although foundation models (FMs) have recently demonstrated impressive generalizability and transfer learning capabilities by leveraging vast and diverse datasets, their performance can be undermined by spurious correlations that arise from variations in image quality, labeling uncertainty, and sensitive patient attributes. In this work, we explore the fairness and bias of FMs for breast mammography classification by leveraging a large pool of datasets from diverse sourcesincluding data from underrepresented regions and an in-house dataset. Our extensive experiments show that while modality-specific pre-training of FMs enhances performance, classifiers trained on features from individual datasets fail to generalize across domains. Aggregating datasets improves overall performance, yet does not fully mitigate biases, leading to significant disparities across under-represented subgroups such as extreme breast densities and age groups. Furthermore, while domainadaptation strategies can reduce these disparities, they often incur a performance trade-off. In contrast, fairness-aware techniques yield more stable and equitable performance across subgroups. These findings underscore the necessity of incorporating rigorous fairness evaluations and mitigation strategies into FM-based models to foster inclusive and generalizable AI.", "filename": "2025_0318.pdf", "year": 2025, "institution": "University Hospital Bonn", "country": "Germany", "authors": ["Elodie Germani", "Ilayda Selin-Türk", "Fatima Zeineddine", "Charbel Mourad", "Shadi Albarqouni"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0319", "x": 2.529, "y": 7.63, "title": "Bridging Knowledge Discrepancy in Retinal Image Analysis Through Federated Multi-task Learning", "abstract": "Retinal image analysis not only reveals the microscopic structure of the eye but also provides insights into overall health status. Therefore, employing multi-task learning to simultaneously address disease recognition and segmentation in retinal images can improve the accuracy and comprehensiveness of the analysis. Given the need for medical privacy, federated multi-task learning provides an effective solution for retinal image analysis. However, existing federated multi-task learning studies fail to address client resource constraints or knowledge discrepancies between global and local models. To address these challenges, we propose FedBKD, a novel federated multi-task learning framework for retinal image analysis. FedBKD leverages a server-side foundation model and effectively bridges the knowledge discrepancy between the clients and the server. Before local training, the adaptive sub-model extraction module ranks the activation values of neurons in the global model. It extracts the most representative sub-model based on computational resources, thereby facilitating the local adaptation of the global model. Additionally, we design a feature consistency optimization strategy to ensure alignment between the local model and the global foundation model's prior knowledge. This reduces error accumulation in the client sub-model during multi-task learning and ensures better adaptation to local tasks. Experimental results on the multi-center retinal image dataset demonstrate that FedBKD achieves state-of-the-art performance. Our code is available at https://github.com/Yjing07/FedBKD.git.", "filename": "2025_0319.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Jing Yang", "Yuxi Ma", "Jin-Gang Yu", "Feng Gao", "Shuting Yang", "Du Cai", "Jiacheng Wang", "Liansheng Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0320", "x": 1.609, "y": 3.633, "title": "Conformal Risk Control for Semantic Uncertainty Quantification in Computed Tomography", "abstract": "Uncertainty quantification is necessary for developers, physicians, and regulatory agencies to build trust in machine learning predictors and improve patient care. Beyond measuring uncertainty, it is crucial to express it in clinically meaningful terms that provide actionable insights. This work introduces a conformal risk control (CRC) procedure for organ-dependent uncertainty estimation, ensuring high-probability coverage of the ground-truth image. We first present a high-dimensional CRC procedure that leverages recent ideas of length minimization. We make this procedure semantically adaptive to each patient's anatomy and positioning of organs. Our method, sem-CRC, provides tighter uncertainty intervals with valid coverage on real-world computed tomography data while communicating uncertainty with clinically relevant features.", "filename": "2025_0320.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Jacopo Teneggi", "J Webster Stayman", "Jeremias Sulam"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0321", "x": 1.739, "y": 5.621, "title": "Conservative-Radical Complementary Learning for Class-Incremental Medical Image Analysis with Pre-trained Foundation Models", "abstract": "Class-incremental learning (CIL) in medical image-guided diagnosis requires models to retain diagnostic expertise on historical disease classes while adapting to newly emerging categories-a critical challenge for scalable clinical deployment. While pretrained foundation models (PFMs) have revolutionized CIL in the general domain by enabling generalized feature transfer, their potential remains underexplored in medical imaging, where domain-specific adaptations are critical yet challenging due to anatomical complexity and data heterogeneity. To address this gap, we first benchmark recent PFM-based CIL methods in the medical domain and further propose Conservative-Radical Complementary Learning (CRCL), a novel framework inspired by the complementary learning systems in the human brain. CRCL integrates two specialized learners built upon PFMs: (i) a neocortex-like conservative learner, which safeguards accumulated diagnostic knowledge through stability-oriented parameter updates, and (ii) a hippocampus-like radical learner, which rapidly adapts to new classes via dynamic and taskspecific plasticity-oriented optimization. Specifically, dual-learner feature and cross-classification alignment mechanisms harmonize their complementary strengths, reconciling inter-task decision boundaries to mitigate catastrophic forgetting. To ensure long-term knowledge retention while enabling adaptation, a consolidation process progressively transfers learned representations from the radical to the conservative learner. During task-agnostic inference, CRCL integrates outputs from both learners for robust final predictions. Comprehensive experiments on four medical imaging datasets show CRCL's superiority over state-of-the-art methods. X. Wu and Z. Xu-Equal contribution.", "filename": "2025_0321.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Xinyao Wu", "Zhe Xu", "Donghuan Lu", "Jinghan Sun", "Hong Liu", "Sadia Shakil", "Jiawei Ma", "Yefeng Zheng", "Raymond Kai-Yu Tong"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0322", "x": 2.342, "y": 6.465, "title": "CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis", "abstract": "The transparency of deep learning models is essential for clinical diagnostics. Concept Bottleneck Model provides clear decisionmaking processes for diagnosis by transforming the latent space of blackbox models into human-understandable concepts. However, conceptbased methods still face challenges in concept capture capabilities. These methods often rely on encode features solely from the final layer, neglecting shallow and multiscale features, and lack effective guidance in concept encoding, hindering fine-grained concept extraction. To address these issues, we introduce Concept Prompting and Aggregating (CoPA), a novel framework designed to capture multilayer concepts under prompt guidance. This framework utilizes the Concept-aware Embedding Generator (CEG) to extract concept representations from each layer of the visual encoder. Simultaneously, these representations serve as prompts for Concept Prompt Tuning (CPT), steering the model towards amplifying critical concept-related visual cues. Visual representations from each layer are aggregated to align with textual concept representations. With the proposed method, valuable concept-wise information in the images is captured and utilized effectively, thus improving the performance of concept and disease prediction. Extensive experimental results demonstrate that CoPA outperforms state-of-the-art methods on three public datasets. Code is available at https://github.com/yihengd/CoPA.", "filename": "2025_0322.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Yiheng Dong", "Yi Lin", "Xin Yang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0323", "x": 2.948, "y": 6.3, "title": "CytoSAE: Interpretable Cell Embeddings for Hematology", "abstract": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic interpretability of transformer-based foundation models. Recently, SAEs were also adopted for the visual domain, enabling the discovery of visual concepts and their patch-wise attribution to input images. While foundation models are increasingly applied to medical imaging, tools for interpreting their predictions remain limited. In this work, we propose CytoSAE, a sparse autoencoder trained on over 40,000 peripheral blood single-cell images. CytoSAE generalizes well to diverse and out-of-domain datasets, including bone marrow cytology. Here, it identifies morphologically relevant concepts which we validated with medical experts. Furthermore, we demonstrate scenarios in which CytoSAE can generate patient-specific and disease-specific concepts, enabling the detection of pathognomonic cells and localized cellular abnormalities at patch-level. We quantified the effect of concepts on a patient-level AML subtype classification task and show that CytoSAE concepts reach performance comparable to the state-of-the-art, while offering explainability on the sub-cellular level. Source code and model weights are available at https://github.com/dynamical-inference/cytosae.", "filename": "2025_0323.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Muhammed Furkan Dasdelen", "Hyesu Lim", "Michele Buck", "Katharina S Götze", "Carsten Marr", "Steffen Schneider"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0324", "x": 3.25, "y": 4.034, "title": "DCKAN: A Dual-Coordinate KAN Framework for Fibrous Cap Segmentation on Carotid OCT", "abstract": "Fibrous cap thickness is a key clinical marker for assessing carotid plaque vulnerability. While intravascular optical coherence tomography (OCT) enables in vivo visualization of fibrous caps, its design for coronary arteries poses challenges in carotid imaging, such as larger vessel size, faster blood flow, limited penetration, and restricted imaging range, leading to incomplete visualization and poor image quality. To address these limitations, we propose a dual-coordinate segmentation framework for carotid OCT fibrous cap segmentation. This framework integrates Cartesian images, which preserve global spatial context, with linear-polar transformed images, effectively representing the annular geometry of fibrous caps. The fusion of dual-coordinate features mitigates incomplete vascular walls and blood artifacts, enhancing segmentation accuracy and robustness. We introduce a Cross-Coordinate Feature Fusion Module (CCFFM) to efficiently integrate features from both coordinate systems and reduce interference from redundant information. Additionally, the Kolmogorov-Arnold Network (KAN) block is incorporated to extract complex nonlinear features while improving model interpretability. Our method achieves state-of-the-art performance on an external carotid OCT dataset, demonstrating the potential of OCT for advancing carotid imaging and improving plaque vulnerability assessment.", "filename": "2025_0324.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Tonghua Wan", "Sihan Liu", "Yuxin Cai", "Shengcai Chen", "Yan Wan", "Bo Hu", "Wu Qiu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0325", "x": 2.603, "y": 7.562, "title": "Decentralized Noise Handling in Medical Imaging: Encoder-Decoder Based Federated Imputation for Robust Training", "abstract": "Noise in medical imaging is an inevitable challenge, often stemming from acquisition artifacts, varying imaging protocols, and external interference. While some studies suggest that noise can enhance model robustness, excessive or unstructured noise degrades training quality and classification performance. This issue is further exacerbated in federated learning settings, where individual clients have limited local data, making it difficult to train robust models independently. Federated imputation has been explored as a solution, yet existing methods do not fully leverage federated learning settings for optimal noise reconstruction. In this work, we introduce a novel encoder-decoder based federated imputation method, designed to replace noisy images with more representative reconstructions before training. Experimental results demonstrate that classification models trained with images imputed by the proposed method consistently outperform those trained with raw noisy images and without noisy images, highlighting the importance of effective noise handling in federated learning-based medical imaging.", "filename": "2025_0325.pdf", "year": 2025, "institution": "Gachon University", "country": "Republic of Korea", "authors": ["Yunyoung Chang", "Yeonwoo Noh", "Sang-Woong Lee", "Minwoo Lee", "Wonjong Noh"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0326", "x": 3.141, "y": 6.131, "title": "DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology", "abstract": "While Whole Slide Imaging (WSI) scanners remain the gold standard for digitizing pathology samples, their high cost limits accessibility in many healthcare settings. Other low-cost solutions also face critical limitations: automated microscopes struggle with consistent focus across varying tissue morphology, traditional auto-focus methods require time-consuming focal stacks, and existing deep-learning approaches either need multiple input images or lack generalization capability across tissue types and staining protocols. We introduce a novel automated microscopic system powered by DeepAf, a novel auto-focus framework that uniquely combines spatial and spectral features through a hybrid architecture for single-shot focus prediction. The proposed network automatically regresses the distance to the optimal focal point using the extracted spatiospectral features and adjusts the control parameters for optimal image outcomes. Our system transforms conventional microscopes into efficient slide scanners, reducing focusing time by 80% compared to stack-based methods while achieving focus accuracy of 0.18 µm on same-lab samples-matching the performance of dual-image methods (0.19 µm) with half the input requirements. DeepAf demonstrates robust cross-lab generalization with only 0.72% false focus predictions and 90% of predictions within the depth of field. Through an extensive clinical study of 536 brain tissue samples, our system achieves 0.90 AUC in cancer classification at 4× magnification, a significant achievement at lower magnification than typical 20× WSI scans. This results in a comprehensive hardware-software design enabling accessible, real-time digital", "filename": "2025_0326.pdf", "year": 2025, "institution": "TU Munich", "country": "Germany", "authors": ["Yousef Yeganeh", "Maximilian Frantzen", "Michael Lee", "Kun-Hsing Yu", "Nassir Navab", "Azade Farshad"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0327", "x": 4.608, "y": 5.951, "title": "Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction", "abstract": "To improve the prediction of cancer survival using wholeslide images and transcriptomics data, it is crucial to capture both modality-shared and modality-specific information. However, multimodal frameworks often entangle these representations, limiting interpretability and potentially suppressing discriminative features. To address this, we propose Disentangled and Interpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that separates the intra-and inter-modal interactions within an attention-based fusion mechanism to learn distinct modality-specific and modality-shared representations. We introduce a loss based on Distance Correlation to promote disentanglement between these representations and integrate Shapley additive explanations to assess their relative contributions to survival prediction. We evaluate DIMAF on four public cancer survival datasets, achieving a relative average improvement of 1.85% in performance and 23.7% in disentanglement compared to current state-of-theart multimodal models. Beyond improved performance, our interpretable framework enables a deeper exploration of the underlying interactions between and within modalities in cancer biology. Code and checkpoints are publicly available at: https://github.com/Trustworthy-AI-UU-NKI/ DIMAF.", "filename": "2025_0327.pdf", "year": 2025, "institution": "Utrecht University", "country": "The Netherlands", "authors": ["Aniek Eijpe", "Soufyan Lakbir", "Melis Erdal Cesur", "Sara P Oliveira", "Sanne Abeln", "Wilson Silva"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0328", "x": -0.361, "y": 4.666, "title": "E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation", "abstract": "Although the Segment Anything Model (SAM) has advanced medical image segmentation, its Bayesian adaptation for uncertaintyaware segmentation remains hindered by three key issues: ( 1) instability in Bayesian fine-tuning of large pre-trained SAMs; (2) high computation cost due to SAM's massive parameters; (3) SAM's black-box design limits interpretability. To overcome these, we propose E-BayesSAM, an efficient framework combining Token-wise Variational Bayesian Inference (T-VBI) for efficiency Bayesian adaptation and Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving interpretability. T-VBI innovatively reinterprets SAM's output tokens as dynamic probabilistic weights and reparameterizes them as latent variables without auxiliary training, enabling training-free VBI for uncertainty estimation. SO-KAN improves token prediction with learnable spline activations via self-supervised learning, providing insight to prune redundant tokens to boost efficiency and accuracy. Experiments on five ultrasound datasets demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03 s/image), (ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0% vs. E-BayesSAM's 88.0% vs. Med-SAM's 88.3%), and (iii) identification of four critical tokens governing SAM's decisions. By unifying efficiency, reliability, and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs, advancing", "filename": "2025_0328.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Bin Huang", "Zhong Liu", "Huiying Wen", "Bingsheng Huang", "Xin Chen", "Shuo Li"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0329", "x": 2.693, "y": 7.668, "title": "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", "abstract": "Deep Learning (DL) has revolutionized medical imaging, yet its adoption is constrained by data scarcity and privacy regulations, limiting access to diverse datasets. Federated Learning (FL) enables decentralized training but suffers from high communication costs and is often restricted to a single downstream task, reducing flexibility. We propose a data-sharing method via Differentially Private (DP) generative models. By adopting foundation models, we extract compact, informative embeddings, reducing redundancy and lowering computational overhead. Clients collaboratively train a Differentially Private Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware data distribution, supporting diverse downstream tasks. Our approach, validated across multiple feature extractors, enhances privacy, scalability, and efficiency, outperforming traditional FL classifiers while ensuring differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings than DP-CGAN while requiring 5× fewer parameters.", "filename": "2025_0329.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Francesco Di Salvo", "Hanh Huyen My Nguyen", "Christian Ledig"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0330", "x": 4.472, "y": 4.576, "title": "Enhancing AI-Assisted Stroke Emergency Triage with Adaptive Uncertainty Estimation", "abstract": "Stroke diagnosis in emergency rooms (ERs) is challenging due to limited access to MRI scans and a shortage of neurologists. Although AI-assisted triage has shown promise, existing methods typically use MRI-derived training labels, which may not align with stroke patterns in patient multimedia data. To address this mismatch, we propose an Adaptive Uncertainty-aware Stroke TrIage Network (AUSTIN) (Source code for the framework is at https://github.com/shuashua0608/ AUSTIN), that leverages inconsistencies between clinician triage decisions and MRI-derived labels to enhance AI-driven stroke triage. This approach mitigates overfitting to clinician-MRI disagreement cases during training, significantly improving test accuracy. Additionally, it identifies high-uncertainty samples during inference, prompting further imaging or expert review. Evaluated on a clinical stroke patient dataset collected in an ER setting, AUSTIN achieves over 20% performance gain over human triage and a 13% improvement over a prior state-of-theart method. The learned uncertainty scores also show strong alignment with discrepancies in clinical assessments, highlighting the framework's potential to enhance the reliability of AI-assisted stroke triage.", "filename": "2025_0330.pdf", "year": 2025, "institution": "The Pennsylvania State University", "country": "USA", "authors": ["Shuhua Yang", "Tongan Cai", "Haomiao Ni", "Wenchao Ma", "Yuan Xue", "Kelvin Wong", "John Volpi", "James Z Wang", "Sharon X Huang", "Stephen T C Wong"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0331", "x": 3.497, "y": 6.166, "title": "Enhancing Soft Tissue Sarcoma Classification by Mitigating Patient-Specific Bias in Whole Slide Images", "abstract": "Soft tissue sarcomas (STS) are a rare and heterogeneous group of malignant tumors that arise in soft tissues throughout the body. Accurate classification from whole slide images (WSIs) is essential for diagnosis and treatment planning. However, STS classification faces a significant challenge due to patient-specific biases, where WSIs from the same patient share confounding non-tumor-related features, such as anatomical site and demographic characteristics. These biases can lead models to learn spurious correlations, compromising their generalization. To address this issue, we propose a novel multiple instance learning framework that explicitly mitigates patient-specific biases from WSIs. Our method leverages supervised contrastive learning to extract patient-specific features and integrates a bias-mitigation strategy based on propensity score matching. Extensive experiments on two STS datasets demonstrate that our approach significantly improves classification performance. By mitigating patient-specific biases, our method improves the reliability and generalization of the model, contributing to a more accurate and clinically reliable STS classification. To facilitate direct clinical application and support decision-making, the code, trained models, and testing pipeline will be publicly available at https://github.com/Lanman-Z/MPSF.", "filename": "2025_0331.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Weiping Lin", "Runchen Zhu", "Wentai Hou", "Jiacheng Wang", "Yixuan Lin", "Rui Chen", "Na Ta", "Liansheng Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0332", "x": 2.589, "y": 7.747, "title": "Equitable Federated Learning with NCA", "abstract": "Federated Learning (FL) is enabling collaborative model training across institutions without sharing sensitive patient data. This approach is particularly valuable in low-and middle-income countries (LMICs), where access to trained medical professionals is limited. However, FL adoption in LMICs faces significant barriers, including limited high-performance computing resources and unreliable internet connectivity. To address these challenges, we introduce FedNCA, a novel FL system tailored for medical image segmentation tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training on lowcost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication. By overcoming infrastructural and security challenges, FedNCA paves the way for inclusive, efficient, lightweight, and encryption-ready medical imaging solutions, fostering equitable healthcare advancements in resource-constrained regions. We make our implementation publicly available at: https://github.com/MECLabTUDA/FedNCA.", "filename": "2025_0332.pdf", "year": 2025, "institution": "Technical University of Darmstadt", "country": "Germany", "authors": ["Nick Lemke", "Mirko Konstantin", "Henry John Krumb", "John Kalkhof", "Jonathan Stieber", "Anirban Mukhopadhyay"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0333", "x": 2.682, "y": 7.693, "title": "FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation", "abstract": "Federated learning (FL) has emerged as a promising approach for collaborative medical image analysis, enabling multiple institutions to build robust predictive models while preserving sensitive patient data. In the context of Whole Slide Image (WSI) classification, FL faces significant challenges, including heterogeneous computational resources across participating medical institutes and privacy concerns. To address these challenges, we propose FedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn and transmit synthetic slides. On the server side, FedWSIDD aggregates synthetic slides from participating centres and distributes them across all centres. On the client side, we introduce a novel DD algorithm tailored to histopathology datasets which incorporates stain normalisation into the distillation process to generate a compact set of highly informative synthetic slides. These synthetic slides, rather than model parameters, are transmitted to the server. After communication, the received synthetic slides are combined with original slides for local tasks. Extensive experiments on multiple WSI classification tasks, including CAMELYON16 and CAMELYON17, demonstrate that FedWSIDD offers flexibility for heterogeneous local models, enhances local WSI classification performance, and preserves patient privacy. This makes it a highly effective solution for complex WSI classification tasks. The code is available at FedWSIDD.", "filename": "2025_0333.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Haolong Jin", "Shenglin Liu", "Cong Cong", "Qingmin Feng", "Yongzhi Liu", "Lina Huang", "Yingzi Hu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0334", "x": 0.312, "y": 2.35, "title": "Feeling the Stakes: Realism and Ecological Validity in User Research for Computer-Assisted Interventions", "abstract": "User research is increasingly recognized as an essential strategy for ensuring the usability, safety, and effectiveness of emerging technologies in surgery. From a human-centered perspective, user studies are key to evaluating how technology-assisted interventions affect human behavior and system perceptions. For feasibility and scalability, these studies are typically conducted in controlled, desk-based lab settings. However, these settings often lack ecological validity, raising questions about how well they capture the actual surgical environment's emotional, perceptual, and interactive complexities. Previous work in humancentered assurance for image-based navigation, for example, described office-like laboratory studies where participants were asked to assess the adequacy of image-based 2D/3D registration, revealing that evaluators struggled to identify misalignments reliably. For that same task in robotic surgery, this study investigates whether-and how-the environment in which user studies are administered influences user behavior and performance. Specifically, we compare a conventional office-like lab to a high-fidelity mock operating room (mock OR) with an active robotic system, where the latter is contextually more relevant to the surgical task. Twenty-one participants first trained in an office, then were randomly assigned to either return to the office or proceed to the mock OR. Although task performance did not differ significantly, likely due to task difficulty, participants in the mock OR showed significantly higher interaction, perceived stakes, and NASA-TLX workload changes, despite completing the same task. These findings suggest that realistic, contextually relevant environments modulate user responses and behavior, with important implications for how user studies are designed, interpreted, and applied in computer-assisted interventions.", "filename": "2025_0334.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Sue Min Cho", "Winnie Wu", "Ethan Kilmer", "Russell H Taylor", "Mathias Unberath"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0335", "x": 2.537, "y": 6.02, "title": "Fine-Tuning Vision Language Models with Graph-Based Knowledge for Explainable Medical Image Analysis", "abstract": "Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timely interventions and preventing vision loss. However, current staging models are hardly interpretable, and most public datasets contain no clinical reasoning or interpretation beyond image-level labels. In this paper, we present a novel method that integrates graph representation learning with vision-language models (VLMs) to deliver explainable DR diagnosis. Our approach leverages optical coherence tomography angiography (OCTA) images by constructing biologically informed graphs that encode key retinal vascular features such as vessel morphology and spatial connectivity. A graph neural network (GNN) then performs DR staging while integrated gradients highlight critical nodes and edges and their individual features that drive the classification decisions. We collect this graph-based knowledge which attributes the model's prediction to physiological structures and their characteristics. We then transform this reasoning into textual descriptions for VLMs. We perform instruction-tuning with these textual descriptions and the corresponding image to train a student VLM. This final agent can classify the disease and explain its decision in a human interpretable way solely based on a single image input. Experimental evaluations on both proprietary and public datasets demonstrate that our method not only improves classification accuracy but also offers more clinically interpretable results. An expert study further demonstrates that our agent provides more accurate diagnostic explanations and enables precise localization of pathologies in OCTA images.", "filename": "2025_0335.pdf", "year": 2025, "institution": "Cornell University", "country": "USA", "authors": ["Chenjun Li", "Laurin Lux", "Alexander H Berger", "Martin J Menten", "Mert R Sabuncu", "Johannes C Paetzold"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0336", "x": 2.975, "y": 2.419, "title": "Flip Distribution Alignment VAE for Multi-phase MRI Synthesis", "abstract": "Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at https://github.com/QianMuXiao/FDA-VAE.", "filename": "2025_0336.pdf", "year": 2025, "institution": "Central South University", "country": "China", "authors": ["Xiaoyan Kui", "Qianmu Xiao", "Qinsong Li", "Zexin Ji", "Jielin Zhang", "Beiji Zou"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0337", "x": 1.337, "y": 4.41, "title": "FOCUS: Feature Replay with Optimized Channel-Consistent Dropout for U-Net Skip-Connections", "abstract": "We address image segmentation in the domain-incremental continual learning scenario, a use-case frequently encountered in medical diagnostics where privacy regulations and storage constraints prevent access to historical data. In this scenario, segmentation models must learn to cope with new domains (e.g., difference in imaging protocols or patient population) while maintaining performance on previously learned domains without full access to past data. Feature-based replay addresses the privacy concerns by only storing latent feature representations instead of original images. However, existing feature replay approaches have a critical limitation: they sacrifice U-Net skipconnections, which are essential for achieving high segmentation accuracy and fast convergence. This limitation significantly impacts clinical viability, especially when alternatives such as full model retraining or maintaining domain-specific models are available. Therefore, we propose feature replay with optimized channel-consistent dropout for U-Net skip-connections (FOCUS). FOCUS enables crucial skip-connections in feature replay while respecting privacy and storage constraints, and integrates recent domain generalization techniques based on data augmentation. Evaluation across two domain-incremental continual MRI segmentation settings demonstrates that FOCUS achieves substantial improvements (up to 21% average DSC) over existing methods, while saving only 0.5% of the original feature information per domain. The code is available at https://github.com/imigraz/FOCUS/.", "filename": "2025_0337.pdf", "year": 2025, "institution": "Medical University of Graz", "country": "Austria", "authors": ["Simon Johannes Joham", "Franz Thaler", "Arnela Hadzic", "Martin Urschler"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0338", "x": 4.587, "y": 2.895, "title": "Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images", "abstract": "Accurate gestational age (GA) estimation, ideally through fetal ultrasound measurement, is a crucial aspect of providing excellent antenatal care. However, deriving GA from manual fetal biometric measurements depends on the operator and is time-consuming. Hence, automatic computer-assisted methods are demanded in clinical practice. In this paper, we present a novel feature fusion framework to estimate GA using fetal ultrasound images without any measurement information. We adopt a deep learning model to extract deep representations from ultrasound images. We extract radiomic features to reveal patterns and characteristics of fetal brain growth. To harness the interpretability of radiomics in medical imaging analysis, we estimate GA by fusing radiomic features and deep representations. Our framework estimates GA with a mean absolute error of 8.0 days across three trimesters, outperforming current machine learning-based methods at these gestational ages. Experimental results demonstrate the robustness of our framework across different populations in diverse geographical regions. Our code is publicly available on GitHub.", "filename": "2025_0338.pdf", "year": 2025, "institution": "University College Dublin", "country": "Ireland", "authors": ["Fangyijie Wang", "Yuan Liang", "Sourav Bhattacharjee", "Abey Campbell", "Kathleen M Curran", "Guénolé Silvestre"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0339", "x": 4.274, "y": 2.616, "title": "GeneMorphFormer: Transformer-Driven Cross-Scale Mapping from Gene Expression to Cortical Morphology", "abstract": "We present GeneMorphFormer, a Transformer-based model to decode nonlinear interactions between gene expression and cortical morphology. We align expression maps with gray matter and white matter boundary curves through spatial normalization by leveraging marmoset in situ hybridization (ISH) data. Our model employs multi-head self-attention to model global dependencies across 1024 gene features, optimized by a hybrid loss (MSE and Hausdorff distance) balancing local precision and global shape fidelity. SHapley Additive exPlanations (SHAP) analysis is subsequently employed to quantify the contribution of genes to morphological shape. Wavelet-based clustering further reveals distinct gene sets governing smooth versus fluctuating morphologies, suggesting hierarchical genetic regulation. Experimental results demonstrate that GeneMorphFormer outperforms traditional networks in both global shape matching and local precision. This work proposed a biologically interpretable Transformer architecture for cross-scale gene-morphology mapping and enables systematic exploration of genetic drivers in cortical morphology malformations. Our code is publicly available at https:// github.com/Leveup/GeneMorphFormer.", "filename": "2025_0339.pdf", "year": 2025, "institution": "Northwest University", "country": "China", "authors": ["Xiao Li", "Han Zhang", "Qitai Sun", "Chenjie Jia", "Xiaowei He", "Yudan Ren"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0340", "x": 1.421, "y": 7.036, "title": "GoCa: Trustworthy Multi-modal RAG with Explicit Thinking Distillation for Reliable Decision-Making in Med-LVLMs", "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown promise in enhancing medical diagnosis by enabling interactive and knowledge-driven healthcare applications. However, these models often suffer from factual hallucinations which may lead to incorrect diagnoses. Retrieval-augmented generation (RAG) has been proposed to mitigate these issues, yet its effectiveness in multi-modal medical applications is hindered by over-reliance on retrieved data and the opacity of text-based reasoning. To address these challenges, we propose GoCa, a multi-modal RAG system based on chain-of-thought (CoT) distillation and explicit thought optimization, which is designed to enhance both the factuality and explainability of Med-LVLMs. Our GoCa consists of three key components: (1) a self-evolving CoT framework that leverages multiagent collaboration to refine diagnostic reasoning iteratively and (2) a seamless, preference-guided optimization mechanism that distills highquality CoT reasoning using preference tuning and (3) an adaptive Monte Carlo-like top-k selection strategy. These innovations ensure that the RAG process remains logically transparent and adaptable, significantly improving consistency when integrating retrieve contexts. Experimental results across multiple datasets on medical visual question answering (Med-VQA) demonstrate that GoCa outperforms several recent stateof-the-art methods, achieving superior factual accuracy and coherence. The code can be found at https://github.com/Da1daidaidai/Goca", "filename": "2025_0340.pdf", "year": 2025, "institution": "Institute of Science Tokyo", "country": "Japan", "authors": ["Pengyu Dai", "Yafei Ou", "Yuqiao Yang", "Ze Jin", "Kenji Suzuki"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0341", "x": 2.723, "y": 7.513, "title": "GradInvDiff: Stealing Medical Privacy in Federated Learning via Diffusion-Based Gradient Inversion", "abstract": "Federated learning (FL) has become a crucial technique for medical imaging analysis, enabling multiple institutions to train machine learning models while preserving patient privacy collaboratively. However, recent research has uncovered the vulnerability of shared gradients in FL, which can be exploited through the gradient inversion attack (GIA) to reconstruct private medical images. While existing methods show promise in generic image tasks, their application to high-resolution medical images remains underexplored and ineffective due to data complexity. This paper introduces GradInvDiff, a novel GIA tailored for medical FL scenarios. Unlike traditional methods that rely solely on gradient guidance, our approach combines diffusion models with gradient matching optimization to iteratively refine the inference process. By replacing the standard random noise in the diffusion process with a direction derived from the difference between the optimized and original means, we inject a gradient-based condition into the noise to enhance image reconstruction quality. This method enables high-quality, pixel-level reconstruction of private medical images, even in the presence of large batch sizes or gradient noise. Our experiments demonstrate that GradInvDiff outperforms existing state-of-the-art gradient inversion methods and shows better accuracy and visibility when attacking medical FL models. We hope that this paper can raise public awareness of privacy leakage risks when using medical FL.", "filename": "2025_0341.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Zhiyuan Wang", "Daisong Gan", "Wenzhuo Fang", "Yuliang Zhu", "Kun Liu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0342", "x": 2.677, "y": 3.025, "title": "High-Order Progressive Trajectory Matching for Medical Image Dataset Distillation", "abstract": "Medical image analysis faces significant challenges in data sharing due to privacy regulations and complex institutional protocols. Dataset distillation offers a solution to address these challenges by synthesizing compact datasets that capture essential information from real, large medical datasets. Trajectory matching has emerged as a promising methodology for dataset distillation; however, existing methods primarily focus on terminal states, overlooking crucial information in intermediate optimization states. We address this limitation by proposing a shape-wise potential that captures the geometric structure of parameter trajectories, and an easy-to-complex matching strategy that progressively addresses parameters based on their complexity. Experiments on medical image classification tasks demonstrate that our method improves distillation performance while preserving privacy and maintaining model accuracy comparable to training on the original datasets. Our code is available at https://github.com/Bian-jh/HoP-TM.", "filename": "2025_0342.pdf", "year": 2025, "institution": "Xidian University", "country": "China", "authors": ["Le Dong", "Jinghao Bian", "Jingyang Hou", "Jingliang Hu", "Yilei Shi", "Weisheng Dong", "Xiao Xiang Zhu", "Lichao Mou"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0343", "x": 2.32, "y": 6.731, "title": "Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Experts", "abstract": "AI-based systems have achieved high accuracy in skin disease diagnostics but often exhibit biases across demographic groups, leading to inequitable healthcare outcomes and diminished patient trust. Most existing bias mitigation methods attempt to eliminate the correlation between sensitive attributes and diagnostic prediction, but those methods often degrade performance due to the lost of clinically relevant diagnostic cues. In this work, we propose an alternative approach that incorporates sensitive attributes to achieve fairness. We introduce FairMoE, a framework that employs layer-wise mixture-of-experts modules to serve as group-specific learners. Unlike traditional methods that rigidly assign data based on group labels, FairMoE dynamically routes data to the most suitable expert, making it particularly effective for handling cases near group boundaries. Experimental results show that, unlike previous fairness approaches that reduce performance, FairMoE achieves substantial accuracy improvements while preserving comparable fairness metrics.", "filename": "2025_0343.pdf", "year": 2025, "institution": "University of Notre", "country": "USA", "authors": ["Gelei Xu", "Yuying Duan", "Zheyuan Liu", "Xueyang Li", "Meng Jiang", "Michael Lemmon", "Wei Jin", "Yiyu Shi"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0344", "x": 6.226, "y": 3.608, "title": "Interpretable fMRI Captioning via Contrastive Learning", "abstract": "Recent advances in deep learning and generative AI have enhanced our understanding of brain function and enabled braincomputer interfaces to reconstruct stimuli from non-invasive neuroimaging data. In this work, we introduce an efficient two-stage training framework for captioning stimulus images from fMRI data, leveraging the compact representations of vision-language models and incorporating contrastive learning with text embeddings. Our approach demonstrates strong performance in fMRI captioning across multiple evaluation metrics and enables multimodal retrieval, highlighting the advantages of the contrastive learning. Additionally, we conduct an analysis with region-ofinterests (ROI) to examine the contributions of specific brain regions to the decoding process, providing interpretable results that align with neuroscience theories. Our findings contribute to advancing brain decoding techniques and improving model interpretability.", "filename": "2025_0344.pdf", "year": 2025, "institution": "KAIST", "country": "South Korea", "authors": ["Vyacheslav Shen", "Kassymzhomart Kunanbayev", "Donggon Jang", "Daeshik Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0345", "x": 0.681, "y": 6.697, "title": "IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports", "abstract": "The development of AI-based methods to analyze radiology reports could lead to significant advances in medical diagnosis, from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability of AI-based methods could hinder their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying chest radiology reports. First, we extract a set of representative facts from a large set of reports. Then, given a new report, we query whether a small subset of the representative facts is entailed by the report, and predict a diagnosis based on the selected subset of query-answer pairs. The explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select the most informative queries, a natural language inference model to determine if a fact is entailed by the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI. Code is available at: https://github.com/Glourier/ MICCAI2025-IP-CRR.", "filename": "2025_0345.pdf", "year": 2025, "institution": "University of Pennsylvania", "country": "USA", "authors": ["Yuyan Ge", "Kwan Ho Ryan Chan", "Pablo Messina", "René Vidal"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0346", "x": 4.476, "y": 3.287, "title": "Latent Motion Profiling for Annotation-Free Cardiac Phase Detection in Adult and Fetal Echocardiography Videos", "abstract": "The identification of cardiac phase is an essential step for analysis and diagnosis of cardiac function. Automatic methods, especially data-driven methods for cardiac phase detection, typically require extensive annotations, which is time-consuming and labour-intensive. In this paper, we present an unsupervised framework for end-diastole (ED) and end-systole (ES) detection through self-supervised learning of latent cardiac motion trajectories from 4-chamber-view echocardiography videos. Our method eliminates the need for manual annotationsincluding ED ES indices, segmentation, or volumetric measurements-by training a reconstruction model to encode interpretable spatiotemporal motion patterns. Evaluated on the EchoNet-Dynamic benchmark, the approach achieves mean absolute error (MAE) of 3.0 frames (58.3 ms) for ED and 2.0 frames (38.8 ms) for ES detection, matching state-of-theart supervised methods. Extended to fetal echocardiography, the model demonstrates robust performance with MAE 1.5 frames (20.7 ms) for ED and 1.7 frames (25.3 ms) for ES, despite the fact that the fetal heart model is built using non-standardized heart views due to fetal heart positioning variability. Our results demonstrate the potential of the proposed latent motion trajectory strategy for cardiac phase detection in adult and fetal echocardiography. This work advances unsupervised cardiac motion analysis, offering a scalable solution for clinical populations lacking annotated data. Code is released at https://github.com/YingyuYyy/ CardiacPhase.", "filename": "2025_0346.pdf", "year": 2025, "institution": "University of Oxford", "country": "UK", "authors": ["Yingyu Yang", "Qianye Yang", "Kangning Cui", "Can Peng", "Elena D’alberti", "Netzahualcoyotl Hernandez-Cruz", "Olga Patey", "Aris T Papageorghiou", "J Alison Noble"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0347", "x": 1.075, "y": 3.956, "title": "LHU-Net: A Lean Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Segmentation", "abstract": "The rise of Transformer architectures has advanced medical image segmentation, leading to hybrid models that combine Convolutional Neural Networks (CNNs) and Transformers. However, these models often suffer from excessive complexity and fail to effectively integrate spatial and channel features, crucial for precise segmentation. To address this, we propose LHU-Net, a Lean Hybrid U-Net for volumetric medical image segmentation. LHU-Net prioritizes spatial feature extraction before refining channel features, optimizing both efficiency and accuracy. Evaluated on four benchmark datasets (Synapse, Left Atrial, BraTS-Decathlon, and Lung-Decathlon), LHU-Net consistently outperforms existing models across diverse modalities (CT/MRI) and output configurations. It achieves state-of-the-art Dice scores while using four times fewer parameters and 20% fewer FLOPs than competing models, without the need for pre-training, additional data, or model ensembles. With an average of 11 million parameters, LHU-Net sets a new benchmark for computational efficiency and segmentation accuracy. Our implementation is available on github.com/xmindflow/LHUNet.", "filename": "2025_0347.pdf", "year": 2025, "institution": "University of Regensburg", "country": "Germany", "authors": ["Yousef Sadegheih", "Afshin Bozorgpour", "Pratibha Kumari", "Reza Azad", "Dorit Merhof"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0348", "x": 5.851, "y": 4.337, "title": "LLM-Powered Cross-Modal Alignment for Explainable Seizure Detection from EEG", "abstract": "While artificial intelligence (AI) has revolutionized the field of epileptic seizure detection from electroencephalography (EEG), its clinical adoption remains limited, largely due to the lack of transparency in AI models and their inability to explain the underlying seizure etiology. This paper introduces SzXAI, a novel framework to enhance the reasoning abilities of AI models for EEG-based seizure detection. SzXAI employs a contrastive training mechanism, which uses cross-modality similarity layers to align the EEG encodings with textual concept embeddings derived from clinical notes using LLMs. Along with the alignment, SzXAI leverages an attention-weighted pooling mechanism to detect underlying seizure and baseline etiologies. We validate SzXAI via 10fold cross validation on the publicly available Temple University Hospital dataset. Our results demonstrate that the alignment-powered training mechanism of SzXAI vastly outperforms direct etiology prediction, thus improving the reliability of the predicted seizure etiologies. Furthermore, structured sentence generation using the model output provided insights in a human-readable format. Thus, SzXAI provides an effective platform to boost clinical trust and AI usability in epilepsy management", "filename": "2025_0348.pdf", "year": 2025, "institution": "Boston University", "country": "USA", "authors": ["Maryam Riazi", "Deeksha M Shama", "Archana Venkataraman"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0349", "x": 6.968, "y": 4.591, "title": "MAGNET-AD: Multitask Spatiotemporal GNN for Interpretable Prediction of PACC and Conversion Time in Preclinical Alzheimer", "abstract": "Preclinical Alzheimer's Disease (AD) detection remains challenging due to the complex interplay of biological, structural, and temporal factors. Existing methods often struggle to integrate multimodal longitudinal data and predict key clinical outcomes. We propose MAGNET-AD, a novel multitask spatiotemporal graph neural network designed to predict the Preclinical Alzheimer's Cognitive Composite (PACC) score and time to AD conversion. MAGNET-AD offers three key contributions: (1) A dynamic heterogeneous graph architecture with weighted edges for hybrid fusion mechanisms, integrating static and dynamic multimodal data; (2) a temporal importance weighting loss function that adaptively learns critical time points while jointly optimizing time prediction and cognitive decline estimation; and (3) an interpretable attention framework that highlights key brain regions and genetic factors driving disease progression. MAGNET-AD achieves state-of-the-art performance with a concordance index of 0.858 for conversion time prediction and a mean square error of 1.983 for PACC prediction, outperforming existing deep learning approaches. These results underscore MAGNET-AD's potential for early AD risk assessment and monitoring, enabling broader clinical applications. The code is available at https://github.com/BioMedIA- MBZUAI/MAGNET-AD", "filename": "2025_0349.pdf", "year": 2025, "institution": "University of Artificial Intelligence", "country": "UAE", "authors": ["Salma Hassan", "Mostafa Salem", "Vijay Ram Kumar Papineni", "Ayman Elsayed", "Mohammad Yaqub"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0350", "x": 2.651, "y": 7.678, "title": "Maverick: Collaboration-Free Federated Unlearning for Medical Privacy", "abstract": "Federated Learning (FL) enables decentralized model training while preserving patient privacy, making it essential for medical AI applications. However, regulatory frameworks such as GDPR, CCPA, and LGPD mandate \"the right to be forgotten\", requiring patient data removal from trained models upon request. This has driven growing interest in Federated Unlearning (FU), but existing methods require the collaborative participation of all clients, which is often impractical and raises privacy concerns. This paper proposes Maverick, a novel Collaboration-free FU framework that enables localized unlearning at the target client by minimizing model sensitivity, without requiring global collaboration from all clients to unlearn a target client. Theoretical analysis and extensive experiments on three medical imaging datasets, Colorectal Cancer Histology, Pigmented Skin Lesions, and Blood Cells, demonstrate Maverick's effectiveness in sample, class, and client unlearning scenarios. Maverick ensures trustworthy FL in healthcare while complying with regulations. The code is publicly available at https://github. com/OngWinKent/Maverick.", "filename": "2025_0350.pdf", "year": 2025, "institution": "Universiti Malaya", "country": "Malaysia", "authors": ["Win Kent Ong", "Chee Seng Chan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0351", "x": 1.535, "y": 7.113, "title": "Med-BiasX: Robust Medical Visual Question Answering with Language Biases", "abstract": "In Medical Visual Question Answering (Med-VQA), accurate interpretation of clinical questions alongside medical images is crucial for reliable diagnostic support. However, conventional methods often exhibit pronounced medical language biases that stem from imbalanced data distribution and question shortcut dependence, causing models to disproportionately rely on textual priors at the expense of valuable visual semantics. To mitigate this challenge, we propose a novel Med-VQA debiasing approach called \"Med-BiasX\" that synergistically combines two strategies, i.e., Energy-aware Confidence Constraint (ECC) and Distribution-aware Dependence Calibration (DDC). Specifically, ECC aims to reinforce correct answers and adjust the energy associated with incorrect answers by leveraging the global normalization property of free energy and the intrinsic properties of energy. DDC is designed to shift the model's dependency from question shortcuts to multimodal information by explicitly measuring the similarity between predicted distributions from different branches and prior distributions. Extensive experiments on multiple medical standard benchmarks and bias-sensitive benchmarks, SLAKE-BIAS and VQA-RAD-BIAS, consistently demonstrate the robustness and superiority of our Med-BiasX approach over state-of-the-art competitors.", "filename": "2025_0351.pdf", "year": 2025, "institution": "South China Normal University", "country": "China", "authors": ["Huanjia Zhu", "Yishu Liu", "Chengju Zhou", "Guangming Lu", "Bingzhi Chen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0352", "x": 2.688, "y": 5.712, "title": "MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification", "abstract": "Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-ofconcept towards better mitigating data biases with generative models.", "filename": "2025_0352.pdf", "year": 2025, "institution": "Technische Universität Berlin", "country": "Germany", "authors": ["David Jacob Drexlin", "Jonas Dippel", "Julius Hense", "Niklas Prenißl", "Grégoire Montavon", "Frederick Klauschen", "Klaus-Robert Müller"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0353", "x": 3.535, "y": 6.082, "title": "Multi-task Screening for Cervical Diseases via Feature Routing and Asymmetric Distillation", "abstract": "Cervical diseases present a significant global health challenge, especially in resource-limited regions with scarce specialized healthcare. Traditional analysis methods for thin-prep cytologic tests and whole slide images are hindered by their reliance on time-consuming processes and expert knowledge. Although AI-driven approaches have advanced single-task screening, they often face difficulties adapting to multi-task workflows and handling extreme class imbalance, thereby limiting their practical deployment in real clinical settings. To address these challenges, we propose a novel framework, MECDS, for multi-task early screening of cervical diseases. Specifically, we design dynamic feature routing to prevent inter-task interference and selectively process taskrelevant features. Furthermore, we employ asymmetric attention levels during knowledge distillation to address class imbalance, thus enhancing performance across diverse classes. Our extensive experiments on a largescale dataset comprising 29,774 whole slide images demonstrate that MECDS surpasses existing single-task and multi-task models across three key screening tasks: cervical cancer, candidiasis, and clue cell detection. Additionally, MECDS exhibits remarkable extensibility, allowing for the efficient integration of novel diagnostic tasks without the need for exhaustive retraining. This unified framework holds great promise for improving comprehensive screening programs in resource-constrained healthcare environments, potentially advancing early detection and improving health outcomes. Our code is released at Github.", "filename": "2025_0353.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Haotian Jiang", "Haolin Huang", "Jiangdong Cai", "Mengjie Xu", "Zhenrong Shen", "Manman Fei", "Xinyu Wang", "Lichi Zhang", "Qian Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0354", "x": 2.337, "y": 6.82, "title": "Mutual Information Regularization for Fairness-Aware Deep Imaging Representations", "abstract": "Fairness in medical imaging ML models is dependent on ensuring they are not impacted by sensitive attributes such as race and gender. Building on popularly considered in-processing fairness mitigation strategies, we present a novel approach to leveraging mutual information (MI) regularization to learn fairness-aware deep imaging representations. Based on analytical and theoretical justification, we develop a unique gradient-based mutual information penalty which bypasses the need for MI estimation within our Fairness-aware MI (FaMI) framework which avoids unstable approximations and scales effectively to large datasets. FaMI was implemented in conjunction with popular DenseNet and Vision Transformer architectures and evaluated against nine alternative fairness-aware alternatives as well as alternative MI estimators. Experiments on multi-institutional retinal OCT and rectal cancer MRI cohorts demonstrate that FaMI-ViT achieves the highest overall classification AUC (0.83 in distinguishing glaucoma vs non-glaucoma, 0.81 in distinguishing responders vs non-responders) while also improving fairness-related metrics across disparity subgroups, increasing EOM up to 0.84 and reducing EOdd by up to 0.85. These results highlight the potential of fairness-aware MI constraints in developing robust and equitable imaging-based ML models.", "filename": "2025_0354.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Amir Reza Sadri", "Thomas Desilvio", "Satish E Viswanath"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0355", "x": 6.301, "y": 4.672, "title": "NeuroXVocal: Detection and Explanation of Alzheimer’s Disease Through Non-Invasive Analysis of Picture-Prompted Speech", "abstract": "The early diagnosis of Alzheimer's Disease (AD) through non-invasive methods remains a significant healthcare challenge. We present NeuroXVocal, the first end-to-end explainable AD classification system that achieves state-of-the-art performance while providing clinically interpretable explanations. Our novel dual-component architecture consists of: (1) Neuro, a multimodal classifier implementing a unique transformer-based fusion strategy that projects acoustic, textual, and speech embeddings into a common dimensional space for complex crossmodal interactions; and (2) XVocal, a specialized RAG-based explainer that retrieves relevant clinical literature to generate evidence-based explanations. Unlike previous approaches using late fusion or simple concatenation, our architecture enables both robust classification and meaningful clinical insights. Using the IS2021 ADReSSo Challenge benchmark dataset, NeuroXVocal achieved 95.77% accuracy, significantly outperforming previous state-of-the-art. Medical professionals validated the clinical relevance of XVocal's explanations through structured evaluation. This work advances beyond pure classification to bridge the gap between machine learning predictions and clinical decision-making. Code available at: https://github.com/NNtamp/NeuroXVocal.", "filename": "2025_0355.pdf", "year": 2025, "institution": "International Hellenic University", "country": "Greece", "authors": ["Nikolaos Ntampakis", "Konstantinos Diamantaras", "Ioanna Chouvarda", "Magda Tsolaki", "Panagiotis Sarigianndis", "Vasileios Argyriou"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0356", "x": 2.397, "y": 6.775, "title": "On the Interplay of Human-AI Alignment, Fairness, and Performance Trade-Offs in Medical Imaging", "abstract": "Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. The code is available at https://github.com/Roypic/Aligner.", "filename": "2025_0356.pdf", "year": 2025, "institution": "University of Bern", "country": "Switzerland", "authors": ["Haozhe Luo", "Ziyu Zhou", "Shelley Zixin Shu", "Aurélie Pahud De Mortanges", "Robert Berke", "Mauricio Reyes"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0357", "x": 2.114, "y": 6.388, "title": "One For All: A Unified Approach to Classification and Self-explanation", "abstract": "The integration of deep learning into medical vision applications has led to a growing demand for interpretable predictions. Typically, classification and explainability are treated as separate processes, with explainability methods applied post hoc to pre-trained classifiers. However, this decoupling introduces additional computational costs and may lead to explanations misaligned with the underlying model. In this paper, we propose One For All (OFA), an efficient, single-stage approach that jointly optimizes classification accuracy and self-explanation during training. OFA achieves this through a multi-objective framework, eliminating the need for separate explainability models while ensuring faithful and robust explanations. Extensive experiments on medical datasets confirm that OFA delivers competitive classification performance while providing high-quality, inherently interpretable explanations, making it a scalable and versatile solution for fully explainable classification.", "filename": "2025_0357.pdf", "year": 2025, "institution": "University of Freiburg", "country": "Germany", "authors": ["Mehdi Naouar", "Yannick Vogt", "Joschka Boedecker", "Gabriel Kalweit", "Maria Kalweit"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0358", "x": 0.882, "y": 6.171, "title": "Patient-Specific Radiomic Feature Selection with Reconstructed Healthy Persona of Knee MR Images", "abstract": "Classical radiomic features (e.g., entropy, energy) have been designed to describe image appearance and intensity patterns. These features are directly interpretable and readily understood by radiologists. Compared with end-to-end deep learning (DL) models, lower dimensional parametric models that use such radiomic features offer enhanced interpretability but lower comparative performance in clinical tasks. In this study, we propose an approach where a standard logistic regression model performance is substantially improved by learning to select radiomic features for individual patients, from a pool of candidate features. This approach has potentials to maintain the interpretability of such approaches while offering comparable performance to DL. In addition, we also propose to expand the feature pool by generating a patient-specific healthy persona via mask-inpainting using a denoising diffusion model trained on healthy subjects. Such a pathology-free baseline feature set allows not only further opportunity in novel feature discovery but also improved condition classification. We demonstrate our method on multiple clinical tasks of classifying general abnormalities, anterior cruciate ligament tears, and meniscus tears. Experimental results demonstrate that our approach achieved comparable or even superior performance than stateof-the-art DL approaches while offering added interpretability through the use of radiomic features extracted from images and supplemented by generating healthy personas. Example clinical cases are discussed in-depth to demonstrate the interpretability-enabled utilities such as human-explainable feature discovery and patient-specific location/view selection. These findings highlight the potentials of the combination of subject-specific feature selection with generative models in augmenting radiomic analysis for more interpretable decision-making. The codes are available at: https://github.com/YaxiiC/RadiomicsPersona.git.", "filename": "2025_0358.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Yaxi Chen", "Simin Ni", "Shaheer U Saeed", "Aleksandra Ivanova", "Rikin Hargunani", "Jie Huang", "Chaozong Liu", "Yipeng Hu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0359", "x": 2.518, "y": 7.581, "title": "Personalized Federated Side-Tuning for Medical Image Classification", "abstract": "Large Vision-Language Models (VLMs) capture rich multimodal knowledge through pretraining and demonstrate remarkable performance across various tasks. However, adapting these foundation models to medical image analysis through fine-tuning faces significant challenges, including constrained computing resources, data privacy concerns, and data heterogeneity. Federated Parameter-Efficient Fine-Tuning (PEFT) emerges as a promising solution, enabling multiple clinical institutions to collaboratively fine-tune VLMs with a small number of parameters. However, it still suffers from data heterogeneity across clients and high training memory requirements. In this work, we propose a personalized Federated Side-Tuning (pFedST) method. Specifically, we equip each client with a frozen pre-trained CLIP model and a lightweight, learnable, personalized side network for fine-tuning. Only a portion of the side network parameters participates in model aggregation, while the personalized LoRA modules within the side network address data heterogeneity with minimal additional parameters. Extensive experiments demonstrate that pFedST consistently outperforms 12 state-of-the-art methods across two real multi-center medical image classification tasks.", "filename": "2025_0359.pdf", "year": 2025, "institution": "Ningbo Institute of Northwestern Polytechnical University", "country": "China", "authors": ["Jiayi Chen", "Benteng Ma", "Yongsheng Pan", "Bin Pu", "Hengfei Cui", "Yong Xia"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0360", "x": 2.249, "y": 5.825, "title": "PRADA: Protecting and Detecting Dataset Abuse for Open-Source Medical Dataset", "abstract": "Open-source datasets play a crucial role in data-centric AI, particularly in the medical field, where data collection and access are often restricted. While these datasets are typically opened for research or educational purposes, their unauthorized use for model training remains a persistent ethical and legal concern. In this paper, we propose PRADA, a novel framework for detecting whether a Deep Neural Network (DNN) has been trained on a specific open-source dataset. The main idea of our method is exploiting the memorization ability of DNN and designing a hidden signal-a carefully optimized signal that is imperceptible to humans yet covertly memorized in the models. Once the hidden signal is generated, it is embedded into a dataset and makes protected data, which is then released to the public. Any model trained on this protected data will inherently memorize the characteristics of hidden signals. Then, by analyzing the response of the model on the hidden signal, we can identify whether the dataset was used during training. Furthermore, we propose the Exposure Frequency-Accuracy Correlation (EFAC) score to verify whether a model has been trained on protected data or not. It quantifies the correlation between the predefined exposure frequency of the hidden signal, set by the data provider, and the accuracy of models. Experiments demonstrate that our approach effectively detects whether the model is trained on a specific dataset or not. This work provides a new direction for protecting open-source datasets from misuse in medical AI research.", "filename": "2025_0360.pdf", "year": 2025, "institution": "ETRI", "country": "South Korea", "authors": ["Jinhyeok Jang", "Hong Joo Lee", "Nassir Navab", "Seong Tae Kim"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0361", "x": 4.524, "y": 3.599, "title": "PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis", "abstract": "Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of the aortic valve, leading to impaired blood flow. Despite its high prevalence, access to echocardiography (echo)-the gold-standard diagnostic tool-is often limited due to resource constraints, particularly in rural and underserved areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative but is restricted by operator expertise and the challenge of selecting the most relevant imaging views. To address this, we propose a reinforcement learning (RL)-driven active video acquisition framework that dynamically selects each patient's most informative echo videos. Unlike traditional methods that rely on a fixed set of videos, our approach continuously evaluates whether additional imaging is needed, optimizing both accuracy and efficiency. Tested on data from 2,572 patients, our method achieves 80.6% classification accuracy while using only 47% of the echo videos compared to a full acquisition. These results demonstrate the potential of active feature acquisition to enhance AS diagnosis, making echocardiographic assessments more efficient, scalable, and personalized. Our source code is available at: https://github. com/Armin-Saadat/PRECISE-AS.", "filename": "2025_0361.pdf", "year": 2025, "institution": "The University of British Columbia", "country": "Canada", "authors": ["Armin Saadat", "Nima Hashemi", "Hooman Vaseli", "Michael Y Tsang", "Christina Luong", "Michiel Van De Panne", "Teresa S M Tsang", "Purang Abolmaesumi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0362", "x": 2.774, "y": 7.278, "title": "Privacy Preserving Chest X-Ray Classification in Latent Space with Homomorphically Encrypted Neural Inference", "abstract": "Medical imaging data contain sensitive patient information requiring strong privacy protection. Many analytical setups require data to be sent to a server for inference purposes. Homomorphic encryption (HE) provides a solution by allowing computations to be performed on encrypted data without revealing the original information. However, HE inference is computationally expensive, particularly for large images (e.g., chest X-rays). In this study, we propose an HE inference framework for medical images that uses VQGAN to compress images into latent representations, thereby significantly reducing the computational burden while preserving image quality. We approximate the activation functions with lower-degree polynomials to balance the accuracy and efficiency in compliance with HE requirements. We observed that a downsampling factor of eight for compression achieved an optimal balance between performance and computational cost. We further adapted the squeeze and excitation module, which is known to improve traditional CNNs, to enhance the HE framework. Our method was tested on two chest X-ray datasets for multi-label classification tasks using vanilla CNN backbones. Although HE inference remains relatively slow and introduces minor performance differences compared with unencrypted inference, our approach shows strong potential for practical use in medical images. Our code is available at https://github.com/jongdory/Latent-HE.", "filename": "2025_0362.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "Korea", "authors": ["Jonghun Kim", "Gyeongdeok Jo", "Sinyoung Ra", "Hyunjin Park"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0363", "x": 1.386, "y": 2.523, "title": "ProgreSpine: Inherently Explainable Prototypical Regression for Spine Age Estimation", "abstract": "Spine aging is a complicated process shaped by pathologies, genetic factors, and lifestyle influences. Radiologists routinely use MR images to assess the spinal health of patients in different age brackets. Quantifying spinal health as an organ age would allow ranking and monitoring of patients within the same and across different demographics. However, spine age estimation has been limited to classical machine learning methods which suffer from high error rates and a lack of interpretability. Moreover, inherently explainable state-of-the-art models in organ age estimation, such as prototypical networks, are limited to 2D and are not extendable to repeated prototype labels. This is important as organs typically degenerate in different ways as a result of aging. We propose ProgreSpine, the first deep-learning-based 3D spine age estimation model based on prototypical regression with a loss specifically tailored to repeated prototype labels. We trained and tuned our proposed model on a large dataset of 9542 samples and performed a thorough evaluation on 1069 samples to demonstrate improved performance against the state-of-the-art with a mean absolute error of 3.61 years. Furthermore, the results suggest that the model learns the prototypes based on clinical conditions that will facilitate monitoring disease progression with a transparent model. The source code is available at https://github.com/ prenuvo/progrespine.", "filename": "2025_0363.pdf", "year": 2025, "institution": "Prenuvo , Redwood City , USA\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tPrenuvo Redwood City\n\t\t\t\t\t\t\t\t\tUSA", "country": "USA", "authors": ["Roozbeh Bazargani", "Saqib Basar", "Sam Hashemi", "Siavash Khallaghi"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0364", "x": 3.783, "y": 6.938, "title": "Prototype-Based Multiple Instance Learning for Gigapixel Whole Slide Image Classification", "abstract": "Multiple Instance Learning (MIL) methods have succeeded remarkably in histopathology whole slide image (WSI) analysis. However, most MIL models only offer attention-based explanations that do not faithfully capture the model's decision mechanism and do not allow human-model interaction. To address these limitations, we introduce ProtoMIL, an inherently interpretable MIL model for WSI analysis that offers user-friendly explanations and supports human intervention (Code can be found at https://github.com/ss-sun/ProtoMIL). Our approach employs a sparse autoencoder to discover human-interpretable concepts from the image feature space, which are then used to train ProtoMIL. The model represents predictions as linear combinations of concepts, making the decision process transparent. Furthermore, ProtoMIL allows users to perform model interventions by altering the input concepts. Experiments on two widely used pathology datasets demonstrate that ProtoMIL achieves a classification performance comparable to state-ofthe-art MIL models while offering intuitively understandable explanations. Moreover, we demonstrate that our method can eliminate reliance on diagnostically irrelevant information via human intervention, guiding the model toward being right for the right reason.", "filename": "2025_0364.pdf", "year": 2025, "institution": "University of Tübingen", "country": "Germany", "authors": ["Susu Sun", "Dominique Van Midden", "Geert Litjens", "Christian F Baumgartner"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0365", "x": 4.028, "y": 3.694, "title": "RegScore: Scoring Systems for Regression Tasks", "abstract": "Scoring systems are widely adopted in medical applications for their inherent simplicity and transparency, particularly for classification tasks involving tabular data. In this work, we introduce RegScore, a novel, sparse, and interpretable scoring system specifically designed for regression tasks. Unlike conventional scoring systems constrained to integer-valued coefficients, RegScore leverages beam search and ksparse ridge regression to relax these restrictions, thus enhancing predictive performance. We extend RegScore to bimodal deep learning by integrating tabular data with medical images. We utilize the classification token from the TIP (Tabular Image Pretraining) transformer to generate Personalized Linear Regression parameters and a Personalized RegScore, enabling individualized scoring. We demonstrate the effectiveness of RegScore by estimating mean Pulmonary Artery Pressure using tabular data and further refine these estimates by incorporating cardiac MRI images. Experimental results show that RegScore and its personalized bimodal extensions achieve performance comparable to, or better than, state-of-the-art black-box models. Our method provides a transparent and interpretable approach for regression tasks in clinical settings, promoting more informed and trustworthy decision-making. We provide our code at https://github.com/SanoScience/RegScore.", "filename": "2025_0365.pdf", "year": 2025, "institution": "Sano Centre for Computational Medicine", "country": "Poland", "authors": ["Michal K Grzeszczyk", "Tomasz Szczepański", "Pawel Renc", "Siyeop Yoon", "Jerome Charton", "Tomasz Trzciński", "Arkadiusz Sitek"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0366", "x": 2.625, "y": 7.725, "title": "Revisit the Stability of Vanilla Federated Learning Under Diverse Conditions", "abstract": "Federated Learning (FL) is a distributed machine learning paradigm enabling collaborative model training across decentralized clients while preserving data privacy. In this paper, we revisit the stability of the vanilla FedAvg method under diverse conditions. Despite its conceptual simplicity, FedAvg exhibits remarkably stable performance compared to more advanced FL techniques. Our experiments assess the performance of various FL methods on blood cell and skin lesion classification tasks using Vision Transformer (ViT). Additionally, we evaluate the impact of different representative classification models and analyze sensitivity to hyperparameter variations. The results consistently demonstrate that, regardless of dataset, classification model employed, or hyperparameter settings, FedAvg maintains robust performance. Given its stability, robust performance without the need for extensive hyperparameter tuning, FedAvg is a safe and efficient choice for FL deployments in resource-constrained hospitals handling medical data. These findings highlight the value of the vanilla FedAvg as a reliable baseline for clinical practice.", "filename": "2025_0366.pdf", "year": 2025, "institution": "KAIST", "country": "South Korea", "authors": ["Youngjoon Lee", "Jinu Gong", "Sun Choi", "Joonhyuk Kang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0367", "x": 2.133, "y": 3.102, "title": "Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model Using X-Ray Images", "abstract": "Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2%, achieving an AUC of 0.71 in predicting knee OA progression while offering 9× faster inference time.", "filename": "2025_0367.pdf", "year": 2025, "institution": "University of Surrey", "country": "UK", "authors": ["David Butler", "Adrian Hilton", "Gustavo Carneiro"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0368", "x": 2.949, "y": 4.544, "title": "Seeing Beyond the Surface: Retinal Thickness Prediction from Color Fundus Photography for DME Management", "abstract": "Diabetic macular edema (DME) is a leading cause of severe vision loss in the working-age population. Optical coherence tomography (OCT) is the gold standard for DME management and primary care referrals, providing retinal thickness maps (RTMs) that quantify retinal pathologies. However, its limited accessibility in resource-constrained settings necessitates more efficient solutions. While color fundus photography (C-FP) is a cost-effective screening tool, its potential for quantitative thickness evaluation remains underexplored. In this paper, we propose a novel Global-to-Local conditional Diffusion model for Retinal Thickness prediction (GLD-RT), the first attempt to predict RTM solely from C-FP. Our framework predicts thickness distributions of macular region from 2D inputs through a diffusion process guided by hierarchical global-to-local retinal features. Experimental results demonstrate that GLD-RT accurately depicts both physiological and pathological retinal morphology, achieving superior performance in thickness quantification and enabling a more detailed examination of retinal structures. Furthermore, C-FP-generated RTMs exhibit promising utility in facilitating DME diagnosis. This approach transforms conventional fundus imaging into a comprehensive and cost-effective diagnostic tool for DME screening and monitoring in resource-limited settings, thereby holding significant clinical implications.", "filename": "2025_0368.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Wenquan Cheng", "Yihua Sun", "Jinyuan Wang", "Jia Guo", "Zihan Li", "Zhuhao Wang", "Guochen Ning", "Yingfeng Zheng", "Hongen Liao", "Tien Yin Wong", "Su Jeong Song"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0369", "x": 3.53, "y": 4.369, "title": "Separable Tissue Representations for Attributable Risk Prediction", "abstract": "Attributing model predictions to a set of variables is a crucial part of methods in medicine and the sciences. However, in multimodal settings, ablating the contribution of a particular part of an image is often challenging. We present the STRAP framework (separable tissue representations for attributable risk prediction) using a novel masked autoencoder (MAE) enabling learning representations of a varying number of image patch tokens, enhancing memory efficiency and flexibility. We apply this framework on a fracture risk prediction task using clinical features and high-resolution peripheral quantitative computed tomography (HR-pQCT) images, to investigate the contribution of bone vs. muscle and fat tissues. Unlike previous work, we are able to selectively include specific tissues in risk prediction, and attribute their contribution to the risk using ablation and state-of-the-art interpretability methods. For the first time, we demonstrate that including soft-tissue from HR-pQCT increases prediction performance both in terms of Cindex and overall AUC. Source-code is openly published online: https:// github.com/waahlstrand/strap.", "filename": "2025_0369.pdf", "year": 2025, "institution": "Chalmers University of Technology", "country": "Sweden", "authors": ["Victor Wåhlstrand", "Jennifer Alvén", "Lisa Johansson", "Kristian Axelsson", "Mattias Lorentzon", "Ida Häggström"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0370", "x": 2.692, "y": 7.694, "title": "Shuffle-Diversity Collaborative Federated Learning for Imbalanced Medical Image Analysis", "abstract": "Data imbalance presents a significant challenge for the application of federated learning in medical image analysis. To address this challenge, we propose FedSDC, an innovative federated approach designed to effectively tackle the issue of data imbalance, as well as heterogeneity in distributed federated learning environments. The proposed FedSDC framework comprises a shared body network and multiple taskspecific head networks. By incorporating a shuffle-diversity collaborative strategy, FedSDC effectively addresses data imbalanc and heterogeneity challenges while improving cross-client generalization. Furthermore, training multiple heads under this strategy enables ensemble predictions, which enhances decision stability and accuracy. To balance efficiency and performance, FedSDC employs the sparse-head scheme during inference phase. Extensive experiments on medical image classification tasks validate that FedSDC achieves state-of-the-art results under imbalanced and heterogeneous data conditions. The source code will be available at https://github.com/wpnine/FedSDC.", "filename": "2025_0370.pdf", "year": 2025, "institution": "South China Agricultural University", "country": "China", "authors": ["Wenpeng Gao", "Liantao Lan", "Yumeng Liu", "Ruxin Wang", "Xiaomao Fan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0371", "x": 1.511, "y": 1.983, "title": "Spatial Regularisation for Improved Accuracy and Interpretability in Keypoint-Based Registration", "abstract": "Unsupervised registration strategies bypass requirements in ground truth transforms or segmentations by optimising similarity metrics between fixed and moved volumes. Among these methods, a recent subclass of approaches based on unsupervised keypoint detection stand out as very promising for interpretability. Specifically, these methods train a network to predict feature maps for fixed and moving images, from which explainable centres of mass are computed to obtain point clouds, that are then aligned in closed-form. However, the features returned by the network often yield spatially diffuse patterns that are hard to interpret, thus undermining the purpose of keypoint-based registration.Here, we propose a three-fold loss to regularise the spatial distribution of the features. First, we use the KL divergence to model features as point spread functions that we interpret as probabilistic keypoints. Then, we sharpen the spatial distributions of these features to increase the precision of the detected landmarks. Finally, we introduce a new repulsive loss across keypoints to encourage spatial diversity. Overall, our loss considerably improves the interpretability of the features, which now correspond to precise and anatomically meaningful landmarks. We demonstrate our three-fold loss in foetal rigid motion tracking and brain MRI affine registration tasks, where it not only outperforms state-of-the-art unsupervised strategies, but also bridges the gap with state-of-the-art supervised methods. Our code is available at https://github.com/BenBillot/spatial regularisation.", "filename": "2025_0371.pdf", "year": 2025, "institution": "Inria", "country": "France", "authors": ["Benjamin Billot", "Ramya Muthukrishnan", "Esra Abaci Turk", "P Ellen Grant", "Nicholas Ayache", "Hervé Delingette", "Polina Golland"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0372", "x": 2.378, "y": 6.712, "title": "Subgroup Performance Analysis in Hidden Stratifications", "abstract": "Machine learning (ML) models may suffer from significant performance disparities between patient groups. Identifying such disparities by monitoring performance at a granular level is crucial for safely deploying ML to each patient. Traditional subgroup analysis based on metadata can expose performance disparities only if the available metadata (e.g., patient sex) sufficiently reflects the main reasons for performance variability, which is not common. Subgroup discovery techniques that identify cohesive subgroups based on learned feature representations appear as a potential solution: They could expose hidden stratifications and provide more granular subgroup performance reports. However, subgroup discovery is challenging to evaluate even as a standalone task, as ground truth stratification labels do not exist in real data. Subgroup discovery has thus neither been applied nor evaluated for the application of subgroup performance monitoring. Here, we apply subgroup discovery for performance monitoring in chest x-ray and skin lesion classification. We propose novel evaluation strategies and show that a simplified subgroup discovery method without access to classification labels or metadata can expose larger performance disparities than traditional metadata-based subgroup analysis. We provide the first compelling evidence that subgroup discovery can serve as an important tool for comprehensive performance validation and monitoring of trustworthy AI in medicine (Code available at https://github.com/alceubissoto/hidden-subgroup-perf).", "filename": "2025_0372.pdf", "year": 2025, "institution": "University of Bern", "country": "Switzerland", "authors": ["Alceu Bissoto", "Trung-Dung Hoang", "Tim Flühmann", "Susu Sun", "Christian F Baumgartner", "Lisa M Koch"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0373", "x": 2.414, "y": 7.601, "title": "Temporal Model-Based Federated Active Medical Image Classification", "abstract": "Traditional federated learning relies on fully labeled datasets in each medical institution, which is impractical in real-world clinical scenarios. Federated Active Learning (FAL) addresses this by selecting a few informative samples for labeling, but it faces challenges such as domain shift across institutions. Besides, existing FAL methods rely on singleround model knowledge to estimate prediction-level uncertainty, ignoring uncertainty from features and model evolution during training. In this work, we propose TM-FAL, a novel framework for federated active medical image classification under domain shift. TM-FAL proposes a new uncertainty by integrating feature differences and prediction confidence from temporal local and global models to capture both local-global differences and the inherent complexity of images. Additionally, we use the prediction of the global model as pseudo labels to group images to mitigate class imbalance caused by uncertainty-based selection. Experiments on two medical image classification datasets demonstrate that TM-FAL outperforms various state-of-the-art methods.", "filename": "2025_0373.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Yunlu Yan", "Chun-Mei Feng", "Yuexiang Li", "Jinheng Xie", "Jun Chen", "Mohamed Elhoseiny", "Ming Hu", "Kaishun Wu", "Lei Zhu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0374", "x": 5.837, "y": 4.269, "title": "This EEG Looks Like These EEGs: Interpretable Interictal Epileptiform Discharge Detection With ProtoEEG-kNN", "abstract": "The presence of interictal epileptiform discharges (IEDs) in electroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even trained neurologists find detecting IEDs difficult, leading many practitioners to turn towards machine learning for help. Although deep learning algorithms have shown state-of-the-art accuracy on this task, most models are uninterpretable and cannot justify their conclusions. Absent the ability to understand model reasoning, doctors cannot leverage their expertise to identify incorrect model predictions and intervene accordingly. To improve human-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable IED-detection model that follows a simple case-based reasoning process. Specifically, ProtoEEG-kNN compares input EEGs to samples from the training set that contain similar IED morphology (shape) and spatial distribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art accuracy while providing visual explanations that experts prefer over existing approaches.", "filename": "2025_0374.pdf", "year": 2025, "institution": "Duke University", "country": "USA", "authors": ["Dennis Tang", "Jon Donnelly", "Alina Jade Barnett", "Lesia Semenova", "Jin Jing", "Peter Hadar", "Ioannis Karakis", "Olga Selioutski", "Kehan Zhao", "M Brandon Westover", "Cynthia Rudin"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0375", "x": 2.205, "y": 5.683, "title": "Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method", "abstract": "The rapid advancement of generative AI in medical imaging has introduced both significant opportunities and serious challenges, especially the risk that fake medical images could undermine healthcare systems. These synthetic images pose serious risks, such as diagnostic deception, financial fraud, and misinformation. However, research on medical forensics to counter these threats remains limited, and there is a critical lack of comprehensive datasets specifically tailored for this field. Additionally, existing media forensic methods, which are primarily designed for natural or facial images, are inadequate for capturing the distinct characteristics and subtle artifacts of AI-generated medical images. To tackle these challenges, we introduce MedForensics, a large-scale medical forensics dataset encompassing six medical modalities and twelve state-of-the-art medical generative models. We also propose DSKI, a novel Dual-Stage Knowledge Infusing detector that constructs a vision-language feature space tailored for the detection of AIgenerated medical images. DSKI comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for extracting subtle forgery clues from both spatial and noise domains during training, and 2) a medical forensic retrieval module (MFRM) that boosts detection accuracy through few-shot retrieval during testing. Experimental results demonstrate that DSKI significantly outperforms both existing methods and human experts, achieving superior accuracy across multiple medical modalities.", "filename": "2025_0375.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Shuaibo Li", "Zhaohu Xing", "Hongqiu Wang", "Pengfei Hao", "Xingyu Li", "Zekai Liu", "Lei Zhu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0376", "x": 2.083, "y": 5.245, "title": "Training-Free Test-Time Improvement for Explainable Medical Image Classification", "abstract": "Deep learning-based medical image classification techniques are rapidly advancing in medical image analysis, making it crucial to develop accurate and trustworthy models that can be efficiently deployed across diverse clinical scenarios. Concept Bottleneck Models (CBMs), which first predict a set of explainable concepts from images and then perform classification based on these concepts, are increasingly being adopted for explainable medical image classification. However, the inherent explainability of CBMs introduces new challenges when deploying trained models to new environments. Variations in imaging protocols and staining methods may induce concept-level shifts, such as alterations in color distribution and scale. Furthermore, since CBM training requires explicit concept annotations, fine-tuning models solely with image-level labels could compromise concept prediction accuracy and faithfulness -a critical limitation given the high cost of acquiring expert-annotated concept labels in medical domains. To address these challenges, we propose a training-free confusion concept identification strategy. By leveraging minimal new data (e.g., 4 images per class) with only image-level labels, our approach enhances out-of-domain performance without sacrificing source domain accuracy through two key operations: masking misactivated confounding concepts and amplifying under-activated discriminative concepts. The efficacy of our method is validated on both skin and white blood cell images. Our code is available at: https://github.com/ riverback/TF-TTI-XMed.", "filename": "2025_0376.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Hangzhou He", "Jiachen Tang", "Lei Zhu", "Kaiwen Li", "Yanye Lu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0377", "x": 1.441, "y": 4.045, "title": "Uncertainty-Supervised Interpretable and Robust Evidential Segmentation", "abstract": "Uncertainty estimation has been widely studied in medical image segmentation as a tool to provide reliability, particularly in deep learning approaches. However, previous methods generally lack effective supervision in uncertainty estimation, leading to low interpretability and robustness of the predictions. In this work, we propose a self-supervised approach to guide the learning of uncertainty. Specifically, we introduce three principles about the relationships between the uncertainty and the image gradients around boundaries and noise. Based on these principles, two uncertainty supervision losses are designed. These losses enhance the alignment between model predictions and human interpretation. Accordingly, we introduce novel quantitative metrics for evaluating the interpretability and robustness of uncertainty. Experimental results demonstrate that compared to state-of-the-art approaches, the proposed method can achieve competitive segmentation performance and superior results in out-of-distribution (OOD) scenarios while significantly improving the interpretability and robustness of uncertainty estimation. Code is available via https://github.com/suiannaius/SURE.", "filename": "2025_0377.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Yuzhu Li", "An Sui", "Fuping Wu", "Xiahai Zhuang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0378", "x": 4.366, "y": 2.537, "title": "Unraveling Brainstem Deformations in Joubert Syndrome: A Statistical Shape Analysis of MRI-Derived Structures", "abstract": "Statistical shape analysis (SSA) is a powerful tool for studying anatomical structures and their geometric variations in medical imaging. In this work, we analyze real MRI-derived data to explore correlations between geometric deformations and Joubert syndrome (JS). Building on prior SSA research, we tailor the preprocessing pipeline to an in-house dataset and perform a detailed shape variability analysis using principal component analysis (PCA). A random forest classifier is then applied, achieving high classification accuracy. To ensure robustness, we test multiple train-test splits and evaluate their impact. In addition, we support clinical interpretation by providing visualizations that combine 3D and 2D information, resembling typical diagnostic paradigms on MRI planes. Our work offers some methodological insights into shape-based analysis and aims to serve as a practical tool for the medical community. Code and data are openly available at: https://github.com/Franca-exe/ SSA-brainstem.", "filename": "2025_0378.pdf", "year": 2025, "institution": "University of Milano-Bicocca", "country": "Italy", "authors": ["Francesca Maccarone", "Giorgio Longari", "Filippo Arrigoni", "Denis Peruzzo", "Simone Melzi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0379", "x": 1.735, "y": 4.008, "title": "Variational Visible Layers: A Practical Framework for Uncertainty Estimation", "abstract": "Uncertainty estimation is critical for reliable decision-making in medical imaging. State-of-the-art uncertainty methods require significant computational overhead and complex modeling. In this work, we present and explore a simple, effective approach to incorporating Bayesian uncertainty into deterministic networks by replacing the first and/or last layer (visible layers) with their variational Bayesian counterpart. This lightweight modification enables uncertainty quantification through mean-field variational estimation, making it practical for realworld medical applications. We evaluate the methods on ISIC and LIDC-IDRI for the segmentation task and DermaMNIST and ChestMNIST for the classification task using post-hoc and jointly-trained visible layers. We demonstrate that variational visible layers enable uncertainty-based failure detection for both in-distribution and near-out-of-distribution samples, preserving task performance while reducing the number of variational parameters required for Bayesian estimation. We provide an easyto-implement solution for integrating uncertainty estimation into existing pipelines.", "filename": "2025_0379.pdf", "year": 2025, "institution": "Polytechnique Montreal", "country": "Canada", "authors": ["Zeinab Abboud", "Herve Lombaert", "Samuel Kadoury"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0380", "x": 6.074, "y": 4.661, "title": "X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis", "abstract": "Interpretable models are crucial for supporting clinical decision-making, driving advances in their development and application for medical images. However, the nature of 3D volumetric data makes it inherently challenging to visualize and interpret intricate and complex structures like the cerebral cortex. Cortical surface renderings, on the other hand, provide a more accessible and understandable 3D representation of brain anatomy, facilitating visualization and interactive exploration. Motivated by this advantage and the widespread use of surface data for studying neurological disorders, we present the eXplainable Surface Vision Transformer (X-SiT). This is the first inherently interpretable neural network that offers human-understandable predictions based on interpretable cortical features. As part of X-SiT, we introduce a prototypical surface patch decoder for classifying surface patch embeddings, incorporating case-based reasoning with spatially corresponding cortical prototypes. The results demonstrate state-of-the-art performance in detecting Alzheimer's disease and frontotemporal dementia while additionally providing informative prototypes that align with known disease patterns and reveal classification errors.", "filename": "2025_0380.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Fabian Bongratz", "Tom Nuno Wolf", "Jaume Gual Ramon", "Christian Wachinger"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0381", "x": 4.539, "y": 5.791, "title": "A Flexible Deep Learning Framework for Survival Analysis with Medical Data", "abstract": "Medical imaging data and electronic health records are an integral part of clinical routine and research for prognostication of patient survival and thus directly inform patient management. However, standard regression models used to derive patient prognoses are ill-equipped to handle such non-tabular data directly. Several neural network architectures based on classification or the Cox model have been proposed. Here, we present deep conditional transformation models (DCTMs) for survival applications with medical imaging data. DCTMs include the Cox model as a special case, but parameterize the log cumulative baseline hazards via Bernstein polynomials and allow the specification of nonlinear and non-proportional hazards for both tabular and non-tabular data and extend to all types of uninformative censoring. DCTMs yield moderate to large performance gains over state-of-the-art deep learning approaches to survival analysis on a multitude of publicly available datasets featuring tabular or imaging data from radiology and pathology.", "filename": "2025_0381.pdf", "year": 2025, "institution": "Windreich Department of AI and Human Health", "country": "USA", "authors": ["Gabriele Campanella", "Ida Häggström", "Lucas Kook", "Torsten Hothorn", "Thomas J Fuchs"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0382", "x": 4.564, "y": 4.92, "title": "Asynchronous Multi-modal Learning for Dynamic Risk Monitoring of Acute Respiratory Distress Syndrome in Intensive Care Units", "abstract": "Acute Respiratory Distress Syndrome (ARDS) is a critical adverse event with high modality rates, yet its recognition in ICU settings is often delayed. Clinicians face significant challenges in integrating asynchronous, multi-modal data streams with misaligned temporal resolutions during rapid deterioration. This work introduces a deep learning model for continuous ARDS risk monitoring, designed to dynamically integrate diverse ICU data sources and generate timely, actionable predictions of ARDS onset. We extend existing settings for ARDS detection from static, single-modality prediction to continuous, multi-modal monitoring that aligns with clinical workflows. To address the inherent complexities of this task, we propose tailored solutions for hierarchical fusion across irregular sampling points, heterogeneous data modalities, and sequential predictions, while ensuring robust training against dynamic, irregular inputs and severe class imbalance. V alidated on 1,985 MIMIC-IV patients, our model demonstrates superior performance, achieving average AUROC scores of 0.94, 0.91, and 0.87 across 6, 24, and 48 h pre-onset, respectively, outperforming previous models (AUROC 0.78-0.85). Furthermore, the model quantifies emergency level to aid in resource prioritization and identifies highrisk patients with peak relative risk reaching 25, demonstrating exceptional discrimination between cohorts. The code is publicly released at https://github.com/YidFeng/MICCAI25-ARDS-Risk-Prediction.", "filename": "2025_0382.pdf", "year": 2025, "institution": "The Hong Kong Polytechnic University", "country": "China", "authors": ["Yidan Feng", "Bohan Zhang", "Sen Deng", "Zhanli Hu", "Jing Qin"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0383", "x": 4.577, "y": 4.34, "title": "Attention-Based Multimodal Deep Learning Model for Post-stroke Motor Impairment Prediction", "abstract": "Accurately predicting post-stroke motor impairment remains a challenge due to the complexity of functional recovery and its association with neuroimaging biomarkers. This study presents a deep learning (DL) framework that integrates Magnetic Resonance Imaging (MRI)-based measures such as Diffusion Tensor Imaging (DTI) metrics-fractional anisotropy (FA), mean diffusivity (MD), radial diffusivity (RD), and axial diffusivity (AD)-along with white matter (WM) and gray matter (GM) intensities to classify upper limb motor function. Unlike previous approaches, the proposed model directly extracts wholebrain volumetric features without predefined region-of-interest constraints. Feature representation is enhanced using residual connections, attention mechanisms, and Global Average Pooling (GAP), improving classification performance while maintaining computational efficiency. The ensemble framework combines six independently trained models to optimize multi-modality integration. The results demonstrate that the WM + FA combination achieved the highest accuracy (0.97), outperforming the full ensemble model (0.96). These findings exceed the performance reported in prior studies, emphasizing the effectiveness of microstructural and structural biomarkers in motor recovery prediction. This optimized DL framework has the potential to improve post-stroke motor impairment classification, supporting early rehabilitation planning, and personalized treatment strate gies.", "filename": "2025_0383.pdf", "year": 2025, "institution": "Sivas Cumhuriyet University", "country": "Turkey", "authors": ["Rukiye Karakis", "Kali Gurkahraman", "Georgios D Mitsis", "Marie-Hèléne Boudrias"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0384", "x": 4.279, "y": 5.606, "title": "Boosting Generalizability in NPC ART Prediction via Multi-omics Feature Mapping", "abstract": "Adaptive radiotherapy (ART) improves treatment precision by adapting to anatomical changes, but its clinical adoption is limited by high costs, patient burden, and institutional variability. To address this, we propose a robust multi-omics nomogram for predicting ART eligibility in nasopharyngeal carcinoma (NPC) patients by integrating multi-modality Genomap signatures with clinical factors. Using retrospective data from 311 patients at Queen Elizabeth Hospital (training set) and 192 patients at Queen Mary Hospital (external test set), we extracted 7,956 radiomics features from six regions-of-interest (ROIs) across contrast-enhanced computed tomography (CECT), magnetic resonance imaging (MRI), and dose modalities, alongside 132 geometric features capturing spatial relationships between ROIs. Feature selection via LASSO identified 35 radiomic, 8 dosiomic, and 4 geometric features for analysis. The Genomap model achieved an accuracy of 80% and an AUC of 90% across modalities, while the integrated nomogram demonstrated superior performance with 88% accuracy and 96% AUC. Our results show that Genomap ensures generalizability and robustness, providing a reliable tool for personalized ART planning in NPC patients.", "filename": "2025_0384.pdf", "year": 2025, "institution": "The Hong Kong Polytechnic University", "country": "China", "authors": ["Jiabao Sheng", "Zhe Li", "Jiang Zhang", "Saikit Lam", "Zhi Chen", "Lei Xing", "Jing Cai"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0385", "x": 6.9, "y": 4.744, "title": "Brain-Heart-Gut Guided Multi-constraint Knowledge Distillation for Early Alzheimer’s Disease Diagnosis", "abstract": "Alzheimer's disease (AD) is a progressive and irreversible brain disorder. Emerging evidence suggests that Aβ deposition in the heart and microbiota dysbiosis in the gut may also contribute to the pathogenesis of AD. However, currently no studies have integrated heart and gut imaging information into AD diagnosis. To address this gap, we propose the first framework to integrate brain, heart, and gut information based on whole-body PET imaging and leverage these multiorgan interactions to guide brain-only model for early AD diagnosis in clinical applications. To this end, we collect multi-cohort data, including 1,475 unlabeled whole-body FDG-PET images, 1,730 brain FDG-PET images, and 70 labeled high-quality whole-body FDG-PET images. Our AD diagnostic model consists of two stages: (1) feature extraction and alignment, where AD-related features across brain, heart, and gut are extracted and aligned via hierarchical Transformers using contrastive learning; and (2) m ulti-constraint knowledge distillation, which utilizes sample-level contrastive distillation, group-level distribution distillation, and response-level distillation to transfer the performance of brain-heart-gut model to the brain-only model. Experimental results show that, guided by the learned interactions of brain, heart, and gut, our brain-only model improves the area under the receiver operating characteristic curve (AUC) from 75.4% to 80.3% for normal control vs. mild cognitive impairment (MCI) classification, achieving comparable diagnostic performance of using whole-body PET.", "filename": "2025_0385.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Fan Li", "Shilun Zhao", "Shuwei Bai", "Yuxiao Liu", "Kai Zhang", "Yin Xu", "Ya Zhang", "Kaicong Sun", "Dinggang Shen"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0386", "x": 3.457, "y": 6.842, "title": "Calibration in Multiple Instance Learning: Evaluating Aggregation Methods for Ultrasound-Based Diagnosis", "abstract": "Accurate probability estimates are critical for clinical decision-making, yet many Multiple Instance Learning (MIL) methods prioritize classification performance alone. We investigate the calibration quality of various MIL aggregation strategies, comparing them against simpler instance-based probability pooling in both in-distribution and out-of-distribution ultrasound imaging scenarios. Our findings reveal that attention-based aggregators yield stronger discrimination but frequently produce overconfident predictions, leading to higher calibration errors. In contrast, simpler instance-level methods o ffer more reliable risk estimates, albeit with a modest reduction in classification metrics. These results underscore a trade-off between predictive strength and calibration in MIL, emphasizing the importance of evaluating both aspects for clinically robust applications.", "filename": "2025_0386.pdf", "year": 2025, "institution": "KU Leuven", "country": "Belgium", "authors": ["Axel Geysels", "Ben Van Calster", "Bart De Moor", "Wouter Froyman", "Dirk Timmerman"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0387", "x": 3.791, "y": 5.35, "title": "CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis", "abstract": "Clinicians usually combine information from multiple sources to achieve the most accurate diagnosis, and this has sparked increasing interest in leveraging multimodal deep learning for diagnosis. However, in real clinical scenarios, due to differences in incidence rates, multimodal medical data commonly face the issue of class imbalance, which makes it difficult to adequately learn the features of minority classes. Most existing methods tackle this issue with resampling or loss reweighting, but they are prone to overfitting or underfitting and fail to capture cross-modal interactions. Therefore, we propose a Curriculum Learning (CL) framework for Imbalanced Multimodal Diagnosis (CLIMD). Specifically, we first design multimodal curriculum measurer that combines two indicators, intra-modal confidence and inter-modal complementarity, to enable the model to focus on key samples and gradually adapt to complex category distributions. Additionally, a class distribution-guided training scheduler is introduced, whic h enables the model to progressively adapt to the imbalanced class distribution during training. Extensive experiments on multiple multimodal medical datasets demonstrate that the proposed method outperforms state-of-the-art approaches across various metrics and excels in handling imbalanced multimodal medical data. Furthermore, as a plugand-play CL framework, CLIMD can be easily integrated into other models, offering a promising path for improving multimodal disease diagnosis accuracy. Code is publicly available at https://github.com/KHan-UJS/ CLIMD.", "filename": "2025_0387.pdf", "year": 2025, "institution": "Jiangsu University", "country": "China", "authors": ["Kai Han", "Chongwen Lyu", "Lele Ma", "Chengxuan Qian", "Siqi Ma", "Zheng Pang", "Jun Chen", "Zhe Liu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0388", "x": 4.007, "y": 5.475, "title": "Clinical Prior Guided Cross-Modal Hierarchical Fusion for Histological Subtyping of Lung Cancer in CT Scans", "abstract": "Accurate lung cancer localization and classification in computed tomography (CT) images are vital for effective treatment. However, existing approaches still face challenges such as redundant information in CT images, ineffective integration of clinical prior knowledge, and difficulty in distinguishing subtle histological differences across lung cancer subtypes. To address these, we propose Cross-Modal Detection Auxiliary Classification (CM-DAC), a framework enhancing classification accuracy. It employs a YOLO-based slice detection module to extract lesion areas, which are processed using the Multimodal Contrastive Learning Pretrain (MCLP) module, minimizing redundancy. Specifically, MCLP aligns 3D patches with clinical records via a crossmodal hierarchical fusion module, integrating image and clinical features through attention mechanisms and residual connections. Additionally, we employ multi-scale fusion strategies to further enhance histological distinction by capturing features at different resolutions. Simultaneously, a text path expands category labels into semantic vectors using a medical ontology-driven text augmentation approach. These vectors are encoded and aligned with fusion feature vectors. Experimental results on both private and public datasets confirm that the proposed CM-DAC outperforms competitive methods, achieving superior classification performance. The code is available at https://github.com/fancccc/CM-DAC.", "filename": "2025_0388.pdf", "year": 2025, "institution": "Zhejiang University of Finance and Economics", "country": "China", "authors": ["Chenchen Fan", "Ahmed Elazab", "Songqi Zhang", "Yuxuan Wang", "Qinghua Liang", "Danna Li", "Yongquan Zhang", "Ying Xiang", "Bo Liu", "Changmiao Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0389", "x": 4.632, "y": 5.984, "title": "CoC: Chain-of-Cancer Based on Cross-Modal Autoregressive Traction for Survival Prediction", "abstract": "Survival prediction aims to evaluate the risk level of cancer patients. Existing methods primarily rely on pathology and genomics data, either individually or in combination. From the perspective of cancer pathogenesis, epigenetic changes, such as methylation data, could also be crucial for this task. Furthermore, no previous endeavors have utilized textual descriptions to guide the prediction. To this end, we are the first to explore the use of four modalities, including three clinical modalities and language, for conducting survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT) to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and inter-learning. We encode the clinical data as the raw features, which remain domain-specific knowledge for intra-learning. In terms of in terlearning, we use language to prompt the raw features and introduce an Autoregressive Mutual Traction module for synergistic representation. This tailored framework facilitates joint learning among multiple modalities. Our approach is evaluated across five public cancer datasets, and extensive experiments validate the effectiveness of our methods and proposed designs, leading to producing state-of-the-art results. Codes will be released (https://github.com/haipengzhou856/CoC).", "filename": "2025_0389.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Haipeng Zhou", "Sicheng Yang", "Sihan Yang", "Jing Qin", "Lei Chen", "Lei Zhu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0390", "x": 4.173, "y": 4.295, "title": "Collateral Circulation Guided Multi-Modality Fusion Network for Postoperative Infarct Prediction", "abstract": "Acute ischemic stroke is one of the major causes of mortality and disability worldwide. Although thrombectomy is an effective intervention, it carries a lot of risks such as hemorrhage and vascular injury. Thus, it is crucial to accurately predicting postoperative infarct before intervention, providing the guidance for treatment. The existing perfusion imaging techniques relying on fixed thresholding approaches mostly fail to account for individual differences in collateral circulation recruitment, which has been proven to effectively reflect infarct severity. In this work, we take the first step toward integrating collateral circulation status into deep neural net work, enabling the model to learn and capture hemodynamic cues for infarct prediction. Specifically, we establish the first brain computed tomography perfusion (CTP) dataset including collateral circulation status and further conduct a thorough analysis of its effectiveness in predicting infarcts. Based on the findings, we propose a novel multi-modal fusion module (Codes are available at https://github. com/Frankenstein2026/CCGM) that integrates spatiotemporal features of multiple modalities. Specifically, a bi-directional Mamba structure is developed to extract the sequential information, which is then fused with collateral priors via a mixture-of-experts mechanism. In addition, a twostage infarct prediction module is developed to successively localize and segment the infarct region under the guidance of collateral circulation status. Finally, both infarct localization and segmentation performance of our method are validated to outperform 14 state-of-the-art methods.", "filename": "2025_0390.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Yichen Guo", "Xinyi Zhao", "Lisong Dai", "Heming Dong", "Lai Jiang", "Mai Xu", "Shengxi Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0391", "x": 2.959, "y": 3.531, "title": "Conditional Latent Diffusion Models for Irregularly Spaced Longitudinal Radiological Data", "abstract": "Longitudinal medical image studies often involves multiple scans of the same patient taken at different times, potentially with different modalities such as (2D vs. 3D volumetric medical imaging). In this work, we propose a single diffusion-based framework that can predict future embeddings of imaging data for predefined time points. Our approach uses a universal vision encoder, able to ingest either 2D or 3D scans, combined with a temporal transformer to fuse embeddings across multiple timepoints. A conditional latent diffusion model then produces the future output in latent space encoding the longitudinal information of the patient. We challenged our method in two crucial tasks involving radiological imaging: (1) predicting future pathology in the form o f segmentation masks, exemplified by Interstitial Lung Disease (ILD) progression on 3D chest CT scans of Systemic Sclerosis (SSc) patients, and (2) generating radiology reports that incorporate prior imaging context, exemplified by longitudinal chest X-rays from MIMIC-CXR. Results indicate that this unified diffusion approach outperforms existing baselines in both pixel-level forecasting and report generation, highlighting its versatility and effectiveness for longitudinal medical imaging.", "filename": "2025_0391.pdf", "year": 2025, "institution": "CentraleSupélec", "country": "France", "authors": ["Nabil Mouadden", "Othmane Laousy", "Rafael Marini", "Valentin Ong", "Marie-Pierre Revel", "Guillaume Chassagnon", "Stergios Christodoulidis", "Maria Vakalopoulou"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0392", "x": 1.576, "y": 2.733, "title": "Confidence in Angle Predictions for Clinical Decision Support", "abstract": "Anatomical landmarks are used for clinical measurements, screening, and to guide treatment decisions. In this work, we explore the clinical application of landmark-based angle measurements, with a particular aim of screening infants for Developmental Dysplasia of the Hip (DDH).Our automated machine method uses a simple UNet++ architecture. The network is used to predict landmark heatmaps, which represent landmark localisation certainty. A Monte Carlo-like approach is then used to approximate an angle distribution from landmark heatmaps. We propose a confidence metric from the derived angle distributions.Multiple clinician annotations are combined and compared to the machine predictions. The machine-generated angle distribution is verified by confirming the correlation of the mean angle values and standard deviations per scan, between the multiple clinicians and the machine. The confidence scores correlate for the clinicians combined and the machine. The confidence of the machine strongly correlates with the sum of the confidence scores given by clinicians for each scan.This work is the first to present a method for estimating the distribution of clinically relevant angles from predicted landmarks. Landmarkbased angle confidence can establish robust methods and increase clinician trust in using automated or computer-aided methods.", "filename": "2025_0392.pdf", "year": 2025, "institution": "Oxford Universit y", "country": "UK", "authors": ["Allison Clement", "James Willoughby", "Irina Voiculescu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0393", "x": 3.752, "y": 4.627, "title": "Cross-Modal CXR-CTPA Knowledge Distillation Using Latent Diffusion Priors Towards CXR Pulmonary Embolism Diagnosis", "abstract": "Pulmonary Embolism (PE) is a life-threatening condition. Computed tomography pulmonary angiography (CTPA) is the gold standard for PE diagnosis, offering high-resolution soft tissue visualization and three-dimensional imaging. However, its high cost, increased radiation exposure, and limited accessibility restrict its widespread use. In this work, we aim to introduce faster diagnosis opportunities by using 2D chest X-ray (CXR) data. CXR provides only limited two-dimensional visualization and is not typically used for PE diagnosis due to its inability to capture soft tissue contrast effectively. Here, we develop a novel methodology that distills knowledge from a trained CTPA-based teacher classifier model embedding to a CXR-based student embedding, by feature alignment -leveraging paired CTPA and CXR features as supervision, which can be readily acquired. This enables us to train without requiring annotated data. Our approach utilizes a latent diffusion model to generate CTPA-based PE classifier em beddings from CXR embeddings. In addition, we show that incorporating cross-entropy loss together with the corresponding loss of the teacher-student embeddings increases performance, bringing it close to clinical-level performance. We show state-of-the-art AUC in a PE categorization task using only the initial CXR input. This approach broadens the diagnostic capabilities of CXRs by enabling their use in PE classification, thereby extending their applicability beyond traditional imaging roles. The code for this project is available: https://github.com/meshims/Cross-Modal CXR-CTPA Knowledge Distillation.", "filename": "2025_0393.pdf", "year": 2025, "institution": "Tel Aviv Universit y", "country": "Israel", "authors": ["Noa Cahan", "Meshi Sizikov", "Hayit Greenspan"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0394", "x": 4.618, "y": 5.844, "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients", "abstract": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing immunotherapy is essential for personalized treatment planning, enabling informed patient decisions, and improving both treatment outcomes and quality of life. However, the lack of large, relevant datasets and effective multi-modal feature fusion strategies pose significant challenges in this domain. To address these challenges, we present a large-scale dataset and introduce a novel framework for multimodal feature fusion aimed at enhancing the accuracy of survival prediction. The dataset comprises 3D CT images and corresponding clinical records from NSCLC patients treated with immune checkpoint inhibitors (ICI), along with progression-free survival (PFS) and overall survival (OS) data. We further propose a cross-modality masked learning approach for medical feature fusion, consisting of two distinct branches, each tailored to its respective modality: a Slice-Depth Transformer for extracting 3D features from CT images and a graph-based Transformer for learning node features and relationships among clinical v ariables in tabular data. The fusion process is guided by a masked modality learning strategy, wherein the model utilizes the intact modality to reconstruct missing components. This mechanism improves the integration of modality-specific features, fostering more effective inter-modality relationships and feature interactions. Our approach demonstrates superior performance in multi-modal integration for NSCLC survival prediction, surpassing existing methods and setting a new benchmark for prognostic models in this context.", "filename": "2025_0394.pdf", "year": 2025, "institution": "Huazhong University of S cience and Technology", "country": "China", "authors": ["Qilong Xing", "Zikai Song", "Bingxin Gong", "Lian Yang", "Junqing Yu", "Wei Yang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0395", "x": 4.312, "y": 3.749, "title": "CTSL: Codebook-Based Temporal-Spatial Learning for Accurate Non-contrast Cardiac Risk Prediction Using Cine MRIs", "abstract": "Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction from cine MRI sequences remains a critical challenge. Existing methods typically necessitate supervised learning based on human-refined masks in the ventricular myocardium, which become impractical without contrast agents. We introduce a selfsupervised framework, namely Codebook-based Temporal-Spatial Learning (CTSL), that learns dynamic, spatiotemporal representations from raw cine data without requiring segmentation masks. CTSL decouples temporal and spatial features through a multi-view distillation strategy, where the teacher model processes multiple cine views, and the student model learns from reduced-dimensional cine-SA sequences. By leveraging codebook-based feature representations and dynamic lesion selfdetection through motion cues, CTSL captures intricate temporal dependencies and motion patterns. High-confidence MACE risk predictions are achieved through our model, providing a rapid, non-invasive solution for cardiac risk assessment that outperforms traditional contrast-dependent methods, thereby enabling timely and accessible heart disease diagnosis in clinical settings.", "filename": "2025_0395.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Haoyang Su", "Shaohao Rui", "Jinyi Xiang", "Lianming Wu", "Xiaosong Wang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0396", "x": 4.267, "y": 4.958, "title": "CXR-TFT: Multi-modal Temporal Fusion Transformer for Predicting Chest X-Ray Trajectories", "abstract": "In intensive care units (ICUs), patients with complex clinical conditions require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a vital diagnostic tool, providing insights into clinical trajectories, but their irregular acquisition limits their utility. Existing tools for CXR interpretation are constrained by cross-sectional analysis, failing to capture temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal framework that integrates temporally sparse CXR imaging and radiology reports with high-frequency clinical data-such as vital signs, laboratory values, and respiratory flow sheets-to predict the trajectory of CXR findings in critically ill patients. CXR-TFT leverages latent embeddings from a vision encoder that are temporally aligned with hourly clinical data through interpolation. A transformer is trained to predict CXR embeddings at each hour, conditioned on previous CXR embeddings and clinical measurements. In a retrospective study of 20,000 ICU patients, CXR-TFT demonstrated 95% accuracy in predicting abnormal CXR findings 12 h before they became radiographically evident, indicating that clinical data contains valuable respiratory state progression information. By providing distinctive temporal resolution in prognostic CXR analysis, CXR-TFT offers actionable predictions with the potential to improve the management of time-sensitive critical conditions, where early intervention is crucial but timely diagnosis is challenging.", "filename": "2025_0396.pdf", "year": 2025, "institution": "Duke University", "country": null, "authors": ["Mehak Arora", "Ayman Ali", "Kaiyuan Wu", "Carolyn Davis", "Takashi Shimazui", "Mahmoud Alwakeel", "Victor Moas", "Philip Yang", "Annette Esper", "Rishikesan Kamaleswaran"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0397", "x": 3.987, "y": 5.783, "title": "Deep Knowledge-Infused Transformer for NSCLC Lymph Node Station Metastasis Prediction: Development of an AI-Powered Intraoperative Decision System", "abstract": "Non-small cell lung cancer (NSCLC) is one of the leading causes of cancer-related mortality, with lymph node metastasis serving as a critical factor in both prognosis and treatment decisions. Lymph node station (LNS) dissection is an essential procedure in the management of NSCLC patients; however, over-dissection may expose patients to unnecessary risks, while under-dissection could lead to undetected metastases. Despite its importance, predicting the exact metastasis status during surgery remains challenging. To address this challenge and meet the urgent need in clinical practice, this study presents the Deep Knowledge-infused Transformer (DKiT) model, designed to predict LNS metastasis in previously unexamined regions by capturing the relationships between LNSs. Furthermore, DKiT is augmented with clinical prior knowledge through a multi-stage infusion mechanism during the decoding phase, enhancing both model performance and interpretability. Additionally, we developed an AI-powered in traoperative decision support system based on DKiT, which provides real-time surgical recommendations informed by frozen pathology results. Experimental results show that DKiT achieves an AUC score of 0.812 for LNS-level metastasis prediction, outperforming other comparative methods. The clinical system achieves a recall of 0.930 and precision of 0.865 in the retrospective cohort collected from collaborating hospitals, highlighting its potential in guiding NSCLC treatment decisions. The source code is available at https:// github.com/czifan/DKiT.", "filename": "2025_0397.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Jie Zhao", "Zifan Chen", "Guangzhengao Yang", "Yijiang He", "Li Zhang", "Bin Dong"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0398", "x": 7.202, "y": 4.723, "title": "DISCLOSE the Neurodegeneration Dynamics: Individualized ODE Discovery for Alzheimer’s Disease Precision Medicine", "abstract": "Monitoring progression from Mild Cognitive Impairment due to Alzheimer's Disease (MCI-AD) is critical for patient care. However, current approaches to model AD progression overlook complex interrelated neurodegeneration in different regions of the brain and how AD pathology and genotypes manipulate it. This study defines neurodegeneration dynamics and proposes the Dynamics Individualized by Static Covariates without LOngitudinal ScrEening (DISCLOSE) framework. This method predicts individualized neurodegeneration dynamics from only baseline amyloid-beta deposition and the number of APOE4 alleles with an Ordinal Differential Equation (ODE). We evaluated DISCLOSE using longitudinal MRI samples in the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. The results demonstrate that DISCLOSE outperforms existing methods in long-term trajectory prediction, particularly for predictions beyond three years. This work presents a significant step toward modeling individualized disease trajectories. Also, DISCLOSE could quantitatively interpret the effects of AD-related genotypes and pathophysiology on regional atrophy progression.", "filename": "2025_0398.pdf", "year": 2025, "institution": "VUNO Inc", "country": "South Korea", "authors": ["Wooseok Jung", "Joonhyuk Park", "Won Hwa Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0399", "x": 3.732, "y": 6.785, "title": "Dual Selective Gleason Pattern-Aware Multiple Instance Learning for Grade Group Prediction in Histopathology Images", "abstract": "The Gleason Grade Group is the gold standard for diagnosing and prognosticating prostate cancer. Existing multiple instance learning (MIL) methods for Grade Group classification have overlooked domain-specific knowledge that the Grade Group is collaboratively determined by different Gleason Patterns, limiting their performance. In this study, we propose DSPA-MIL, a Dual Selective Gleason Pattern-Aware MIL model for patient-level Grade Group prediction. Our approach incorporates a dual selective instance aggregation strategy, combining selective aggregator tokens and patch-level Gleason pattern expert concept-guided aggregation. Furthermore, to effectively utilize patient-level Grade Group expert concepts, we i ntroduce a knowledgedistillation-based framework for training and inference, enabling accurate Grade Group score prediction. Experimental results on five datasets comprising 10,809 whole slide images (WSIs) and 1,133 tissue microarray (TMA) images demonstrate the superiority of our method, which outperforms state-of-the-art (SOTA) MIL approaches. The code is available at https://github.com/AlexNmSED/DSPA-MIL.", "filename": "2025_0399.pdf", "year": 2025, "institution": "Dalian University o f Technology", "country": "China", "authors": ["Xinyu Hao", "Hongming Xu", "Qibin Zhang", "Qi Xu", "Ilkka Polonen", "Fengyu Cong"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0400", "x": 4.269, "y": 6.314, "title": "Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-distillation", "abstract": "Survival analysis based on Whole Slide Images (WSIs) is crucial for evaluating cancer prognosis, as they offer detailed microscopic information essential for predicting patient outcomes. However, traditional WSI-based survival analysis usually faces noisy features and limited data accessibility, hindering their ability to capture critical prognostic features effectively. Although pathology reports provide rich patientspecific information that could assist analysis, their potential to enhance WSI-based survival analysis remains largely unexplored. To this end, this paper proposes a novel Report-auxiliary self-distillation (Rasa) framework for WSI-based survival analysis. First, advanced large language models (LLMs) are utilized to extract fine-grained, WSI-relevant textual descriptions from original noisy pathology reports via a carefully designed task prompt. Next, a self-distillation-based pipeline is designed to filter out irrelevant or redundant WSI features for the studen t model under the guidance of the teacher model's textual knowledge. Finally, a risk-aware mix-up strategy is incorporated during the training of the student model to enhance both the quantity and diversity of the training data. Extensive experiments carried out on our collected data (CRC) and public data (TCGA-BRCA) demonstrate the superior effectiveness of Rasa against state-of-the-art methods. Our code is available at https://github.com/zhengwang9/Rasa.", "filename": "2025_0400.pdf", "year": 2025, "institution": "Xiamen Universit y", "country": "China", "authors": ["Zheng Wang", "Hong Liu", "Zheng Wang", "Danyi Li", "Min Cen", "Baptiste Magnier", "Li Liang", "Liansheng Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0401", "x": 4.258, "y": 5.726, "title": "Feature Copy-Paste Network for Lung Cancer EGFR Mutation Status Prediction in CT Images", "abstract": "Epidermal growth factor receptor (EGFR) mutation status is crucial for targeted therapy planning in lung cancer. Current identification relies on invasive biopsy and expensive gene sequencing. Recent studies indicate that CT imaging with advanced deep learning techniques offer a non-invasive alternative for predicting EGFR mutation status. However, CT scanning parameters, such as slice thickness, vary significantly between different scanners and centers, making the predicting models highly sensitive to data types, and thus not robust in clinical practice. In this study, we propose Feature Copy-Paste Network (FCP-Net), an innovative and robust model for predicting EGFR mutation status using CT images. First, we propose a novel Feature Copy-Paste Consistency (FCPC) module to exchange the information from CT scans with different slice thicknesses and impose consistency constrain to make model more robust. Second, we introduce a F eature Refinement (FR) module to filter redundant features during information fusion, thereby enhancing the accuracy of mutation prediction. Extensive experiments demonstrate the outstanding performance of the FCPC and FR modules. When the trained model is tested on both thin-slice and thick-slice CT images, it achieves at least 2.6% and 2.1% improvements in AUC, respectively, indicating the models' robustness and stability. Our code is available at https://github.com/499huangxingyu/FCPNet.", "filename": "2025_0401.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Xingyu Huang", "Shuo Wang", "Chengcai Liu", "Haolin Sang", "Yi Wu", "Jie Tian"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0402", "x": 1.788, "y": 3.161, "title": "Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-label Classification Model", "abstract": "Accurate rib fracture identification and classification are essential for treatment planning. However, existing datasets often lack fine-grained annotations, particularly regarding rib fracture characterization, type, and precise anatomical location on individual ribs. To address this, we introduce a novel rib fracture annotation protocol tailored for fracture classification. Further, we enhance fracture classification by leveraging cross-modal embeddings that bridge radiological images and clinical descriptions. Our approach employs hyperbolic embeddings to capture the hierarchical nature of fracture, mapping visual features and textual descriptions into a shared non-Euclidean manifold. This framework enables more nuanced similarity computations between imaging characteristics and clinical descriptions, accounting for the inherent hierarchical relationships in fracture taxonomy. Experimental results demonstrate that our approach outperforms existing methods across multiple classification tasks, with average recall improvements of 6% on the Air-Rib dataset and 17.5% on the public RibFrac dataset. The code and annotation files can be accessed at https://github.com/ribfracture123/ classification.", "filename": "2025_0402.pdf", "year": 2025, "institution": "Indian Institute of Technology Jodhpur", "country": "India", "authors": ["Shripad Pate", "Aiman Farooq", "Suvrankar Datta", "Musadiq Aadil Sheikh", "Atin Kumar", "Deepak Mishra"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0403", "x": 3.901, "y": 2.512, "title": "Flexibly Distilled 3D Rectified Flow with Anatomical Constraints for Developmental Infant Brain MRI Prediction", "abstract": "Longitudinal prediction of infant brain MRIs is crucial for individualized neurodevelopment tracking and disorder forecasting. However, existing methods, such as diffusion-based generative models, often struggle to capture the complex spatiotemporal dynamics of developing brains, leading to unreliable predictions that lack subject-specific, anatomically consistent growth patterns. To address this, we propose a Flexibly Distilled 3D Rectified Flow (FDRF) framework, which integrates anatomical constraints for dual-stream predictions of volumetric images and tissue maps along developmental trajectories. Our framework features an age-conditioned feature fusion module for controllable prediction with targeted age appearances and employs anatomical constraints derived from segmentation labels and high-frequency image details to ensure subject-level spatiotemporal consistency. Additionally, we introduce a flexible distillation of rectified flow, enabling a unified one-step generative model for high-fidelity cross-time predictions while preserving individualized anatomical details. Given 6-month MRIs and tissue maps as the input, our model reliably predicts their spatiotemporal growths at 12 and 24 months, outperforming existing diffusion-based baselines by relatively large margins. Our codes can be found at https:// github.com/ladderlab-xjtu/FDRF.", "filename": "2025_0403.pdf", "year": 2025, "institution": "Xian Jiaotong University", "country": "China", "authors": ["Haifeng Wang", "Zehua Ren", "Heng Chang", "Xinmei Qiu", "Fan Wang", "Chunfeng Lian", "Jianhua Ma"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0404", "x": 5.987, "y": 4.063, "title": "Foundation-Model-Boosted Multimodal Learning for fMRI-Based Neuropathic Pain Drug Response Prediction", "abstract": "Neuropathic pain, affecting up to 10% of adults, remains difficult to treat due to limited therapeutic efficacy and tolerability. Although resting-state functional MRI (rs-fMRI) is a promising noninvasive measurement of brain biomarkers to predict drug response in therapeutic development, the complexity of fMRI demands machine learning models with substantial capacity. However, extreme data scarcity in neuropathic pain research limits the application of high-capacity models. To address the challenge of data scarcity, we propose FMMT C , a Foundation-Model-boosted Multimodal learning framework for fMRIbased neuropathic pain drug response prediction, which leverages both internal multimodal information in pain-specific data and external knowledge from large pain-agnostic data. Specifically, to maximize the value of limited pain-specific data, FMMT C integrates complementary information from two rs-fMRI modalities: Time series and functional Connectivity. FMMT C is further boosted by an fMRI foundation model with its external knowledge from extensive pain-agnostic fMRI datasets enriching limited pain-specific information. Evaluations with an in-house dataset and a public dataset from OpenNeuro demonstrate FMMT C 's superior represen tation ability, generalizability, and cross-dataset adaptability over existing unimodal fMRI models that only consider one of the rs-fMRI modalities. The ablation study validates the effectiveness of multimodal learning and foundation-model-powered external knowledge transfer in FMMTC. An integrated gradient-based interpretation study explains how FMMTC's cross-dataset dynamic behaviors enhance its adaptability. In conclusion, FMMTC boosts clinical trials in neuropathic pain therapeutic development by accurately predicting drug responses to improve the participant stratification efficiency. The code is released on https://github.com/Shef-AIRE/FMM_TC.", "filename": "2025_0404.pdf", "year": 2025, "institution": "University o f Sheffield", "country": "UK", "authors": ["Wenrui Fan", "L M Riza Rizky", "Jiayang Zhang", "Chen Chen", "Haiping Lu", "Kevin Teh", "Dinesh Selvarajah", "Shuo Zhou"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0405", "x": 3.448, "y": 4.204, "title": "From Pixels to Prognosis: A Multi-modal Attention-Based Framework for Visceral Adipose Tissue Estimation", "abstract": "Obesity is a chronic disease that increases the risk of multiorgan damage as well as cardiovascular disease, diabetes, and certain cancers. It is strongly related to Visceral Adipose Tissue (VAT), which is the fat stored around the internal organs. New approaches to assessing VAT in large populations are essential to understand how obesity contributes to chronic disease progression. Various direct and indirect measures have been developed to quantify VAT. However, many of these techniques either fail to distinguish between various types of body fats (e.g., subcutaneous versus visceral) or involve high radiation imaging and/or are costly (e.g., Computed Tomography). Annually, millions of individuals globally undergo hip or spine Dual-energy X-ray Absorptiometry (DXA) scans to screen for osteoporosis as well as lateral spine (LS) scans to detect vertebral fractures. In this paper, we develop a multi-modal attention-based framework for VAT estimation from LS DXA scans and patient demographic information. We compare our results on two LS DXA datasets with baseline methods and also perform clinical analysis to demonstrate its effectiveness. The proposed approach has the potential to enable costeffective, non-invasive, and efficient quantification of VAT in people undergoing bone density assessment with LS scans. To the best of our knowledge, this is the first paper to predict VAT from DXA LS scans.", "filename": "2025_0405.pdf", "year": 2025, "institution": "Edith Cow an University", "country": "Australia", "authors": ["Arooba Maqsood", "Afsah Saleem", "Marc Sim", "David Suter", "Simone Radavelli-Bagatini", "Jonathan M Hodgson", "Richard L Prince", "Kun Zhu", "William D Leslie", "John T Schousboe", "Joshua R Lewis", "Syed Zulqarnain Gilani"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0406", "x": 3.688, "y": 5.091, "title": "Hard Sample Mining-Based Tongue Diagnosis for Fatty Liver Disease Severity Classification", "abstract": "Fatty liver disease (FLD) negatively affects over 30% of the global population and can ultimately lead to cirrhosis and death. Early detection and intervention on the severity of FLD help control its progression. However, facilities for assessing the severity of FLD are lacking in economically disadvantaged regions, highlighting an urgent need for a cost-effective and scalable screening method. Traditional Chinese Medicine (TCM) suggests a strong correlation between tongue characteristics and liver health, positioning tongue diagnosis as a non-invasive means for assessing FLD severity. Establishing an automated tongue diagnosis method holds promise for large-scale and rapid classification of FLD severity among rural populations. In this paper we present a Hard sample Mining-based Tongue Diagnosis Framework (HM-TDF) for multi-class classification of FLD severity. The HM-TDF identifies hard samples using a novel uncertainty estimation approach and addresses them through a multi-expert classifier. We introduce a Multi-source Feature Fusion Kolmogorov-Arnold Network (MFF-KAN) to model the relationship between tongue images plus basic physiological indicators and FLD severity. We propose a three-step training strategy to train this heterogeneous model. We construct and release a novel tongue diagnosis dataset for FLD severity classification, named Tongue-FLD, to advance research in automated tongue diagnosis. Experimental results on this dataset indicate that the proposed method surpasses existing automated tongue diagnosis methods in the classification of FLD severity. Moreover, MFF-KAN effectively visualizes the key pathways from input to output, providing strong interpretability. The dataset and code are available at https://github.com/MLDMXM2017/HM-TDF.T. Chen and J. Gao-Contributed equally.", "filename": "2025_0406.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Tao Chen", "Jie Gao", "Yong Xu", "Weihong Qiu", "Yijie Wu", "Weimin Ye", "Kunhong Liu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0407", "x": 3.92, "y": 4.775, "title": "Integrate Semantic Radiomics as Prior Evidence Into Evidential Deep Learning for Pelvic Lipomatosis Diagnosis", "abstract": "Pelvic Lipomatosis (PL) is a rare disorder characterized by abnormal fat proliferation in the pelvic region, where subtle imaging differences between pathological and normal fat pose significant diagnostic challenges. Existing deep-learning-based computer-aided diagnosis methods struggle to integrate high-level clinical semantics, which limits the diagnosis accuracy. This paper proposes a novel Evidential Deep Learning (EDL) method that synergistically fuses multi-type semantic radiomics priors derived from clinical expertise to enhance PL diagnosis. First, referring to clinical experiences, the critical PL semantic radiomics including bladder-rectal fat distance, rectal circularity, bladder-seminal vesicle angle, and relative pelvic fat volume are extracted from 3D abdominal CT images. Second, these semantic radiomics are probabilistically formulated as prior evidences to quantify their diagnostic relevance. Finally, the prior evidences are fused into the EDL backbone to implement PL diagnosis. Comparing with the pure deep learning methods, the EDL method with prior evidences not only reduces overconfident predictions but also enables in terpretable decision-making by involving clinical knowledge. Experiments demonstrate the state-of-the-art performance of the proposed method, which achieves great improvements over conventional deep learning baselines. Ablation studies also validate the necessity of integrating the semantic features. Theoretical proofs further confirm that clinically consistent priors minimize prediction loss and enhance model stability. This work advances the diagnosis by bridging clinical radiomics with data-driven deep learning and provides a paradigm for interpretable PL medical image analysis.", "filename": "2025_0407.pdf", "year": 2025, "institution": "Shanghai University", "country": "China", "authors": ["Zheran Zhang", "Xiaodong Yue", "Maoyu Wang", "Zhikang Xu", "Yufei Chen", "Zhipeng Wei"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0408", "x": 4.169, "y": 5.036, "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "abstract": "As medical diagnoses increasingly leverage multimodal data, machine learning models are expected to effectively fuse heterogeneous information while remaining robust to missing modalities. In this work, we propose a novel multimodal learning framework that integrates enhanced modalities dropout and contrastive learning to address realworld limitations such as modality imbalance and missingness. Our approach introduces learnable modality tokens for improving missingnessaware fusion of modalities and augments conventional unimodal contrastive objectives with fused multimodal representations. We validate our framework on large-scale clinical datasets for disease detection and prediction tasks, encompassing both visual and tabular modalities. Experimental results demonstrate that our method achieves state-ofthe-art performance, particularly in challenging and practical scenarios where only a single m odality is available. Furthermore, we show its adaptability through successful integration with a recent CT foundation model. Our findings highlight the effectiveness, efficiency, and generalizability of our multimodal learning approach, offering a scalable, low-cost solution with significant potential for more complicated clinical applications that allow missing modality input. The code is available at https://github.com/omron-sinicx/medical-modality-dropout.", "filename": "2025_0408.pdf", "year": 2025, "institution": "OMRON SINIC X Corporation", "country": "Japan", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0409", "x": 3.938, "y": 4.115, "title": "Lesion-Centered Vision Transformer for Stroke Outcome Prediction from Image and Clinical Data", "abstract": "Accurate prediction of stroke functional outcome, particularly the 3-month modified Rankin Scale (mRS), is crucial for personalized treatment. Vision Transformers excel in medical imaging and multimodal fusion but struggle with stroke MRI due to data scarcity and rigid tokenization, which may miss subtle anomalies. In response, we propose the Lesion-Centered Vision Transformer (LC-ViT), integrating lesion-focused MRI preprocessing, adaptive token merging, and multimodal fusion. LC-ViT extracts axial, coronal and sagittal views centered on ischemic lesions to optimize visibility and employs a pretrained TCFormer (token-clustering transformer) for adaptative token generation. A m utual cross-attention mechanism further integrates imaging and clinical data. Evaluated on a retrospective private cohort comprising DWI MRI and 62 clinical variables (e.g. demographics, neurological assessments.) of 119 stroke patients treated with thrombectomy (65% favorable outcome), LC-ViT achieves a new state-of-the-art performance (AUC: .0.80 ± 0.03, Accuracy: . 0.77 ± 0.02) significantly outperforming single modality based deep architectures. Our results highlight the potential of lesion-focused tokenization for stroke outcome prediction and interpretability and broader applications in lesion-localized multimodal analysis. Our code is available at https:// github.com/mingtian12345/LC-VIT.", "filename": "2025_0409.pdf", "year": 2025, "institution": "CREATIS", "country": null, "authors": ["Mingtian Liu", "Nima Hatami", "Laura Mechtouff", "Tae-Hee Cho", "Carole Lartizien", "Carole Frindel"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0410", "x": 3.262, "y": 5.313, "title": "Leveraging Visual Prompt with Diffusion Adversarial Network for Radiotherapy Dose Prediction", "abstract": "Automatic prediction of dose distribution maps wields considerable influence in clinical radiotherapy treatment. Recently, deep learning-based approaches have been explored to automatically predict the dose map from structure images and obtain promising results. However, these methods mainly focus on extracting anatomical features from CT and organ masks, ignoring abundant visual knowledge inherent in the domain of dose map. To address this limitation, we innovatively propose a visual prompt-guided dose prediction model, named ViPDose, to effectively predict radiotherapy dose distribution for cancer patients. Specifically, our ViPDose is structured with two key stages: 1) a prompt pretraining stage and 2) a prompt generation stage. In the pre-training stage, we train a prompt encoder to encode dose maps alongside structure im-ages into compact prompt vectors. Then, in the prompt generation stage, we design a fast prompt generator fulfilled with a diffusion adversarial network (DAN) to efficiently produce the prompt vectors that closely approximate those generated by the prompt encoder, thus enriching the model with abundant visual prompt information. By adopting DAN in such highly compressed latent space, our method can guarantee high-quality predictions with relatively low computation costs. Comprehensive experiments on a clinical rectal cancer dataset with 130 cases have verified the superior performance of our method over other state-of-the-art methods.", "filename": "2025_0410.pdf", "year": 2025, "institution": "Sichuan University", "country": "China", "authors": ["Zhenghao Feng", "Lu Wen", "Jiaqi Cui", "Xi Wu", "Jianghong Xiao", "Xingchen Peng", "Dinggang Shen", "Yan Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0411", "x": 7.212, "y": 4.593, "title": "LNODE: Uncovering the Latent Dynamics of $$\\text {A}\\beta $$ in Alzheimer’s Disease", "abstract": "Aβ Positron Emission Tomography (PET) is often used to manage Alzheimer's disease (AD).To better understand Aβ progression, we introduce and evaluate a mathematical model that couples Aβ at parcellated gray matter regions. We term this model LNODE for \"latent network ordinary differential equations\". At each region, we track normal Aβ, abnormal Aβ, and m latent states that intend to capture unobservable mechanisms coupled to Aβ progression. LNODE is parameterized by subject-specific parameters and cohort parameters. We jointly invert for these parameters by fitting the model to Aβ-PET data from 585 subjects from the ADNI dataset. Although underparameterized, our model achieves population R 2 ≥ 98% compared to R 2 ≤ 60% when fitting without latent states. Furthermore, these preliminary results suggest the existence of different subtypes of Aβ progression.", "filename": "2025_0411.pdf", "year": 2025, "institution": "The University of Texas at Austin", "country": "USA", "authors": ["Zheyu Wen", "George Biros"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0412", "x": 4.379, "y": 5.511, "title": "Longitudinal MRI-Clinical Multimodal Fusion for pCR Prediction in Breast Cancer", "abstract": "Pathologic complete response (pCR) prediction for breast cancer patients undergoing neoadjuvant chemotherapy (NAC) is crucial for optimizing treatment strategies. Nowadays, an increasing number of studies focus on predicting NAC response using preoperative imaging, and with the advancement of deep learning, different modalities of imaging and other clinical data can be effectively integrated to provide more comprehensive information. However, existing deep learning methods primarily focus on multimodal fusion or longitudinal modeling but often suffer from inadequate feature focus and overlook specific treatment effects. To address these limitations, we propose a novel multimodal-learning framework LMF(Longitudinal MRI-Clinical Multimodal Fusion) that enhances feature extraction and explicitly models treatment-induced imaging changes. Our method consists of two key components: (1) Molecular-Aware Deformable Attention (MADA), which integrates molecular subtype information with MRI features and refines spatial representations via deformable cross-attention mechanism; and (2) Treatment-Aware Longitudinal Modeling (TALM), which incorporates t reatment embeddings to capture NAC-driven feature variations. The model is trained and evaluated on the ISPY-2 dataset, using pre-and post-NAC DCE-MRI alongside clinical data. Experimental results demonstrate that our approach outperforms existing methods, confirming that MADA effectively enhances feature extraction while TALM strengthens longitudinal modeling. These findings highlight the potential of integrating multimodal feature refinement with treatment-aware temporal modeling for improved pCR prediction. Our code is available at https://github. com/martin-bro/LMF.", "filename": "2025_0412.pdf", "year": 2025, "institution": "Shanghai Jiao T ong University", "country": "China", "authors": ["Dingrui Ma", "Jiawei Cao", "Hao Cheng", "Dan Zhou", "Jianping Liu", "Xiaofeng Zhang", "Kaijie Wu", "Chaochen Gu", "Xinping Guan"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0413", "x": 2.596, "y": 3.81, "title": "MedSoft-Diffusion: Medical Semantic-Guided Diffusion Model with Soft Mask Conditioning for Vertebral Disease Diagnosis", "abstract": "Accurate diagnosis of vertebral diseases is vital for preventing severe complications, but data imbalance between abundant normal and rare pathological cases poses a substantial challenge to diagnostic performance. Medical image generation offers a promising solution by synthesizing pathological samples. However, existing diffusionbased methods, pre-trained on natural images, often fall short in capturing complex pathological features due to the pre-training knowledge gap, as well as struggling to obtain precise lesion masks and ensure seamless integration between lesions and the background. To overcome these challenges, we propose a novel diffusion-based medical image generation framework called MedSoft-Diffusion, which involves leveraging detailed medical knowledge to ensure that generated images are not only semantically consistent with the specified pathological conditions but also anatomically accurate. Our framework includes a Medical Semantic Controller (MSC) designed to enhance the alignmen t between textual prompts and lesion characteristics, ensuring the synthesis of semantically accurate pathological images. Furthermore, the Soft Mask Inpainting Strategy (SMIS) is proposed to combine soft masks with blurring techniques to improve the realism of synthesized images. Experimental results on two vertebral disease datasets demonstrate notable improvements in both image quality and classification performance using our approach. Code is available at MedSoft-Diffusion.", "filename": "2025_0413.pdf", "year": 2025, "institution": "Sun Yat-Sen University", "country": "China", "authors": ["Shidan He", "Enyuan Hu", "Zixuan Tang", "Bin Chen", "Dongdong Yu", "Yuan Hong", "Zhenzhong Liu", "Mengtang Li", "Lei Liu", "Shen Zhao"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0414", "x": 4.674, "y": 4.669, "title": "MELON: Multimodal Mixture-of-Experts with Spectral-Temporal Fusion for Long-Term MObility EstimatioN in Critical Care", "abstract": "Patient mobility monitoring in intensive care is critical for ensuring timely interventions and improving clinical outcomes. While accelerometry-based sensor data are widely adopted in training artificial intelligence models to estimate patient mobility, existing approaches face two key limitations highlighted in clinical practice: (1) modeling the longterm accelerometer data is challenging due to the high dimensionality, variability, and noise, and (2) the absence of efficient and robust methods for long-term mobility assessment. To overcome these challenges, we introduce MELON, a novel multimodal framework designed to predict 12-h mobility status in the critical care setting. MELON leverages the power of a dual-branch network architecture, combining the strengths of spectrogram-based visual representations and sequential accelerometer statistical features. MELON effectively captures global and fine-grained mobility patterns by integrating a pre-trained image encoder for rich frequency-domain feature extraction and a Mixture-of-Experts encoder for sequence modeling. We trained and evaluated the MELON model on the multimodal dataset of 126 patients recruited from nine Intensive Care Units. Experiments showed that MELON outperforms conventional approaches for 12-h mobility status estimation with an overall area under the receiver operating characteristic curve (AU-ROC) of 0.82 (95% confidence interval 0.78-0.86). Notably, our experiments also revealed that accelerometer data collected from the wrist provides robust predictive performance compared with data from the ankle, suggesting a singlesensor solution that can reduce patient burden and lower deployment costs. Project repository: https://github.com/iheallab/MELON", "filename": "2025_0414.pdf", "year": 2025, "institution": "University of Florida", "country": "USA", "authors": ["Jiaqing Zhang", "Miguel Contreras", "Jessica Sena", "Andrea Davidson", "Yuanfang Ren", "Ziyuan Guan", "Tezcan Ozrazgat-Baslanti", "Tyler J Loftus", "Subhash Nerella", "Azra Bihorac", "Parisa Rashidi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0415", "x": 4.314, "y": 6.02, "title": "Multi-modal Knowledge Decomposition Based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology", "abstract": "Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data fusion analysis. However, the simultaneous acquisition of multi-modal data, such as genomic and pathological information, is often challenging due to cost or technical limitations. To address this challenge, we propose an online distillation approach based on Multimodal Knowledge Decomposition (MKD) to enhance IHC biomarker prediction in haematoxylin and eosin (H&E) stained histopathology images. This method leverages paired genomic-pathology data during training while enabling inference using either pathology slides alone or both modalities. Two teacher and one student models are developed to extract modality-specific and modality-general features by minimizing the MKD loss. To maintain the internal structural relationships between samples, Similarity-preserving K nowledge Distillation (SKD) is applied. Additionally, Collaborative Learning for Online Distillation (CLOD) facilitates mutual learning between teacher and student models, encouraging diverse and complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU datasets demonstrate that our approach achieves superior performance in IHC biomarker prediction using uni-modal data. Our code is available at https://github.com/qiyuanzz/ MICCAI2025 MKD.", "filename": "2025_0415.pdf", "year": 2025, "institution": "Dalian University of Technology", "country": "China", "authors": ["Qibin Zhang", "Xinyu Hao", "Qiao Chen", "Rui Xu", "Fengyu Cong", "Cheng Lu", "Hongming Xu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0416", "x": 3.745, "y": 6.732, "title": "Multi-scale Attention-Based Multiple Instance Learning for Breast Cancer Diagnosis", "abstract": "Multiple Instance Learning (MIL) is a powerful weakly supervised learning framework for high-resolution medical images, but its application in mammographic breast cancer (BC) diagnosis overlooks instance interactions and the multi-scale nature of BC lesions. In this work, we propose a novel Feature Pyramid Network (FPN)-MIL model for BC classification and detection in high-resolution mammograms, integrating (1) a FPN-based instance encoder that enables a multi-scale analysis across different receptive-field granularities while operating on single-scale input patches; (2) deep-supervised scale-specific instance aggregators that support conventional attention (AbMIL) or transformer-based (SetTrans) mechanisms; (3) an attention-based multi-scale aggregator that dynamically com bines scale-specific features, improving robustness to lesion scale variability. Our experiments show that FPN-MIL is superior to conventional single-and multi-scale patchbased MIL models, with FPN-SetTrans outperforming baselines in calcification classification and detection while FPN-AbMIL performs best for mass classification. Code is available publicly at: https://github.com/ marianamourao-37/Multi-scale-Attention-based-MIL.", "filename": "2025_0416.pdf", "year": 2025, "institution": "Instituto Superior Técnico", "country": "Portugal", "authors": ["Mariana Mourão", "Jacinto C Nascimento", "Carlos Santiago", "Margarida Silveira"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0417", "x": 6.803, "y": 4.708, "title": "Multistage Alignment and Fusion for Multimodal Multiclass Alzheimer’s Disease Diagnosis", "abstract": "For the early diagnosis of Alzheimer's disease (AD), it is essential that we have effective multiclass classification methods that can distinct subjects with mild cognitive impairment (MCI) from cognitively normal (CN) subjects and AD patients. However, significant overlaps of biomarker distributions among these groups make this a difficult task. In this work, we propose a novel framework for multimodal, multiclass AD diagnosis that can integrate information from diverse and complex modalities to resolve ambiguity among the disease groups and hence enhance classification performances. More specifically, our approach integrates T1-weighted MRI, tau PET, fiber orientation distribution (FOD) from diffusion MRI (dMRI), and Montreal Cognitive Assessment (MoCA) scores to classify subjects into AD, MCI, and CN groups. We introduce a Swin-FOD model to extract order-balanced features from FOD and use contrastive learning to align MRI and PET features. These aligned features and MoCA scores are then processed with a Tabular Prior-data Fitted In-context Learning (TabPFN) method, which selects model parameters based on the alignment between input data and prior data during pre-training, eliminating the need for additional training or fine-tuning. Evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (n = 1147), our model achieved a diagnosis accuracy of 73.21%, outperforming all comparison models (n = 10). We also performed Shapley analysis and quantitatively evaluated the essential contributions of each modality.", "filename": "2025_0417.pdf", "year": 2025, "institution": "University of Southern California (USC)", "country": "USA", "authors": ["Shuo Huang", "Lujia Zhong", "Yonggang Shi"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0418", "x": 5.252, "y": 5.041, "title": "Multiview Feature Fusion and Contrastive Learning for Drug-Target Interaction Prediction", "abstract": "Drug-target interaction (DTI) prediction is crucial for drug discovery, as it accelerates candidate screening and reduces development costs. However, existing computational methods are often limited to a single perspective and cannot simultaneously consider the biological information and complex associations of drugs and targets. Although multimodal data have been introduced, the complementarity and interaction of multi-source information remain underutilized, making efficient multi-view feature fusion a key challenge. In this paper, we propose a DTI prediction framework based on multi-view feature fusion and contrastive learning, named MFCL-DTI. It integrates sequence feature as well as structural and semantic information of heterogeneous graph. A multi-view adaptive fusion module facilitates cross-view feature fusion, while multi-view contrastive learning enhances feature representation. Experimental results demonstrate that MFCL-DTI outperforms existing methods, validating its effectiveness in DTI prediction.", "filename": "2025_0418.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Xiaoting Zeng", "Li Li", "Yu Liang", "Weilin Chen", "Baiying Lei"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0419", "x": 4.559, "y": 6.033, "title": "MurreNet: Modeling Holistic Multimodal Interactions Between Histopathology and Genomic Profiles for Survival Prediction", "abstract": "Cancer survival prediction requires integrating pathological Whole Slide Images (WSIs) and genomic profiles, a challenging task due to the inherent heterogeneity and the complexity of modeling both interand intra-modality interactions. Current methods often employ straightforward fusion strategies for multimodal feature integration, failing to comprehensively capture modality-specific and modality-common interactions, resulting in a limited understanding of multimodal correlations and suboptimal predictive performance. To mitigate these limitations, this paper presents a Multimodal Representation Decoupling Network (MurreNet) to advance cancer survival analysis. Specifically, we first propose a Multimodal Representation Decomposition (MRD) module to explicitly decompose paired input data into modality-specific and modality-shared representations, thereby reducing redundancy between modalities. Furthermore, the disentangled representations are further refined then updated through a no vel training regularization strategy that imposes constraints on distributional similarity, difference, and representativeness of modality features. Finally, the augmented multimodal features are integrated into a joint representation via proposed Deep Holistic Orthogonal Fusion (DHOF) strategy. Extensive experiments conducted on six TCGA cancer cohorts demonstrate that our MurreNet achieves state-of-the-art (SOTA) performance in survival prediction.", "filename": "2025_0419.pdf", "year": 2025, "institution": "Nanjing University of Information Science and Technology", "country": "China", "authors": ["Mingxin Liu", "Chengfei Cai", "Jun Li", "Pengbo Xu", "Jinze Li", "Jiquan Ma", "Jun Xu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0420", "x": 6.895, "y": 3.783, "title": "MvHo-IB: Multi-view Higher-Order Information Bottleneck for Brain Disorder Diagnosis", "abstract": "Recent evidence suggests that modeling higher-order interactions (HOIs) in functional magnetic resonance imaging (fMRI) data can enhance the diagnostic accuracy of machine learning systems. However, effectively extracting and leveraging HOIs remains a significant challenge. In this paper, we propose MvHo-IB, a novel multi-view learning framework that seamlessly integrates pairwise interactions and HOIs for diagnostic decision-making while automatically compressing taskirrelevant redundant information. Our approach introduces several key innovations: (1) a principled framework combining O-information from information theory with the recently developed matrix-based Rényi's αorder entropy functional estimator to quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder designed to effectively utilize these interactions, and (3) a novel multiview learning information bottleneck objective to enhance representation learning. Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves state-ofthe-art performance, outperforming existing methods, including modern hypergraph-based techniques, by significant margins. The code of our MvHo-IB is available at https://github.com/zky04/MvHo-IB.", "filename": "2025_0420.pdf", "year": 2025, "institution": "Zhengzhou University", "country": "China", "authors": ["Kunyu Zhang", "Qiang Li", "Shujian Yu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0421", "x": 6.68, "y": 4.555, "title": "Neuro-AMS: Neuro-Informed Age-Aware and Medical Knowledge-Integrated Strategy for Diagnosis of Multiple Brain Disorders", "abstract": "Neurological diseases encompass a diverse range of conditions such as neurodegenerative diseases and neurodevelopmental disorders. Developing a general model to assist in the diagnosis of multiple neurological diseases is essential in clinical practice, as it can help reduce misdiagnosis rates and alleviate the burden on physicians. However, most existing diagnostic models are designed for specific neurological disease scenarios and show poor performance when applied to multiple diseases. To this end, we present a semantic-assisted framework, called Neuro-AMS, a Neuro-informed Age-aware and Medical knowledge-integrated Strategy for diagnosis of multiple brain disorders. Specifically, we employ a vision encoder based on age-aware strategy to further enhance performance by leveraging the potential relationship between age and neurological diseases. Additionally, we extract semantic features from labels and integrate corresp onding medical knowledge embeddings, constructing knowledge-level label features with enhanced semantics. These knowledge-level label features guide the vision encoder for capturing higher-level semantic representations through the alignment of image-text pairs. Our method is evaluated on four public brain disease datasets, and experimental results demonstrate that our method achieves consistent and statistically significant improvement compared with three public benchmarks and three specialized models.", "filename": "2025_0421.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Zhenguo Zhang", "Lin Teng", "Nan Zhao", "Yuxiao Liu", "Zhaoyu Qiu", "Zehao Weng", "Jinwei Kong", "Feng Shi", "Dinggang Shen"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0422", "x": 1.842, "y": 2.737, "title": "Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-supervision, Mixture of Experts and Multi-task Integration", "abstract": "Osteoporosis, characterized by reduced bone mineral density (BMD) and compromised bone microstructure, increases fracture risk in aging populations. While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD assessment, its limited accessibility hinders diagnosis in resource-limited regions. Opportunistic computed tomography (CT) analysis has emerged as a promising alternative for osteoporosis diagnosis using existing imaging data. Current approaches, however, face three limitations: (1) underutilization of unlabeled vertebral data, (2) systematic bias from device-specific DXA discrepancies, and (3) insufficient integration of clinical knowledge such as spatial BMD distribution patterns. To address these, we propose a unified deep learning framework with three innovations. First, a self-supervised learning method using radiomic representations to leverage unlabeled CT data and preserve bone texture. Second, a Mixture of Experts (MoE) architecture with learned gating mechanisms to enhance cross-device adaptability. Third, a multi-task learning framework integrating osteoporosis diagnosis, BMD regression, and vertebra location prediction. Validated across three clinical sites and an external hospital, our approach demonstrates superior generalizability and accuracy over existing methods for opportunistic osteoporosis screening and diagnosis.", "filename": "2025_0422.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Jiaxing Huang", "Heng Guo", "Le Lu", "Fan Yang", "Minfeng Xu", "Ge Yang", "Wei Luo"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0423", "x": 4.436, "y": 6.183, "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-Aware Optimal Transport", "abstract": "Survival prediction using whole slide images (WSIs) can be formulated as a multiple instance learning (MIL) problem. However, existing MIL methods often fail to explicitly capture pathological heterogeneity within WSIs, both globally -through long-tailed morphological distributions, and locally -through tile-level prediction uncertainty. Optimal transport (OT) provides a principled way of modeling such heterogeneity by incorporating marginal distribution constraints. Building on this insight, we propose OTSurv, a novel MIL framework from an optimal transport perspective. Specifically, OTSurv formulates survival predictions as a heterogeneity-aware OT problem with two constraints: (1) global long-tail constraint that models prior morphological distributions to avert both mode collapse and excessive uniformity by regulating transport mass allocation, and (2) local uncertainty-aware constraint that prioritizes high-confidence patches while suppressing noise by progressively raising the total transport mass. We then recast the initial OT problem, augmented by these constraints, i nto an unbalanced OT formulation that can be solved with an efficient, hardware-friendly matrix scaling algorithm. Empirically, OTSurv sets new state-of-the-art results across six popular benchmarks, achieving an absolute 3.6% improvement in average C-index. In addition, OTSurv achieves statistical significance in log-rank tests and offers high interpretability, making it a powerful tool for survival prediction in digital pathology. Our codes are available at https://github.com/Y-Research-SBU/OTSurv.", "filename": "2025_0423.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0424", "x": 3.111, "y": 4.594, "title": "Parameterized Diffusion Optimization Enabled Autoregressive Ordinal Regression for Diabetic Retinopathy Grading", "abstract": "As a long-term complication of diabetes, diabetic retinopathy (DR) progresses slowly, potentially taking years to threaten vision. An accurate and robust evaluation of its severity is vital to ensure prompt management and care. Ordinal regression leverages the underlying inherent order between categories to achieve superior performance beyond traditional classification. However, there exist challenges leading to lower DR classification performance: 1) The uneven distribution of DR severity levels, characterized by a long-tailed pattern, adds complexity to the grading process. 2) The ambiguity in defining category boundaries introduces additional challenges, making the classification process more complex and prone to inconsistencies. This work proposes a novel autoregressive ordinal regression method called AOR-DR to address the above challenges by leveraging the clinical knowledge of inherent ordinal information in DR grading dataset settings. Specifically, we decompose the DR grading task into a series of ordered s teps by fusing the prediction of the previous steps with extracted image features as conditions for the current prediction step. Additionally, we exploit the diffusion process to facilitate conditional probability modeling, enabling the direct use of continuous global image features for autoregression without relearning contextual information from patch-level features. This ensures the effectiveness of the autoregressive process and leverages the capabilities of pre-trained large-scale foundation models. Extensive experiments were conducted on four large-scale publicly available color fundus datasets,", "filename": "2025_0424.pdf", "year": 2025, "institution": "University of Exeter", "country": "UK", "authors": ["Qinkai Yu", "Wei Zhou", "Hantao Liu", "Yanyu Xu", "Meng Wang", "Yitian Zhao", "Huazhu Fu", "Xujiong Ye", "Yalin Zheng", "Yanda Meng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0425", "x": 3.759, "y": 4.691, "title": "PLUS: Plug-and-Play Enhanced Liver Lesion Diagnosis Model on Non-contrast CT Scans", "abstract": "Focal liver lesions (FLL) are common clinical findings during physical examination. Early diagnosis and intervention of liver malignancies are crucial to improving patient survival. Although the current 3D segmentation paradigm can accurately detect lesions, it faces limitations in distinguishing between malignant and benign liver lesions, primarily due to its inability to differentiate subtle variations between different lesions. Furthermore, existing methods predominantly rely on specialized imaging modalities such as multi-phase contrast-enhanced CT and magnetic resonance imaging, whereas non-contrast CT (NCCT) is more prevalent in routine abdominal imaging. To address these limitations, we propose PLUS, a plug-and-play framework that enhances FLL analysis on NCCT images for arbitrary 3D segmentation models. In extensive experiments involving 8,651 patien ts, PLUS demonstrated a significant improvement with existing methods, improving the lesion-level F1 score by 5.66%, the malignant patient-level F1 score by 6.26%, and the benign patient-level F1 score by 4.03%. Our results demonstrate the potential of PLUS to improve malignant FLL screening using widely available NCCT imaging substantially. Code is available at https://github.com/alibaba-damo-academy/plug-and-play-diagnosis.", "filename": "2025_0425.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Jiacheng Hao", "Xiaoming Zhang", "Wei Liu", "Xiaoli Yin", "Yuan Gao", "Chunli Li", "Ling Zhang", "Le Lu", "Yu Shi", "Xu Han", "Ke Yan"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0426", "x": 7.2, "y": 4.802, "title": "Predicting Alzheimer’s Disease Progression Using a Regression-Based Survival Model with Longitudinal Data", "abstract": "Alzheimer's disease (AD) progression is characterized by slow, heterogeneous, and subtle changes that span decades, making transition points difficult to determine. This challenge is compounded by the complexity of longitudinal clinical data, including irregular followup patterns and varying observation durations that traditional survival analysis models cannot handle. We present a novel regression-based survival framework with three key innovations: (1) Robust longitudinal data handling, (2) Enhanced early-stage prediction, and (3) Flexible integration with existing models. Using a partial optimization approach for mean squared error l oss, our method achieves state-of-the-art performance in AD progression prediction, particularly excelling in early-stage scenarios. Ablation studies identify the regression loss component as the key driver of improved long-term prediction capability, advancing AD prognosis and broader applications in longitudinal survival analysis.", "filename": "2025_0426.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Ling Dai", "Yiqun Sun", "Jincheng Gu", "Qinsen Bao", "Feihong Liu", "Dinggang Shen"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0427", "x": 6.828, "y": 4.757, "title": "Prior-Guided Prototype Aggregation Learning for Alzheimer’s Disease Diagnosis", "abstract": "Alzheimer's disease (AD) diagnosis faces the challenge of capturing complex patterns of subtle structural and functional changes in neuroimaging and the underutilization of clinical prior knowledge. Current deep learning methods primarily focus on structural magnetic resonance imaging (sMRI) analysis, often overlooking the critical disease concepts that clinicians rely on. To address this limitation, we propose a Prior-guided Prototype Aggregation Learning (PPAL) framework. This framework leverages structured prompts to large language models (LLMs) to extract disease-related anatomical descriptions as clinical prior knowledge and progressively aggregates the visual features of AD and cognitively normal (CN) individuals, bridging the semantic gap between sMRI features and LLM-derived clinical concepts to construct category prototype representations. Meanwhile, we design a slice selection and compression module that adaptively learns the importance of different slices, prioritizing those most critical for AD diagnosis. Ultimately, AD diagnosis is achieved by computing the semantic similarity between MRI slice features and the category prototypes. Experimental results demonstrate that, compared to state-of-the-art 2D slice-based methods, incorporating clinical prior knowledge not only enhances the identification of pathological regions but also shows significant advantages in the zero-shot mild cognitive impairment (MCI) conversion task. The code is available at: https://github.com/diaoyq121/PPAL.", "filename": "2025_0427.pdf", "year": 2025, "institution": "South China University of Technology", "country": "China", "authors": ["Yueqin Diao", "Huihui Fang", "Hanyi Yu", "Yuning Wang", "Yaling Tao", "Ziyan Huang", "Si Yong Yeo", "Yanwu Xu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0428", "x": 7.114, "y": 4.682, "title": "Progression-Aware Generative Model Enhancing Baseline Visit Prediction of Early Alzheimer’s Disease", "abstract": "Benefiting from longitudinal pair-wise brain 18 F-fluorodeoxyglucose ( 18 F-FDG) positron emission tomography (PET) images, disease progression characterized by the generative model may assist the baseline visit prediction of early Alzheimer's Disease. However, most existing methods focused on diagnosing disease from single-timepoint scans or a simple stacking of sequential images, which ignore the importance of disease progression and are not in line with actual clinical scenarios. Moreover, decoupling the low-level disease representations is quite challenging for similar changes between normal aging and neurodegenerative changing. In this paper, we propose a classifier induced generative model to generate the next-timepoint brain images. Then, we design a statistical prior knowledge vision transformer to extract features from the generated next-timepoint images for disease diagnosis. The main contribution is to build a disease progression model that can effectively improve diagnosis performance from single-timepoint images. Meanwhile, we provide pixel-level disease representations for explanation. Experiments on ADNI datasets demonstrate that our method outperforms other state-of-the-art techniques.", "filename": "2025_0428.pdf", "year": 2025, "institution": "Donghua University", "country": "China", "authors": ["Xingyu Gao", "Runyi Guo", "Zhe Zhao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0429", "x": 3.765, "y": 6.835, "title": "PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis", "abstract": "Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with the complexity and heterogeneity of WSIs. Existing MIL methods face challenges in aggregating diverse patch information into robust WSI representations. While ViTs and clustering-based approaches show promise, they are computationally intensive and fail to capture task-specific and slide-specific variability. To address these limitations, we propose PTCMIL, a novel Prompt Token Clustering-based ViT for MIL aggregation. By introducing learnable prompt tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in an end-to-end manner. It dynamically aligns clustering with downstream tasks, using projection-based clustering tailored to each WSI, reducing complexity w hile preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL efficiently captures task-relevant patterns. Extensive experiments on eight datasets demonstrate its superior performance in classification and survival analysis tasks, outperforming state-of-the-art methods. Systematic ablation studies confirm its robustness and strong interpretability. The code is released at https:// github.com/ubc-tea/PTCMIL.", "filename": "2025_0429.pdf", "year": 2025, "institution": "The University of British Columbia", "country": "Canada", "authors": ["Beidi Zhao", "Sangmook Kim", "Hao Chen", "Chen Zhou", "Zu-Hua Gao", "Gang Wang", "Xiaoxiao Li"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0430", "x": 4.143, "y": 5.531, "title": "RadKAM: Attention-Driven Kolmogorov-Arnold Model for Automatic Radiation-Induced Lymphopenia Prediction by Multimodal Learning", "abstract": "Accurate prediction of radiation-induced lymphopenia (RIL), a common complication of radiation therapy (RT), is clinically crucial for ensuring the safety of cancer treatment. However, accurately predicting RIL before RT is highly challenging due to the complexity of immune damage and various input data. In this study, we propose a novel multimodal learning framework named RadKAM to predict RIL severity using heterogeneous data, including CT images, dose maps, and meta-data. The proposed RadKAM leverages a \"divide and conquer\" strategy to learn the multimodal representation and model the dose-damage relationship for RIL prediction in an end-to-end framework. For the first time, an Attention-driven Kolmogorov-Arnold Fusion (AKaF) scheme is designed by injecting modality-adaptive attention into KAN for intra-and inter-modality interactions. Specifically, Rad-KAM is constructed with Multimodal Interactive AKaF (MI-AKaF) and Cross-modality Guided AKaF (CG-AKaF) to capture f eatures related to lymphocyte-related organs, and model the dose-damage relationships by multimodal feature interactions. By leveraging the advantages of nonlinear representation, RadKAM effectively models the complex interactions of heterogeneous multimodal data, resulting in a comprehensive representation for RIL prediction. Extensive experiments validate the effectiveness of the proposed RadKAM framework, demonstrating its ability to accurately predict RIL severity through multimodal learning.", "filename": "2025_0430.pdf", "year": 2025, "institution": "Central South University", "country": "China", "authors": ["Rongchang Zhao", "Zhangyue Wu", "Jian Zhang", "Zijian Zhang", "Shuo Li"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0431", "x": 3.718, "y": 5.409, "title": "RANDose: A Region-Aware Attention Network for Accurate Radiation Dose Prediction", "abstract": "External Radiation Therapy (ERT) is a key treatment in oncology, aiming to deliver high radiation doses to the Planned Target Volume (PTV) while minimizing exposure to surrounding healthy tissues and Organs At Risk (OARs). However, the proximity of PTVs to OARs, the presence of multiple OARs, and the time-consuming nature of manual subjective dose planning present significant challenges. While recent advancements in Deep Learning (DL) have led to various DL-based methods for dose prediction, it is still challenging to effectively capture multiscale features and propagate essential information to related regions. In this work, we propose the Region-aware Attention Net (RANDose), which addresses these issues by integrating Multi-Scale Channel Spatial Attention (MSCSA), PTV In tegration (PI), and Attention Fusion (AF) modules. Additionally, we introduce a Region-Aware Loss function to ensure accurate dose distribution within the PTV while minimizing radiation exposure to OARs. Experiments on the OpenKBP dataset demonstrate that RANDose outperforms existing models in both Dose Score and Dose Volume Histogram (DVH) Score, highlighting its superior performance.", "filename": "2025_0431.pdf", "year": 2025, "institution": "", "country": null, "authors": ["G Jignesh Chowdary", "Tiezhi Zhang", "Xin Qian", "Zhaozheng Yin"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0432", "x": 3.759, "y": 4.486, "title": "Regression-Assisted Classification for CT-Based Portal Hypertension Diagnosis", "abstract": "Portal hypertension (PHT), a critical complication of liver disease, is primarily assessed via invasive procedures that carry inherent risks and discomfort. Recent advancements in deep learning have demonstrated potential for non-invasive diagnostic assistance based on computed tomography (CT) images. However, the small sample size and notable imbalance in PHT clinical data severely restrict the performance of deep learning methods, while the bias introduced by label discretization further compromises model robustness. To address these challenges, we propose a Regression-assisted Classification (RAC) method for noninvasive PHT diagnosis. Firstly, we propose the RAC method instead of direct classification, enabling fine-grained estimation of hepatic venous pressure gradient (HVPG) values before making categorical decisions, thereby reducing the bias caused by discrete label assignment. Moreover, the boundary-aware weighted learning method is proposed to jointly optimize model parameters and the loss function by dynamically assigning online bucket-based weights and enforcing gradient balance across decision boundaries. We show that this approach can significantly reduce the impact of data imbalance and help handle the challenges of smallsample learning in PHT diagnosis. Experiments on our collected clinical CT dataset achieve 83.28% accuracy and 82.69% for the area under the receiver operating characteristic curve in the three-class classification task of PHT, outperforming the cross-entropy baseline by +1.01% and +2.38%, respectively. These results demonstrate leading performance in PHT multi-class classification diagnostic tasks and offer an effective solution for the direct diagnosis of PHT based on CT images.", "filename": "2025_0432.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Wuque Cai", "Jiayi He", "Xu Guo", "Hongze Sun", "Huan Tong", "Bo Wei", "Hao Wu", "Dezhong Yao", "Daqing Guo"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0433", "x": 2.935, "y": 4.227, "title": "Reliable and Interpretable Visual Field Progression Prediction with Diffusion Models and Conformal Risk Control", "abstract": "Accurately predicting visual field progression is critical for early intervention and personalized treatment of glaucoma. However, existing methods struggle with both predictive accuracy and reliable uncertainty quantification. This paper introduces a framework that leverages diffusion models and conformal risk control to generate robust and interpretable forecasts of visual field deterioration. We first train a diffusion model to predict future visual fields based on a patient's past examinations. To ensure trustworthy predictions, we design a novel archetypalbased conformal risk control method, which provides finite-sample coverage guarantees on intervals of archetypal contributions. This framework captures the underlying structures within uncertainty, enabling clinicians to interpret a range of potential progression patterns rather than a single deterministic outcome. Experimental results illustrate that our method achieves the target archetypal contribution coverage while providing tighter prediction intervals than baselines. Visualizations show how archetypal visual field patterns contribute to prediction uncertainty, offering interpretable insights into disease progression. By combining diffusion models with conformal methods, our framew ork enhances the reliability of AI-assisted visual field forecasting, ultimately supporting improved clinical decision-making. Our code is available at: https:// github.com/averysi224/abci.git.", "filename": "2025_0433.pdf", "year": 2025, "institution": "University of Pennsylv ania", "country": "USA", "authors": ["Wenwen Si", "Vivian Lin", "Bo Sun", "Kuk Jin Jang", "Rubo Xing", "Almiqdad Saeed", "Rina Nagatani", "Oleg Sokolsky", "Insup Lee", "Lama Al-Aswad"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0434", "x": 3.482, "y": 4.869, "title": "Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport", "abstract": "Multimodal ophthalmic imaging-based diagnosis integrates colour fundus imaging with optical coherence tomography (OCT) to provide a comprehensive view of ocular pathologies. However, the uneven global distribution of healthcare resources often leads to real-world clinical scenarios with incomplete multimodal data, which significantly compromises diagnostic accuracy. Existing deep learning pipelines, such as modality imputation and distillation methods, face notable limitations. Imputation methods struggle to accurately reconstruct key lesion features because OCT lesions are localized, while fundus images vary greatly in style. Distillation methods rely heavily on fully paired multimodal training data. To address these challenges, we propose a novel multimodal alignment and fusion framework capable of robustly handling missing modalities in ophthalmic diagnostics. By considering the distinctive feature characteristics of OCT and fundus images, we emphasize semantic feature alignment within the same category and explicitly learn soft matching between modalities, allowing missing modalities to leverage information from available modalities and achieve robust cross-modal feature alignment under modality absence. Specifically, we leverage an Optimal Transport (OT) mechanism for multi-scale modality feature alignment, including class-wise alignment through predicted class prototypes and feature-wise alignment via cross-modal shared feature transport. Furthermore, we propose an asymmetric fusion strategy that effectively exploits the distinct characteristics of OCT and fundus modalities. Extensive evaluations on three large-scale ophthalmic multimodal datasets demonstrate superior performance under various modality-incomplete scenarios, achieving state-of-the-art results in both complete-modality and inter-modality incompleteness settings. The implementation code is available at https://github.com/Qinkaiyu/RIMA", "filename": "2025_0434.pdf", "year": 2025, "institution": "University of Exeter", "country": "UK", "authors": ["Qinkai Yu", "Jianyang Xie", "Yitian Zhao", "Cheng Chen", "Lijun Zhang", "Liming Chen", "Jun Cheng", "Lu Liu", "Yalin Zheng", "Yanda Meng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0435", "x": 4.207, "y": 4.211, "title": "Self-Propagative Multi-Task Learning for Predicting Cardiometabolic Risk Factors", "abstract": "CardioMetabolic Risk (CMR) assessment requires numerous risk factors derived from anthropometric measurements, sphygmomanometry, and blood tests. Deep Learning enables CMR factors to be acquirable from a medical image (e.g., fundus), however, model-perfactor approach is insufficient solution in cost-efficiency. It is also challenge to predict multiple factors simultaneously from a single image, since the CMR factors are inter-correlated among themselves but also correlated with fundus features in various depths. To address this challenge, we propose Self-Propagative multi-task Learning (SePL) which utilizes comparatively simple 6 CMR factor predictions as prior knowledge to guide predicting more complex CMR factors. The proposed SePL propagates its initial predictions to a latent space, enriching unimodal features in to multimodal representation. A discriminative mixture of experts leverages the relevant prior for 9 CMR factor predictions. The training and testing of SePL use 5,232 sets of fundus images and corresponding CMR factors. Experimental results demonstrate that the proposed SePL outperforms the existing methods up to 10.46% of AUC and 8.07% of MAE across all 15 CMR factor predictions. The code is available at https://github. com/shko0215/SePL.", "filename": "2025_0435.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "South Korea", "authors": ["Seonghyeon Ko", "Huigyu Yang", "Junghyun Bum", "Duc-Tai Le", "Hyunseung Choo"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0436", "x": 4.48, "y": 6.719, "title": "Spatially Gene Expression Prediction Using Dual-Scale Contrastive Learning", "abstract": "Spatial transcriptomics (ST) provides crucial insights into tissue micro-environments, but is limited to its high cost and complexity. As an alternative, predicting gene expression from pathology whole slide images (WSI) is gaining increasing attention. However, existing methods typically rely on single patches or a single pathology modality, neglecting the complex spatial and molecular interactions between target and neighboring information (e.g., gene co-expression). This leads to a failure in establishing connections among adjacent regions and capturing intricate cross-modal relationships. To address these issues, we propose NH 2 2ST, a framework that integrates spatial context and both pathology and gene modalities for gene expression prediction. Our model comprises a query branch and a neighbor branch to pro cess paired target patch and gene data and their neighboring regions, where cross-attention and contrastive learning are employed to capture intrinsic associations and ensure alignments between pathology and gene expression. Extensive experiments on six datasets demonstrate that our model consistently outperforms existing methods, achieving over 20% in PCC metrics. Codes are available at https://github.com/MCPathology/NH2ST.", "filename": "2025_0436.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Mingcheng Qu", "Yuncong Wu", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0437", "x": 7.093, "y": 4.789, "title": "Surface-Based Multi-axis Longitudinal Disentanglement Using Contrastive Learning for Alzheimer’s Disease", "abstract": "Accurate modeling of disease progression is essential for comprehending the heterogeneous neuropathologies such as Alzheimer's Disease (AD). Traditional neuroimaging analysis often confound disease effects with normal aging, complicating the differential diagnosis. Recent advancements in deep learning have catalyzed the development of disentanglement techniques in Autoencoder networks, aiming to segregate longitudinal changes attributable to aging from those due to diseasespecific alterations within the latent space. However, existing longitudinal disentanglement methods usually model disease as a single axis factor which ignores the complexity and heterogeneity of Alzheimer's Disease. In response to this issue, we propose a novel Surface-based Multiaxis Disentanglement framework. This framework posits multiple disease axes within the latent space, enhancing the model's capacity to encapsulate the multifaceted nature of AD, which includes various disease trajectories. To assign axes to data trajectories without explicit ground truth labels, we implement a longitudinal contrastive loss leveraging selfsupervision, thereby refining the s eparation of disease trajectories. Evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (N=1321), our model demonstrates superior performance in delineating between cognitively normal (CN), mild cognitive impairment (MCI), and AD subjects, classification of stable MCI vs converting MCI and Amyloid status, compared to the single-axis model. This is further substantiated through an ablation study on the contrastive loss, underscoring the utility of our multi-axis approach in capturing the complex progression patterns of AD. The code is available at: https://github.com/jianweizhang17/ MultiAxisDisentanglement.git.", "filename": "2025_0437.pdf", "year": 2025, "institution": "University of Southern California (USC)", "country": "USA", "authors": ["Jianwei Zhang", "Yonggang Shi"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0438", "x": 3.748, "y": 2.674, "title": "Temporal Atlas-Guided Generation of Longitudinal Data via Geometric Latent Embeddings", "abstract": "The spatiotemporal changes of a developing anatomical structure is a dynamic process, and quantifying this process within a population and between populations is a fundamental yet challenging task in medical image analysis. Central to this task is the availability of longitudinal imaging data for 4D statistical shape analysis. Unfortunately, this type of longitudinal data is expensive, time-consuming, and difficult to collect. Practically, the majority of imaging data are 3D cross-sectional data, which are inadequate in describing the dynamic shape changes of anatomical structures. In this paper, we introduce a novel temporal atlas-guided deep learning model for longitudinal data generation. Unlike existing methods that directly generate longitudinal data from input images or sequences, we characterize distinctive geometric shape representations in both cross-sectional and longitudinal latent spaces of diffeomorphisms, while optimizing the quality of both atlas and longitudinal data generation. To the best of our knowledge, this is the first deep learning approach t hat leverages temporal atlas-based representation for longitudinal data generation. The innovative nature of our framework lies in its ability to jointly perform within-age and cross-age shape registration, thus maximizing registration performance while maintaining desirable deformation qualities. Our work's ability to model spatiotemporal dynamics makes it highly versatile and applicable to a wide range of domains, including modeling the normal and abnormal development of anatomical structures for improved clinical diagnosis and treatment planning.", "filename": "2025_0438.pdf", "year": 2025, "institution": "Harvard Medical Scho ol", "country": "USA", "authors": ["Shaoju Wu", "Jian Wang", "Sila Kurugol", "Andy Tsai"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0439", "x": 4.34, "y": 5.478, "title": "Temporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer", "abstract": "Effective therapy decisions require models that predict the individual response to treatment. This is challenging since the progression of disease and response to treatment vary substantially across patients. Here, we propose to learn a representation of the early dynamics of treatment response from imaging data to predict pathological complete response (pCR) in breast cancer patients undergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magnetic resonance imaging (MRI) data of the breast forms trajectories in the latent space, serving as basis for prediction of successful response. The multi-task model represents appearance, fosters temporal continuity and accounts for the comparably high heterogeneity in the non-responder cohort. In experiments on the publicly available ISPY-2 dataset, a linear classifier in the latent trajectory space achieves a balanced accuracy of 0.761 using only pre-treatment data (T0), 0.811 using early response (T0 + T1), and 0.861 using four imaging time points (T0 → T3). The full code can be found here: https://github.com/cirmuw/temporal- representation-learning.", "filename": "2025_0439.pdf", "year": 2025, "institution": "Medical University of Vienna", "country": "Austria", "authors": ["Ivana Janíčková", "Yen Y Tan", "Thomas H Helbich", "Konstantin Miloserdov", "Zsuzsanna Bago-Horvath", "Ulrike Heber", "Georg Langs"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0440", "x": 3.044, "y": 5.265, "title": "ThyroidXL: Advancing Thyroid Nodule Diagnosis with an Expert-Labeled, Pathology-Validated Dataset", "abstract": "Thyroid nodules are among the most prevalent endocrine disorders, with incidence rates increasing in recent years. Ultrasonography remains the primary method for thyroid nodule diagnosis due to its non-invasive nature and cost-effectiveness; however, the process is subjective and skill-intensive. To assist radiologists, Computer-Aided Diagnosis systems (CAD) have been developed to provide a second opinion. Despite these advancements, the absence of publicly available medical datasets has resulted in inconsistent validation methods, deterring comparability across studies. This paper introduces ThyroidXL, an open benchmark dataset for thyroid nodule classification, segmentation, and detection. With over 11,000 images from more than 4,000 patients, the dataset-collected and annotated by expert radiologists at the Vietnam National Hospital of Endocrinology-stands as the largest publicly available resource for thyroid nodule diagnosis in terms of both patient count and image volume. Additionally, we provide multiple deep-learning baseline models on three key tasks, including malignancy classification, thyroid nodule detection, and segmentation. The proposed dataset and benchmark can serve as a foundational resource for advancing CAD system development, fostering reproducible research, and accelerating progress in thyroid nodule diagnosis. Our dataset can be accessed at: https://huggingface.co/datasets/hunglc007/ThyroidXL.", "filename": "2025_0440.pdf", "year": 2025, "institution": "Hanoi University of Science and Technology, H anoi", "country": "Vietnam", "authors": ["Viet Hung Duong", "Huan Vu", "Huong Duong Phan", "Duc Quyen Nguyen", "Duc Hao Pham", "Quang Toan Le", "Ba Sy Nguyen", "Tien Dung Do", "Viet Sang Dinh", "Tien Cuong Nguyen", "Huy Hoang Pham", "Dien Hy Ngo"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0441", "x": 5.872, "y": 4.355, "title": "Uncertainty-Aware Multimodal MRI Fusion for HIV-Associated Asymptomatic Neurocognitive Impairment Prediction", "abstract": "Asymptomatic neurocognitive impairment (ANI) is an early stage of HIV-associated neurocognitive disorder. Recent studies have investigated magnetic resonance imaging (MRI) for ANI analysis, but most of them rely on single modality, neglecting to utilize complementary information derived from multiple MRI modalties. For a few multimodal MRI fusion studies, they usually suffer from \"modality laziness\", where dominant modalities suppress weaker ones due to misalignment and scale disparities, limiting fusion efficacy. To address these issues, we propose Uncertainty-aware Multimodal MRI Fusion (UMMF), a novel framework integrating structural MRI, functional MRI, and diffusion tensor imaging for ANI identification. The UMMF employs modality-specific encoders with an uncertainty-aware alternating unimodal training strategy to reduce modality dominance and enhance feature extraction. Moreover, a random network prediction method is designed to estimate uncertainty weights for each modality, enabling robust uncertainty-aware fusion that prioritizes reliable modalities. Extensive experiments demonstrate UMMF's superior performance over SOTA methods, achieving significant improvements in prediction accuracy. Additionally, our approach can help identify critical brain regions associated with ANI, offering potential clinical biomarkers for its early intervention. Our code is available at https://github.com/IsaacKingCzg/IK_MICCAI25_UMMF.", "filename": "2025_0441.pdf", "year": 2025, "institution": "Nanjing University", "country": "China", "authors": ["Zige Chen", "Haonan Qin", "Wei Wang", "Zhongkai Zhou", "Chen Zhao", "Yuqi Fang", "Caifeng Shan"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0442", "x": 6.684, "y": 4.799, "title": "UniCross: Balanced Multimodal Learning for Alzheimer’s Disease Diagnosis by Uni-modal Separation and Metadata-Guided Cross-Modal Interaction", "abstract": "Early and accurate diagnosis of Alzheimer's disease (AD) is crucial for effective treatment and patient care. In clinical practice, physicians can achieve precise diagnoses through the integration of multimodal image information, and it is desired to develop automated diagnosis approaches based on the multimodal information. However, existing multimodal deep learning methods face a critical paradox: although models excel at leveraging joint features to improve task performance, they often neglect the optimization of independent representation capabilities for uni-modal. This shortcoming, known as Modality Laziness, stems from imbalanced modality contributions within conventional joint training frameworks, where models predominantly rely on dominant modalities and neglect to learn weaker ones. To address this challenge, we propose UniCross, a novel balanced multimodal learning paradigm. Specifically, UniCross employs separate learning pathways with specialized training objectives for each modality to ensure comprehensive uni-modal feature learning. In addition, we design a Metadata Weighted Contrastive Loss (MWCL) to facilitate effective cross-modal information interaction. The MWCL lev erages patient metadata (e.g., age, gender, and years of education) to adaptively calibrate both cross-modal and intra-modal feature distances between individuals. We validated our approach through extensive experiments on the ADNI dataset, using structural MRI and FDG-PET modalities for AD diagnosis and mild cognitive impairment (MCI) conversion prediction tasks. The results demonstrate that UniCross not only achieves state-of-the-art overall performance, but also significantly improves the diagnosis performance when only a single modality is available. Our code is available at https://github.com/Alita-song/UniCross", "filename": "2025_0442.pdf", "year": 2025, "institution": "Beijing Institute of T echnology", "country": "China", "authors": ["Lisong Yin", "Chuyang Ye", "Tiantian Liu", "Jinglong Wu", "Tianyi Yan"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0443", "x": 3.725, "y": 4.093, "title": "VAMPIRE: Uncovering Vessel Directional and Morphological Information from OCTA Images for Cardiovascular Disease Risk Factor Prediction", "abstract": "Cardiovascular disease (CVD) remains the leading cause of death worldwide, requiring urgent development of effective risk assessment methods for timely intervention. While current research has introduced non-invasive and efficient approaches to predict CVD risk from retinal imaging with deep learning models, the commonly used fundus photographs and Optical Coherence Tomography (OCT) fail to capture detailed vascular features critical for CVD assessment compared with OCT angiography (OCTA) images. Moreover, existing methods typically classify CVD risk only as high or low, without providing a deeper analysis on CVD-related blood factor conditions, thus limiting prediction accuracy and clinical utility. As a result, we propose a novel multipurpose paradigm of CVD risk assessment that jointly performs CVD risk and CVD-related condition prediction, aligning with clinical experiences. Based on this core idea, we introduce OCTA-CVD, the first OCTA dataset for CVD risk assessment, and a Vessel-Aware Mamba-based Prediction model with Informative Enhancement (VAMPIRE) based on OCTA enface images. Our proposed model aims to extract crucial vascular characteristics through two key components: (1) a Mamba-Based Directional (MBD) Module that captures fine-grained vascular trajectory features and (2) an Information-Enhanced Morphological (IEM) Module that incorporates comprehensive vessel morphology knowledge. Experimental results demonstrate that our method can surpass standard classification backbones, OCTA-based detection methods, and ophthalmologic foundation models. Our codes and the collected OCTA-CVD dataset are available at https://github.com/xmed-lab/VAMPIRE.", "filename": "2025_0443.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Lehan Wang", "Hualiang Wang", "Chubin Ou", "Lushi Chen", "Yunyi Liang", "Xiaomeng Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0444", "x": 4.168, "y": 5.266, "title": "VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction", "abstract": "Breast cancer remains a leading cause of mortality worldwide and is typically detected via screening programs where healthy people are invited in regular intervals. Automated risk prediction approaches have the potential to improve this process by facilitating dynamically screening of high-risk groups. While most models focus solely on the most recent screening, there is growing interest in exploiting temporal information to capture evolving trends in breast tissue, as inspired by clinical practice. Early methods typically relied on two time steps, and although recent efforts have extended this to multiple time steps using Transformer architectures, challenges remain in fully harnessing the rich temporal dynamics inherent in longitudinal imaging data. In this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. To further enhance our approach, we incorporate an a symmetry module that utilizes a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant bilateral differences. This integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies. Our code is available at this URL.", "filename": "2025_0444.pdf", "year": 2025, "institution": "University of Bologna", "country": "Italy", "authors": ["Zijun Sun", "Solveig Thrun", "Michael Kampffmeyer"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0445", "x": 6.326, "y": 3.549, "title": "VT-SNN: Variable Time-Step Spiking Neural Network Based on Uncertainty Measure and Its Application in Brain Disease Diagnosis", "abstract": "The integration of neural networks with Magnetic Resonance Imaging (MRI) data for brain disease diagnosis has become a significant research focus. However, the inherent complexity of 3D MRI data poses challenges for traditional models like CNNs and Transformers, leading to high computational costs and difficulties in clinical deployment. Spiking Neural Networks (SNNs), inspired by biological neurons, offer a promising alternative with enhanced efficiency and robustness. Yet, their application to MRI data is limited by fixed time-steps that fail to account for inter-sample variability. To address this, we propose a Variable Time- Step Spiking Neural Network (VT-SNN) that dynamically adjusts the time-step based on sample-specific uncertainty.Our method e mploys an SNN-based Transformer module to convert MRI data into spike form and extract features, followed by a variable time-step module that measures decision uncertainty using Fisher information and PAC-Bayes theory. Experiments on AHNU and AMRD datasets demonstrate superior classification performance and reduced computational costs. Our codes are available at https://github.com/UAIBC-Brain/MICCAI-2025- Paper-VT-SNN.", "filename": "2025_0445.pdf", "year": 2025, "institution": "Nantong University", "country": "China", "authors": ["Haonan Rao", "Shaolong Wei", "Shu Jiang", "Mingliang Wang", "Weiping Ding", "Jiashuang Huang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0446", "x": 1.184, "y": 2.064, "title": "3D Acetabular Surface Reconstruction from 2D Pre-operative X-Ray Images Using SRVF Elastic Registration and Deformation Graph", "abstract": "Accurate and reliable selection of the appropriate acetabular cup size is crucial for restoring joint biomechanics in total hip arthroplasty (THA). This paper proposes a novel framework integrating squareroot velocity function (SRVF)-based elastic shape registration technique with an embedded deformation (ED) graph approach to reconstruct the 3D articular surface of the acetabulum by fusing multiple views of 2D pre-operative pelvic X-ray images and a hemispherical surface model. The SRVF-based elastic registration establishes 2D-3D correspondences between the parametric hemispherical model and X-ray images, and the ED framework incorporates the SRVF-derived correspondences as constraints to optimize the 3D acetabular surface reconstruction using nonlinear least-squares optimization. Validations using both simulation and real patient datasets are performed to demonstrate the robustness and the potential clinical value of the proposed algorithm. The reconstruction result can assist surgeons in selecting the correct acetabular cup o n the first attempt in THA, minimising the need for revision surgery. Code and data are available at: https://github.com/zsustc/3D-ASR.", "filename": "2025_0446.pdf", "year": 2025, "institution": "UCL Hawk es Institute", "country": "UK", "authors": ["Shuai Zhang", "Jinliang Wang", "Xu Wang", "Sujith Konan", "Danail Stoyanov", "Evangelos B Mazomenos"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0447", "x": 2.467, "y": 3.492, "title": "A Boundary-Aware Cold-Diffusion Model for Electron Microscopy Segmentation", "abstract": "The advancement of electron microscopy (EM) imaging technology has expanded its applications in life science research, making the automation of EM image analysis a key focus in biomedical imaging. As a core task in EM image analysis, semantic segmentation has garnered significant attention, and convolutional neural networks (CNNs) have been extensively studied, currently emerging as the mainstream method. However, existing methods still face several unresolved challenges. One issue arises from the convolution process, which makes it difficult to efficiently balance global and local information, thus limiting further improvements in segmentation accuracy. Another issue stems from the nature of CNNs, which aim to establish an optimal mapping between images and labels, achieving high accuracy in in-domain data segmentation but at the cost of a noticeable performance drop on outof-domain data. In this paper, we explore the potential of diffusion probabilistic models (DPMs), known for their exceptional image modeling capabilities, to address t hese challenges. Specifically, we introduce a diffusion probabilistic model for the semantic segmentation of EM images, which we call EM-Cold-SegDiffusion (ECSD). We adopt a cold or deterministic diffusion framework to achieve higher inference efficiency and a more deterministic segmentation process. Additionally, by introducing an edge-sensitive loss function, we significantly enhance both training efficiency and model performance. Experimental results on common EM segmentation tasks demonstrate that ECSD outperforms mainstream models, offering a promising and superior solution for EM segmentation.", "filename": "2025_0447.pdf", "year": 2025, "institution": "Peking Univ ersity", "country": "China", "authors": ["Muge Qi", "Ruohua Shi", "Yu Cai", "Liuyuan He", "Wenyao Wang", "Lei Ma"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0448", "x": 1.024, "y": 4.701, "title": "A Virtual Domain Collaborative Learning Framework for Semi-supervised Microscopic Hyperspectral Image Segmentation", "abstract": "Microscopic hyperspectral image segmentation faces dual challenges of limited labeled data and insufficient utilization of unlabeled data. However, existing semi-supervised methods often isolate the training processes for labeled and unlabeled data, neglecting their potential synergistic effects. To address this, we propose a semi-supervised method based on Virtual Domain Collaborative Learning (VDCL) to enhance the collaborative learning ability between labeled and unlabeled data and improve the quality of pseudo-labels. Specifically, by combining unlabeled background with labeled foreground and labeled background with unlabeled foreground to construct virtual domain data pairs, we established a collaborative learning bridge between labeled and unlabeled samples. Furthermore, we establish a repository of optimal models and employ an alternating co-training strategy. The current and historically optimal models jointly guide training, and this dynamic framework significantly improves pseudo-labels quality. We have verified the novel semi-supervised segmentation method on the widely-used public microscopic hyperspectral choledoch dataset from Kaggle and the oral squamous cell carcinoma dataset. On these datasets, our method has achieved the state-of-the-art performance. The code is available at https://github. com/Qugeryolo/Virual-Domain.", "filename": "2025_0448.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Geng Qin", "Huan Liu", "Wei Li", "Haihao Zhang", "Yuxing Guo"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0449", "x": 2.122, "y": 4.961, "title": "ADAptation: Reconstruction-Based Unsupervised Active Learning for Breast Ultrasound Diagnosis", "abstract": "Deep learning-based diagnostic models often suffer performance drops due to distribution shifts between training (source) and test (target) domains. Collecting and labeling sufficient target domain data for model retraining represents an optimal solution, yet is limited by time and scarce resources. Active learning (AL) offers an efficient approach to reduce annotation costs while maintaining performance, but struggles to handle the challenge posed by distribution variations across different datasets. In this study, we propose a novel unsupervised Active learning framework for Domain Adaptation, named ADAptation, which efficiently selects informative samples from multi-domain data pools under limited annotation budget. As a fundamental step, our method first utilizes the distribution homogenization capabilities of diffusion models to bridge cross-dataset gaps by translating target images into sourcedomain style. We then introduce two ke y innovations: (a) a hypersphereconstrained contrastive learning network for compact feature clustering, and (b) a dual-scoring mechanism that quantifies and balances sample uncertainty and representativeness. Extensive experiments on four breast ultrasound datasets (three public and one in-house/multi-center) across five common deep classifiers demonstrate that our method surpasses existing strong AL-based competitors, validating its effectiveness and generalization for clinical domain adaptation. The code is available at the anonymized link: https://github.com/miccai25-966/ADAptation.", "filename": "2025_0449.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Yaofei Duan", "Yuhao Huang", "Xin Yang", "Luyi Han", "Xinyu Xie", "Zhiyuan Zhu", "Ping He", "Ka-Hou Chan", "Ligang Cui", "Sio-Kei Im", "Dong Ni", "Tao Tan"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0450", "x": 0.939, "y": 1.128, "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View Co-training", "abstract": "Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing t he detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at https://github.com/ShanghaiTech- IMPACT/DVCTNet.", "filename": "2025_0450.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Tao Luo", "Han Wu", "Tong Yang", "Dinggang Shen", "Zhiming Cui"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0451", "x": 0.966, "y": 4.369, "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-supervised Medical Image Segmentation", "abstract": "Vision Transformer (ViT) has recently gained tremendous popularity in medical image segmentation task due to its superior capability in capturing long-range dependencies. However, transformer requires a large amount of labeled data to be effective, which hinders its applicability in annotation scarce semi-supervised learning scenario where only limited labeled data is available. State-of-the-art semisupervised learning methods propose combinatorial CNN-Transformer learning to cross teach a transformer with a convolutional neural network (CNN), which achieves promising results. However, it remains a challenging task to effectively train the transformer with limited labeled data. In this paper, we propose an adversarial masked image modeling (AdvMIM) method to fully unleash the potential of transformer for semi-supervised medical image segmentation. The key challenge in semisupervised learning with transformer lies in the lack of sufficient supervision signal. To this end, we propose to construct an auxiliary masked domain from original domain with masked image modeling and train the transformer to predict the entire segmentation mask with masked inputs to increase supervision signal. We leverage the original labels from labeled data and pseudo-labels from unlab eled data to learn the masked domain. To further benefit the original domain from masked domain, we provide a theoretical analysis of our method from a multi-domain learning perspective and devise a novel adversarial training loss to reduce the domain gap between the original and masked domain, which boosts semisupervised learning performance. We also extend adversarial masked image modeling to CNN network. Extensive experiments on three public medical image segmentation datasets demonstrate the effectiveness of our method, where our method outperforms existing methods significantly. Our code is publicly available at https://github.com/zlheui/AdvMIM.", "filename": "2025_0451.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Lei Zhu", "Jun Zhou", "Rick Siow Mong Goh", "Yong Liu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0452", "x": 2.777, "y": 2.03, "title": "All-in-One Medical Image Restoration with Latent Diffusion-Enhanced Vector-Quantized Codebook Prior", "abstract": "All-in-one medical image restoration (MedIR) aims to address multiple MedIR tasks using a unified model, concurrently recovering various high-quality (HQ) medical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts. However, all-in-one MedIR presents significant challenges due to the heterogeneity across different tasks. Each task involves distinct degradations, leading to diverse information losses in LQ images. Existing methods struggle to handle these diverse information losses associated with different tasks. To address these challenges, we propose a latent diffusion-enhanced vector-quantized codebook prior and develop DiffCode, a novel framework leveraging this prior for all-in-one MedIR. Specifically, to compensate for diverse information losses associated with different tasks, DiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ prior features across tasks, capturing a comprehensive prior. Furthermore, to enhance prior retrieval from the codebook bank, DiffCode introduces a latent diffusion strategy that utilizes the diffusion model's powerful mapping capabilities to iteratively refine the latent feature distribution, estimating more accurate HQ prior features during restoration. With the help of the task-adaptive codebook bank and latent diffusion strategy, DiffCode achieves superior performance in both quantitative metrics and visual quality across three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis.", "filename": "2025_0452.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Haowei Chen", "Zhiwen Yang", "Haotian Hou", "Hui Zhang", "Bingzheng Wei", "Gang Zhou", "Yan Xu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0453", "x": 2.872, "y": 3.463, "title": "AVDM: Controllable Adversarial Diffusion Model for Vessel-to-Volume Synthesis", "abstract": "3D blood vessel segmentation remains a critical yet challenging task in medical image analysis. The heterogeneity of clinical imaging protocols introduces substantial domain gaps, limiting the generalizability of supervised learning methods that rely on manually annotated pixel-level labels for individual datasets. Furthermore, the large labeled volumetric datasets are difficult to collect because of data privacy issues. While diffusion models offer potential solutions by generating shareable synthetic data, existing approaches often exhibit poor alignment between synthesized volumes and their corresponding vascular structure input. To address these limitations, we propose Controllable Adversarial Diffusion Model (AVDM), which integrates adversarial supervision into the diffusion training framework. Unlike conventional methods that generate imperceptible perturbations, AVDM synthesizes adversarial instances emphasizing structural variations critical for volume synthesis. Specifically, we design a segmentation-guided discriminator that enforces both the photorealism of generated volumes and pixellevel consistency with original vessel annotations. This supervision mechanism enables high-resolution synthesis of anatomically plausible vascular structures. Experiments demonstrate that AVDM surpasses state-of-the-art methods in generative fidelity and enhances performance on downstream tasks. Our code is available at https://github.com/jdai22/AVDM.", "filename": "2025_0453.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Jian Dai", "Wanchen Liu", "Honghao Cui", "Xiao Liu", "Jiajun Wang", "Zhiji Zheng", "Daoying Geng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0454", "x": 2.736, "y": 1.275, "title": "Bowsher Prior Enhanced Unsupervised PET Image Denoising", "abstract": "Positron Emission Tomography (PET) is an advanced nuclear medicine imaging technique widely used in the diagnosis and treatment of oncology and neurological diseases. However, PET images suffer from high noise levels due to statistical fluctuations and physical degradation factors during image acquisition. Recently, deep learningbased denoising methods have shown great performance for PET image quality enhancement. Most of these methods attempt to incorporate high-quality anatomical image (such as CT or MR), as network input to provide prior information into the PET denoising process. However, directly using CT or MR image as network input has limited effectiveness and lacks interpretability due to the significant differences between two modalities. Exploring how to make better use of anatomical prior remains a valuable research direction. In this study, we proposed an unsupervised PET image denoising framework that leverages the Bowsher prior to achieving cross-modality fusion and anatomical information extraction. Specifically, we compute the Bowsher prior using the denoised result from the Conditional Deep Image Prior (CDIP) method and the corresponding MR image. The Bowsher prior and MR image are concatenated along the channel dimension and then fed into a designed Spatial Attention Network (SA-Net) to enhance PET image quality. Experiments on both simulation and clinical datasets demonstrated that the proposed framework can effectively utilize Bowsher prior to generating high-quality PET image.", "filename": "2025_0454.pdf", "year": 2025, "institution": "Zhejiang University o f Technology", "country": "China", "authors": ["Zhongxue Wu", "Jiankai Wu", "Jianan Cui", "Yuanjing Feng", "Zan Chen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0455", "x": 0.911, "y": 3.832, "title": "BraTS-UMamba: Adaptive Mamba UNet with Dual-Band Frequency Based Feature Enhancement for Brain Tumor Segmentation", "abstract": "Brain tumor segmentation (BraTS) of 3D Magnetic Resonance Imaging (MRI) aims to facilitate clinical analysis of brain cancer. Existing BraTS segmentation works tend to exploit convolutional neural networks (CNNs) or vision transformers (ViTs), yet CNNs have a restricted receptive field that focuses on local context only and ViTs suffer from high computational overheads due to quadratic complexity. Recently, Mamba has shown superior performance over ViTs in longrange dependency modeling, offering linear computational complexity and lower memory consumption. However, these methods primarily learn feature representation in the spatial domain, overlooking valuable heuristics embedded in the frequency domain. Inspired by this, we propose BraTS-UMamba, a novel Mamba-based U-Net designed to enhance brain tumor segmentation by c apturing and adaptively fusing bi-granularity based long-range dependencies in the spatial domain while integrating both low-and high-band spectrum clues from the frequency domain to refine spatial feature representation. We further enhance segmentation through an auxiliary brain tumor classification loss. Extensive experiments on two public benchmark datasets demonstrate the superiority of our BraTS-UMamba over state-of-the-art methods.", "filename": "2025_0455.pdf", "year": 2025, "institution": "Henan Normal University", "country": "China", "authors": ["Haoran Yao", "Hao Xiong", "Dong Liu", "Hualei Shen", "Shlomo Berkovsky"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0456", "x": 1.114, "y": 4.343, "title": "Compact Training-Free NAS with Alternating Evolution Game for Medical Image Segmentation", "abstract": "Neural Architecture Search (NAS) has shown significant potential in designing deep neural networks for medical image segmentation. However, even emerging training-free NAS frameworks often incur substantial computational costs and lengthy search times. To address the critical challenges of computational efficiency and architecture interpretability, the paper proposes a compact training-free NAS framework based on an Alternating Evolution Game (AEG-cTFNAS). The proposed method alternates the search and contribution evaluation of the encoder and decoder within the UNet architecture via alternating games. It employs a truncated normal distribution for compact encoding, sampling, and updating to minimize computational overhead, while Baye sian inference is utilized to estimate the contribution of each block, adaptively adjusting the search strategy and facilitating process visualization. Experimental results on two benchmark datasets reveal that AEG-cTFNAS outperforms both manually designed architectures and NAS-based algorithms, underscoring its efficacy and potential on medical image segmentation. Code is available at https://github.com/spcity/ AEG-cTFNAS.", "filename": "2025_0456.pdf", "year": 2025, "institution": "Nankai Univ ersity", "country": "China", "authors": ["Xiaoxue Sun", "Hongpeng Wang", "Pei-Cheng Song"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0457", "x": 3.166, "y": 2.538, "title": "Controllable Flow Matching for 3D Contrast-Enhanced Brain MRI Synthesis from Non-contrast Scans", "abstract": "Magnetic resonance imaging (MRI) enhanced by gadolinium-based contrast agents (GBCAs) is crucial in the assessment and management of cancer. However, the use of GBCAs introduces additional costs and raises potential safety concerns, including the risk of gadolinium accumulation in the brain. Several generative learning methods based on GANs and diffusion models have been proposed to generate contrast-enhanced MRI from non-contrast-enhanced MRI. However, GANs face challenges such as gradient vanishing and mode collapse. Diffusion models also face several challenges, such as generation instability and long sampling times. In this paper, we propose a controllable flow matching (CFM) model for efficient synthesis of 3D contrast-enhanced brain MRI with fine-grained details of targets of interest. CFM adopts a straight-line generation path, enabling image generation in a single step. We design a multi-stage training strategy integrating controllable constraints to ensure that single-step sampling can generate contrast-enhanced MRI meeting specific controllable conditions. Our CFM model has been evaluated on both the BraTS2023 and an in-house dataset. Experimental results demonstrate that CFM achieves state-of-the-art image generation and tumor delineation performance with promising generalizability. Our code is available at [https://github.com/ladderlab-xjtu/CFM](https://github.com/ladderlab-xjtu/CFM).", "filename": "2025_0457.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Heng Chang", "Yu Shang", "Haifeng Wang", "Yuxia Liang", "Haoyu Wang", "Fan Wang", "Chen Niu", "Chunfeng Lian"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0458", "x": 2.735, "y": 4.971, "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "abstract": "Skin images from real-world clinical practice are often limited, resulting in a shortage of training data for deep-learning models. While many studies have explored skin image synthesis, existing methods often generate low-quality images and lack control over the lesion's location and type. To address these limitations, we present LF-VAR, a model leveraging quantified lesion measurement scores and lesion type labels to guide the clinically relevant and controllable synthesis of skin images. It enables controlled skin synthesis with specific lesion characteristics based on language prompts. We train a multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to encode images into discrete latent representations for structured tokenization. Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized representations facilitates image synthesis. Lesion measurement from the lesion region and t ypes as conditional embeddings are integrated to enhance synthesis fidelity. Our method achieves the best overall FID score (average 0.74) among seven lesion types, improving upon the previous state-of-the-art (SOTA) by 6.3%. The study highlights our controllable skin synthesis model's effectiveness in generating high-fidelity, clinically relevant synthetic skin images. Our framework code is available at https://github.com/echosun1996/LF-VAR.", "filename": "2025_0458.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J Ong", "Lei Zhang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0459", "x": 2.372, "y": 2.167, "title": "Cross-View Generalized Diffusion Model for Sparse-View CT Reconstruction", "abstract": "Sparse-view computed tomography (CT) reduces radiation exposure by subsampling projection views, but conventional reconstruction methods produce severe streak artifacts with undersampled data. While deep-learning-based methods enable single-step artifact suppression, they often produce over-smoothed results under significant sparsity. Though diffusion models improve reconstruction via iterative refinement and generative priors, they require hundreds of sampling steps and struggle with stability in highly sparse regimes. To tackle these concerns, we present the Cross-view Generalized Diffusion Model (CvG-Diff), which reformulates sparse-view CT reconstruction as a generalized diffusion process. Unlike existing diffusion approaches that rely on stochastic Gaussian degradation, CvG-Diff explicitly models imagedomain artifacts caused by angular subsampling as a deterministic degradation operator, leveraging correlations across sparse-view CT at different sample rates. To address the inherent artifact propagation and inefficiency of sequential sampling in generalized diffusion model, we introduce two innovations: Error-Propagating Composite Training (EPCT), which facilitates identifying error-prone regions and suppresses propagated artifacts, and Semantic-Prioritized Dual-Phase Sampling (SPDPS), an adaptive strategy that prioritizes semantic correctness before detail refinement. Together, these innovations enable CvG-Diff to achieve highquality reconstructions with minimal iterations, achieving 38.34 dB PSNR and 0.9518 SSIM for 18-view CT using only 10 steps on AAPM-LDCT dataset. Extensive experiments demonstrate the superiority of CvG-Diff over state-of-the-art sparse-view CT reconstruction methods. The code is available at https://github.com/xmed-lab/CvG-Diff.", "filename": "2025_0459.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Jixiang Chen", "Yiqun Lin", "Yi Qin", "Hualiang Wang", "Xiaomeng Li"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0460", "x": 3.169, "y": 2.405, "title": "D$$^3$$M: Deformation-Driven Diffusion Model for Synthesis of Contrast-Enhanced MRI with Brain Tumors", "abstract": "Contrast-enhanced magnetic resonance images (CEMRIs) provide valuable information for brain tumor diagnosis and treatment planning. However, CEMRI acquisition requires contrast agent injection, which poses problems such as health risks, high costs, and environmental concerns. To address these drawbacks, researchers have synthesized CEMRIs from non-contrast magnetic resonance images (NCM-RIs) to remove the need for contrast agents. However, CEMRI synthesis from NCMRIs is highly ill-posed, where false positive and false negative enhancement can be produced, especially for brain tumors. In this study, we propose a deformation-driven diffusion model (D 3 M) for CEMRI synthesis with brain tumors from NCMRIs. Instead of modeling enhancement errors as intensity errors, we formulate them as incorrect interpretation of tumor subcomponents, where enhanced tumors are misinterpreted as non-enhanced tumors and vice versa. In this way, the enhancement can be geometrically corrected with spatial deformation. This reduces the difficulty of CEMRI synthesis, as the intensity error is usually large to correct whereas the geometry correction is relatively small. Specifically, we first introduce a multi-step spatial deformation module (MSSDM) in D 3 M. MSSDM performs image deformation to adjust the enhancement, displacing enhanced regions to remove false positive and false negative enhancement. Moreover, as the denoising process of diffusion models is stepwise, MSSDM is applied at these multiple diffusion steps. Second, to further guide the spatial deformation, we incorporate an auxiliary task of segmenting the enhanced tumor, which aids the model understanding of contrast enhancement. Accordingly, we introduce a dualstream image-mask decoder (DSIMD) that jointly produces intermediate enhanced images and masks of enhanced tumors. Results on two public datasets demonstrate that D 3 M outperforms existing methods in CEMRI synthesis.", "filename": "2025_0460.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Haowen Pang", "Peng Zhang", "Xiaoming Hong", "Shannan Chen", "Chuyang Ye"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0461", "x": 2.586, "y": 3.658, "title": "DiffAtlas: GenAI-Fying Atlas Segmentation via Image-Mask Diffusion", "abstract": "Accurate medical image segmentation is crucial for precise anatomical delineation. Deep learning models like U-Net have shown great success but depend heavily on large datasets and struggle with domain shifts, complex structures, and limited training samples. Recent studies have explored diffusion models for segmentation by iteratively refining masks. However, these methods still retain the conventional image-to-mask mapping, making them highly sensitive to input data, which hampers stability and generalization. In contrast, we introduce DiffAtlas, a novel generative framework that models both images and masks through diffusion during training, effectively \"GenAI-fying\" atlasbased segmentation. During testing, the model is guided to generate a specific target image-mask pair, from which the corresponding mask is obtained. DiffAtlas retains the robustness of the atlas paradigm while overcoming its scalability and domain-specific limitations. Extensive experiments on CT and MRI across same-domain, cross-modality, varying-domain, and different data-scale settings using the MMWHS and TotalSegmentator datasets demonstrate that our approach outperforms existing methods, particularly in limited-data and zero-shot modality segmentation. Code is available at https://github.com/M3DV/DiffAtlas.", "filename": "2025_0461.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "Switzerland", "authors": ["Hantao Zhang", "Yuhe Liu", "Jiancheng Yang", "Weidong Guo", "Xinyuan Wang", "Pascal Fua"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0462", "x": 3.2, "y": 2.385, "title": "Diffusion-Based Multi-modal MR Fusion for TOF-MRA Image Synthesis", "abstract": "Time-of-flight magnetic resonance angiography (TOF-MRA) is widely recognized as the gold standard for non-invasive assessment of cerebrovascular lesions. However, its long scanning times and susceptibility to motion artifacts often result in image blurring and loss of diagnostic information. To address these limitations, the synthesis of TOF-MRA images from multi-modal MR images has emerged as an effective solution. In this paper, we propose a novel Multi-Modal Diffusion Model (MMDM) for TOF-MRA image synthesis, which fully leverages complementary anatomical and pathological information from multi-modal MR images to enhance synthesis performance. Specifically, we introduce modality-specific diffusion modules, each of which independently models the deterministic mapping from a source domain to the target domain, preserving modality-specific prior knowledge. Then, we propose a cross-modal dynamic fusion module to integrate multi-path diffusion features. Additionally, we present a Maximum Intensity Projection (MIP) loss, which constrains the consistency of adjacent slices in the maximum intensity projection space, addressing the issue of vascular discontinuities caused by 2D training. Finally, we propose a Noiseadaptive Weighting Strategy (NAWS) that dynamically balances the multiobjective loss weights based on the data distribution of the diffusion model, ensuring stable convergence during training. Experimental results demonstrate that our method significantly outperforms existing approaches on both the original images and MIP images. Our code is available at https:// github.com/taozh2017/MMDM-Syn.", "filename": "2025_0462.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Tianen Yu", "Xinyu Song", "Lei Xiang", "Tao Zhou"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0463", "x": 0.967, "y": 1.158, "title": "DuoDent: Tooth Generation Using Dual-Stream Diffusion with Normal Consistency", "abstract": "Generating high-precision 3D dental data is crucial for clinical practice, virtual simulation, and education. However, it is challenging to synthesize smooth and detailed tooth models. In this work, we introduce DuoDent, a dual-stream diffusion-based framework for the synthesis of accurate 3D tooth point clouds followed by a refined mesh generation. Our framework combines Transformer-based diffusion and CNN-based diffusion to capture both global dental structures and fine local features, thereby enhancing surface detail while reducing artifacts such as staircase and rough textures. The generated point clouds are optimized using normal consistency constraints for proper alignment of surface normals, which is key to high-quality mesh reconstruction. In addition, we apply a normal estimation with orientation consistency to the generated p oint clouds prior to converting them to output meshes, which enables the generation of smoother and anatomically precise tooth models. Extensive experiments validate that our method not only outperforms existing approaches in quantitative metrics but also delivers superior qualitative results, demonstrating its potential to significantly improve tooth modeling in dentistry. Our code is available at https://github.com/kdy-ku/ DuoDent.", "filename": "2025_0463.pdf", "year": 2025, "institution": "Korea University", "country": "South Korea", "authors": ["Doeyoung Kwon", "Seongjun Kim", "In-Seok Song", "Seung Jun Baek"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0464", "x": 0.821, "y": 4.621, "title": "EFFDNet: A Scribble-Supervised Medical Image Segmentation Method with Enhanced Foreground Feature Discrimination", "abstract": "Medical image segmentation, a critical task in medical image analysis, plays a key role in assisting clinical diagnostic workflows. However, traditional fully supervised learning methods for segmentation require large, high-quality annotations from expert physicians, which is resource-intensive and time-consuming. To mitigate this, scribble supervised segmentation approaches use simplified annotations to reduce annotation costs. Nevertheless, the simplistic nature of scribble annotations limits the model's ability to accurately distinguish foreground anatomical structures from the background and differentiate between various anatomical classes. This limitation results in low accuracy in capturing foreground morphology and hinders the model's generalization ability. To address this, we propose an Enhanced Foreground Feature Discrimination Network (EFFDNet) that better leverages semantic information in scribble annotations to improve the network's foreground discrimination ability. EFFDNet introduces an innovative Foreground-Background Separation Loss (FBSL), enhancing the model's ability to distinguish between foreground and bac kground features, and improving the morphological accuracy of foreground anatomical region recognition. Additionally, we propose a new Foreground Augmentation with Diverse Context (FADC) strategy to further enhance the network's attention on the foreground and increase training sample diversity, mitigating overfitting and improving generalization. We validate our approach through systematic experiments on two publicly available datasets, demonstrating significant improvements over existing methods. The code is available at: https://github.com/Aurora-003-web/EFFDNet.", "filename": "2025_0464.pdf", "year": 2025, "institution": "Nanyang Technological University", "country": "Singapore", "authors": ["Jinhua Liu", "Shu Yun Tan", "Xulei Yang", "Yanwu Xu", "Si Yong Yeo"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0465", "x": 3.142, "y": 1.958, "title": "FilterDiff: Noise-Free Frequency-Domain Diffusion Models for Accelerated MRI Reconstruction", "abstract": "Accelerated MRI reconstruction has garnered increasing attention due to its significant clinical value. Recently, the exceptional capabilities of diffusion models in image generation have led to their widespread application in accelerated MRI reconstruction. However, the inherent noisy diffusion process in these models introduces uncertainty during the reverse diffusion restoration, which can compromise the consistency of the results. Moreover, adding Gaussian noise contradicts the actual MRI imaging process. To address these issues, we propose Filter-Diff, a noise-free frequency-domain diffusion framework. In FilterDiff, the diffusion process is modeled as a filtering operation, similar to the MRI acquisition process, thereby eliminating the dependence on noise and simplifying the diffusion procedure. To better capture frequency-domain long-range information, we proposed a Swin-DiTs network, which modifies the DiT transformer network by replacing the self-attention mechanism with Swin-attention to reduce computational cost, and removing the position embedding to mitigate feature artifacts. Extensive experiments on two public datasets demonstrate that our model achieves stateof-the-art performance in accelerated MRI reconstruction, both in indistribution and out-of-distribution scenarios.", "filename": "2025_0465.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Tao Song", "Fang Nie", "Yi Guo", "Feng Xu", "Shaoting Zhang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0466", "x": 3.1, "y": 2.766, "title": "Flow Matching for Medical Image Synthesis: Bridging the Gap Between Speed and Quality", "abstract": "Deep learning models have emerged as a powerful tool for various medical applications. However, their success depends on large, high-quality datasets that are challenging to obtain due to privacy concerns and costly annotation. Generative models, such as diffusion models, offer a potential solution by synthesizing medical images, but their practical adoption is hindered by long inference times. In this paper, we propose the use of an optimal transport flow matching approach to accelerate image generation. By introducing a straighter mapping between the source and target distribution, our method significantly reduces inference time while preserving and further enhancing the quality of the outputs. Furthermore, this approach is highly a daptable, supporting various medical imaging modalities, conditioning mechanisms (such as class labels and masks), and different spatial dimensions, including 2D and 3D. Beyond image generation, it can also be applied to related tasks such as image enhancement. Our results demonstrate the efficiency and versatility of this framework, making it a promising advancement for medical imaging applications. Code is available on: https://github.com/milad1378yz/ MOTFM.", "filename": "2025_0466.pdf", "year": 2025, "institution": "University of British Columbia (UBC)", "country": "Canada", "authors": ["Milad Yazdani", "Yasamin Medghalchi", "Pooria Ashrafian", "Ilker Hacihaliloglu", "Dena Shahriari"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0467", "x": 3.097, "y": 2.308, "title": "FMM-Diff: A Feature Mapping and Merging Diffusion Model for MRI Generation with Missing Modality", "abstract": "Brain disease diagnosis and treatment planning rely on complementary information from multiple MRI modalities. Compared to routine modalities (RM) such as T1, T2, and FLAIR, modalities like DWI and T1ce provide unique diagnostic information but are less commonly used due to longer scan times, higher costs, or the need for contrast agents. To mitigate this, multi-modal MRI synthesis methods are proposed to generate advanced MRIs from routine MRIs. However, in clinical practice, missing modality is a known issue in MRI generation which degrades the synthesis quality. Existing methods typically use shared encoders and masking strategies to compensate for missing modality. However, as the number of missing modalities increases, it becomes harder to capture the inter-modal correlations, causing a sharp performance drop. To address this, we propose the Feature Mapping and Merging Diffusion Model (FMM-Diff). Instead of using a shared encoder, we introduce dedicated mapping encoders for each modality. When a modality is missing, its latent representation is inferred from the available ones via its dedicated encoder. This ensures complete latent representations, allowing the Merge Module to selectively extract and fuse inter-modal correlations, significantly improving synthesis performance. Evaluated on two public MRI datasets, including CGGA and BraTS2021, FMM-Diff not only outperforms the state-of-the-art models by 4.35% in terms of Structural Similarity Index Measure (SSIM) while demonstrating exceptional stability, with less than a 1.0% SSIM drop, which is significantly lower than the 2.0-3.45% drop observed with other methods, across various missing modality scenarios. The source code will be available at: https://github. com/ZJohnWenjin/FMMDIFF.git.", "filename": "2025_0467.pdf", "year": 2025, "institution": "Macquarie University", "country": "Australia", "authors": ["Wenjin Zhong", "Cong Cong", "Zihan Wang", "Zeya Yan", "Antonio Di Ieva", "Sidong Liu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0468", "x": 2.546, "y": 3.69, "title": "GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation", "abstract": "Accurate segmentation of tubular structures, such as vascular networks, plays a critical role in various medical domains. A remaining significant challenge in this task is structural fragmentation, which can adversely impact downstream applications. Existing methods primarily focus on designing various loss functions to constrain global topological structures. However, they often overlook local discontinuity regions, leading to suboptimal segmentation results. To overcome this limitation, we propose a novel Global-to-Local Connectivity Preservation (GLCP) framework that can simultaneously perceive global and local structural characteristics of tubular networks. Specifically, we propose an Interactive Multi-head Segmentation (IMS) module to jointly learn global segmentation, skeleton maps, and local discontinuity maps, respectively. This enables our model to explicitly target local discontinuity regions while maintaining global topological int egrity. In addition, we design a lightweight Dual-Attention-based Refinement (DAR) module to further improve segmentation quality by refining the resulting segmentation maps. Extensive experiments on both 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy and continuity in tubular structure segmentation compared to several state-of-the-art approaches. The source codes will be available at https://github.com/FeixiangZhou/GLCP.", "filename": "2025_0468.pdf", "year": 2025, "institution": "University of Liverpo ol", "country": "UK", "authors": ["Feixiang Zhou", "Zhuangzhi Gao", "He Zhao", "Jianyang Xie", "Yanda Meng", "Yitian Zhao", "Gregory Y H Lip", "Yalin Zheng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0469", "x": 3.284, "y": 1.812, "title": "Guiding Quantitative MRI Reconstruction with Phase-Wise Uncertainty", "abstract": "Quantitative magnetic resonance imaging (qMRI) requires multiphase acquisition, often relying on reduced data sampling and reconstruction algorithms to accelerate scans, which inherently poses an ill-posed inverse problem. While many studies focus on measuring uncertainty during this process, few explore how to leverage it to enhance reconstruction performance. In this paper, we introduce PUQ, a novel approach that pioneers the use of uncertainty information for qMRI reconstruction. PUQ employs a two-stage reconstruction and parameter fitting framework, where phase-wise uncertainty is estimated during reconstruction and utilized in the fitting stage. This design allows uncertainty to reflect the reliability of different phases and guide information integration during parameter fitting. We evaluated PUQ on in vivo T1 and T2 mapping datasets from healthy subjects. Compared to existing qMRI reconstruction methods, PUQ achieved the state-of-the-art performance in parameter mappings, demonstrating the effectiveness of uncertainty guidance. Our code is available at https://github. com/Haozhoong/PUQ.", "filename": "2025_0469.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Haozhong Sun", "Zhongsen Li", "Chenlin Du", "Haokun Li", "Yajie Wang", "Huijun Chen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0470", "x": 2.738, "y": 2.774, "title": "High-Fidelity Unified One-to-Many Medical Image Synthesis via Text-Conditioned Latent Diffusion", "abstract": "Current deep learning approaches for medical image synthesis require training multiple specialized models for different modality conversions, leading to inefficient parameter utilization. In this work, we propose a unified text-conditioned latent diffusion framework that achieves one-to-many medical image synthesis through two key innovations: (1) With text-guided dynamic gating, a shared latent space construction using pre-trained modality-specific encoders is proposed, reducing model parameters compared to training several separate models. (2) An adaptive hybrid frequency processor combining wavelet decomposition and Fourier analysis is designed to preserve both local textures and global anatomical structures. Our comprehensive experimental evaluation in various datasets validates that this framework is capable of transforming a single medical imaging mo dality into multiple target modalities using only one model, surpassing existing methods based on Generative Adversarial Networks and diffusion models in terms of generation quality. The success of this work establishes a new paradigm for efficient multi-modal medical image synthesis through latent space unification and frequencyaware diffusion, significantly advancing the practicality of virtual medical image generation systems.", "filename": "2025_0470.pdf", "year": 2025, "institution": "JancsiLab", "country": "China", "authors": ["Youjian Zhang", "Jian Huang", "Jie Wang", "Zezhou Li", "Zhongya Wang", "Guanqun Zhou", "Zhicheng Zhang", "Gang Yu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0471", "x": 1.337, "y": 4.28, "title": "Improving Medical Image Segmentation with Implicit Representation and Noisy Label Robustness", "abstract": "Medical image segmentation plays a vital role in healthcare by identifying and delineating specific structures, such as organs, tumors, or lesions, from medical images. While deep learning has significantly advanced this field, existing methods face two major challenges. First, they rely on pixel-wise discrete representations, which lead to difficulties in scaling to different input sizes and create ambiguity in fine boundary delineation. Second, the presence of noisy labels in medical datasets hinders model accuracy. To address these challenges, we propose a novel approach that leverages continuous representations and incorporates three key components: the Hierarchical Channel-Attention Encoder (HCAE), the Three-Stage Implicit Decoder with Noise-Based Index Selector (NBIS), and the High-Frequency Noise Modulator (HFNM). HCAE enhances feature extraction by capturing both fine and c oarse details through hierarchical attention mechanisms. NBIS refines segmentation by identifying stable and unstable feature indices, improving performance in challenging regions. Meanwhile, HFNM selectively introduces noise to high-frequency components, helping the model mitigate the effects of label noise. This comprehensive solution demonstrates improved segmentation accuracy, particularly in the presence of noisy labels, making it a promising approach for medical image analysis.", "filename": "2025_0471.pdf", "year": 2025, "institution": "Indian Institute of Technology", "country": "India", "authors": ["Suruchi Kumari", "Harshdeep Singh", "Pravendra Singh"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0472", "x": 1.567, "y": 5.872, "title": "InstructX2X: An Interpretable Local Editing Model for Counterfactual Medical Image Generation", "abstract": "Counterfactual medical image generation have emerged as a critical tool for enhancing AI-driven systems in medical domain by answering \"what-if\" questions. However, existing approaches face two fundamental limitations: First, they fail to prevent unintended modifications, resulting collateral changes in demographic attributes when only disease features should be affected. Second, they lack interpretability in their editing process, which significantly limits their utility in realworld medical applications. To address these limitations, we present InstructX2X, a novel interpretable local editing model for counterfactual medical image generation featuring Region-Specific Editing. This approach restricts modifications to specific regions, effectively preventing unintended changes while simultaneously providing a Guidance Map that offers inherently interpretable visual explanations of the editing process. Additionally, we i ntroduce MIMIC-EDIT-INSTRUCTION, a dataset for counterfactual medical image generation derived from expert-verified medical VQA pairs. Through extensive experiments, InstructX2X achieve state-of-the-art performance across all major evaluation metrics. Our model successfully generates high-quality counterfactual chest X-ray i mages along with interpretable explanations, as validated by experienced radiologists. Our code and dataset are publicly available at https://github.com/hgminn/InstructX2X.", "filename": "2025_0472.pdf", "year": 2025, "institution": "Seoul National University", "country": "South K orea", "authors": ["Hyungi Min", "Taeseung You", "Hangyeul Lee", "Yeongjae Cho", "Sungzoon Cho"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0473", "x": 1.331, "y": 4.639, "title": "Investigating Voxel-Level Brain Age Prediction as a Pretext Task for Brain MRI Segmentation", "abstract": "To address the challenge of few annotated datasets for training brain magnetic resonance imaging (MRI) segmentation models, we propose to use voxel-level brain age prediction as a domain-specific pretext task for self-supervised learning before adapting models to a segmentation downstream task. We combined publicly available T1-weighted, normative brain MRI datasets to create a large (N = 1,710), representative dataset with a balanced distribution across age groups and sexes, minimizing potential biases in our model. We then compared three stateof-the-art architectures, Swin UNETR, UNETR, and UNET, on the voxel-level brain age prediction pretext task. Swin UNETR achieved the best performance with a mean absolute error (MAE) of 5.9 ± 4.4 years, outperforming UNETR (MAE: 7.2 ± 4.4 years) and UNET (MAE: 6.2 ± 4.2 years). Based on this performance, we selected Swin UNETR for a brain MRI segmentation downstream task to evaluate the effectiveness of the voxel-level brain age prediction as a self-supervised learning pretext task. We fine-tuned it and compared its performance against two baselines: (1) training from scratch and (2) fine-tuning a model pretrained on an image inpainting task, a non-domain-specific pretext task. The Swin UNETR model pre-trained on voxel-level brain age prediction achieved the highest Dice coefficient on an out-of-distribution test set and performed comparably to the inpainting-pretrained model on an in-distribution test set. These results demonstrate the potential of voxel-level brain age prediction as a domain-specific pretext task for self-supervised learning in neuroimaging, improving segmentation performance, especially in challenging, low-data scenarios.", "filename": "2025_0473.pdf", "year": 2025, "institution": "University of Calgary", "country": "Canada", "authors": ["Tasneem Nasser", "Roberto Souza", "Naser El-Sheimy"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0474", "x": 1.953, "y": 3.549, "title": "Learning with Explicit Topological Priors for Chest X-Ray Rib Segmentation", "abstract": "Chest X-ray (CXR) image examination is a primary tool for assessing thoracic abnormalities. It is widely utilized for initial diagnosis and screening of diseases due to its cost-effectiveness and low radiation dose. Segmentation of ribs in CXR images (CXR rib segmentation) facilitates rapid determination of lesion types and locations, thereby alleviating the workload of medical professionals. Deep learning-based methods have achieved significant progress but still face some challenges in CXR rib segmentation, such as the occlusion challenge caused by artifacts and the interlace challenge caused by the spatial overlap of ribs. Therefore, it can be observed that the topological knowledge of ribs is crucial for CXR rib segmentation but neglected in existing methods, including the connectivity and interactivity of ribs. To address these challenges, we propose a novel learning framework that integrates explicit topological priors into segmentation networks for precise CXR rib segmentation. In particular, we introduce two modules including the connectivity prior embedding module and the interactivity prior embedding module. These modules are designed to explicitly encode the continuity and interactivity of ribs into deep learning models for end-to-end training. Both modules are plug-and-play and can be integrated into various networks. We conduct extensive experiments on VinDr-RibCXR and CXRS datasets to evaluate the segmentation accuracy of each rib using multiple metrics. Evaluation and visual results show that our method exhibits strong adaptability, seamlessly integrating with diverse architectures and enhancing performance across various networks. Our code is publicly available at https://github.com/XWei98/LTSeg.", "filename": "2025_0474.pdf", "year": 2025, "institution": "Anhui University", "country": "China", "authors": ["Xiaowei Zhao", "Chenglong Li", "Jin Tang", "Chuanfu Li"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0475", "x": 1.277, "y": 5.243, "title": "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of Pan-Tumors with Knowledge-Driven Prompts", "abstract": "Accurate tumor segmentation is crucial for cancer diagnosis and treatment. While foundation models have advanced general-purpose segmentation, existing methods still struggle with: (1) limited incorporation of medical priors, (2) imbalance between generic and tumorspecific features, and (3) high computational costs for clinical adaptation. To address these challenges, we propose MAST-Pro (Mixtureof-experts for Adaptive Segmentation of pan-Tumors with knowledgedriven Prompts), a novel framework that integrates dynamic Mixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor segmentation. Specifically, text and anatomical prompts provide domain-specific priors, guiding tumor representation learning, while D-MoE dynamically selects experts to balance generic and tumor-specific feature learning, improving segmentation a ccuracy across diverse tumor types. To enhance efficiency, we employ Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with significantly reduced computational overhead. Experiments on multi-anatomical tumor datasets demonstrate that MAST-Pro outperforms State-of-The-Art approaches, achieving up to a 5.20% improvement in average DSC while reducing trainable parameters by 91.04%, without compromising accuracy.", "filename": "2025_0475.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Runqi Meng", "Sifan Song", "Pengfei Jin", "Lin Teng", "Yulin Wang", "Yiqun Sun", "Ling Chen", "Yujin Oh", "Xiang Li", "Quanzheng Li", "Ning Guo", "Dinggang Shen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0476", "x": 2.613, "y": 3.99, "title": "MatchGen: Detecting Medical Abnormal Region by Generating Matched Normal Regions", "abstract": "Accurate abnormal region detection in medical images is critical for early diagnosis. Unlike supervised and self-supervised methods, unsupervised methods require no annotated training data and generalize well to unseen abnormalities. Such advantages are achieved by detecting abnormal regions from the differences between an input image and a generated pseudo-normal image, which is similar to the input image but excludes abnormal regions. However, existing unsupervised methods often suffer from high false positive rate at test time due to poor pixel-level matching between the normal regions of the input image and the pseudo-normal image. To address this challenge, we propose Match-Gen, a novel plug-and-play framework t o enhance the detection performance of existing unsupervised methods by optimizing the pseudonormal image at test time. This generates an optimized pseudo-normal image that accurately matches the normal regions of the input while maintaining a clear distinction from the abnormal regions, which significantly improves the detection performance. Extensive experiments on four real-world datasets demonstrate the outstanding effectiveness of MatchGen.", "filename": "2025_0476.pdf", "year": 2025, "institution": "McMaster University", "country": null, "authors": ["Xinyu Ma", "Jinhui Ma", "Shiqi He", "Xin Che", "Hon Yiu So", "Lingyang Chu"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0477", "x": 2.499, "y": 6.061, "title": "MedGNN: General Medical Image Recognition Network via GNN Visual Representations", "abstract": "Existing medical image representations are typically processed into grid or sequence structures via Convolutional Neural Network (CNN) or Vision Transformers. However, these methods struggle to flexibly capture irregular lesion regions and reveal relationships between lesions, especially in 3D medical imaging. To address this, we transform medical images into graph structures and propose MedGNN, a general recognition network based on Graph Neural Network (GNN) visual representations. We first segment the image into patches and treat each patch as a node, constructing graph visual embeddings via the K-Nearest Neighbor algorithm. Then, we propose multi-scale dynamic max-relative graph convolution for feature aggregation and updating. To mitigate over-smoothing in graph models, we design a feature-enhanced feed-forward network to refine feature representations. Experiments show that MedGNN achieves strong competitive performance across various 2D and 3D medical image recognition datasets. Moreover, it visualizes lesion relationships through graphs, enabling interpretable analysis based on graph structures. Code is available at: https://github.com/IMCTGD/ MedGNN.", "filename": "2025_0477.pdf", "year": 2025, "institution": "Guangdong University of Technology", "country": "China", "authors": ["Jiayu Ye", "An Zeng", "Dan Pan", "Junhao Chen", "Guanwei Cheng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0478", "x": 4.107, "y": 2.89, "title": "Mesh4D: A Motion-Aware Multi-view Variational Autoencoder for 3D+t Mesh Reconstruction", "abstract": "Reconstructing temporally coherent 3D meshes of the beating heart from multi-view MR images is an important but challenging problem. The challenge is entangled by the complexity in integrating multi-view data, the sparse coverage of a 3D geometry by 2D image slices, and the interplay between geometry and motion. Current approaches often treat mesh reconstruction and motion estimation as two separate problems. Here we propose Mesh4D, a novel motion-aware method that jointly learns cardiac shape and motion, directly from multi-view MR image sequences. The method introduces three key innovations: (1) A cross-attention encoder that fuses multi-view image information, (2) A transformer-based variational autoencoder (VAE) that jointly model the image feature and motion, and (3) A deformation decoder that generates continuous deformation fields and temporally smooth 3D+t cardiac meshes. Incorporating geometric regularisation and motion consistency constraints, Mesh4D can reconstruct high-quality 3D+t meshes (7,698 vertices, 15,384 faces) of the heart ventricles across 50 time frames, within less than 3 s. When compared to existing approaches, Mesh4D achieves notable improvements in reconstruction accuracy and motion smoothness, offering an efficient image-to-mesh solution for quantifying shape and motion of the heart and creating digital heart models. M. Qiao and J. Zheng-Equal contribution.", "filename": "2025_0478.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Mengyun Qiao", "Jin Zheng", "Weitong Zhang", "Qiang Ma", "Liu Li", "Bernhard Kainz", "Declan P O'regan", "Paul M Matthews", "Steven Niederer", "Wenjia Bai"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0479", "x": 3.151, "y": 1.341, "title": "NeRF-Based CBCT Reconstruction Needs Normalization and Initialization", "abstract": "Cone Beam Computed Tomography (CBCT) is widely used in medical imaging. However, the limited number and intensity of Xray projections make reconstruction an ill-posed problem with severe artifacts. NeRF-based methods have achieved great success in this task. However, they suffer from a local-global training mismatch between their two key components: the hash encoder and the neural network. Specifically, in each training step, only a subset of the hash encoder's parameters is used (local sparse), whereas all parameters in the neural network participate (global dense). Consequently, hash features generated in each step are highly misaligned, as they come from different subsets of the hash encoder. These misalignments from different training steps are then fed into the neural network, causing repeated inconsistent updates, which leads to unstable training, slower convergence, and degraded reconstruction quality. Aiming to alleviate the impact of this local-global optimization mismatch, we introduce a Normalized Hash Encoder, which enhances feature consistency and mitigates the mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI) strategy that initializes the neural network before training by leveraging the global mapping property from a well-trained model. The initialized neural network exhibits improved early training stability, faster convergence and enhanced reconstruction performance. Our method is simple yet effective, requiring only a few lines of code while substantially improving training efficiency on 128 CT cases from 4 different datasets, covering 7 distinct anatomical regions. https://github. com/iddifficult/NI_NeRF", "filename": "2025_0479.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "China", "authors": ["Zhuowei Xu", "Han Li", "Dai Sun", "Zhicheng Li", "Yujia Li", "Qingpeng Kong", "Zhiwei Cheng", "Nassir Navab", "Shaohua Kevin Zhou"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0480", "x": 3.031, "y": 3.618, "title": "New Multiple Sclerosis Lesion Segmentation via Calibrated Inter-patch Blending", "abstract": "Longitudinal monitoring of multiple sclerosis (MS) lesions provides crucial biomarkers for assessing disease progression and treatment efficacy. However, it remains challenging to detect and segment numerous MS lesion instances accurately. One key limitation lies in the common average blending of sliding-window predictions during inference, where unreliable patch-level outputs often lead to many false-positive results. To address this issue, we propose a Calibrated Inter-patch Blending (CIB) framework for new MS lesion segmentation, leveraging patch-level segmentation performance as blending weights. Specifically, our CIB model incorporates a multi-scale design with two additional prediction heads: one estimates the overall segmentation performance of the input patch, while the other predicts the performance of smaller grids within the patch. This dual-head architecture enables the model to capture both global a nd local contextual information, reducing overconfident lesion predictions. During inference, the predicted segmentation scores serve as calibration weights for adaptively blending patch predictions. Extensive experiments on the MSSEG-2 dataset demonstrate that our CIB model can significantly enhance both new MS lesion detection (e.g., a 12.82% F1 gain) and segmentation (e.g., a 4.01% Dice gain) across various backbones. Our code is available at https:// github.com/Yejin0111/CIB.", "filename": "2025_0480.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Jin Ye", "Son Duy Dao", "Yicheng Wu", "Yasmeen George", "Thanh Nguyen-Duc", "Daniel F Schmidt", "Hengcan Shi", "Winston Chong", "Jianfei Cai"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0481", "x": 0.832, "y": 4.228, "title": "No More Sliding Window: Efficient 3D Medical Image Segmentation with Differentiable Top-K Patch Sampling", "abstract": "3D models surpass 2D models in CT/MRI segmentation by effectively capturing inter-slice relationships. However, the added depth dimension substantially increases memory consumption. While patchbased training alleviates memory constraints, it significantly slows down the inference speed due to the sliding window (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel end-to-end trainable framework that enhances the efficiency of generic 3D segmentation backbone during an inference step by eliminating the need for SW. NMSW employs a differentiable Top-k module to selectively sample only the most relevant patches, thereby minimizing redundant computations. When patch-level predictions are insufficient, the framework intelligently leverages coarse global predictions to refine results. Evaluated across 3 tasks using 3 segmentation backbones, NMSW achieves comp etitive accuracy compared to SW inference while significantly reducing computational complexity by 91% (88.0 to 8.00 TMACs). Moreover, it delivers a 9.1× faster inference on the H100 GPU (99.0 to 8.3 sec) and a 11.1× faster inference on the Xeon Gold CPU (2110 to 189 sec). NMSW is modelagnostic, further boosting efficiency when integrated with any existing efficient segmentation backbones. The code is available: https://github. com/Youngseok0001/open_nmsw.", "filename": "2025_0481.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Young Seok Jeon", "Hongfei Yang", "Huazhu Fu", "Yeshe Kway", "Mengling Feng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0482", "x": 3.719, "y": 3.076, "title": "Pairwise-Constrained Implicit Functions for 3D Human Heart Modeling", "abstract": "Accurate 3D models of the human heart require not only correct outer surfaces but also realistic inner structures, such as the ventricles, atria, and myocardial layers. Approaches relying on implicit surfaces, such as signed distance functions (SDFs), are primarily designed for single watertight surfaces, making them ill-suited for multi-layered anatomical structures. They often produce gaps or overlaps in shared boundaries. Unsigned distance functions (UDFs) can model non-watertight geometries but are harder to optimize, while voxelbased methods are limited in resolution and struggle to produce smooth, anatomically realistic surfaces. We introduce a pairwise-constrained SDF approach that models the heart as a set of interdependent SDFs, each representing a distinct anatomical component. By enforcing proper contact between adjacent SDFs, w e ensure that they form anatomically correct shared walls, preserving the internal structure of the heart and preventing overlaps, or unwanted gaps. Our method significantly improves inner structure accuracy over single-SDF, UDF-based, voxel-based, and segmentation-based reconstructions. We further demonstrate its generalizability by applying it to a vertebrae dataset, preventing unwanted contact between structures.", "filename": "2025_0482.pdf", "year": 2025, "institution": "EPFL", "country": null, "authors": ["Hieu Le", "Jingyi Xu", "Nicolas Talabot", "Jiancheng Yang", "Pascal Fua"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0483", "x": 2.55, "y": 4.023, "title": "Pathology-Aware Adaptive Watermarking for Text-Driven Medical Image Synthesis", "abstract": "As recent text-conditioned diffusion models have enabled the generation of high-quality images, concerns over their potential misuse have also grown. This issue is critical in the medical domain, where text-conditioned generated medical images could enable insurance fraud or falsified records, highlighting the urgent need for reliable safeguards against unethical use. While watermarking techniques have emerged as a promising solution in general image domains, their direct application to medical imaging presents significant challenges. A key challenge is preserving fine-grained disease manifestations, as even minor distortions from a watermark may lead to clinical misinterpretation, which compromises diagnostic integrity. To overcome this gap, we present MedSign, a deep learning-based watermarking framework specifically designed for text-to-medical image synthesis, which preserves pathologically significant regions by adaptively adjusting watermark strength. Specifically, we generate a pathology localization map using c ross-attention between medical text tokens and the diffusion denoising network, aggregating token-wise attention across layers, heads, and time steps. Leveraging this map, we optimize the LDM decoder to incorporate watermarking during image synthesis, ensuring cohesive integration while minimizing interference in diagnostically critical regions. Experimental results show that our MedSign preserves diagnostic integrity while ensuring watermark robustness, achieving state-of-the-art performance in image quality and detection accuracy on MIMIC-CXR and OIA-ODIR datasets.", "filename": "2025_0483.pdf", "year": 2025, "institution": "Yonsei University", "country": "Republic of Korea", "authors": ["Chanyoung Kim", "Dayun Ju", "Jinyeong Kim", "Woojung Han", "Roberto Alcover-Couso", "Seong Jae Hwang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0484", "x": 2.584, "y": 5.713, "title": "PathoPainter: Augmenting Histopathology Segmentation via Tumor-Aware Inpainting", "abstract": "Tumor segmentation plays a critical role in histopathology, but it requires costly, fine-grained image-mask pairs annotated by pathologists. Thus, synthesizing histopathology data to expand the dataset is highly desirable. Previous works suffer from inaccuracies and limited diversity in image-mask pairs, both of which affect training segmentation, particularly in small-scale datasets and the inherently complex nature of histopathology images. To address this challenge, we propose PathoPainter, which reformulates image-mask pair generation as a tumor inpainting task. Specifically, our approach preserves the background while inpainting the tumor region, ensuring precise alignment between the generated image and its corresponding mask. To enhance dataset diversity while maintaining biological plausibility, we incorporate a sampling mechanism that conditions tumor inpainting on regional embeddings from a different image. Additionally, we introduce a filtering strategy to exclude uncertain synthetic regions, further improving the quality of the generated data. Our comprehensive evaluation spans multiple datasets featuring diverse tumor types and various training data scales. As a result, segmentation improved significantly with our synthetic data, surpassing existing segmentation data synthesis approaches, e.g., 75.69% → 77.69% on CAMELYON16. The code is available at https://github.com/HongLiuuuuu/PathoPainter.", "filename": "2025_0484.pdf", "year": 2025, "institution": "Eindhoven University of Technology", "country": "The Netherlands", "authors": ["Hong Liu", "Haosen Yang", "Evi M C Huijben", "Mark Schuiveling", "Ruisheng Su", "Josien P W Pluim", "Mitko Veta"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0485", "x": 0.975, "y": 1.154, "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "abstract": "Apical periodontitis is a prevalent oral pathology that presents significant public health challenges. Despite advances in automated diagnostic systems across various medical fields, the development of Computer-Aided Diagnosis (CAD) applications for apical periodontitis is still constrained by the lack of a large-scale, high-quality annotated dataset. To address this issue, we release a large-scale panoramic radiograph benchmark called \"PerioXrays\", comprising 3,673 images and 5,662 meticulously annotated instances of apical periodontitis. To the best of our knowledge, this is the first benchmark dataset for automated apical periodontitis diagnosis. This paper further proposes a clinicaloriented apical periodontitis detection (PerioDet) paradigm, which jointly incorporates Background-Denoising Attention (BDA) and IoU-Dynamic Calibration (IDC) mechanisms to address the c hallenges posed by background noise and small targets in automated detection. Extensive experiments on the PerioXrays dataset demonstrate the superiority of PerioDet in advancing automated apical periodontitis detection. Additionally, a well-designed human-computer collaborative experiment underscores the clinical applicability of our method as an auxiliary diagnostic tool for professional dentists. The project is publicly accessible at https://github.com/XiaochengFang/MICCAI2025_PerioDet.", "filename": "2025_0485.pdf", "year": 2025, "institution": "South China Normal University", "country": "China", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0486", "x": 3.334, "y": 1.727, "title": "Physics Informed Guided Diffusion for Accelerated Multi-parametric MRI Reconstruction", "abstract": "We introduce MRF-DiPh, a novel physics informed denoising diffusion approach for multiparametric tissue mapping from highly accelerated, transient-state quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our method is derived from a proximal splitting formulation, incorporating a pretrained denoising diffusion model as an effective image prior to regularize the MRF inverse problem. Further, during reconstruction it simultaneously enforces two key physical constraints: (1) k-space measurement consistency and (2) adherence to the Bloch response model. Numerical experiments on in-vivo brain scans data show that MRF-DiPh outperforms deep learning and compressed sensing MRF baselines, providing more accurate parameter maps while better preserving measurement fidelity and physical model consistency-critical for solving reliably inverse problems in medical imaging.", "filename": "2025_0486.pdf", "year": 2025, "institution": "University of Bristol", "country": "UK", "authors": ["Perla Mayo", "Carolin M Pirkl", "Alin Achim", "Bjoern Menze", "Mohammad Golbabaee"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0487", "x": 4.22, "y": 5.504, "title": "Predicting Radiation Therapy Response Based on Dynamic Temporal Feature Difference Fusion from Longitudinal MRI", "abstract": "Significant progress has been made in AI-based prediction of therapeutic response to neoadjuvant chemotherapy (NAC) in breast cancer. However, current studies primarily rely on data from a single time point, neglecting the dynamic changes in tumor characteristics during treatment. To address this limitation, we propose a novel Dynamic Temporal Feature Difference Fusion (DTFDF) framework, which integrates image features from multiple time points throughout the treatment process to predict therapy response more precisely. Based on tumor spatial features, we design an innovative DTFDF strategy and introduce a treatment response-based triplet contrastive loss function to facilitate the learning of longitudinal tumor changes and enhance feature representation. Additionally, we incorporate biomarker prediction as an auxiliary task and introduce a feature decoupling-based multi-task learning module. This module generates feature representations for different tasks by accounting for both shared and task-specific information, improving response prediction. Experiments with data from 786 patients in the I-SPY 2 trial dataset demonstrate that our method achieves the highest AUC of 0.835 in predicting radiation therapy response, outperforming state-of-the-art (SOTA) approaches on longitudinal dynamic contrastenhanced MRI data. Our source code is available at https://github.com/ AlexNmSED/DTFDF.", "filename": "2025_0487.pdf", "year": 2025, "institution": "Dalian University of Technology", "country": "China", "authors": ["Xinyu Hao", "Hongming Xu", "Qibin Zhang", "Qi Xu", "Xiaofeng Wang", "Ilkka Polonen", "Fengyu Cong"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0488", "x": 2.263, "y": 2.691, "title": "R2B-WFC Ultrasound Reconstruction: Wavelet Fourier Convolution-Based Reconstruction from Radio Frequency to Image", "abstract": "Plane-wave ultrasound (PWUS) facilitates functional imaging through a high frame rate of a few thousand Hz. However, its application remains constrained due to the inferior B-mode image quality in comparison to conventional ultrasound imaging such as focused beam ultrasound (FBUS). In this paper, a data-driven approach is proposed through two steps to enhance the quality of PWUS images. In the first step, the unpaired neural Schrödinger bridge (UNSB) is employed to synthesize high-fidelity images that structurally correspond to the low-quality PWUS images. In the second step, our proposed model, R2B-WFC, is trained to reconstruct high-quality images from the PWUS radio frequency signals, incorporating a wavelet Fourier convolution (WFC) module. Multiple losses are also suggested, combining perceptual loss from a USNB pre-trained model and a Markovian discriminator to preserve high-frequency detail more effectively. As a result, Fréchet Inception Distance (FID), Kernel Inception Distance (KID), Learned Perceptual Image Patch Similarity (LPIPS), Feature Similarity Index Measure (FSIM), Signal to noise ratio (SNR), and Contrast Ratio (CR) scores were 136.32, 0.0356, 0.1956, 0.9514, 41.18 dB, and 27.48 dB, respectively. Compared to image-to-image translation methods, R2B-WFC from RF signal-to-image also shows faster inference time.", "filename": "2025_0488.pdf", "year": 2025, "institution": "Pohang University of Science and Technology (POSTECH)", "country": "South Korea", "authors": ["Hyunsu Jeong", "Chiho Yoon", "Minsik Sung", "Kiduk Kim", "Dougho Park", "Chulhong Kim"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0489", "x": 1.059, "y": 4.776, "title": "RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation", "abstract": "High-quality pixel-level annotations of medical images are essential for supervised segmentation tasks, but obtaining such annotations is costly and requires medical expertise. To address this challenge, we propose a novel coarse-to-fine segmentation framework that relies entirely on coarse-level annotations, encompassing both target and complementary drawings, despite their inherent noise. The framework works by introducing transition matrices in order to model the inaccurate and incomplete regions in the coarse annotations. By jointly training on multiple sets of coarse annotations, it progressively refines the network's outputs and infers the true segmentation distribution, achieving a robust approximation of precise labels through matrix-based modeling. To va lidate the flexibility and effectiveness of the proposed method, we demonstrate the results on two public cardiac imaging datasets, ACDC and MSCMRseg, and further evaluate its performance on the UK Biobank dataset. Experimental results indicate that our approach surpasses the state-of-the-art weakly supervised methods and closely matches the fully supervised approach. Our code is available at https:// github.com/AnghongDu/RefineSeg-MICCAI2025.", "filename": "2025_0489.pdf", "year": 2025, "institution": "Univ ersity of Birmingham", "country": "UK", "authors": ["Anghong Du", "Nay Aung", "Theodoros N Arvanitis", "Stefan K Piechnik", "Joao A C Lima", "Steffen E Petersen", "Le Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0490", "x": 2.928, "y": 4.903, "title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "abstract": "The scarcity of high-quality, labelled retinal imaging data, which presents a significant challenge in the development of machine learning models for ophthalmology, hinders progress in the field. Existing methods for synthesising Colour Fundus Photographs (CFPs) largely rely on predefined disease labels, which restricts their ability to generate images that reflect fine-grained anatomical variations, subtle disease stages, and diverse pathological features beyond coarse class categories. To overcome these challenges, we first introduce an innovative pipeline that creates a large-scale, captioned retinal dataset comprising 1.4 million entries, called RetinaLogos-1400k. Specifically, RetinaLogos-1400k uses the visual language model (VLM) to describe retinal conditions and key structures, such as optic disc configuration, vascular distribution, nerve fibre layers, and pathological features. Building on this dataset, we employ a novel three-step training framework, called RetinaLogos, which enables fine-grained semantic control over retinal images and accurately captures different stages of disease progression, subtle anatomical variations, and specific lesion types. Through extensive experiments, our method demonstrates superior performance across multiple datasets, with 62.07% of text-driven synthetic CFPs indistinguishable from real ones by ophthalmologists. Moreover, the synthetic data improves accuracy by 5%-10% in diabetic retinopathy grading and glaucoma detection. Codes are available at Link.", "filename": "2025_0490.pdf", "year": 2025, "institution": "Shanghai Artificial Intelligence Laboratory", "country": "China", "authors": ["Junzhi Ning", "Cheng Tang", "Kaijing Zhou", "Diping Song", "Lihao Liu", "Ming Hu", "Wei Li", "Huihui Xu", "Yanzhou Su", "Tianbin Li", "Jiyao Liu", "Jin Ye", "Sheng Zhang", "Yuanfeng Ji", "Junjun He"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0491", "x": 6.582, "y": 4.191, "title": "RSAD: Region-Specific Anomaly Detection in fMRI for Disease Diagnosis", "abstract": "Functional magnetic resonance imaging (fMRI), a noninvasive neuroimaging technique for mapping neural activity, has demonstrated substantial potential in identifying brain disease. However, clinical applications face a critical challenge: patient data are typically scarce compared to abundant healthy control samples. This severe class imbalance significantly limits the performance of classification-based diagnostic models. To address this issue, we propose the Region-Specific Anomaly Detection (RSAD) framework, which formulates the brain disease identification as an anomaly detection task. We first employ pretraining to capture normal patterns of healthy data through a reconstruction task, and then develop the discrepancy score to enhance the model's ability to perceive potential abnormalities, thereby improving the AD performance. Specifically, we design an affinity matrix learning module and an adaptive region of interest (ROI) masking strategy to improve the performance of signal representation learning. Additionally, we propose a region-based discrepancy score weighting strategy to amplify the distinction between potential abnormalities and healthy controls by assigning higher weights to key brain regions, thereby improving the model's ability to detect anomalies. We conduct experiments across six different brain diseases, and the superior results demonstrate that RSAD effectively enables disease diagnosis, even with extreme class imbalance. Our code is available at https://github.com/kylin1112/RSAD.", "filename": "2025_0491.pdf", "year": 2025, "institution": "Shanghai Jiao Tong U niversity", "country": "China", "authors": ["Yusong Sun", "Dongdong Chen", "Mengjun Liu", "Zhenrong Shen", "Zhiyun Song", "Yuqi Hu", "Manman Fei", "Xu Han", "Zelin Liu", "Xingkai Fang", "Lu Bai", "Lichi Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0492", "x": 1.509, "y": 5.113, "title": "Segmenting Vessels Encapsulating Tumor Clusters via Fine-Grained Visual Prompt", "abstract": "Segmenting hepatocellular carcinoma (HCC) and vessels encapsulating tumor clusters (VETC) are new paradigm for prognostic analysis. However, the clustered morphology of VETC nuclei, which is difficult to represent at the patch level, makes segmentation highly challenging. Recent visual prompt-based methods incorporating nucleus prior knowledge have shown promise but assume patch pixels lack spatial correlation, failing to capture nuclei morphology at the pixel level. To address this, we propose a Patch-to-Pixel Visual Prompt (VPP2P) framework, which models VETC morphological features by propagating visual prompts from patches to pixels. Built on contrastive learning, our semi-supervised approach samples positive and negative pairs within patches to enhance feature learning. Experiments show that VPP2P achieves performance c omparable to fully supervised methods using only 10% of the training data. With 30% of the training data, VPP2P attains a Dice score of 90.52%, outperforming state-of-the-art visual promptbased methods by an average margin of 6.6%. To the best of our knowledge, this is the first semi-supervised deep learning approach for VETC morphological analysis, offering new insights into HCC clinical research. Code is available at https://github.com/sm8754/VPP2P.", "filename": "2025_0492.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Jiahui Yu", "Tianyu Ma", "Shenjian Gu", "Yuping Guo", "Feng Chen", "Xiaoxiao Li", "Yingke Xu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0493", "x": 1.81, "y": 4.639, "title": "Sequence-Independent Continual Test-Time Adaptation with Mixture of Incremental Experts for Cross-Domain Segmentation", "abstract": "Continual Test-Time Adaptation (CTTA) adapts a model pretrained on the source domain to sequentially arriving unlabeled target domains. However, existing approaches predominantly assume that model would complete adaptation to all samples within the same target domain before transitioning to the next domain, deviating from realistic clinical scenarios where samples from diverse domains arrive stochastically. Such gradual adaptation strategies suffer from performance drop under rapid domain shifts and limits their clinical applicability. To address this issue, we propose Mixture of Incremental Experts (MoIE), a lightweight network structure that maps new patterns to established knowledge. Specifically, MoIE incorporates two key innovations: 1) Progressive Expert Expansion (PEE), which dynamically adds experts when existing ones fail to effectively process the current sample, enabling stable and swift adaptation to target domains; 2) Knowledge-Transfer Initialization (KTI), which initializes new experts by combining existing ones through domain-similarity based weights, enabling fast adaptation to unseen domains while preserving learned knowledge to prevent immediate forgetting. Experiments on two CTTA tasks (prostate and fundus segmentations) indicate its superiority by achieving SOTA performance with minimal performance gaps across diverse inference sequences. (Code available at https://github.com/dyxu-cuhkcse/MoIE.", "filename": "2025_0493.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Dunyuan Xu", "Yuchen Yuan", "Donghao Zhou", "Xikai Yang", "Jingyang Zhang", "Jinpeng Li", "Pheng-Ann Heng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0494", "x": 3.613, "y": 1.606, "title": "Single-Spoke Motion-Compensated Dynamic 3D MRI Reconstruction via Neural Representation", "abstract": "Dynamic 3D Magnetic Resonance Imaging (MRI) is a powerful imaging technique for motion monitoring and tracking, offering both excellent soft-tissue contrast and the ability to capture dynamic changes in tissue. Current reconstruction methods typically assume that multiple spokes share the same motion state. However, this assumption does not align with the complex realities of patient motion and clinical acquisition protocols, often resulting in anatomical discontinuities or blurring artifacts in the reconstructed images. In this work, we propose an unsupervised Single-sPoke motion-compensated Implicit NEural Representation method (SPINER) for dynamic volumetric MRI reconstruction. We address a more challenging yet realistic scenario, single-spoke motion modeling, which assigns a unique motion state for each spoke measurement. To address this highly ill-posed inverse problem, we propose a motion-ignoring static initialization strategy that exploits static anatomical information across all spokes. We find that a good initialization of the canonical volume significantly improves the optimization process and facilitates better dynamic volumetric reconstruction based on implicit neural representation learning. Experiments on abdomen MRI datasets demonstrate that our methods can reconstruct high-quality dynamic volumetric MRI while capturing continuous and accurate motion.", "filename": "2025_0494.pdf", "year": 2025, "institution": "University of Michigan", "country": "USA", "authors": ["Lixuan Chen", "James M Balter", "Liyue Shen", "Jeong Joon Park"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0495", "x": 2.662, "y": 2.72, "title": "Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention", "abstract": "Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a cross-sectional flow image via Doppler phase-subtraction of raw A-scans along B-line. To obtain a high-fidelity B-scan, densely sampledA-scans are required currently, leading to prolonged scanning time and increased storage demands. Addressing this issue, we propose a novel sparse ODT reconstruction framework with an Alternative State Space Attention Network (ASSAN) that effectively reduces raw A-scans needed. Inspired by the distinct distributions of information along A-line and B-line, ASSAN applies 1D State Space Model (SSM) to each A-line to learn the intra-A-scan representation, while using 1D gated self-attention along B-line to capture the inter-A-scan features. In addition, an effective feedforward network based on sequential 1D convolutions along different axes is employed to enhance the local feature. In validation experiments on real animal data, ASSAN shows clear effectiveness in the reconstruction in comparison with state-of-the-art reconstruction methods. The source code is available at: https://github.com/ZhenghLi/ASSAN.", "filename": "2025_0495.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Zhenghong Li", "Jiaxiang Ren", "Wensheng Cheng", "Yanzuo Liu", "Congwu Du", "Yingtian Pan", "Haibin Ling"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0496", "x": 4.115, "y": 3.356, "title": "Spatiotemporal-Sensitive Network for Microvascular Obstruction Segmentation from Cine Cardiac Magnetic Resonance", "abstract": "Accurate diagnosis of microvascular obstruction (MVO) in acute myocardial infarction (AMI) patients typically relies on Cine Cardiac Magnetic Resonance Imaging (CMR) (video sequences) and Late Gadolinium Enhancement (LGE) CMR (images). However, LGE imaging is contraindicated in approximately 20% of AMI patients with chronic kidney disease, underscoring the need for Cine CMR as a standalone diagnostic alternative. Although recent advancements in deep learning have improved video data processing, current methods fail to adequately capture complementary temporal motion features. This limits their efficacy and poses significant challenges for MVO segmentation with Cine CMR, as MVO regions are defined by dynamic motion rather than clear boundaries or contrast on Cine CMR. To address this limitation, we propose a Spatiotemporal-Sensitive Network that integrates static and motion encoders to effectively process Cine CMR. Further through a guided decoder utilizing the rich spatiotemporal information and an uncertainty-driven refinement leveraging uncertainty maps and low-level features, our method enhances segmentation accuracy and refines boundary delineation. Extensive experiments on 621 Cine CMR demonstrate superior performance over competing methods with a Dice score of 0.56 in Cine CMR-based MVO identification and highlight its potential to advance video analysis in clinical settings. The code is available at https://github.com/MICCAI25-MVO-Segmentation/miccai25-mvo-seg", "filename": "2025_0496.pdf", "year": 2025, "institution": "Institute for Infocomm Research (I2R)", "country": "Singapore", "authors": ["Yang Yu", "Christopher Kok", "Jiahao Wang", "Jun Cheng", "Shuang Leng", "Ru San Tan", "Liang Zhong", "Xulei Yang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0497", "x": 3.644, "y": 2.368, "title": "Spherical Diffusion Process for Score-Guided Cortical Correspondence via Spectral Attention", "abstract": "Cortical shape correspondence is a crucial problem in medical image analysis, primarily focused on aligning cortical geometric patterns across individuals. This task is particularly challenging due to the intricate geometry of the cortex and the substantial anatomical variability among individuals. In this work, we introduce a novel approach comprising (1) a spherical diffusion process and (2) a spectral attention for robust shape correspondence construction, wherein a score function from the diffusion process guides a deformation to align cortical geometric features on sphere. Specifically, we propose a smooth diffusion process on sphere by introducing a stochastic differential equation in a spherical harmonic space, where we learn the score function that encodes the distribution of subjects. F urthermore, to effectively guide the alignment of cortical geometric patterns using the learned score function, we propose a novel attention mechanism that computes frequency correlations in the spectral domain, enabling efficient conditioning of the score function in this domain. Experimental results demonstrate that our method achieves highly accurate shape correspondence while minimizing the distortions.", "filename": "2025_0497.pdf", "year": 2025, "institution": "AI Research Team", "country": "South Korea", "authors": ["Seungeun Lee", "Sergey Pyatkovskiy", "Jaejun Yoo", "Ilwoo Lyu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0498", "x": 2.462, "y": 5.514, "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-World Breast MRI Classification", "abstract": "Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn nonclinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at github.com/utkuozbulak/spurbreast.", "filename": "2025_0498.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Jong Bum Won", "Wesley De Neve", "Joris Vankerschaver", "Utku Ozbulak"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0499", "x": 2.559, "y": 2.083, "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration", "abstract": "Medical image restoration (MedIR) aims to recover highquality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating taskspecific weight parameters for eac h task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks-PET synthesis, CT denoising, and MRI super-resolution-both in task-specific and All-in-One settings. Code is available at this https URL.", "filename": "2025_0499.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Zhiwen Yang", "Jiaju Zhang", "Yang Yi", "Jian Liang", "Bingzheng Wei", "Yan Xu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0500", "x": 1.634, "y": 4.595, "title": "tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation and Its Application in Medical Image Segmentation", "abstract": "Transfer learning, by leveraging knowledge from pre-trained models, has significantly improved the performance of downstream tasks. However, as deep neural networks continue to scale, full fine-tuning poses substantial computational and storage challenges in resource-constrained environments, limiting its practical adoption. To address this, parameterefficient fine-tuning (PEFT) methods have been proposed to reduce computational complexity and memory requirements by updating only a small subset of parameters. Among them, matrix decomposition-based approaches such as LoRA have shown promise, but often struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-order tensors offer a more natural representation of neural network parameters, enabling richer modeling of multi-dimensional interactions and higher-order features. In this paper, we propose tCUR-LoRA, a novel fine-tuning method based on tensor CUR decomposition. By stacking pre-trained weight matrices into a third-order tensor and applying tensor CUR decomposition, our method updates only the compressed tensor components during fine-tuning, thereby substantially reducing both computational and storage costs. Experimental results show that tCURLoRA consistently outperforms existing PEFT approaches on medical image segmentation tasks. The source code is publicly available at: https://github.com/WangangCheng/t-CURLora.", "filename": "2025_0500.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Guanghua He", "Wangang Cheng", "Hancan Zhu", "Xiaohao Cai", "Gaohang Yu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0501", "x": 1.358, "y": 3.505, "title": "TransSino: Prior Sinogram Pattern-Based Transformer for Limited-Angle CT Image Segmentation", "abstract": "Medical diagnosis using limited-angle computed tomography (LACT) is a beneficial approach for patients due to advantages such as faster scanning times and lower radiation doses. However, images reconstructed from LACT contain limited information, leading to significant artifacts and making an accurate diagnosis more challenging. Although various methods have been proposed to reconstruct LACT images into full-angle computed tomography (CT) images, they primarily focus on improving image quality and operate independently of lesion segmentation models, neglecting critical lesion-related information. In this paper, we propose TransSino, a transformer-based medical image segmentation model that operates in the sinogram domain of LACT. TransSino learns an auxiliary task to reconstruct the unmeasured regions in the sinogram domain for robust segmentation performance. Sp ecifically, it analyzes the sequential nature of the sinogram using the transformer from language models and reconstructs features for the unmeasured regions by using prior sinogram patterns. Moreover, we introduce a contrastive abnormal feature loss to enhance the contrast between abnormal and normal feature regions. Experimental results confirm that TransSino outperforms existing medical segmentation methods on LACT images. The code is available at https://github.com/jhyoon964/TransSino.", "filename": "2025_0501.pdf", "year": 2025, "institution": "Chonnam National University", "country": "Korea", "authors": ["Jae Hyun Yoon", "Yeong Jong Lee", "Seok Bong Yoo"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0502", "x": 3.504, "y": 1.627, "title": "Ultra-Low-Field MRI Enhancement via INR-Based Style Transfer", "abstract": "Ultra-low-field (ULF) Magnetic Resonance Imaging (MRI) improves accessibility and affordability but suffers from lower image quality compared to high-field MRI. This study proposes a novel enhancement framework that integrates Implicit Neural Representations (INRs) with Neural Style Transfer (NST) to improve ULF MRI quality by transferring high-resolution structural details from 7T MRI. Unlike conventional methods, our approach does not require paired datasets or extensive pre-training, leveraging INR's continuous representation and NST's perceptual refinement to enhance contrast, sharpness, and noise suppression. Quantitative evaluations on T1-weighted ULF MRI demonstrate significant improvements in perceptual quality (PIQE), contrastto-noise ratio (CNR), and structural consistency (MLC/MSLC), outperforming state-of-the-art methods. These findings underscore the potential of INR-driven learning for advancing MRI reconstruction, enabling higher-quality imaging in resource-limited settings. Our method is fully unsupervised and operates in an unpaired setting, requiring no voxelwise correspondence or labeled training data. The implementation of our proposed method and model hyperparameters is publicly available at https://github.com/khtohidulislam/ULF-MRI-Enhance.", "filename": "2025_0502.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Kh Tohidul Islam", "Mevan Ekanayake", "Zhaolin Chen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0503", "x": 3.876, "y": 3.19, "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "abstract": "Echocardiography is routine for cardiac examination. However, 2D ultrasound (US) struggles with accurate metric calculation and direct observation of 3D cardiac structures. Moreover, 3D US is limited by low resolution, small field of view and scarce availability in practice. Constructing the cardiac anatomical twin from 2D images is promising to provide precise treatment planning and clinical quantification. However, it remains challenging due to the rare paired data, complex structures, and US noises. In this study, we introduce a novel generative framework UltraTwin, to obtain cardiac anatomical twin from sparse multi-view 2D US. Our contribution is three-fold. First, pioneered the construction of a real-world and high-quality dataset containing strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we propose a coarse-tofine scheme to achieve hierarchical reconstruction optimization. Last, we introduce an implicit autoencoder for topology-aware constraints. Extensive experiments show that UltraTwin reconstructs high-quality anatomical twins versus strong competitors. We believe it advances anatomical twin modeling for potential applications in personalized cardiac care.", "filename": "2025_0503.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0504", "x": 1.108, "y": 3.948, "title": "U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation", "abstract": "Despite their remarkable success in medical image segmentation, the life cycle of deep neural networks remains a challenge in clinical applications. These models must be regularly updated to integrate new medical data and customized to meet evolving diagnostic standards, regulatory requirements, commercial needs, and privacy constraints. Model merging offers a promising solution, as it allows working with multiple specialized networks that can be created and combined dynamically instead of relying on monolithic models. While extensively studied in standard 2D classification, the potential of model merging for 3D segmentation remains unexplored. This paper presents an efficient framework that allows effective model merging in the domain of 3D image segmentation. Our approach builds upon theoretical analysis and encourages wide minima during pre-training, which we demonstrate to facilitate subsequent model merging. Using U-Net 3D, we evaluate the method on distinct anatomical structures with the ToothFairy2 and BTCV Abdomen datasets. To support further research, we release the source code and all the model weights in a dedicated repository: https://github.com/ LucaLumetti/UNetTransplant.", "filename": "2025_0504.pdf", "year": 2025, "institution": "University of Modena and Reggio Emilia", "country": "Italy", "authors": ["Luca Lumetti", "Giacomo Capitani", "Elisa Ficarra", "Simone Calderara", "Costantino Grana", "Angelo Porrello", "Federico Bolelli"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0505", "x": 2.755, "y": 4.432, "title": "UniOCTSeg: Towards Universal OCT Retinal Layer Segmentation via Hierarchical Prompting and Progressive Consistency Learning", "abstract": "Accurate segmentation and quantitative thickness analysis of retinal layers in optical coherence tomography (OCT) are crucial for early diagnoses of ocular disorders. To address the clinical needs of diagnosing various ocular and systemic diseases, numerous multi-granularity OCT datasets are constructed. While deep learning achieves impressive results in retinal layer segmentation, general training paradigms require separate models for datasets with different annotation granularities. Universal models are developed to unify diverse datasets and tasks via advanced techniques such as prompt learning, but they overlook across-granularity information and struggle to generalize to new granularities. In this paper, we propose a universal OCT segmentation model, named UniOCTSeg, which builds its basis upon Hierarchical Prompting Strategy (HPS) and Progressive Consistency Learning (PCL). HPS employs a granularitymerging strategy to construct prompts at various granularities, based on the finest-grained prompts, and develops a universal segmentation model t hat utilizes these hierarchical prompts. Meanwhile, PCL leverages an Exponential Moving Average teacher model to generate pseudosupervision signals, guiding the student model through easy-to-hard progression to ensure consistency across hierarchical levels. Extensive experiments across eight publicly available OCT datasets involving six distinct granularity levels demonstrate UniOCTSeg's superior performance compared with state-of-the-art methods, while also illustrating its high flexibility and strong generalizability. Our code and data are available at https://github.com/Halcyon1010/UniOCTSeg.", "filename": "2025_0505.pdf", "year": 2025, "institution": "Southern University of Science and Technology", "country": "China", "authors": ["Jian Zhong", "Li Lin", "Chaoran Miao", "Kenneth K Y Wong", "Xiaoying Tang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0506", "x": 1.65, "y": 3.602, "title": "UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction of Clinical Brain MRIs", "abstract": "We propose UNSURF, a novel uncertainty measure for cortical surface reconstruction of clinical brain MRI scans of any orientation, resolution, and contrast. It relies on the discrepancy between predicted voxel-wise signed distance functions (SDFs) and the actual SDFs of the fitted surfaces. Our experiments on real clinical scans show that traditional uncertainty measures, such as voxel-wise Monte Carlo variance, are not suitable for modeling the uncertainty of s urface placement. Our results demonstrate that UNSURF estimates correlate well with the ground truth errors and: (i) enable effective automated quality control of surface reconstructions at the subject-, parcel-, mesh node-level; and (ii) improve performance on a downstream Alzheimer's disease classification task.", "filename": "2025_0506.pdf", "year": 2025, "institution": "Massachusetts General Hospital", "country": "USA", "authors": ["Karthik Gopinath", "Raghav Mehta", "Ben Glocker", "Juan Eugenio Iglesias"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0507", "x": 1.755, "y": 2.285, "title": "Vascular Photoacoustic Volume Registration via 2D Feature Matching with Reverse Mapping Based on Maximum Intensity Projection", "abstract": "Photoacoustic (PA) imaging is an emerging biomedical imaging modality well-suited for visualizing blood vessels due to its noninvasive, label-free nature. Registering vascular PA volumetric images acquired at different times or viewpoints is crucial for tracking longitudinal changes and expanding the field of view for comprehensive vascular assessment. However, PA volumes exhibit characteristics such as sparsity, ambiguity, vascular network changes, and unavoidable body hair, which pose significant challenges and limit the accuracy and robustness of existing registration methods. We propose a robust affine registration framework to address PA registration challenges, integrating feature-based alignment, intensity-based refinement, and hair removal. We leverage 2D feature matching with reverse mapping based on maximum intensity projections (MIPs) to handle sparsity and ambiguity, enabling robust alignment. A n intensity-based refinement further enhances accuracy by incorporating our feature-guided sampling strategy to mitigate the impact of vascular network changes. Additionally, we introduce a hair removal procedure to prevent hairs from affecting registration. Experimental evaluation, conducted in collaboration with medical experts, demonstrates that our method outperforms existing approaches in both accuracy and robustness on real PA volumes.", "filename": "2025_0507.pdf", "year": 2025, "institution": "The University of Tokyo", "country": "Japan", "authors": ["Junda Liao", "Chu Zhou", "Yuta Asano", "Yushi Suzuki", "Ryoma Bise", "Nobuaki Imanishi", "Kazuo Kishi", "Sadakazu Aiso", "Imari Sato"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0508", "x": 3.296, "y": 3.418, "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry", "abstract": "Anatomical trees are critical for clinical diagnosis and treatment planning, yet their complex and diverse geometry make accurate representation a significant challenge. Motivated by the latest advances in large language models, we introduce an autoregressive method for synthesizing anatomical trees. Our approach first embeds vessel structures into a learned discrete vocabulary using a VQ-VAE architecture, then models their generation autoregressively with a GPT-2 model. This method effectively captures intricate geometries and branching patterns, enabling realistic vascular tree synthesis. Comprehensive qualitative and quantitative evaluations reveal that our technique achieves high-fidelit y tree reconstruction with compact discrete representations. Moreover, our B-spline representation of vessel cross-sections preserves critical morphological details that are often overlooked in previous' methods parameterizations. To the best of our knowledge, this work is the first to generate blood vessels in an autoregressive manner. Code is available at https:// github.com/LIA-DiTella/VesselGPT-MICCAI.", "filename": "2025_0508.pdf", "year": 2025, "institution": "Consejo Nacional de Investigaciones Científicas y Técnicas", "country": "Argentina", "authors": ["Paula Feldman", "Martin Sinnona", "Claudio Delrieux", "Viviana Siless", "Emmanuel Iarussi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0509", "x": 2.482, "y": 1.926, "title": "VQ-SCD: Vector Quantization Meets Unknown Scan Condition Self-supervised Low-Dose CT Denoising", "abstract": "Artifacts and noise in low-dose CT images can degrade image quality, potentially hindering accurate diagnosis. In recent years, imagedomain post-processing denoising methods have gained flexibility by eliminating the need for raw data. However, clinical scanning conditions vary widely, with most existing studies focusing on CT denoising under fixed or known conditions. Moreover, obtaining paired CT data in clinical settings is challenging, limiting the practical applicability of supervised learning methods. To address these challenges, we propose the selfsupervised VQ-SCD, capable of denoising low-dose CT (LDCT) images under varying unknown scanning conditions using only normal-dose CT (NDCT) training data. For the first time, VQ-SCD uses a discretized codebook to approximate the distribution of LDCT features across various scanning conditions, enabling uniform characterization and denoising of data from multiple scanning setups. A dditionally, we design a miniature diffusion model that uses up-sampled features as guidance to enhance image details. Our method outperforms both supervised and state-of-the-art self-supervised methods in terms of both quantitative metrics and visual quality, with a test time of only 0.25 s per image. Furthermore, training the model using only animal and phantom data still results in excellent denoising performance on human data. The code will be available at https://github.com/WHUSU/VQSCD.", "filename": "2025_0509.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Bo Su", "Jiabo Xu", "Xiangyun Hu", "Kai Deng", "Jiancheng Li", "Zhouxian Lu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0510", "x": 2.65, "y": 1.274, "title": "WiD-PET: PET Image Reconstruction from Low-Dose Data Using a Wavelet-Informed Diffusion Model with Fast Inference", "abstract": "Reconstruction of standard-dose Positron Emission Tomography is vital for clinical diagnosis, while recent diffusion-denoising probabilistic models offer strong generative capabilities, when applied to this task, they often struggle with fine detail recovery, slow inference, and inadequate cross-slice continuity in 3D volumes. To overcome these issues, we introduce WiD-PET, which employs a wavelet transform to produce smaller wavelet-transformed inputs, and thereby reduces inference time to 10% of that required by the DDPM model. Additionally, a high-frequency enhancer is adopted for reconstructing fine and rich image details. Moreover, a spatial consistency feature extractor and spatial consistency attention are implemented to enhance cross-slice continuity in 3D PET reconstructions. Evaluations across dose levels (1/20, 1/50, and 1/100) reveal that WiD-PET consistently achieves superior reconstruction quality, detail preservation, and inference efficiency. Project page: https://github.com/SwingM/WiD.git.", "filename": "2025_0510.pdf", "year": 2025, "institution": "University of Sydney", "country": "Australia", "authors": ["Qingcheng Lyu", "Tong Chen", "Yiran Wang", "Erjian Guo", "Luping Zhou"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0511", "x": 6.78, "y": 3.628, "title": "A Large-Scale Neural Model Inversion Framework for Effective Connectivity Estimation", "abstract": "The development of a computational framework that can infer largescale brain-wide effective connectivity (EC) based on resting-state functional MRI (rs-fMRI) represents a grand challenge to computational neuroimaging. Towards the goal of estimating full-scale, whole-brain EC, we developed a new computational framework termed Large-scale nEural Model Inversion (LEMI) by utilizing a linear neural mass model with an efficient Kalman-filter based gradient descent algorithm. Key advantages of LEMI include fast estimation of both intra-regional and inter-regional connection strengths for large-scale networks, allowing exploration of both intrinsic and external mechanisms in neuroscience problems. Using ground-truth simulations, we demonstrated that LEMI can accurately and efficiently recover model parameters in a large network (100 regions) within 90 min. We then applied the LEMI model to an empirical rs-fMRI dataset from the ADNI database and identified widespread reduced excitation-inhibition (E-I) ratio in patients with Alzheimer's disease (AD). Overall, LEMI provides an efficient and accurate computational framework to estimate large-scale EC and whole-brain E-I balance based on non-invasive neuroimaging data.", "filename": "2025_0511.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Guoshi Li", "Pew-Thian Yap"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0512", "x": 2.373, "y": 1.746, "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning", "abstract": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose computed tomography (LDCT), numerous deep learning-based denoising methods have been developed to mitigate noise and artifacts. However, most of these approaches ignore the anatomical semantics of human tissues, which may potentially result in suboptimal denoising outcomes. To address this problem, we propose ALDEN, an anatomy-aware LDCT denoising method that integrates semantic features of pretrained vision models (PVMs) with adversarial and contrastive learning. Specifically, we introduce an anatomy-aware discriminator that dynamically fuses hierarchical semantic features from reference normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific realism evaluation in the discriminator. In addition, we propose a semantic-guided contrastive learning module that enforces anatomical consistency by contrasting PVM-derived features from LDCT, denoised CT and NDCT, preserving tissue-specific patterns through positive pairs and suppressing artifacts via dual negative pairs. Extensive experiments conducted on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art performance, offering superior anatomy preservation and substantially reducing over-smoothing issue of previous work. Further validation on a downstream multi-organ segmentation task (encompassing 117 anatomical structures) affirms the model's ability to maintain anatomical awareness.", "filename": "2025_0512.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Runze Wang", "Zeli Chen", "Zhiyun Song", "Wei Fang", "Jiajin Zhang", "Danyang Tu", "Yuxing Tang", "Minfeng Xu", "Xianghua Ye", "Le Lu", "Dakai Jin"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0513", "x": 0.828, "y": 4.519, "title": "Autoregressive Medical Image Segmentation via Next-Scale Mask Prediction", "abstract": "While deep learning has significantly advanced medical image segmentation, most existing methods still struggle with handling complex anatomical regions. Cascaded or deep supervision-based approaches attempt to address this challenge through multi-scale feature learning but fail to establish sufficient inter-scale dependencies, as each scale relies solely on the features of the immediate predecessor. Towards this end, we propose the AutoRegressive Segmentation framework via next-scale mask prediction, termed AR-Seg, which progressively predicts the next-scale mask by explicitly modeling dependencies across all previous scales within a unified architecture. AR-Seg introduces three innovations: (1) a multi-scale mask autoencoder that quantizes the mask into multi-scale token maps to capture hierarchical anatomical structures, (2) a next-scale autoregressive mechanism that progressively predicts next-scale masks to enable sufficient interscale dependencies, and (3) a consensus-aggregation strategy that combines multiple sampled results to generate a more accurate mask, further improving segmentation robustness. Extensive experimental results on two benchmark datasets with different modalities demonstrate that AR-Seg outperforms state-of-the-art methods while explicitly visualizing the intermediate coarse-to-", "filename": "2025_0513.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Tao Chen", "Chenhui Wang", "Zhihao Chen", "Hongming Shan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0514", "x": 3.026, "y": 5.197, "title": "BenchReAD: A Systematic Benchmark for Retinal Anomaly Detection", "abstract": "Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, resulting in a more powerful and stable approach. The benchmark is publicly available at https://github. com/DopamineLcy/BenchReAD.", "filename": "2025_0514.pdf", "year": 2025, "institution": "The Hong Kong Polytechnic University", "country": "China", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0515", "x": 2.162, "y": 1.989, "title": "BiMSRec: A Progressive Image Reconstruction Framework for Medical Image Fusion Guided by Multi-scale Deformation Fields", "abstract": "Traditional multi-modal medical image fusion methods typically employ a hierarchical feature fusion strategy. However, due to inconsistencies among features at different scales, these approaches often introduce unanticipated deformations during the fusion process. Such deformations accumulate through successive registration steps and ultimately result in oscillatory distortions at the fine-detail level. To address this challenge, we propose a progressive image reconstruction framework that is guided by multi-scale deformation fields. Specifically, the input images are first mapped into feature spaces at multiple scales and a deformation field prediction strategy is employed to generate multiple deformation fields that capture both local and global transformation trends simultaneously. Notably, the deformation fields generated across all scales possess the intrinsic capability to directly perform image registration. This capability eliminates the need for sequential propagation of registration outcomes and effectively mitigates cumulative deformation issues. In the image reconstruction phase, we adopt a progressive coarse-to-fine strategy, leveraging multi-scale deformation fields to achieve accurate structure restoration and fusion. Extensive experimental results demonstrate that the proposed method significantly enhances image alignment accuracy and fusion quality across multiple datasets, offering an efficient and robust solution for multi-modal medical image processing.", "filename": "2025_0515.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Nuoer Long", "Kaiwen Yang", "Xinyu Xie", "Zitong Yu", "Tao Tan", "Yue Sun"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0516", "x": 2.86, "y": 2.223, "title": "Blaze3DM: Integrating Triplane Representation with Diffusion for Solving 3D Inverse Problems in Medical Imaging", "abstract": "Solving inverse problems, such as image restoration and reconstruction, is essential in medical imaging. Recently, research on deep learning-based methods for solving 3D data problems has become a focus in the field. Existing diffusion models achieve excellent reconstruction quality but face challenges with volume inconsistency and high computational costs when dealing with 3D medical images. To overcome these challenges, we propose Blaze3DM, a novel approach that combines triplane neural fields with a diffusion model for effective 3D medical image reconstruction. Blaze3DM leverages compact, data-dependent triplane embeddings to ensure volume consistency and significantly improve the computational efficiency of the diffusion model. Furthermore, we introduce a guidance-based sampling method for zero-shot 3D inverse problem solving, enabling Blaze3DM to generate high-fidelity 3D volumes from limited, low-quality 2D slices. We evaluate Blaze3DM on various 3D inverse problem tasks across multiple imaging modalities, including sparse-view CT, limited-angle CT, compressed-sensing MRI, and MRI isotropic super-resolution. The experimental results demonstrate that Blaze3DM not only achieves state-of-the-art reconstruction performance but also markedly improves computational efficiency by approximately 22 to 40 times. Code is available at: https://github.com/Jenn- He/Blaze3DM.", "filename": "2025_0516.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Jia He", "Bonan Li", "Ge Yang", "Ziwen Liu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0517", "x": 2.882, "y": 2.805, "title": "Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations", "abstract": "While the availability of open 3D medical shape datasets is increasing, offering substantial benefits to the research community, we have found that many of these datasets are, unfortunately, disorganized and contain artifacts. These issues limit the development and training of robust models, particularly for accurate 3D reconstruction tasks. In this paper, we examine the current state of available 3D liver shape datasets and propose a solution using diffusion models combined with implicit neural representations (INRs) to augment and expand existing datasets. Our approach utilizes the generative capabilities of diffusion models to create realistic, diverse 3D liver shapes, capturing a wide range of anatomical variations and addressing the problem of data scarcity. Experimental results indicate that our method enhances dataset diversity, providing a scalable solution to improve the accuracy and reliability of 3D liver reconstruction and generation in medical applications. Finally, we suggest that diffusion models can also be applied to other downstream tasks in 3D medical imaging. Our code is available at https://github. com/Khoa-NT/hyperdiffusion_liver", "filename": "2025_0517.pdf", "year": 2025, "institution": "Ghent University", "country": "Belgium", "authors": ["Khoa Tuan Nguyen", "Francesca Tozzi", "Wouter Willaert", "Joris Vankerschaver", "Niki Rashidian", "Wesley De Neve"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0518", "x": 2.132, "y": 2.422, "title": "Boosting Medical Image Synthesis via Registration-Guided Consistency and Disentanglement Learning", "abstract": "Medical image synthesis remains challenging due to misalignment noise during training. Existing methods have attempted to address this challenge by incorporating a registration-guided module. However, these methods tend to overlook the task-specific constraints on the synthetic and registration modules, which may cause the synthetic module to still generate spatially aligned images with misaligned target images during training, regardless of the registration module's function. Therefore, this paper proposes registration-guided consistency and incorporates disentanglement learning for medical image synthesis. The proposed registration-guided consistency architecture fosters taskspecificity within the synthetic and registration modules by applying identical deformation fields before and after synthesis, while enforcing output consistency through an alignment loss. Moreover, the synthetic module is designed to possess the capability of disentangling anatomical structures and specific styles across various modalities. An anatomy consistency loss is introduced to further compel the synthetic module to preserve geometrical integrity within latent spaces. Experiments conducted on both an in-house abdominal CECT-CT dataset and a publicly available pelvic MR-CT dataset have demonstrated the superiority of the proposed method. The code is available at: https://github.com/ pupuchuan/RegConDIS.", "filename": "2025_0518.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Chuanpu Li", "Zeli Chen", "Yiwen Zhang", "Liming Zhong", "Wei Yang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0519", "x": 3.997, "y": 2.895, "title": "CardiacFlow: 3D+t Four-Chamber Cardiac Shape Completion and Generation via Flow Matching", "abstract": "Learning 3D+t shape completion and generation from multiview cardiac magnetic resonance (CMR) images requires a large amount of high-resolution 3D whole-heart segmentations (WHS) to capture shape priors. In this work, we leverage flow matching techniques to learn deep generative flows for augmentation, completion, and generation of 3D+t shapes of four cardiac chambers represented implicitly by segmentations. Firstly, we introduce a latent rectified flow to generate 3D cardiac shapes for data augmentation, learned from a limited number of 3D WHS data. Then, a label completion network is trained on both real and synthetic data to reconstruct 3D+t shapes from sparse multi-view CMR segmentations. Lastly, we propose CardiacFlow, a novel one-step generative flow model for efficient 3D+t four-chamber cardiac shape generation, conditioned on the periodic Gaussian kernel encoding of time frames. The experiments on the WHS datasets demonstrate that flow-based data augmentation reduces geometric errors by 16% in 3D shape completion. The evaluation on the UK Biobank dataset validates that CardiacFlow achieves superior generation quality and periodic consistency compared to existing baselines. The code of CardiacFlow is released publicly at https://github.com/m-qiang/CardiacFlow.", "filename": "2025_0519.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Qiang Ma", "Qingjie Meng", "Mengyun Qiao", "Paul M Matthews", "Declan P O’regan", "Wenjia Bai"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0520", "x": 2.882, "y": 3.658, "title": "Controllable Latent Diffusion Model to Evaluate the Performance of Cardiac Segmentation Methods", "abstract": "In medical imaging, the evaluation of segmentation methods remains confined to a limited set of metrics (e.g. Dice coefficient and Hausdorff distance) and annotated datasets with restricted size and diversity. Besides, segmentation is often a preliminary step for extracting relevant biomarkers, accentuating the need to redirect evaluation efforts towards this objective. To address this, we propose an original methodology to evaluate segmentation methods, based on the generation of realistic synthetic images with explicitly controlled biomarker values. Image synthesis is based on Stable Diffusion, conditioned by either a 1D vector (clinical attributes or latent representation) or a 2D feature map (latent representation). We demonstrate the relevance of this approach in the context of myocardial lesions observed in cardiac late Gadolinium enhancement MR images, controlling the image synthesis with segmentation masks or infarct-related attributes, among which size and transmurality. We evaluate it on two datasets of 3557 and 932 pairs of 2D images and segmentation masks, the second dataset being for testing only. Our conditioning not only leads to very realistic synthetic images but also brings varying levels of task complexity, a must-have to better assess the readiness of segmentation methods.", "filename": "2025_0520.pdf", "year": 2025, "institution": "Universite", "country": null, "authors": ["Romain Deleat-Besson", "Celia Goujat", "Olivier Bernard", "Pierre Croisille", "Magalie Viallon", "Nicolas Duchateau"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0521", "x": 3.944, "y": 2.525, "title": "CortexGen: A Geometric Generative Framework for Realistic Cortical Surface Generation Using Latent Flow Matching", "abstract": "Geometric deep learning has shown great potential for cortical surface analysis, but its performance often depends on a largescale training set of cortical surfaces, which are traditionally derived from MRI scans through complex and time-consuming preprocessing pipelines. Although deep learning-based surface reconstruction methods have streamlined this process, they still rely on MRI data, limiting the availability of training data. To address this, we propose CortexGen, a geometric generative framework that synthesizes highly realistic cortical surfaces without requiring MRI scans. CortexGen employs geometric variational encoders to map cortical surfaces into a latent space, where latent flow matching models efficiently learn the true data distribution. This enables a two-stage cortical surface synthesis process: first, deforming an icosahedron-discretized sphere into a coarse cortical surface, and second, refining it into a high-resolution surface. Experiments show that CortexGen generates diverse, realistic cortical surfaces with 163,842 vertices in just 1.4 seconds per surface. Using these synthetic surfaces as augmented training data significantly improved learning-based cortical surface parcellation in few-shot settings. Our code and pretrained models are available at https://github.com/ladderlab-xjtu/CortexGen.", "filename": "2025_0521.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Yuanzhuo Zhu", "Kehan Li", "Jianhua Ma", "Chunfeng Lian", "Fan Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0522", "x": 0.416, "y": 4.639, "title": "CSAL-3D: Cold-Start Active Learning for 3D Medical Image Segmentation via SSL-Driven Uncertainty-Reinforced Diversity Sampling", "abstract": "Active Learning (AL) is a promising solution in medical image segmentation to reduce annotation costs by selecting the most informative training samples. However, traditional warm-start AL methods rely on iterative querying and fail to address the cold-start dilemma. While Cold-Start Active Learning (CSAL) attempts to resolve this, current methods are limited to 2D images and neglect Self-Supervised Learning (SSL)'s potential for uncertainty estimation in AL. Moreover, while hybrid uncertainty-diversity sampling has been discussed in warm-start setting, the efficacy of this combined approach is not explored in CSAL. In this paper, we present CSAL-3D: a novel Cold-Start Active Learning framework for 3D medical image segmentation. Firstly, a CSAL-adapted SSL pipeline for ensemble-based uncertainty estimation and 3D-oriented feature extraction is proposed. Secondly, a novel Uncertainty-Reinforced Diversity Sampling (URDS) strategy is introduced, which synthesizes cluster representativeness and sample-level uncertainty in a hierarchical process. It can select samples that are both uncertain and representative in one shot. Experiments on Brain Tumor, Heart and Spleen organ segmentation tasks from CT or MRI 3D images show that CSAL-3D outperforms other state-of-the-art CSAL counterparts with an avergae Dice of 87.03%. The source code is available at https://github.com/HiLab-git/CSAL-3D.", "filename": "2025_0522.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Ning Zhu", "Ping Ye", "Lanfeng Zhong", "Qiang Yue", "Shaoting Zhang", "Guotai Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0523", "x": 3.179, "y": 2.189, "title": "D2Diff: A Dual-Domain Diffusion Model for Accurate Multi-Contrast MRI Synthesis", "abstract": "Multi-contrast MRI synthesis is inherently challenging due to the complex and nonlinear relationships among different contrasts. Each MRI contrast highlights unique tissue properties, but their complementary information is difficult to exploit due to variations in intensity distributions and contrast-specific textures. Existing methods for multicontrast MRI synthesis primarily utilize spatial domain features, which capture localized anatomical structures but struggle to model global intensity variations and distributed patterns. Conversely, frequencydomain features provide structured inter-contrast correlations but lack spatial precision, limiting their ability to retain finer details. To address this, we propose a dual-domain learning framework that integrates spatial and frequency domain information across multiple MRI contrasts for enhanced synthesis. Our method employs two mutually trained denoising networks, one conditioned on spatial domain and the other on frequency domain contrast features through a shared critic network. Additionally, an uncertainty-driven mask loss directs the model's focus toward more critical regions, further improving synthesis accuracy. Extensive experiments show that our method outperforms state-of-the-art (SOTA) baselines, and the downstream segmentation performance highlights the diagnostic value of the synthetic results. Code and model hyperparameters are available at https://github.com/sanuwanihewa/D2Diff.", "filename": "2025_0523.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Sanuwani Dayarathna", "Himashi Peiris", "Kh Tohidul Islam", "Tien-Tsin Wong", "Zhaolin Chen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0524", "x": 7.185, "y": 4.715, "title": "DGM: Disentangled Generative Model for Detecting AD Individualized Pathological Changes via Pseudo-Healthy Synthesis", "abstract": "Alzheimer's disease (AD) is a complicated, heterogeneous neurodegenerative disease associated with cognitive decline, behavioral impairment, and brain atrophy. Detecting individualized pathological changes from cognitive normal (CN) to AD is critical for targeted treatment. Current existing methods face challenges, including biases toward specific pathology profiles. To this end, we proposed a disentangled generative model (DGM) to generate pseudo-healthy images and disease-related residual maps that accurately detect universal pathological changes. The framework of DGM consists of three modules: pseudohealthy MRI synthesis, residual map synthesis, and input reconstruction modules. We take into account both the healthiness and subject identity to validate the biological validity of synthetic pseudo-healthy images. Our experiments demonstrated the effectiveness of the DGM in reconstructing healthy brain anatomy, preserving subject identity, and highlighting its direct application in anomaly pathological detection across the transitions from CN to MCI and from CN to AD. Code is available at https:// github.com/zhonghuajiuzhou12138/DDGM_disease_stage_model.", "filename": "2025_0524.pdf", "year": 2025, "institution": "Beijing University of Posts and Telecommunications", "country": "China", "authors": ["Zhuangzhuang Li", "Kun Zhao", "Dong Wang", "Yong Liu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0525", "x": 2.959, "y": 2.4, "title": "DisDiff: Disentanglement Diffusion Network for MR Imaging Translation", "abstract": "Multi-modal MR imaging plays a crucial role in clinical diagnosis and medical research. However, its widespread adoption is hindered by significant time and hardware costs. Medical image translation, which aims to synthesize missing modalities from available data, presents a promising solution. Nevertheless, existing models often struggle to maintain the structural consistency required for clinical applications. We introduced a disentanglement diffusion network -DisDiff, a novel disentangled adversarial diffusion framework designed to address these challenges. DisDiff incorporates a Disentangled module that decouples content and style factors within image features, thereby enabling the generation of anatomically precise images. Conditioned on disentangled representations, compared to traditional diffusion-based models, DisDiff not only accelerates the learning process, but also improves image quality and enhances training efficiency. In addition, we proposed a content discriminator module to further enforce anatomical consistency, effectively addressing the lack of explicit structural guidance in conventional diffusion models. Experimental evaluations on multi-contrast MRI translation demonstrate that DisDiff substantially outperforms existing methods in both image quality and structural preservation, positioning it as a promising solution for real-world clinical applications.", "filename": "2025_0525.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Yipin Zhang", "Ziqi Yu", "Xiange Zhang", "Shengjie Zhang", "Xiang Chen", "Haibo Yang", "Xiao-Yong Zhang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0526", "x": 4.076, "y": 2.77, "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation", "abstract": "Accurate analysis of cardiac motion is crucial for evaluating cardiac function. While dynamic cardiac magnetic resonance imaging (CMR) can capture detailed tissue motion throughout the cardiac cycle, the fine-grained 4D cardiac motion tracking remains challenging due to the homogeneous nature of myocardial tissue and the lack of distinctive features. Existing approaches can be broadly categorized into imagebased and representation-based, each with its limitations. Image-based methods, including both traditional and deep learning-based registration approaches, either struggle with topological consistency or rely heavily on extensive training data. Representation-based methods, while promising, often suffer from loss of image-level details. To address these limitations, we propose Dynamic 3D Gaussian Representation (Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our method simultaneously optimizes cardiac structure and motion in a self-supervised manner, eliminating the need for extensive training data or point-to-point correspondences. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous motion representation with image-space alignment while preserving both topological and temporal consistency. Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art deep learning-based diffeomorphic registration methods in tracking accuracy. The code will be available in https://github.com/windrise/Dyna3DGR.", "filename": "2025_0526.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "China", "authors": ["Xueming Fu", "Pei Wu", "Yingtai Li", "Xin Luo", "Zihang Jiang", "Junhao Mei", "Jian Lu", "Gao-Jun Teng", "Shaohua Kevin Zhou"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0527", "x": 1.279, "y": 2.252, "title": "EUReg: End-to-End Framework for Efficient 2D-3D Ultrasound Registration", "abstract": "Ultrasound (US) is widely used for surgical navigation, and real-time intraoperative 2D US to preoperative 3D US registration is crucial. However, existing methods either lack accuracy, suffer from low efficiency, or are highly prone to overfitting. To address these challenges, we propose a novel and Efficient end-to-end real-time 2D-3D US registration framework (EUReg). Specifically, we introduce a cross dimension flow estimator (CDFE) that is both learn-free and differentiable, along with a decoupled transformation prediction (DTP) network. Furthermore, we design a flow loss to supervise the coarse deformation field, effectively decoupling the entire registration process into four sequential steps: feature extraction, coarse deformation field estimation, translation estimation, and rotation estimation. In addition, we improve the differentiable 2D-3D sampling process. We evaluate our framework through comparative, ablation, and exploratory experiments on two public datasets for cardiac and prostate US. Experimental results demonstrate that our method achieves a registration speed exceeding 100 frames per second (FPS) while maintaining high accuracy, meeting the requirements for clinical interventional procedures. Moreover, our exploration reveals that registration accuracy improves when each frame within the volume is larger than the target frame. Our code is publicly available at https:// github.com/ZAX130/EUReg.", "filename": "2025_0527.pdf", "year": 2025, "institution": "Shenzhen University Medical School", "country": "China", "authors": ["Haiqiao Wang", "Yi Wang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0528", "x": 2.755, "y": 2.203, "title": "Exploring the Feasibility of Zero-Shot Super-Resolution in Preclinical Imaging", "abstract": "Preclinical imaging studies are vital to the research, development, and evaluation of new medical therapies. Images acquired during these studies often have high in-plane resolution but low through-plane resolution, resulting in highly anisotropic volumes that hamper downstream volumetric analysis. Additionally, since there are no image acquisition standards across studies, training data for conventional supervised super-resolution (SR) methods is limited. In this work, we compare two SR methods that do not require additional training data. The first is ECLARE, a self-SR approach that creates its own training data from inplane patches drawn from the anisotropic volume. The second is Biplanar Denoising diffusion null space model (DDNM) Averaging (BiDA), a proposed method leveraging two independently pre-trained denoising diffusion probabilistic models and the DDNM posterior sampling technique. We evaluate both methods first on rat data at two scale factors (.2.5× and .5×) and compare signal recovery and downstream task performance. We further evaluate these methods on a different species (mice) to measure their generalizability. Both methods experimentally resulted in good signal recovery performance, but only the images super-resolved by BiDA were accurately skullstripped downstream. Although both methods performed well on the in-domain rat data, BiDA did not fully generalize to the out-of-domain mouse data.", "filename": "2025_0528.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Omar A M Gharib", "Samuel W Remedios", "Blake E Dewey", "Jerry L Prince", "Aaron Carass"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0529", "x": 1.87, "y": 5.484, "title": "From Generalist to Specialist: Distilling a Mixture of Foundation Models for Domain-Specific Medical Image Segmentation", "abstract": "Segmentation foundation models (SFMs) hold promise for medical image analysis, but their direct clinical application is limited by computational cost, potentially suboptimal accuracy, and fairness concerns. In this paper, we propose a novel framework to address these challenges by distilling knowledge from a heterogeneous ensemble of pre-trained SFMs, generating specialized, high-performance models for domain-specific medical image segmentation. Unlike existing single-SFM approaches, our methodology leverages the collective intelligence of diverse SFMs to enhance accuracy, fairness, and efficiency. A key contribution is a ground-truth-free knowledge distillation strategy using the ensemble's aggregate predictions on unlabeled data to minimize reliance on manual annotation. Evaluated on a large, diverse dataset of CT and MRI scans from 702 individuals, our distilled model significantly outperforms individual SFMs and their ensemble average, achieving state-ofthe-art segmentation accuracy, improved fairness across demographics (sex, age, BMI), and substantially reduced computational cost. These results offer a practical paradigm for leveraging foundation models in real-world clinical settings, mitigating key SFM limitations.", "filename": "2025_0529.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Qing Li", "Yizhe Zhang", "Shengxiao Yang", "Qirong Li", "Zian Wang", "Junhong Liu", "Haoyang Zhang", "Shuo Wang", "Chengyan Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0530", "x": 3.779, "y": 2.503, "title": "Generating Novel Brain Morphology by Deforming Learned Templates", "abstract": "Designing generative models for 3D structural brain MRI that synthesize morphologically-plausible and attribute-specific (e.g., age, sex, disease state) samples is an active area of research. Existing approaches based on frameworks like GANs or diffusion models synthesize the image directly, which may limit their ability to capture intricate morphological details. In this work, we propose a 3D brain MRI generation method based on state-of-the-art latent diffusion models (LDMs), called MorphLDM, that generates novel images by applying synthesized deformation fields to a learned template. Instead of using a reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a latent embedding derived from both an image and a learned template that is itself the output of a template decoder; this latent is passed to a deformation field decoder, whose output is applied to the learned template. A registration loss is minimized between the original image and the deformed template with respect to the encoder and both decoders. Empirically, our approach outperforms generative baselines on metrics spanning image diversity, adherence with respect to input conditions, and voxel-based morphometry. Our code is available at https:// github.com/alanqrwang/morphldm", "filename": "2025_0530.pdf", "year": 2025, "institution": "Stanford University", "country": "USA", "authors": ["Alan Q Wang", "Fangrui Huang", "Bailey Trang", "Wei Peng", "Mohammad Abbasi", "Kilian Pohl", "Mert R Sabuncu", "Ehsan Adeli"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0531", "x": 0.919, "y": 1.098, "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "abstract": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains challenging, especially for fine structures like root apices, which is critical for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel approach that unifies instance detection and multi-class segmentation into a single step tailored to improve root segmentation. Our method integrates a Statistical Shape Model of dentition as a geometric prior, capturing anatomical context and morphological consistency without enforcing restrictive adjacency constraints. We leverage a deep watershed method, modeling each tooth as a continuous 3D energy basin encoding voxel distances to boundaries. This instanceaware representation ensures accurate segmentation of narrow, complex root apices. Trained on publicly available CBCT scans from a single center, our method is evaluated on external test sets from two in-house and two public medical centers. GEPAR3D achieves the highest overall segmentation performance, averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the second-best method) and increasing recall to 95.2% (+9.5%) across all test sets. Qualitative analyses demonstrated substantial improvements in root segmentation quality, indicating significant potential for more accurate root resorption assessment and enhanced clinical decision-making in orthodontics. We provide the implementation and dataset at github.com/tomek1911/GEPAR3D.", "filename": "2025_0531.pdf", "year": 2025, "institution": "Sano Centre for Computational Medicine", "country": "Poland", "authors": ["Tomasz Szczepański", "Szymon Płotka", "Michal K Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemysław Korzeniowski", "Tomasz Trzciński", "Arkadiusz Sitek"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0532", "x": 1.177, "y": 3.805, "title": "Hybrid Local-Window-Attention–Assisted U-Net Model for Multimodal Medical-Image Segmentation", "abstract": "Multimodal image segmentation has been gaining significance with the advancement of deep learning and increasing diversity of datasets. Although researchers have been actively exploring multimodal U-Net structures, improvements in the segmentation of fine features in medical images remain limited. In this study, we propose a novel U-Net model based on hybrid local-window attention, for multimodal medical-image segmentation. This study aims to effectively analyze overlapping brain-tumor lesions and extract essential information from different magnetic-resonance-imaging modalities for more precise segmentation. The proposed hybrid local-window-attention mechanism comprises local-window self-attention and cross-attention, disentangled representation learning (DRL), and region-aware contrastive learning (RCL) modules. We apply local-window self-attention for achieving efficiency over global attention, and local-window cross-attention between the encoder and decoder to enhance the modality interaction. The hybrid localwindow-attention structure extracts modality-specific features, whereas DRL preserves modality and lesion information. RCL utilizes the contrast loss within the lesions to improve segmentation. We perform comprehensive experiments on the BraTS 2023 and BraTS 2024 datasets and confirm that the proposed model provides enhanced medical-image segmentation performance, compared with U-Net based benchmark models without pre-training.", "filename": "2025_0532.pdf", "year": 2025, "institution": "Sejong University", "country": "Korea", "authors": ["Jiwon Kim", "Seyong Jin", "Yeonwoo Noh", "Hyeonjoon Moon", "Minwoo Lee", "Wonjong Noh"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0533", "x": 2.625, "y": 2.509, "title": "Hybrid State-Space Models and Denoising Training for Unpaired Medical Image Synthesis", "abstract": "Unsupervised medical image synthesis faces significant challenges due to the absence of paired data, often resulting in global anatomical distortions and local detail loss. Existing approaches primarily rely on convolutional neural networks (CNNs) for local feature extraction; however, their limited receptive fields hinder effective global anatomical modeling. Recently, Vision Mamba (ViM) has demonstrated efficient global modeling capabilities via state-space models, yet its potential in this task remains unexplored. To address this gap, we propose a hybrid architecture, CRAViM (Convolutional Residual Attention Vision Mamba), which integrates the precise local anatomical feature extraction of CNNs with the long-range dependency modeling of state-space models, thereby enhancing the structural fidelity and detail preservation of synthesized images. Furthermore, we introduce a cycle denoise consistencybased training framework that incorporates transport loss and random denoise loss to jointly optimize global structural constraints and local detail restoration. Experimental results on two public medical imaging datasets demonstrate that CRAViM achieves notable improvements in key metrics such as SSIM and NMI over existing methods, effectively maintaining global anatomical consistency while enhancing local details, thus validating the effectiveness of our approach. The code for this paper can be found at https://github.com/jmzhang-cv/CRAViM.", "filename": "2025_0533.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Junming Zhang", "Shancheng Jiang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0534", "x": 1.536, "y": 2.021, "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "abstract": "Deformable medical image registration is an essential task in computer-assisted interventions. This problem is particularly relevant to oncological treatments, where precise image alignment is necessary for tracking tumor growth, assessing treatment response, and ensuring accurate delivery of therapies. Recent AI methods can outperform traditional techniques in accuracy and speed, yet they often produce unreliable deformations that limit their clinical adoption. In this work, we address this challenge and introduce a novel implicit registration framework that can predict accurate and reliable deformations. Our insight is to reformulate image registration as a signal reconstruction problem: we learn a kernel function that can recover the dense displacement field from sparse keypoint correspondences. We integrate our method in a novel hierarchical architecture, and estimate the displacement field in a coarse-to-fine manner. Our formulation also allows for efficient refinement at test time, permitting clinicians to easily adjust registrations when needed. We validate our method on challenging intra-patient thoracic and abdominal zero-shot registration tasks, using public as well as internal datasets from the Innsbruck University Hospital. Our method not only shows competitive accuracy to state-of-the-art approaches, but also bridges the generalization gap between implicit and explicit registration techniques. In particular, our method generates deformations that better preserve anatomical relationships and matches the performance of specialized commercial systems, underscoring its potential for clinical adoption.", "filename": "2025_0534.pdf", "year": 2025, "institution": "University of Innsbruck", "country": "Austria", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0535", "x": -0.084, "y": 4.962, "title": "Improved Baselines with Synchronized Encoding for Universal Medical Image Segmentation", "abstract": "Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. Code and checkpoints are available at https://github.com/Hhankyangg/SyncSAM.", "filename": "2025_0535.pdf", "year": 2025, "institution": "Xi'an Jiaotong University", "country": "China", "authors": ["Sihan Yang", "Jiadong Feng", "Xuande Mi", "Haixia Bi", "Hai Zhang", "Jian Sun"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0536", "x": 3.208, "y": 1.256, "title": "IMREPET: Implicit Neural Representation for Unsupervised Dynamic PET Reconstruction", "abstract": "Deep image prior (DIP) has become an important approach to unsupervised reconstruction of Positron Emission Tomography (PET) images. In the setting of dynamic PET, however, its performance is limited by the frame-by-frame reconstruction, computational cost, and the fixed-size discrete representation of PET images. To address these challenges, we propose IMREPET, a novel dynamic PET reconstruction method based on implicit neural networks (INR). By incorporating temporal information directly into INR's parameterization of dynamic PET images, we overcome the limitation of frame-by-frame reconstructions without the need of complex algorithms or regularization. Results on simulated and real-data experiments showed that IMREPET enabled rapid, high-quality reconstruction with improved signal-to-noise ratio and enhanced image detail recovery, while drastically reducing computation time compared to DIP baselines. The resolution-agnostic nature of INR further allowed IMREPET to reconstruct PET images at any resolution. These results show the feasibility of IMREPET as a robust and efficient solution for dynamic PET imaging.", "filename": "2025_0536.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Kailong Fan", "Yubo Ye", "Huafeng Liu", "Linwei Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0537", "x": 3.017, "y": 3.321, "title": "Lesion-Aware Post-training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion", "abstract": "Image-to-Image translation models can help mitigate various challenges inherent to medical image acquisition. Latent diffusion models (LDMs) leverage efficient learning in compressed latent space and constitute the core of state-of-the-art generative image models. However, this efficiency comes with a trade-off, potentially compromising crucial pixel-level detail essential for high-fidelity medical images. This limitation becomes particularly critical when generating clinically significant structures, such as lesions, which often occupy only a small portion of the image. Failure to accurately reconstruct these regions can severely impact diagnostic reliability and clinical decision-making. To overcome this limitation, we propose a novel post-training framework for LDMs in medical image-to-image translation by incorporating lesion-aware medical pixel space objectives. This approach is essential, as it not only enhances overall image quality but also improves the precision of lesion delineation. We evaluate our framework on brain CT-to-MRI translation in acute ischemic stroke patients, where early and accurate diagnosis is critical for optimal treatment selection and improved patient outcomes. While diffusion MRI is the gold standard for stroke diagnosis, its clinical utility is often constrained by high costs and low accessibility. Using a dataset of 817 patients, we demonstrate that our framework improves overall image quality and enhances lesion delineation when synthesizing DWI and ADC images from CT perfusion scans, outperforming existing image-to-image translation models. Furthermore, our post-training strategy is easily adaptable to pre-trained LDMs and exhibits substantial potential for broader applications across diverse medical image translation tasks.", "filename": "2025_0537.pdf", "year": 2025, "institution": "Seoul National University", "country": "Republic of Korea", "authors": ["Junhyeok Lee", "Hyunwoong Kim", "Hyungjin Chung", "Heeseong Eom", "Joon Jang", "Chul-Ho Sohn", "Kyu Sung Choi"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0538", "x": 1.68, "y": 4.13, "title": "Leveraging Semantic Asymmetry for Accurate Gross Tumor Volume Segmentation of Nasopharyngeal Carcinoma in Planning CT", "abstract": "In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues requires radiation oncologists to delineate the tumors with additional reference from MRI images manually. In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor Segmentation (SATS) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the presence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of deep features to semantic asymmetries. Extensive experiments demonstrate that the proposed SATS achieves the leading NPC GTV segmentation performance in both internal and external testing.", "filename": "2025_0538.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Zi Li", "Ying Chen", "Zeli Chen", "Yanzhou Su", "Tai Ma", "Tony C W Mok", "Yan-Jie Zhou", "Yunhao Bai", "Zhilin Zheng", "Le Lu", "Yirui Wang", "Jia Ge", "Senxiang Yan", "Xianghua Ye", "Dakai Jin"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0539", "x": 3.913, "y": 2.328, "title": "Lifespan Cortical Surface Reconstruction from Thick-Slice Clinical MRI", "abstract": "Accurately characterizing brain morphological changes throughout human lifespan is crucial for understanding brain development, aging, and disorders. At the core of this endeavor lies cortical surface reconstruction (CSR), which underpins the computation of essential brain morphological features. However, existing CSR methods face two major limitations. First, cortical surfaces are typically reconstructed from 3D MRI data with high isotropic resolution, which is confined to research settings. In contrast, clinical MRI scans are collected with high in-plane but low through-plane resolution. Second, most CSR pipelines are designed either for adult or pediatric populations, restricting their applicability across the lifespan. To this end, we develop a deep learning framework that harnesses MRI super-resolution (SR) as a bridging mechanism, leveraging the complementary information SR provides to jointly perform SR and CSR with a coarse-to-fine strategy. Specifically, we introduce a dual-decoder age-conditioned temporal attention network (DATAN) with a shared encoder, which simultaneously performs CSR and SR from thick-slice clinical MRI. By jointly training on the SR task, the shared encoder captures richer cortical features, thereby enhancing CSR performance. Through a two-stage coarse-to-fine approach, incremental refinements in the SR output progressively restore fine-scale details otherwise lost in low-resolution scans, ultimately improving CSR fidelity. Furthermore, to facilitate accurate CSR across the lifespan, we exploit the age-conditioning module of our framework and train our model on a large, diverse MRI dataset spanning ages from 1 to 100 years. Experimental results demonstrate that our method, despite requiring only thick-slice clinical MRI scans, achieves consistently improved CSR performance across the entire human lifespan.", "filename": "2025_0539.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Xiuyu Dong", "Kaibo Tang", "Dan Hu", "Zhengwang Wu", "Li Wang", "Weili Lin", "Gang Li"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0540", "x": 0.853, "y": 4.647, "title": "M$$^{3}$$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-supervised Medical Image Segmentation", "abstract": "Data augmentation methods inspired by CutMix have demonstrated significant potential in recent semi-supervised medical image segmentation tasks. However, these approaches often apply Cut-Mix operations in a rigid and inflexible manner, while paying insufficient attention to feature-level consistency constraints. In this paper, we propose a novel method called Mutual Mask Mix with High-Low level feature consistency (M 3 HL) to address the aforementioned challenges, which consists of two key components: 1) M 3 : An enhanced data augmentation operation inspired by the masking strategy from Masked Image Modeling (MIM), which advances conventional CutMix through dynamically adjustable masks to generate spatially complementary image pairs for collaborative training, thereby enabling effective information fusion between labeled and unlabeled images. 2) HL: A hierarchical consistency regularization framework that enforces high-level and low-level feature consistency between unlabeled and mixed images, enabling the model to better capture discriminative feature representations. Our method achieves state-of-the-art performance on widely adopted medical image segmentation benchmarks including the ACDC and LA datasets. Source code is available at https://github.com/PHPJava666/M3HL.", "filename": "2025_0540.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Yajun Liu", "Zenghui Zhang", "Jiang Yue", "Weiwei Guo", "Dongying Li"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0541", "x": 2.759, "y": 1.445, "title": "MAK-GAN: Multi-level Adaptive Convolutional Kernels for Asymmetric Multi-modal PET Reconstruction", "abstract": "Positron emission tomography (PET) reconstruction from low-dose to standard-dose acquisitions poses a significant challenge in medical imaging. While integrating Magnetic Resonance Imaging (MRI) for complementary guidance shows promise for enhancing reconstruction fidelity, current multi-modal approaches typically treat PET and MRI uniformly, neglecting their inherent asymmetry within the multi-modal context. This leads to insufficient utilization of anatomical guidance provided by MRI and neglects the unique metabolic characteristics of PET. To address these limitations, we propose MAK-GAN, a novel Generative Adversarial Network (GAN) that incorporates Multi-level Adaptive Kernels to distinguish feature extraction and interaction strategies between the primary (PET) and auxiliary (MRI) modalities in the asymmetric multi-modal PET reconstruction task. Specifically, we design a Multi-Kernel Extraction (MKE) block for both PET and MRI branches, replacing linear projections in vanilla Transformers with hierarchical multi-kernel convolutions. This enables efficient extraction of modality-specific features at multiple scales while reducing computational overhead. Subsequently, we asymmetrically introduce an Adaptive-Kernel Interaction (AKI) block in the PET branch. This block integrates self-and crossattention modules to dynamically generate weights for adaptive kernels, preserving PET-specific characteristics while utilizing MRI's anatomical information. Finally, we incorporate two PET-centric optimization strategies to prioritize PET during reconstruction: a residual connection for direct LPET-to-SPET mapping, and an edge-aware consistency loss to enforce structural coherence. Experiments demonstrate superiority on two PET/MRI datasets.", "filename": "2025_0541.pdf", "year": 2025, "institution": "Sichuan University", "country": "China", "authors": ["Xinyi Zeng", "Pinxian Zeng", "Yan Wang", "Jiaqi Cui", "Luping Zhou", "Caiwen Jiang", "Han Zhang", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0542", "x": 4.006, "y": 2.878, "title": "Mask2Surface: Motion Correction and Super-Resolution for Cardiac Surface Reconstruction Using Latent Diffusion", "abstract": "Cardiac magnetic resonance (CMR) imaging is one of the most important imaging modalities for cardiac analysis. However, shortaxis CMR imaging can only produce a sparse set of 2D images with an extremely low inter-slice resolution. Moreover, these 2D slices are usually misaligned due to the respiratory and cardiac motion of the patients, strongly affecting the diagnosis and intervention procedures for cardiac diseases. Deep learning-based approaches have been proposed to tackle these problems, but they mostly focus on voxel representation, yielding rough cardiac surfaces that are difficult to analyze. Therefore, we propose a deep learning-based method to perform CMR motion correction and super-resolution simultaneously to acquire high-fidelity left ventricular myocardial surfaces. Given a set of 2D misaligned sparse segmentation masks of the left ventricular myocardium, our method first leverages an end-to-end convolutional neural network to correct and super-resolve the masks to approach the distribution of the motion-free and high-resolution masks. Then, the acquired super-resolved segmentation masks are estimated to form coarse signed distance grids, guiding a latent diffusion model to produce the corresponding high-fidelity myocardial surfaces. The superior performances of our approach are testified through comprehensive experiments in both simulation and clinical settings.", "filename": "2025_0542.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Zichen Zhang", "Zhentao Liu", "Zeng Zhang", "Zhiming Cui"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0543", "x": 3.191, "y": 1.949, "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction", "abstract": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, undersampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at https://github.com/Zolento/MDPG.", "filename": "2025_0543.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Lingtong Zhang", "Mengdie Song", "Xiaohan Hao", "Huayu Mai", "Bensheng Qiu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0544", "x": 3.198, "y": 1.864, "title": "Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions", "abstract": "In recent years, accelerated MRI reconstruction based on deep learning has led to significant improvements in image quality with impressive results for high acceleration factors. However, from a clinical perspective image quality is only secondary; much more important is that all clinically relevant information is preserved in the reconstruction from heavily undersampled data. In this paper, we show that existing techniques, even when considering resampling for diffusion-based reconstruction, can fail to reconstruct small and rare pathologies, thus leading to potentially wrong diagnosis decisions (false negatives). To uncover the potentially missing clinical information we propose \"Semantically Diverse Reconstructions\" (SDR), a method which, given an original reconstruction, generates novel reconstructions with enhanced semantic variability while all of them are fully consistent with the measured data. To evaluate SDR automatically we train an object detector on the fastMRI+ dataset. We show that SDR significantly reduces the chance of false-negative diagnoses (higher recall) and improves mean average precision compared to the original reconstructions. The code is available on https://github. com/NikolasMorshuis/SDR", "filename": "2025_0544.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Jan Nikolas Morshuis", "Christian Schlarmann", "Thomas Küstner", "Christian F Baumgartner", "Matthias Hein"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0545", "x": -0.197, "y": 4.859, "title": "MoE-SAM: Enhancing SAM for Medical Image Segmentation with Mixture-of-Experts", "abstract": "Recent adaptations of the powerful and promptable Segment Anything Model (SAM), pretrained on a large-scale dataset, have shown promising results in medical image segmentation. However, existing methods fail to fully leverage the intermediate features from SAM's image encoder, limiting its adaptability. To address this, we introduce MoE-SAM, a novel approach that enhances SAM by incorporating a Mixture-of-Experts (MoE) during adaptation. Central to MoE-SAM is a MoE-driven feature enhancing block, which uses learnable gating functions and expert networks to select, refine, and fuse latent features from multiple layers of SAM's image encoder. By combining these features, the model creates a more robust image embedding that captures both low-level local and high-level global information. This comprehensive embedding facilitates prompt embedding generation and mask decoding, thereby enabling more effective self-prompting segmentation. Extensive evaluations across four benchmark medical image segmentation tasks show that MoE-SAM outperforms both task-specialized models and other SAM-based approaches, achieving state-of-the-art segmentation accuracy. The code is available at: https://github.com/Asphyxiate-Rye/ E-SAM.", "filename": "2025_0545.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Ruocheng Li", "Lei Wu", "Jingjun Gu", "Qi Xu", "Wanyi Chen", "Xiaoxu Cai", "Jiajun Bu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0546", "x": 0.888, "y": 3.961, "title": "MSWAL: 3D Multi-class Segmentation of Whole Abdominal Lesions Dataset", "abstract": "With the significantly increasing incidence and prevalence of abdominal diseases, there is a need to embrace greater use of new innovations and technology for the diagnosis and treatment of patients. Although deep-learning methods have notably been developed to assist radiologists in diagnosing abdominal diseases, existing models have the restricted ability to segment common lesions in the abdomen due to missing annotations for typical abdominal pathologies in their training datasets. To address the limitation, we introduce MSWAL, the first 3D Multi-class Segmentation of the Whole Abdominal Lesions dataset, which broadens the coverage of various common lesion types, such as gallstones, kidney stones, liver tumors, kidney tumors, pancreatic cancer, liver cysts, and kidney cysts. With CT scans collected from 694 patients (191,417 slices) of different genders across various scanning phases, MSWAL demonstrates strong robustness and generalizability. The transfer learning experiment from MSWAL to two public datasets, LiTS and KiTS, effectively demonstrates consistent improvements, w ith Dice Similarity Coefficient (DSC) increase of 3.00% for liver tumors and 0.89% for kidney tumors, demonstrating that the comprehensive annotations and diverse lesion types in MSWAL facilitate effective learning across different domains and data distributions. Furthermore, we propose Inception nnU-Net, a novel segmentation framework that effectively integrates an Inception module with the nnU-Net architecture to extract information from different receptive fields, achieving significant enhancement in both voxel-level DSC and region-level F1 compared to the cutting-edge public algorithms on MSWAL. Our dataset Z. Wu, Q. Zhao and M. Hu-Equal contribution. Feilong T ang is the project leader.", "filename": "2025_0546.pdf", "year": 2025, "institution": "Xi'an Jiaotong-Liverpool University", "country": "China", "authors": ["Zhaodong Wu", "Qiaochu Zhao", "Ming Hu", "Yulong Li", "Haochen Xue", "Zhengyong Jiang", "Angelos Stefanidis", "Qiufeng Wang", "Imran Razzak", "Zongyuan Ge", "Junjun He", "Yu Qiao", "Zhong Zheng", "Feilong Tang", "Kang Dang", "Jionglong Su"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0547", "x": 3.288, "y": 2.904, "title": "MultiTransAD: Cross-Sequence Translation-Driven Anomaly Detection in Multi-sequence Brain MRI", "abstract": "Accurate anomaly detection in brain MRI is critical for early disease diagnosis, yet existing single-sequence reconstruction methods often fail to distinguish pathological anomalies from both normal anatomical variations and multi-sequence contrast discrepancies. We propose MultiTransAD, a novel framework that leverages inter-sequence contrast differences as primary biomarkers for unsupervised anomaly detection. Our approach introduces: (1) a disentangled architecture with anatomical edge constraints to decouple sequence-invariant anatomy from sequence features, (2) cross-sequence translation error analysis for direct anomaly quantification, and (3) dual-level anomaly detection combining pixel-level errors and patch-level feature dissimilarities. E valuated on BraTS 2021, MultiTransAD achieves state-of-the-art performance with Dice scores of 0.6334 (14.2% improvement over reconstruction baselines) and AUROC of 0.9722, validating the effectiveness of multi-sequence contrast analysis in anomaly detection while establishing a extensible cross-sequence translation paradigm. The code is publicly available at: https://github.com/zhibaishouheilab/MT-AD", "filename": "2025_0547.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Qi Zhang", "Yibo Hu", "Jianqi Sun"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0548", "x": 1.797, "y": 1.94, "title": "New Multimodal Similarity Measure for Image Registration via Modeling Local Functional Dependence with Linear Combination of Learned Basis Functions", "abstract": "The deformable registration of images of different modalities, essential in many medical imaging applications, remains challenging. The main challenge is developing a robust measure for image overlap despite the compared images capturing different aspects of the underlying tissue. Here, we explore similarity metrics based on functional dependence between intensity values of registered images. Although functional dependence is too restrictive on the global scale, earlier work has shown competitive performance in deformable registration when such measures are applied over small enough contexts. We confirm this finding and further develop the idea by modeling local functional dependence via the linear basis function model with the basis functions learned jointly with the deformation. The measure can be implemented via convolutions, making it efficient to compute on GPUs. We release the method as an easy-touse tool and show good performance on three datasets compared to wellestablished baseline and earlier functional dependence-based methods.", "filename": "2025_0548.pdf", "year": 2025, "institution": "Aalto University", "country": "Finland", "authors": ["Joel Honkamaa", "Pekka Marttinen"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0549", "x": 1.375, "y": 4.411, "title": "Non-salient Object Segmentation in Medical Images via Pre-trained Multi-granularity Masked Autoencoders", "abstract": "The segmentation of non-salient objects in medical images plays a crucial role in the early detection and diagnosis of diseases. However, due to the low contrast and unbalanced distribution of the nonsalient objects, their feature extraction still suffers from dimensional collapse. To address the inherent feature representation challenges of non-salient objects, we propose a pre-trained Multi-Granularity Masked AutoEncoder (MG-MAE) framework with diversified feature learning capabilities. In the global level, masked image reconstruction captures holistic structural and contextual features. Subsequently, in the local level, patches are extracted from the global visible patches, and the Histogram of Oriented Gradient (HOG) features of these patches are then reconstructed to enhance the texture details. Based on local perception, the framework integrates Nuclear Norm Maximization (NNM) constraint to foster diversity of the local representations in the feature encoding process. In the HOG reconstruction pro cess, the framework also adopts a Dynamic Weight Adjustment (DWA) strategy, assigning greater reconstruction weights to challenging image patches, thereby solving the problem of representation bias towards salient objects. We evaluate our method on a private dataset, CCTA139, and two public datasets, BTCV and LiTS, respectively. Our method achieves DSC of 80.71%, 82.60%, and 71.77%, respectively, surpassing the performance of current state-of-the-art methods. The code is available at https://github.com/zhangbbin/mgmae.", "filename": "2025_0549.pdf", "year": 2025, "institution": "Anhui University", "country": "China", "authors": ["Bin Zhang", "Dongsheng Ruan", "Ronghui Qi", "Chenchu Xu", "Yanping Zhang", "Chengjin Yu", "Lei Xu", "Rui Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0550", "x": 3.438, "y": 1.408, "title": "P$$^{2}$$INR-FWI: An Implicit Neural Representation Method for Speed of Sound Image Reconstruction in Ultrasound Computed Tomography", "abstract": "Ultrasound Computed Tomography (USCT) has emerged as a cutting-edge imaging modality, offering quantitative acoustic parameter maps to enhance disease diagnosis. Full-waveform Inversion (FWI), a mainstream reconstruction method, enables high-resolution imaging of the speed of sound (SOS) from USCT measurements. However, its strong sensitivity to the initial model and the anatomical distortions caused by cycle-skipping artifacts significantly hinder its application in complex clinical scenarios. In this paper, we propose P 2 INR-FWI, a Polar coordinate-based Implicit Neural Representation framework with structural Prior, to achieve unsupervised, subject-specific SOS reconstruction. Departing from conventional Cartesian coordinate-based neural representations, our method introduces a polar coordinate encoding mechanism aligned with the geometry of the USCT ring array, which substantially accelerates convergence and improves reconstruction accuracy. Furthermore, we develop a reflected signal-derived structural prior extraction method to guide the reconstruction process toward clinically critical regions, thereby enabling fine-structure restoration. Experiments conducted on numerical phantom, breast-mimicking phantom, and in vivo data demonstrate that our method outperforms traditional approaches in both reconstruction quality and quantitative metrics, without requiring additional regularization constraints.", "filename": "2025_0550.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Zesong Wang", "Weicheng Yan", "Zhaohui Liu", "Ming Yuchi", "Wu Qiu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0551", "x": 4.816, "y": 3.746, "title": "Parameter-Efficient 12-Lead ECG Reconstruction from a Single Lead", "abstract": "With the rise of wearable IoT devices such as smartwatches and smart rings, ECG signals have become more accessible and made cardiovascular monitoring a reality. However, analyzing the ECG signals for complex conditions, such as bundle branch blocks and myocardial infarction, requires multi-lead ECG data. Although various deep learning models for ECG reconstruction have been proposed, they are computationally expensive and unsuitable on resource-constrained wearable IoT devices. To address this challenge, we propose mEcgNet, a parameter-efficient model for reconstructing 12-lead ECG signals from a single lead. mEcgNet introduces a modular deep learning architecture for parameter efficiency and separates the single lead-I signal into multiple frequency segments to improve accuracy. Our experiments demonstrate that mEcgNet significantly reduces the number of parameters and inference time by ∼23.1× and ∼5.4×, respectively, compared to existing state-of-the-art models. Furthermore, it reduces the reconstruction error by ∼22.1%, demonstrating its high accuracy and efficiency.", "filename": "2025_0551.pdf", "year": 2025, "institution": "Korea University", "country": "Republic of Korea", "authors": ["Junseok Lee", "Yeonho Yoo", "Jinkyu Kim", "Dosun Lim", "Gyeongsik Yang", "Chuck Yoo"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0552", "x": 3.186, "y": 6.434, "title": "Pathology Image Compression with Pre-trained Autoencoders", "abstract": "The growing volume of high-resolution Whole Slide Images in digital histopathology poses significant storage, transmission, and computational efficiency challenges. Standard compression methods, such as JPEG, reduce file sizes but often fail to preserve fine-grained phenotypic details critical for downstream tasks. In this work, we repurpose autoencoders (AEs) designed for Latent Diffusion Models as an efficient learned compression framework for pathology images. We systematically benchmark three AE models with varying compression levels and evaluate their reconstruction ability using pathology foundation models. We introduce a fine-tuning strategy to further enhance reconstruction fidelity that optimizes a pathology-specific learned perceptual metric. We validate our approach on dow nstream tasks, including segmentation, patch classification, and multiple instance learning, showing that replacing images with AE-compressed reconstructions leads to minimal performance degradation. Additionally, we propose a K-means clustering-based quantization method for AE latents, improving storage efficiency while maintaining reconstruction quality. We provide the weights of the fine-tuned autoencoders at this link.", "filename": "2025_0552.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Srikar Yellapragada", "Alexandros Graikos", "Kostas Triaridis", "Zilinghan Li", "Tarak Nath Nandi", "Ravi K Madduri", "Prateek Prasanna", "Joel Saltz", "Dimitris Samaras"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0553", "x": 3.027, "y": 5.398, "title": "Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis", "abstract": "Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction pro cess, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: https://github.com/QuIIL/AnoPILaD.", "filename": "2025_0553.pdf", "year": 2025, "institution": "Korea University", "country": "Korea", "authors": ["Jiamu Wang", "Keunho Byeon", "Jinsol Song", "Anh Nguyen", "Sangjeong Ahn", "Sung Hak Lee", "Jin Tae Kwak"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0554", "x": 2.917, "y": 5.968, "title": "PD-UniST: Prompt-Driven Universal Model for Unpaired H&E-to-IHC Stain Translation", "abstract": "Virtual staining, which leverages generative artificial intelligence (AI) to produce immunohistochemistry (IHC)-stained tissue samples from hematoxylin and eosin (H&E)-stained images, has emerged as a cost-effective and accessible alternative to traditional IHC staining. Despite its potential, this approach faces three significant challenges:(1) the necessity of training a separate model for each tumor marker used in IHC staining, (2) the limited availability of large-scale datasets, and (3) the inherent diversity of staining patterns across different tissue types and markers. In this study, we address these challenges by introducing the Prompt-Driven Universal model for unpaired H&E-to-IHC Stain Translation (PD-UniST). Our approach incorporates two key innovations: (1) Structure-Cognizant Organization Prompt ModulE (SCOPE), which employs textual prompts to guide regionspecific generation, and (2) Style-Prompt Unified Mapping Mod-ulE (SPUME), which utilizes learnable prompts to capture task differences between various IHC stains and features a pathology-specific prompt-aware fusion layer for effective integration of visual features with task-specific prompts. Extensive experiments on two public datasets and one private dataset demonstrate that our method achieves state-ofthe-art performance across five different translation tasks, significantly improving both structural preservation and staining pattern accuracy. In clinical evaluation, we further validate the effectiveness of our method through pathologists' assessment of both public and private datasets. The dataset and source code are available on anonymous GitHub at https://github.com/chujie-zhang/PD-UniST.", "filename": "2025_0554.pdf", "year": 2025, "institution": "Ritsumeik an University", "country": "Japan", "authors": ["Chujie Zhang", "Yangyang Xie", "Yinhao Li", "Xiao Liang", "Lanfen Lin", "Yen-Wei Chen"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0555", "x": 6.493, "y": 4.312, "title": "Phenotype Representation and Analysis via Discriminative Atypicality (PRADA) to Capture the Structural Heterogeneity of Autism Spectrum Disorder", "abstract": "Most current neuroimaging analyses in studies of brain disorders assume a homogenous presentation of the disorder such that traditional statistical analysis methods based on Gaussian distributions can be applied. Yet, most brain disorders present with a heterogeneous spectrum of cognitive, behavioral, morphometric as well as functional manifestations. In this paper, we introduce a novel approach called PRADA (Phenotype Representation and Analysis via Discriminant Atypicality) that embraces the heterogeneity of both typical and atypical brain morphometry. This approach employs Multiscale Score Matching Analysis (MSMA), a global and local multiscale out-of-distribution analysis via the gradients of the log density (scores). Combining MSMA and manifold-mapping, we compute a morphospace of brain phenotypes representing deviations from a population of typical subjects. Using these brain phenotypes, disorder-related subtyping can be performed. Fu rthermore, subject-specific profiles of atypicality can be extracted via Spatial-MSMA and summarized per subtype. We show the application of PRADA to structural MRI data in a study of Autism Spectrum Disorder (ASD). The resulting analysis detects disorder-related subtypes and reveals that subtype-specific structural atypicality correlates with cognitive and behavioral outcomes. These results highlight the potential of PRADA to discover disorder relevant phenotypes.", "filename": "2025_0555.pdf", "year": 2025, "institution": "University of North Carolina at Chapel", "country": "USA", "authors": ["Emre Onemli", "Ahsan Mahmood", "Omar Azrak", "Dea Garic", "Meghan R Swanson", "Rebecca Grzadzinski", "Kattia Mata", "Mark D Shen", "Jessica B Girault", "Tanya St. John", "Juhi Pandey", "Lonnie Zwaigenbaum", "Annette M Estes", "Audrey M Shen", "Stephen R Dager", "Robert T Schultz", "Kelly N Botteron", "Alan C Evans", "Jed T Elison", "Essa Yacoub", "Sun Hyung Kim", "Robert C Mckinstry", "Guido Gerig", "Heather C Hazlett", "Natasha Marrus", "Joseph Piven", "John R Pruett", "Martin Styner"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0556", "x": 4.115, "y": 3.163, "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications", "abstract": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenot ypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is available at https://github.com/ Markaeov/CPGG.", "filename": "2025_0556.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Ziyu Li", "Yujian Hu", "Zhengyao Ding", "Yiheng Mao", "Haitao Li", "Fan Yi", "Hongkun Zhang", "Zhengxing Huang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0557", "x": 3.267, "y": 5.445, "title": "Reconsidering Explicit Longitudinal Mammography Alignment for Enhanced Breast Cancer Risk Prediction", "abstract": "Regular mammography screening is essential for early breast cancer detection and deep learning-based risk prediction methods have sparked interest to adjust screening intervals for high-risk groups. While early methods focused only on current mammograms, recent approaches leverage the temporal aspect of screenings to track breast tissue changes over time, requiring spatial alignment across different time points. Two main strategies for this have emerged: explicit feature alignment through deformable registration and implicit learned alignment using techniques like transformers, with the former providing more control over the alignment. However, the optimal approach for explicit alignment in mammography remains underexplored. In this study, we provide insights into where explicit alignment should occur (input space vs. representation space) and if alignment and risk prediction should be jointly optimized. We demonstrate that jointly learning explicit alignment in representation space while optimizing risk estimation performance, as done in the current state-of-the-art approach, results in a trade-off between alignment quality and predictive performance and show that image-level alignment is superior to representation-level alignment, leading to better deformation field quality and enhanced risk prediction accuracy. The code is available at https://github.com/sot176/ Longitudinal_Mammogram_Alignment.git.", "filename": "2025_0557.pdf", "year": 2025, "institution": "UiT The Arctic University of N orway", "country": "Norway", "authors": ["Solveig Thrun", "Stine Hansen", "Zijun Sun", "Nele Blum", "Suaiba A Salahuddin", "Kristoffer Wickstrøm", "Elisabeth Wetzer", "Robert Jenssen", "Maik Stille", "Michael Kampffmeyer"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0558", "x": 2.811, "y": 2.022, "title": "ResMAP: Restoring MRIs of Mixed Artifacts by Prompt Cascading Retrieval", "abstract": "Image Restoration (IR) aims to enhance degraded images to provide high-quality diagnostic references in Magnetic Resonance Imaging (MRI). Although recent All-in-One IR (AiOIR) methods seek to handle multiple artifacts within a unified network, they still struggle with mixed artifacts, where multiple unknown artifacts occur simultaneously in a single MRI scan. To tackle this challenge, we propose ResMAP, a cascading framework for Restoring MRIs of Mixed Artifacts by Prompt Retrieval. It is trained exclusively on individual artifact types but can effectively handle all their mixed forms in inference, offering a feasible solution instead of requiring exhaustive training on mixed artifacts. Specifically, our ResMAP utilizes a coarse-to-fine correction process for mixed artifacts by cascading retrieval of prompts based on the artifact types. In this process, the retrieval guidance is provided through the perception and classification of fine-grained image features, while the prompts are prepared via LLM-based generation and fine-tuning. Validations on three types of artifacts and their mixed forms demonstrate the superiority of ResMAP over current IR methods. Besides, zero-shot experiments on MRIs from multiple field strengths further confirm the promising generalizability of the proposed framework. Our code is available at https://github.com/Tanishabc/ResMAP.", "filename": "2025_0558.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Yuxian Tang", "Feng Li", "Feng Shi", "Qian Wang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0559", "x": 2.839, "y": 4.28, "title": "RetiDiff: Diffusion-Based Synthesis of Retinal OCT Images for Enhanced Segmentation", "abstract": "Optical coherence tomography (OCT) enables detailed visualization and critical segmentation of retinal layers, which is essential for ophthalmological diagnosis. However, the development of automatic segmentation methods has been hindered by limited annotated datasets due to time-consuming manual labeling processes. Therefore, we propose RetiDiff, a three-stage diffusion model-based framework to synthesize realistic annotated OCT retinal images for enhancing segmentation performance. By leveraging the diffusion model, RetiDiff can synthesize diverse and realistic images guided by segmentation masks. To improve synthesis quality and accuracy in pathological regions, we introduce dynamic region masking (DRM), which selectively modifies pathological areas during training. To align the continuous outputs from mask sampling in the diffusion model with discrete segmentation labels, we propose discrete mask clustering (DMC), which converts these outputs in to discrete values consistent with the labels. Experimental results show that RetiDiff effectively mitigates data scarcity by synthesizing realistic and diverse annotated OCT retinal images, which substantially enhance retinal layer segmentation performance. Compared to state-of-the-art methods, RetiDiff-synthesized datasets improve the average Dice score by 8.7% across all retinal layers, with a particularly notable increase of up to 53.8% in pathological regions. The code and dataset are publicly available at: https://github.com/MaybeRichard/RetiDiff.", "filename": "2025_0559.pdf", "year": 2025, "institution": "Binjiang Institute o f Zhejiang University", "country": "China", "authors": ["Sicheng Li", "Mai Dan", "Yuhui Chu", "Jiahui Yu", "Yunpeng Zhao", "Pengpeng Zhao"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0560", "x": 1.56, "y": 5.627, "title": "Segmentor-Guided Counterfactual Fine-Tuning for Locally Coherent and Targeted Image Synthesis", "abstract": "Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. W e propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structurespecific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.", "filename": "2025_0560.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Tian Xia", "Matthew Sinclair", "Andreas Schuh", "Fabio De Sousa Ribeiro", "Raghav Mehta", "Rajat Rasal", "Esther Puyol-Antón", "Samuel Gerber", "Kersten Petersen", "Michiel Schaap", "Ben Glocker"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0561", "x": 1.058, "y": 3.779, "title": "SlimFormer-3D: A Layer-Adaptive Lightweight Transformer for Efficient 3D Medical Image Segmentation", "abstract": "Transformer-based architectures demonstrate strong performance in medical image segmentation but face challenges due to computational redundancy and overparameterization, limiting their deployment in resource-constrained settings. This study identifies redundant computations at the block level, particularly in the deeper layers of transformer encoders, as well as in the token mixer and MLP within each layer, as quantified by cross-layer activation similarity. To operationalize these insights, we propose SlimFormer-3D, a lightweight U-shaped encoderdecoder framework that prunes redundant computations at a granular level. Using feature similarity metrics: Angular Distance and Centered Kernel Alignment (CKA), we locate minimally impactful layers and introduce gating factors to control token mixer and MLP mo dule activations selectively. Experiments on BTCV, AMOS, and AbdomenCT-1K 3D abdominal CT datasets show SlimFormer-3D achieves competitive Dice scores while significantly reducing computational redundancy by 3.5× and cutting model parameters by approximately 83% compared to UNETR. Ablation studies confirm its balance between accuracy and efficiency, making it a promising solution for real-time 3D medical image segmentation.", "filename": "2025_0561.pdf", "year": 2025, "institution": "Guangdong University of Technology", "country": "China", "authors": ["Yang Hong", "Lei Zhang", "Xujiong Ye", "Jianqing Mo"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0562", "x": -0.255, "y": 4.55, "title": "Spatial-Temporal Memory Filtering SAM for Lesion Segmentation in Breast Ultrasound Videos", "abstract": "Lesion segmentation in breast ultrasound videos plays a crucial role in the early detection and intervention of breast cancer. However, it remains a challenging task due to blurred lesion boundaries, substantial background noise, and significant scale variations of lesions across frames. Existing methods typically rely on selecting preceding frames for rudimentary temporal integration but fail to achieve satisfactory segmentation performance. In this paper, we propose STMFSAM, a novel Spatio-Temporal Memory Filtering SAM network, designed to leverage the powerful feature representation and modeling capabilities of SAM for lesion segmentation in breast ultrasound videos. Specifically, we introduce a memory mechanism that stores and propagates essential spatio-temporal features across frames. To enhance segmentation accuracy, we select three relevant reference frames from the memory bank as dense prompts for SAM, enabling it to retain long-term contextual information and effectively guide the segmentation of subsequent frames. To further mitigate the impact of background noise, we present the Spatio-Temporal Memory Filtering module, which selectively refines the memory content by filtering out irrelevant or noisy information. This ensures that only meaningful and informative features are retained for segmentation. We conduct extensive experiments on the UVBSL200 breast ultrasound video dataset, demonstrating that STMFSAM outperforms existing methods. Additionally, to highlight our model's generalization capability, we achieve competitive results on two video polyp segmentation datasets. The code is available at https://github.com/tzz- ahu/STMFSAM.", "filename": "2025_0562.pdf", "year": 2025, "institution": "Anh ui University", "country": "China", "authors": ["Zhengzheng Tu", "Liang Zong", "Bo Jiang", "Haowen Wang", "Kunpeng Wang", "Chaoxue Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0563", "x": 2.605, "y": 2.868, "title": "Spline Refinement with Differentiable Rendering", "abstract": "Detecting slender, overlapping structures remains a challenge in computational microscopy. While recent coordinate-based approaches improve detection, they often produce less accurate splines than pixel-based methods. We introduce a training-free differentiable rendering approach to spline refinement, achieving both high reliability and sub-pixel accuracy. Our method improves spline quality, enhances robustness to distribution shifts, and shrinks the gap between synthetic and real-world data. Being fully unsupervised, the method is a drop-in replacement for the popular active contour model for spline refinement. Evaluated on C. elegans nematodes, a popular model organism for drug discovery and biomedical research, we demonstrate that our approach combines the strengths of both coordinate-and pixel-based methods.", "filename": "2025_0563.pdf", "year": 2025, "institution": "University of Copenhagen", "country": "Denmark", "authors": ["Frans Zdyb", "Albert Alonso", "Julius B Kirkegaard"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0564", "x": 3.582, "y": 3.68, "title": "StyleGAN-Based Brain MRI Anomaly Detection via Latent Code Retrieval and Partial Swap", "abstract": "Medical anomaly detection aims at identifying samples that deviate from normal patterns and localizing specific anomalous regions, playing a critical role in early detection and intervention of diseases. Reconstruction methods based on generative models are a key category among current methods for medical anomaly detection. However, a common challenge for them is achieving accurate reconstruction of normal regions while suppressing the reconstruction of anomalous regions. Style-GAN, with its powerful generative capability and the ability to perform controllable image modifications, has shown huge potential for medical image anomaly detection. However, the latent space of StyleGAN still requires further exploration and utilization. In this paper, we propose a StyleGAN-based latent Code Retrieval and Partial Swap (SCRPS) method for brain image anomaly detection. We construct a healthy image latent code repository by leveraging GAN inversion in StyleGAN's latent space. We then design a coarse-to-fine latent code retrieval mechanism to filter out normal images most similar to test image. We also introduce a partial latent code swap strategy that replaces anomalous latent codes with linear combinations of normal latent codes and employ a perceptual score to perform anomaly localization. Comprehensive experiments on brain tumor and stroke lesion datasets show that our method outperforms several state-of-the-art approaches, with 3.12 and 7.14% points improvements in average volume-level AUROC and maximum achievable Dice score, respectively.", "filename": "2025_0564.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Jie Wei", "Xiaofei Hu", "Shaoting Zhang", "Guotai Wang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0565", "x": 0.116, "y": 4.685, "title": "SUGFW: A SAM-Based Uncertainty-Guided Feature Weighting Framework for Cold Start Active Learning", "abstract": "In medical image segmentation, manual annotation is an exceptionally costly process, highlighting the critical need for selecting the most valuable samples for labeling. Active learning provides an effective solution for selecting informative samples, however, they face the challenge of cold start, where the initial training samples are randomly chosen, potentially leading to suboptimal model performance. In this study, we present a novel cold start active learning framework based on Segment Anything Model (SAM), which leverages the zero-shot capabilities of SAM on downstream datasets to address the cold start issue effectively. Concretely, we employ a multiple augmentation strategy to estimate the uncertainty map for each case, then calculate patch-level uncertainty corresponding to the patch-level features generated from SAM's image encoder. Then we propose a Patch-based Global Distinct Representation (PGDR) strategy that integrates patch-level uncertainty and image features into a unified image-level representation. To select the samples with representative and diverse information, we propose a Greedy Selection with Cluster and Uncertainty (GSCU) strategy, which effectively combines the image-level features and uncertainty to prioritize samples for manual annotation. Experiments on prostate and left atrium segmentation datasets demonstrate that our framework outperforms five state-of-the-art methods as well as random selection in various selection ratios. For both datasets, our method achieves comparable performance to that of the fully-supervised method with only 10% and 1.5% annotation burden. Code is available at https://github.com/Hilab-git/ SUGFW.git.", "filename": "2025_0565.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Xiaochuan Ma", "Jia Fu", "Lanfeng Zhong", "Ning Zhu", "Guotai Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0566", "x": 3.852, "y": 3.496, "title": "Synthesis of Pathological Dual-Channel Color Doppler Echocardiograms for Equitable Diagnosis of Heart Diseases", "abstract": "Rheumatic heart disease (RHD) is the leading global cardiac condition, affecting over 54 million people, predominantly in resource-constrained countries. Early detection via color Doppler echocardiography is crucial but often inaccessible due to reliance on specialized cardiologists. Consequently, such data from patients diagnosed with RHD are scarce. To address data limitations in developing robust RHD detection methods, we propose a novel AI-driven approach to synthesize color Doppler echocardiograms with matched B-mode ultrasound using a multi-factor conditioned diffusion model. To our knowledge, this is the first generative AI design for dual-channel color Doppler synthesis. Our model enhances realism by incorporating temporal information for motion consistency and class label for targeted synthesis. We use B-mode ultrasound to visualize anatomical structures and the Doppler-mode fields of view to define blood flow regions across key echocardiographic views (e.g., parasternal and apical). We synthesize one echocardiographic mode from another using cross-view translation to augment data and improve diversity. We evaluated our approach using synthetic data generated from echocardiograms of 589 Ugandan cases and the public CAMUS dataset. Our model outperformed state-of-the-art generative methods in fidelity and structural similarity. We trained and tested an RHD classifier on limited data from different devices. Training with synthetic data significantly improved detection performance compared to a model trained only on real data. These findings highlight the potential of diffusion-based synthetic data to democratize the diagnosis of heart diseases in marginalized populations and low-resource settings. Our", "filename": "2025_0566.pdf", "year": 2025, "institution": "Children's National Hospital", "country": "USA", "authors": ["Pooneh Roshanitabrizi", "Pengfei Guo", "Artur Arturi Aharonyan", "Kelsey Brown", "Taylor Gloria Broudy", "Abhijeet Parida", "Austin Tapp", "Zhifan Jiang", "Alison Tompsett", "Joselyn Rwebembera", "Emmy Okello", "Andrea Beaton", "Holger R Roth", "Daguang Xu", "Syed Muhammad Anwar", "Craig A Sable", "Marius George Linguraru"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0567", "x": 3.258, "y": 1.3, "title": "Target Prior-Enriched Implicit 3D CT Reconstruction with Adaptive Ray Sampling", "abstract": "Existing implicit 3D reconstruction methods utilizing NeRF and its variants for internal CT often overlook anatomical priors of target objects, limiting accuracy in ultra-sparse view scenarios. We present TP-INR, a novel framework that leverages sparse-view projections to generate high-quality anatomical priors for structural encoding of objects. By combining prior-based structural encoding with positional encoding, TP-INR enhances implicit representations for precise CT reconstruction with minimal supervision in these challenging conditions. Additionally, we tailor the implicit framework for medical applications through refined network design and adaptive ray-based training, improving both accuracy and efficiency. Experimental results across various organ regions demonstrate that TP-INR outperforms state-of-the-art methods in reconstruction quality a nd efficiency, relying solely on projection data. Code is available upon request.", "filename": "2025_0567.pdf", "year": 2025, "institution": "Southwest University", "country": "China", "authors": ["Qinglei Cao", "Ziyao Tang", "Xiaoqin Tang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0568", "x": 1.562, "y": 5.841, "title": "Towards Interpretable Counterfactual Generation via Multimodal Autoregression", "abstract": "Counterfactual medical image generation enables clinicians to explore clinical hypotheses, such as predicting disease progression, facilitating their decision-making. While existing methods can generate visually plausible images from disease progression prompts, they produce silent predictions that lack interpretation to verify how the generation reflects the hypothesized progression-a critical gap for medical applications that require traceable reasoning. In this paper, we propose Interpretable Counterfactual Generation (ICG), a novel task requiring the joint generation of counterfactual images that reflect the clinical hypothesis and interpretation texts that outline the visual changes induced by the hypothesis. To enable ICG, we present ICG-CXR, the first dataset pairing longitudinal medical images with hypothetical p rogression prompts and textual interpretations. We further introduce Pro-gEmu, an autoregressive model that unifies the generation of counterfactual images and textual interpretations. Extensive experimental results demonstrate the superiority of ProgEmu in generating progressionaligned counterfactuals and interpretations, showing significant potential in enhancing clinical decision support and medical education. Project resources are available at: https://progemu.github.io.", "filename": "2025_0568.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Chenglong Ma", "Yuanfeng Ji", "Jin Ye", "Lu Zhang", "Ying Chen", "Tianbin Li", "Mingjie Li", "Junjun He", "Hongming Shan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0569", "x": 2.497, "y": 1.504, "title": "Towards Multi-scenario Generalization: Text-Guided Unified Framework for Low-Dose CT and Total-Body PET Reconstruction", "abstract": "Low-dose computed tomography (LDCT) and low-dose positron emission tomography (LDPET) imaging substantially reduce radiation exposure compared to their normal-dose counterparts, mitigating health risks such as elevated cancer incidence. However, the resulting LDCT and total-body LDPET images are often compromised by noise and artifacts stemming from photon starvation and electronic interference. While supervised reconstruction methods have tackled challenges like over-smoothing and training instability, their generalization is hindered by variations in imaging devices, dosage levels, and modality-specific characteristics. Recent advances in text-guided models have augmented traditional deep learning techniques, offering greater adaptability. Building on this, we propose a Text-guided Unified Framework (TUF) for high-precision reconstruction of LDCT and total-body LDPET images. Leveraging insights from cold diffusion paradigms, TUF introduces a novel mean-preserving degradation operator to model the physical process of image degradation. Additionally, we design a dualdomain fusion network that converts textual inputs into scaling and shifting factors, enabling seamless integration of text cues at each timestep. Extensive experiments across four publicly available datasets reveal that TUF surpasses state-of-the-art methods in both reconstruction quality and generalization across LDCT and total-body LDPET imaging scenarios. The code will be available at TUF-code.", "filename": "2025_0569.pdf", "year": 2025, "institution": "Lanzhou University", "country": "China", "authors": ["Weitao Wang", "Yanyan Huang", "Shunjie Dong", "Le Xue", "Kuangyu Shi", "Yu Fu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0570", "x": 2.798, "y": 4.057, "title": "Towards Robust Retinal Vessel Segmentation via Reducing Open-Set Label Noises from SAM-Generated Masks", "abstract": "Retinal vessel segmentation from fundus images is an important task in intelligent ophthalmology. Because vessel annotation is particularly challenging, the scarcity of training labels hinders the model robustness for real-world scenarios. Recent research has shown that SAM, a foundation model for natural image segmentation, demonstrates impressive performance on medical images after few-shot fine-tuning. Therefore, fine-tuned SAM holds promise as a pseudo label generator to alleviate the label scarcity problem in vessel segmentation. However, the limited labeled data fails to represent real-world distribution, finetuned SAM might produce erroneous predictions in unseen image patterns, which is known as open-set label noise. In this work, we propose SAM-OSLN to reduce open-set label noises and improve the quality of generated p seudo masks. Firstly, we introduce the prototype technique to perform open-set aware SAM fine-tuning and identify open-set label noises accordingly. Subsequently, we design an explicit label denoising method and an implicit training strategy to jointly mitigate the impact of open-set label noises. Extensive experiments demonstrate that SAM-OSLN outperforms previous state-of-the-art methods on multiple fundus datasets under synthetic and real-world scenarios.", "filename": "2025_0570.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "Hong Kong", "authors": ["Minqing Zhang", "Mengxian He", "Wu Yuan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0571", "x": 1.83, "y": 4.361, "title": "Tumor Segmentation with Heterogeneity Clustering in Non-Contrast Breast MRI", "abstract": "Breast tumor segmentation in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) achieves precise delineation of tumor boundaries and subregions by capturing rich tissue heterogeneity information. However, its reliance on contrast agents may cause adverse effects, and the acquisition of complete time-series data involves a complex process. In contrast, current non-contrast image segmentation methods suffer from insufficient accuracy due to the lack of explicit tissue heterogeneity information. To address these limitations, we propose an approach for tumor heterogeneity estimation and segmentation in noncontrast images. The core idea is to extract tissue heterogeneity information from DCE-MRI and transfer it to a non-contrast image segmentation network, achieving tumor segmentation accuracy comparable to DCE-MRI-based methods. Our approach uses a vector quantized-variational autoencoder (VQ-VAE)-based clustering model to transform images into heterogeneity maps, capturing structural features of tumor subregions. These maps serve as the ground truth for training. Then, a heterogeneity information prediction model (HIPM) estimates heterogeneity maps from non-contrast images. These features are utilized as prior information to guide the segmentation network, further improving segmentation accuracy. Experimental results demonstrate that the cluster compactness (CPN) and Davies-Bouldin index (DBN) of the clustering reach approximately 0.05 and 0.001, respectively, indicating high clustering accuracy. Our method provides intuitive visualization of tumor heterogeneity without the need for contrast agents and significantly improves segmentation accuracy, with Dice Similarity Coefficient (DSC), Positive Predictive Value (PPV), and Sensitivity (SEN) increased by 20% compared to other non-contrast image segmentation networks.", "filename": "2025_0571.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Xinyu Xie", "Luyi Han", "Yonghao Li", "Yaofei Duan", "Yue Sun", "Muzhen He", "Tao Tan", "Dinggang Shen"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0572", "x": 2.115, "y": 2.862, "title": "UltraRay: Introducing Full-Path Ray Tracing in Physics-Based Ultrasound Simulation", "abstract": "Traditional ultrasound simulators solve the wave equation to model pressure distribution fields, achieving physical accuracy but requiring significant computational time and resources. Ray tracing approaches have been introduced to address this limitation, modeling wave propagation as rays interacting with boundaries and scatterers. However, existing models simplify ray propagation, generating echoes at interaction points without considering return paths to the sensor. This can result in undesired artifacts and necessitates careful scene tuning for plausible results. We propose UltraRay, a novel framework that models the full path of acoustic waves reflecting from tissue boundaries. We derive the equations for accurate reflection modeling across multiple interaction points and introduce a sampling strategy for an increased likelihood of a ray returning to the transducer. By incorporating a ray emission scheme for plane wave imaging and a standard signal processing pipeline for beamforming, we are able to simulate the ultrasound image formation process end-to-end. Built on a differentiable modular framework, UltraRay introduces an extendable foundation for differentiable ultrasound simulation based on full-path ray t racing. We demonstrate its advantages compared to the state-of-the-art ray tracing ultrasound simulation, shown both on a synthetic scene and a spine phantom.", "filename": "2025_0572.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Felix Duelmer", "Mohammad Farid Azampour", "Magdalena Wysocki", "Nassir Navab"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0573", "x": 2.561, "y": 3.567, "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model", "abstract": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable performance across a variety of generative tasks. The inherent randomness in diffusion models helps address issues such as blurring at the edges of medical images and labels, positioning Diffusion Probabilistic Models (DPMs) as a promising approach for lesion segmentation. However, we find that the current training and inference strategies of diffusion models result in an uneven distribution of attention across different timesteps, leading to longer training times and suboptimal solutions. To this end, we propose UniSegDiff, a novel diffusion model framework designed to address lesion segmentation in a unified manner across multiple modalities and organs. This framework introduces a staged training and inference approach, dynamically adjusting the prediction targets at different stages, forcing the model to maintain high attention across all timesteps, and achieves unified lesion segmentation through pre-training the feature extraction network for segmentation. We evaluate performance on six different organs across various imaging modalities. Comprehensive experimental results demonstrate that UniSegDiff significantly outperforms previous state-of-the-art (SOTA) approaches. The code is available at https://github.com/HUYILONG-Z/UniSegDiff.", "filename": "2025_0573.pdf", "year": 2025, "institution": "Dalian University of Technology", "country": "China", "authors": ["Yilong Hu", "Shijie Chang", "Lihe Zhang", "Feng Tian", "Weibing Sun", "Huchuan Lu"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0574", "x": 2.945, "y": 3.307, "title": "VesselSDF: Distance Field Priors for Vascular Network Reconstruction", "abstract": "Accurate segmentation of vascular networks from sparse CT scan slices remains a significant challenge in medical imaging, particularly due to the thin, branching nature of vessels and the inherent sparsity between imaging planes. Existing deep learning approaches, based on binary voxel classification, often struggle with structural continuity and geometric fidelity. To address this challenge, we present VesselSDF , a novel framework that leverages signed distance fields (SDFs) for robust vessel reconstruction. Our method reformulates vessel segmentation as a continuous SDF regression problem, where each point in the volume is represented by its signed distance to the nearest vessel surface. This continuous representation inherently captures the smooth, tubular geometry of blood vessels and their branching patterns. W e obtain accurate vessel reconstructions while eliminating common SDF artifacts such as floating segments thanks to our adaptive Gaussian regularizer which ensures smoothness in regions far from vessel surfaces while producing precise geometry near the surface boundaries. Our experimental results demonstrate that VesselSDF significantly outperforms existing methods and preserves vessel geometry and connectivity, enabling more reliable vascular analysis in clinical settings.", "filename": "2025_0574.pdf", "year": 2025, "institution": "University of Edinburgh", "country": "UK", "authors": ["Salvatore Esposito", "Daniel Rebain", "Arno Onken", "Changjian Li", "Oisin Mac Aodha"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0575", "x": 3.257, "y": 2.349, "title": "WASABI: A Metric for Evaluating Morphometric Plausibility of Synthetic Brain MRIs", "abstract": "Generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. Despite advances in realistic synthetic MRIs, evaluations focus on texture and perception, lacking sensitivity to crucial morphometric fidelity. This study proposes a new metric, called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the morphometric plausibility of synthetic brain MRIs. WASABI leverages SynthSeg, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each MRI and uses the multivariate Wasserstein distance to compare distributions between real and synthetic anatomies. Based on controlled experiments on two real datasets and synthetic MRIs from five generative models, WASABI demonstrates higher s ensitivity in quantifying morphometric discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. Our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing morphometric fidelity as a crucial benchmark for clinically meaningful brain MRI synthesis.Our code is available at https://github.com/BahramJafrasteh/ wasabi-mri.", "filename": "2025_0575.pdf", "year": 2025, "institution": "Department of Radiology", "country": "USA", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0576", "x": 3.998, "y": 2.966, "title": "4D CardioSynth: Synthesising Dynamic Virtual Heart Populations Through Spatiotemporal Disentanglement", "abstract": "Dynamic virtual populations are critical for realistic in-silico cardiovascular trials, yet current approaches primarily generate static anatomies, limiting their clinical and computational value. In this study, we present 4D CardioSynth, a generative framework for constructing dynamic 3D virtual populations of cardiovascular structures that change over time (3D+t). To model the complex interplay between cardiac structure and motion, we develop a factorised variational approach that disentangles spatial and temporal information in latent space, enabling independent control over anatomical variations and motion patterns. We demonstrate 4D CardioSynth's performance using a diverse dataset of bi-ventricle shapes acquired from 6,500 patients across complete cardiac cycles. Our results illustrate the superiority of 4D CardioSynth over state-of-the-art methods with respect to anatomical specificity, diversity, and generalisability, as well as motion plausibility. This approach enables more accurate virtual trials for cardiovascular interventions.", "filename": "2025_0576.pdf", "year": 2025, "institution": "University of Manchester", "country": "UK", "authors": ["Haoran Dou", "Jinghan Huang", "Arezoo Zakeri", "Zherui Zhou", "Tingting Mu", "Jinming Duan", "Alejandro F Frangi"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0577", "x": 7.052, "y": 4.842, "title": "A Unified Continuous Staging Framework for Alzheimer’s Disease and Lewy Body Dementia via Hierarchical Anatomical Features", "abstract": "Alzheimer's Disease (AD) and Lewy Body Dementia (LBD) often exhibit overlapping pathologies, leading to common symptoms that make diagnosis challenging and protracted in clinical settings. While many studies achieve promising accuracy in identifying AD and LBD at earlier stages, they often focus on discrete classification rather than capturing the gradual nature of disease progression. Since dementia develops progressively, understanding the continuous trajectory of dementia is crucial, as it allows us to uncover hidden patterns in cognitive decline and provides critical insights into the underlying mechanisms of disease progression. To address this gap, we propose a novel multiscale learning framework that leverages hierarchical anatomical features to model the continuous relationships across various neurodegenerative conditions, including Mild Cognitive Impairment, AD, and LBD. Our approach employs the proposed hierarchical graph embedding fusion technique, integrating anatomical features, cortical folding patterns, and structural connectivity at multiple scales. This integration captures both fine-grained and coarse anatomical details, enabling the identification of subtle patterns that enhance differentiation between dementia types. Additionally, our framework projects each subject onto continuous tree structures, providing intuitive visualizations of disease trajectories and offering a more interpretable way to track cognitive decline. To validate our approach, we conduct extensive experiments on our in-house dataset of 308 subjects spanning multiple groups. Our results demonstrate that the proposed tree-based model effectively represents dementia progression, achieves promising performance in intricate classification task of AD and LBD, and highlights discriminative brain regions that", "filename": "2025_0577.pdf", "year": 2025, "institution": "University of Texas at Arlington", "country": "USA", "authors": ["Tong Chen", "Minheng Chen", "Jing Zhang", "Yan Zhuang", "Chao Cao", "Xiaowei Yu", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0578", "x": 3.672, "y": 1.879, "title": "Accelerated Free-Breathing 5D Multi-Echo Respiratory Motion-Resolved R2*, PDFF, and QSM Using Novel Composite Total Variation", "abstract": "We introduce a novel composite total variation (TV) and its solution algorithm with their application to multi-echo, respiratory motion-resolved 5D (3D space + 1D respiratory motion + 1D echo signal evolution) compressed sensing (CS) abdominal MR image reconstruction. The proposed formalism ensures a sparse representation between multi-echo images with varying contrast-a vital feature that needs to be preserved-making it highly suitable for applications in multidimensional computational/quantitative imaging. The key idea of the proposed composite TV and its formal definition were inspired by the observation that the spatial gradient of difference images in multi-echo MRI appears sparse. Throughout extensive experiments on a small number of healthy volunteers, we have demonstrated improved performance of the proposed method in 5D motion-resolved CS reconstruction of multi-echo MRI data compared to the state-of-the-art method. We have also demonstrated improved performance of the proposed method in quantitative tissue parameter mapping (such as R2*, proton density fat fraction, and quantitative susceptibility mapping) across a wide range of undersampling factors. In conclusion, the proposed method enables vastly accelerated motion-resolved multi-echo CS-MRI minimally impacting the quantification of downstream tissue parameters.", "filename": "2025_0578.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Mungsoo Kang", "Or Alus", "Youngwook Kee"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0579", "x": 1.241, "y": 3.909, "title": "AffinityUMamba: Uncertainty-Aware Medical Image Segmentation via Probabilistic Weak Supervision Beyond Gold-Standard Annotations", "abstract": "Owing to its superior soft tissue contrast, Magnetic Resonance Imaging (MRI) has become a cornerstone modality in clinical practice. This prominence has driven extensive research on MRI-based segmentation, supported by the proliferation of publicly available benchmark datasets. Despite employing multi-expert consensus protocols to ensure annotation quality in public datasets, the inherent label noise, particularly prevalent at lesion boundary regions, remains unavoidable. To address this fundamental challenge, we introduce a novel machine learning paradigm that reframes dataset annotations as probabilistic weak supervision rather than deterministic gold standards. We proposed AffinityUMamba, a novel dual-branch Unet-like framework that synergistically integrates convolutional operations with state space models, leveraging local feature coherence and global contextual agreement. And a Local Affinity-guided Label Refinement (LALR) module to identify potential noisy labels in the training data and produce refined pseudo labels. A unified uncertainty constraint paradigm combining marginbased logit smoothing with local affinity refinement, enabling simultaneous optimization of segmentation accuracy and confidence calibration. Training is stabilized through a composite objective combining topological preservation constraints with margin-aware uncertainty penalization, enabling joint optimization of structural coherence and detail fidelity. We comprehensively evaluated the proposed method on 12 public datasets spanning multiple modalities: 10 MRI, 1 Ultrasound, and 1 CT. The results of our experiments demonstrate improved segmentation performance and reduced prediction uncertainty.", "filename": "2025_0579.pdf", "year": 2025, "institution": "Beijing University of Posts and Telecommunications", "country": "China", "authors": ["Yukun Zhang", "Guisheng Wang", "William Henry Nailon", "Kun Cheng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0580", "x": 2.866, "y": 5.188, "title": "Anomaly Detection by Clustering DINO Embeddings Using a Dirichlet Process Mixture", "abstract": "In this work, we leverage informative embeddings from foundational models for unsupervised anomaly detection in medical imaging. For small datasets, a memory-bank of normative features can directly be used for anomaly detection which has been demonstrated recently. However, this is unsuitable for large medical datasets as the computational burden increases substantially. Therefore, we propose to model the distribution of normative DINOv2 embeddings with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model that automatically adjusts the number of mixture components to the data at hand. Rather than using a memory bank, we use the similarity between the component centers and the embeddings as anomaly score function to create a coarse anomaly segmentation mask. Our experiments show that through DPMM, embeddings of DINOv2, despite being trained on natural images, achieve very competitive anomaly detection performance on medical imaging benchmarks and can do this while at least halving the computation time at inference. Our analysis further indicates that normalized DINOv2 embeddings are generally more aligned with anatomical structures than unnormalized features, even in the presence of anomalies, making them great representations for anomaly detection. The code is available at https://github.com/NicoSchulthess/anomalydino-dpmm.", "filename": "2025_0580.pdf", "year": 2025, "institution": "ETH Zurich", "country": "Switzerland", "authors": ["Nico Schulthess", "Ender Konukoglu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0581", "x": 3.092, "y": 3.761, "title": "Aorta Multi-class Segmentation via Anatomically Constrained Plane Detection", "abstract": "Accurate multi-class segmentation of the aorta in medical CT images is essential for the effective diagnosis and treatment of blood flow abnormalities. However, achieving precise segmentation in multizone remains challenging due to the lack of visible boundaries and the similarity in intensity between zones. Although existing methods incorporate anatomical features such as global geometric constraints and landmark-based alignment, they often struggle when these features are difficult to extract, such as in regions with asymmetric deformation or extreme curvature due to dissection. This limitation of relying solely on simple anatomical cues underscores the need to learn and model complex anatomical interrelationships for robust segmentation. To overcome these challenges, we propose a plane detection-based segmentation framework that is constrained by anatomical features and their relationships to accurately detect planes between zones. Specifically, our method detects planes by localizing centerpoints and regressing the corresponding normal vectors, while anatomical landmarks further refine the position and orientation of these planes. Additionally, anatomical regularization losses enforce geometric consistency among these components, thereby enhancing both accuracy and stability of the detected planes. The entire framework is implemented as an end-to-end architecture, enabling efficient learning. The experimental results on the AortaSeg24 dataset demonstrate that our approach achieves state-of-the-art performance. Our code is publicly available at https://github.com/jjong0225/ACP.", "filename": "2025_0581.pdf", "year": 2025, "institution": "Soongsil University", "country": "Republic of Korea", "authors": ["Jonghoon An", "Dong Hyun Lee", "So Hyun Kim", "Taejin Moon", "Minyoung Chung"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0582", "x": 3.934, "y": 3.552, "title": "Automated Characterization of Myocardial Scar Topological Patterns for Ventricular Tachycardia Screening", "abstract": "Ventricular tachycardia screening is crucial for early intervention and prevention of life-threatening cardiac events. Myocardial scar topology on late gadolinium enhancement (LGE) MRI offers detailed structural insights that may be closely associated with the mechanisms underlying ventricular tachycardia. However, accurate characterization presents challenges due to the substantial shape variability of myocardium, indistinct boundaries, small scar volumes, and potential issues with image quality. In this study, we present PolarNet, a novel framework for automatic scar segmentation and topological pattern characterization in polar coordinates. The framework incorporates a boundary-aware segmentation branch that explicitly models boundaries essential for scar characterization (endocardium, scar-start, scar-end, and epicardium), ensuring geometric consistency and anatomical coherence.Our method outperforms nnU-Net in both scar segmentation and topological pattern characterization. Code will be available at https://github. com/Sheng-xc/VTS_PolarNet.", "filename": "2025_0582.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Xicheng Sheng", "Yang Zhang", "Lei Li", "Bailiang Chen", "Freddy Odille", "Xiahai Zhuang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0583", "x": 2.222, "y": 3.051, "title": "Blind Restoration of High-Resolution Ultrasound Video", "abstract": "Ultrasound imaging is widely applied in clinical practice, yet ultrasound videos often suffer from low signal-to-noise ratios (SNR) and limited resolutions, posing challenges for diagnosis and analysis. Variations in equipment and acquisition settings can further exacerbate differences in data distribution and noise levels, reducing the generalizability of pre-trained models. This work presents a self-supervised ultrasound video super-resolution algorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive optimization process of a neural network that enhances the resolution of given ultrasound videos without requiring paired training data while simultaneously removing noise. Quantitative and visual evaluations demonstrate that DUP outperforms existing super-resolution algorithms, leading to substantial improvements for downstream applications.", "filename": "2025_0583.pdf", "year": 2025, "institution": "City University of Hong Kong", "country": "Hong Kong", "authors": ["Chu Chen", "Kangning Cui", "Pasquale Cascarano", "Wei Tang", "Elena Loli Piccolomini", "Raymond H Chan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0584", "x": 1.591, "y": 1.932, "title": "Bridging Classical and Learning-Based Iterative Registration Through Deep Equilibrium Models", "abstract": "Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use weight-tied neural networks to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibriumseeking mechanism, bridging the gap between classical optimizationbased and modern learning-based registration methods.", "filename": "2025_0584.pdf", "year": 2025, "institution": "Delft University of Technology", "country": "The Netherlands", "authors": ["Yi Zhang", "Yidong Zhao", "Qian Tao"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0585", "x": 2.673, "y": 1.566, "title": "Cascaded 3D Diffusion Models for Whole-Body 3D 18-F FDG PET/CT Synthesis from Demographics", "abstract": "We propose a cascaded 3D diffusion model framework to synthesize high-fidelity 3D PET/CT volume directly from demographic variables, addressing the growing need for realistic digital twins in oncologic imaging, virtual trials, and AI-driven data augmentation. Unlike deterministic phantoms, which rely on predefined anatomical and metabolic templates, our method employs a two-stage generative process: an initial score-based diffusion model synthesizes low-resolution PET/CT volumes from the demographic variables only, providing global anatomical structures and approximate metabolic activity, followed by a super-resolution residual diffusion model refining spatial resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET dataset and evaluated using organ-wise volume and standardized uptake value (SUV) distributions, comparing synthetic and real data between demographic subgroups. The organ-wise comparison demonstrated strong concordance between synthetic and real images. In particular, most of the deviations in metabolic uptake values remained within 3-5% of the ground truth in sub-group analysis. These findings highlight the potential of cascaded 3D diffusion models to generate anatomically and metabolically accurate PET/CT images, offering a robust alternative to traditional phantoms and enabling scalable, population-informed synthetic imaging for clinical and research applications. Codes can be found at https://github.com/siyeopyoon/TotalGen.", "filename": "2025_0585.pdf", "year": 2025, "institution": "Massachusetts General Hospital", "country": "USA", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0586", "x": 3.009, "y": 2.563, "title": "Causality-Driven Spatio-Temporal Generator for Multi-phase Contrast-Enhanced CT Synthesis", "abstract": "Synthesizing multi-phase contrast-enhanced CT (CE-CT) images is clinically significant, as it can mitigate clinical risks such as radiation exposure and allergic reactions to contrast agents. However, existing methods treat multi-phase synthesis as separate tasks, failing to maintain the inter-phase dependencies and consistency between synthesized multi-phase CE-CT images. Moreover, the limited variability in CT intensity distributions makes it challenging to capture subtle variations in multi-phase imaging. For the first time, we propose a novel Causality-driven Spatio-temporal Generator (CSGen) for synthesizing multi-phase CE-CT imaging through three key novelties: 1) Using a novel phase-causality to creatively exploit the multi-phase variation content for driving the multi-phase CE-CT synthesizing, addressing the challenge of capturing multi-phase discriminative features through one model. 2) Introducing a new Spatio-temporal Transformer to establish the spatiotemporal correlation between multi-phase CE-CT images for leveraging multi-phase inter-and intra-dependencies and improving synthesis quality. 3) Multi-phase adversarial learning is designed for enhancing multi-phase discriminative feature learning. Experimental results (mean PSNR: 31.15, mean SSIM: 0.9066, mean NMAE: 3.17) demonstrate that CSGen outperforms state-of-the-art synthesis methods, and, for the first time, successfully synthesizes multi-phase CE-CT images.", "filename": "2025_0586.pdf", "year": 2025, "institution": "Case Western Reserve University", "country": "USA", "authors": ["Qikui Zhu", "Hao Wu", "Yanyan Zhang", "Shuo Li"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0587", "x": 4.959, "y": 3.873, "title": "Contrastive Disentanglement Learning Framework for Multi-lead Wearable ECG Denoising", "abstract": "Electrocardiogram (ECG) denoising enhances the clarity of noisy signals while preserving or even improving diagnostic performance. Most existing single-lead denoising algorithms require a preliminary noise assessment across all 12 leads, discarding clean leads and denoising only the noisy leads. In this paper, a novel disentanglement learning denoising network is proposed for 12-lead wearable ECG that directly processes 12lead ECG, denoising noisy leads while preserving clean leads. Specifically, the proposed network takes both raw ECG and its corresponding simulated noisy ECG as inputs, disentangling them into noise codes and signal content codes. To ensure consistency between the content codes from two inputs, a discriminator is introduced. Additionally, considering that clean leads within the same ECG can provide valuable information for denoising noisy leads, a lead encoder is designed to extract lead specific features from the raw ECG. A contrastive loss is then applied between the features of noisy and clean leads to optimize the model. The results demonstrate that our method achieves superior denoising performance across two different lead system test datasets. Furthermore, evaluations on an ST-segment change multi-label classification task indicate that the denoised ECG improve diagnostic AUC and AUPRC. Furthermore, our model can be used into remote wearable ECG diagnostic workflows, providing preliminary noise reduction to assist cardiologists in subsequent clinical assessments.", "filename": "2025_0587.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Yue Zhang", "Chenyu Zhao", "Wen Zhang", "Jinliang Wang", "Jun Guo", "Wei Yang", "Qianjin Feng"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0588", "x": 0.839, "y": 4.706, "title": "CS$$^2$$C: Collaborative Spatial and Spectral Neural Clustering for Organelle Segmentation from Volumetric Electron Microscopy", "abstract": "Organelle segmentation is crucial for understanding the morphology of biological structures. Existing unsupervised methods leverage powerful feature extractors and clustering techniques to uncover organelle structures from volumetric electron microscopy images. However, these methods often struggle with noisy microscopy images and the computational complexity of numerical clustering. In this paper, we propose CS 2 C, a novel collaborative spatial and spectral deep neural clustering framework, for multi-class organelle segmentation. The pillar of our approach is combining unsupervised deep spectral clustering and spatial clustering, which enhances a harmony of learned cluster assignments under the spatial and spectral superpixel-wise representation. Specifically, we adopt a masked autoencoder-based feature extractor to obtain powerful superpixel features, where spatial clustering is performed directly on these features. Beyond that, spectral clustering is applied in the spectral domain, naturally alleviating high-frequency perturbations in the image features. The entire framework is trained end-to-end using a combination of clustering loss and consistency regularization between spatial and spectral clustering. Extensive experiments demonstrate that our method outperforms state-of-the-art unsupervised methods on known benchmarks. Code is available at: https://github. com/JimaoJIANG/CS2C.", "filename": "2025_0588.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Jimao Jiang", "Yuru Pei"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0589", "x": 1.658, "y": 3.36, "title": "CT-Based Hippocampus Segmentation with Dual-Decoder Network (HDD-Net)", "abstract": "The hippocampus in the brain performs a pivotal role for memory formation, spatial navigation, and emotional regulation. Its volume and morphology are known to change with the progression of neurodegenerative diseases such as Alzheimer's disease. Hence, hippocampal atrophy serves as a key biomarker for early diagnosis and monitoring of such diseases. Whereas MRI has been predominantly employed in that regard due to its excellent soft-tissue contrast, CT-based segmentation of the structure has been relatively far less explored because the modality results in ambiguous boundaries between brain subregions. This study aims to address this technical challenge, achieving accurate segmentation of the hippocampus on CT images. To this end, we develop a deep learning model, termed 'Hippocampus Dual Decoder Network (HDD-Net)', characterized by the following four major components: 1) parallel, dual decoders that segment the hippocampal region and its boundaries, respectively, 2) a single, shared encoder in which features combined across multiple blocks are refined via attention, 3) a feature fusion module (FFM) that performs inter-decoder featural supplements, and 4) a cross loss to jointly optimize segmentation and edge predictions. HDD-Net was validated using both internal and external datasets, with its performance assessed using Dice similarity coefficient (DSC) and intersection-over-union (IoU). Our model yielded DSC = 0.823 ± 0.03 and IoU = 0.701 ± 0.04, and DSC = 0.759 ± 0.07 and IoU = 0.617 ± 0.09 for internal and external test datasets, respectively, outperforming seven other SOTA methods. Furthermore, volumetric analysis revealed a good agreement between MRI-and CT-derived hippocampal masks. Our findings suggest feasibility of CT-based hippocampal segmentation via HDD-Net, as a cost-effective alternative to MRI. The implementation of HDD-Net is available at https://github.com/sonwonjun103/HDD_Net.", "filename": "2025_0589.pdf", "year": 2025, "institution": "Kyungpook National University", "country": "Republic of Korea", "authors": ["Wonjun Son", "Ji Young Lee", "Sung Jun Ahn", "Hyunyeol Lee"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0590", "x": 2.342, "y": 2.291, "title": "DCT-Net: Dual-Branch CT Reconstruction from Orthogonal X-Rays with Diffusion Model and Contrastive Learning", "abstract": "Computed tomography (CT) reconstruction from X-ray images possesses significant advantages, including lower radiation exposure, reduced costs, and better accessibility than direct CT imaging. However, insufficient effective input samples caused by data volume under the moderate level or occlusion of partial soft tissues by skeletal structures in X-rays often hold back achieving high-quality image reconstruction. Additionally, contrasted with voxel-level differences, the texture and structure features are significant for image reconstruction. In virtue of these challenges, this study proposes an efficient approach named Dual-branch CT Network (DCT-Net). It first integrates a conditional diffusion model for data augmentation, which mitigates data scarcity and achieves bone suppression. Subsequently, a dual-branch network in DCT-Net is leveraged to parallel process both augmented and raw data. In the framework, a perceptual loss based on high-level semantic features performs as the contrastive loss. Furthermore, it combines the voxel-level and adversarial losses to optimize the generator. However, the discriminator optimization only depends on the adversarial loss. Experimental results on two public datasets demonstrate that DCT-Net outperforms the state-of-the-art works, appearing to have promising potential among clinical applications.", "filename": "2025_0590.pdf", "year": 2025, "institution": "Tianjin University of Technology", "country": "China", "authors": ["Zhiyu Zhang", "Cong Shen", "Jijun Tang", "Zhijun Liao"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0591", "x": 4.019, "y": 3.762, "title": "EFMS-Net: Efficient Frequency-Enhanced Multi-scale Network for Ischemic Stroke Segmentation", "abstract": "The combination of multi-modal medical imaging for ischemic stroke infarct segmentation is crucial for clinical treatment. However, existing methods often improve segmentation accuracy at the cost of efficiency, rendering them impractical for mobile health applications. To overcome this limitation, we integrate Mamba, a state-space model for long-sequence modeling, with convolutional operations to capture both global and local dependencies. To further enhance the feature representation, we incorporate multi-scale feature interaction and frequency-domain processing. As a result, we propose a novel Efficient Frequency-enhanced Multi-Scale Network (EFMS-Net) to achieve an optimal trade-off between segmentation accuracy, inference speed, and parameter efficiency. Extensive experiments on four datasets demonstrate the effectiveness and efficiency of EFMS-Net. We release a new dataset to promote further research in ischemic stroke infarct segmentation. The dataset is available on GitHub.", "filename": "2025_0591.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Jie Yang", "Shaowei Shen", "Xuwei Fan", "Ning Chen", "Zhibin Gao", "Lianfen Huang", "Yihong Zhan"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0592", "x": 3.379, "y": 1.95, "title": "Faster, Self-supervised Super-Resolution for Anisotropic Multi-view MRI Using a Sparse Coordinate Loss", "abstract": "Acquiring images in high resolution is often a challenging task. Especially in the medical sector, image quality has to be balanced with acquisition time and patient comfort. To strike a compromise between scan time and quality for Magnetic Resonance (MR) imaging, two anisotropic scans with different low-resolution (LR) orientations can be acquired. Typically, LR scans are analyzed individually by radiologists, which is time consuming and can lead to inaccurate interpretation. To tackle this, we propose tripleSR, a novel approach for fusing two orthogonal anisotropic LR MR images, to reconstruct anatomical details in a unified representation. Our multi-view neural network is trained in a self-supervised manner, without requiring corresponding high-resolution (HR) data. To optimize the model, we introduce a sparse coordinatebased loss, enabling the integration of LR images with arbitrary scaling. We evaluate our method on MR images from two independent cohorts. Our results demonstrate comparable or even improved super-resolution (SR) performance compared to state-of-the-art (SOTA) self-supervised SR methods for different upsampling scales. By combining a patientagnostic offline and a patient-specific online phase, we achieve a substantial speed-up of up to ten times for patient-specific reconstruction while achieving similar or better SR quality. Code is available at https:// github.com/MajaSchle/tripleSR.", "filename": "2025_0592.pdf", "year": 2025, "institution": "Friedrich-Alexander-Universität Erlangen-Nürnberg", "country": "Germany", "authors": ["Maja Schlereth", "Moritz Schillinger", "Katharina Breininger"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0593", "x": 3.285, "y": 2.076, "title": "FDF-VQVAE: A Frequency Disentanglement and Fusion Learning Framework for Multi-sequence MRI Enhancement", "abstract": "Multi-sequence magnetic resonance imaging (MRI) faces critical challenges in balancing accelerated acquisition and image quality: Rapid scanning typically induces degradation, including resolution reduction, increased noise, motion artifacts, and image blurring. While existing image enhancement models partially mitigate these issues, they often exhibit insufficient exploitation of complementary information across multi-sequence data. To address this issue, we propose an interpretable deep learning framework, FDF-VQVAE, for MRI image enhancement through frequency-domain feature disentanglement and fusion. Our framework constructs a dual-branch frequency-domain disentanglement module (DBFD) that precisely decouples high-frequency and low-frequency features of different sequences through parallel highfrequency feature and low-frequency feature extraction pathways. The multi-frequency-domain feature weighting mechanism (MFDFW) adaptively fuses the high and low-frequency features of different sequences. Finally, feature recombination and decoding achieve MRI enhancement through joint optimization. We conducted denoising, super-resolution, and deblurring experiments on the IXI dataset (546 subjects) with external validation on the BraTS2021 dataset (357 subjects). Experimental results demonstrate that our method significantly outperforms the stateof-the-art approaches in denoising, motion artifact removal, and superresolution tasks. Our code is available at https://github.com/kkllxh/ FDF-VQVAE.", "filename": "2025_0593.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Xinghe Xie", "Luyi Han", "Yue Sun", "Chi Kin Lam", "Jian Zheng", "Tong Tong", "Wei Ke", "Chan-Tong Lam", "Tao Tan"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0594", "x": 1.159, "y": 4.248, "title": "Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation", "abstract": "Implicit neural representations (INRs) have achieved remarkable successes in learning expressive yet compact signal representations. However, they are not naturally amenable to predictive tasks such as segmentation, where they must learn semantic structures over a distribution of signals. In this study, we introduce MetaSeg, a meta-learning framework to train INRs for medical image segmentation. MetaSeg uses an underlying INR that simultaneously predicts per pixel intensity values and class labels. It then uses a meta-learning procedure to find optimal initial parameters for this INR over a training dataset of images and segmentation maps, such that the INR can simply be finetuned to fit pixels of an unseen test image, and automatically decode its class labels. We evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice scores comparable to commonly used U-Net models, but with 90% fewer parameters. MetaSeg offers a fresh, scalable alternative to traditional resource-heavy architectures such as U-Nets and vision transformers for medical image segmentation.", "filename": "2025_0594.pdf", "year": 2025, "institution": "Rice University", "country": "USA", "authors": ["Kushal Vyas", "Ashok Veeraraghavan", "Guha Balakrishnan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0595", "x": 2.703, "y": 7.302, "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "abstract": "Privacy preservation in AI is crucial, especially in healthcare, where models rely on sensitive patient data. In the emerging field of machine unlearning, existing methodologies struggle to remove patient data from trained multimodal architectures, which are widely used in healthcare. We propose Forget-MI, a novel machine unlearning method for multimodal medical data, by establishing loss functions and perturbation techniques. Our approach unlearns unimodal and joint representations of the data requested to be forgotten while preserving knowledge from the remaining data and maintaining comparable performance to the original model. We evaluate our results using performance on the forget dataset ↓, performance on the test dataset ↑, and Membership Inference Attack (MIA) ↓, which measures the attacker's ability to distinguish the forget dataset from the training dataset. Our model outperforms the existing approaches that aim to reduce MIA and the performance on the forget dataset while keeping an equivalent performance on the test set. Specifically, our approach reduces MIA by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305, respectively. Additionally, our performance on the test set matches that of the retrained model, while allowing forgetting. Code is available at https://github. com/BioMedIA-MBZUAI/Forget-MI.git.", "filename": "2025_0595.pdf", "year": 2025, "institution": "University of Artificial Intelligence", "country": "UAE", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0596", "x": 3.385, "y": 4.598, "title": "Generative Unsupervised Anomaly Detection with Coarse-Fine Ensemble for Workload Reduction in 3D Non-contrast Brain CT of Emergency Room", "abstract": "Neurologic emergencies need to treat unspecified anomalies with various shapes, intensities, and locations in 3D non-contrast brain CT. However, in practice, patients with anomalies take a relatively small portion of total CT volumes. In this situation, excluding unremarkable scans could reduce radiologists' workload. We used a generative unsupervised anomaly detection (GUAD) with 3D Hierarchical Diffusion AutoEncoder (HDAE) model to develop this. In this study, we considered anomalies in two perspectives and made models. One is a Coarse-Morphological anomaly detection Model (CMM), and the other is a Fine-Grained anomaly detection Model (FGM). We ensembled these models' decisions for the exclusion of the unremarkable scans. Models were trained with normal scans of 28,510 from Asan Medical Center (AMC). For evaluation, we mainly used two consecutive test sets of 544 scans from AMC and 1,795 scans from Gangneung Asan Hospital (GNAH). Among clinically significant and unremarkable scans, our study showed [NPV (Negative Predictive Value)/workload reduction] of [98.1%/9.7%] and [96.7%/19.9%] for AMC and GNAH, respectively. Additionally, we used a public dataset (NPV of 98.5%) and five other external hospitals' hemorrhage sets (NPV of 96.0%) to evaluate robustness. Under the reasonable NPV, models showed the potential for workload reduction by omitting unremarkable scans. Compared to individual results of CMM or FGM, the ensembled decision usually shows NPV advantages. Also, with visual results, we observed our model could detect various types of anomalies.", "filename": "2025_0596.pdf", "year": 2025, "institution": "University of Ulsan College of Medicine", "country": "Republic of Korea", "authors": ["Jongjun Won", "Jihwan Kim", "Joonseo Oh", "Yereen Yoo", "Jieun Yum", "Joonsang Lee", "Joon Hyung Park", "Wooyoung Jo", "Yoojin Nam", "Hyunki Lee", "Gil-Sun Hong", "Namkug Kim"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0597", "x": 0.566, "y": 3.662, "title": "GLM-SFNet: Global-Local Vision-Mamba with Semantic Fusion for Medical Image Segmentation", "abstract": "Mamba-based architectures have shown promising performance in medical image segmentation. Accurate segmentation demands effective capture and integration of both global context and local details. However, existing methods often lack a balanced approach to extracting and fusing global and local information within the encoder and decoder. To address this issue, we introduce Global-Local Vision-Mamba with Semantic Fusion Network (GLM-SFNet), which is designed for balanced global-local feature processing in medical image segmentation. In the encoder, GLM-SFNet employs a Local-Global Vision State Space block (LGVSS).LGVSS strategically integrates four-directional scanning Mamba to capture comprehensive global context while incorporating Learnable Descriptive Convolution (LDC) to ensure detailed local feature extraction. For the decoder, we propose a Semantic Fusion Decoder (SFD), which achieves enhanced information integration and boundary precision by strategically combining global and local semantic fusion modules. Extensive experiments on three benchmark datasets demonstrate that GLM-SFNet achieves state-of-the-art segmentation performance while maintaining a lightweight architecture.", "filename": "2025_0597.pdf", "year": 2025, "institution": "Xidian University", "country": "China", "authors": ["Jiahui Chen", "Fei Qi", "Chengyuan Chang", "Qinjie Hu", "Kaiwen Fu", "Xiaotian Wang", "Kun Liu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0598", "x": 3.308, "y": 4.358, "title": "Harnessing EHRs for Diffusion-Based Anomaly Detection on Chest X-Rays", "abstract": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for identifying pathological abnormalities without requiring extensive labeled data. However, existing diffusion-based UAD models rely solely on imaging features, limiting their ability to distinguish between normal anatomical variations and pathological anomalies. To address this, we propose Diff3M, a multi-modal diffusion-based framework that integrates chest X-rays and structured Electronic Health Records (EHRs) for enhanced anomaly detection. Specifically, we introduce a novel Image-EHR Cross-Attention module to incorporate structured clinical context into the image generation process, improving the model's ability to differentiate normal from abnormal features. Additionally, we develop a static masking strategy to enhance the reconstruction of normal-like images from anomalies. Extensive evaluations on CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-ofthe-art performance, outperforming existing UAD methods in medical imaging. Our implementation is available at https://github.com/nth221/ Diff3M", "filename": "2025_0598.pdf", "year": 2025, "institution": "Handong Global University", "country": "South Korea", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0599", "x": 3.312, "y": 2.705, "title": "Hierarchical Anatomy-Aware Guidance for Brain Tissue Microstructure Reconstruction from T1-Weighted MRI", "abstract": "Tissue microstructure information reconstructed from diffusion magnetic resonance imaging (MRI) provides crucial brain tissue information for brain disease analysis. However, clinical imaging time constraints often limit the availability of diffusion MRI, thus prompting research into tissue microstructure reconstruction from clinically feasible MRI modalities, such as T1-weighted MRI. Recent Transformerbased generative adversarial networks demonstrate potential by capturing long-range dependencies via self-attention in general MRI synthesis tasks, yet the significant gap between diffusion and T1-weighted MRI limits their ability to achieve optimal performance, leading to anatomical inconsistency in the reconstructed tissue microstructure maps. To address the problem, we propose a hierarchical anatomy-aware guidance (HAAG) framework for brain tissue microstructure reconstruction from T1-weighted MRI. First, we consider a two-level strategy to introduce the anatomical priors for the Transformer. At the input level of the Transformer, we propose an adaptive semantic embedding module that seamlessly integrates anatomical structure category information, providing semantic-level guidance for tissue microstructure reconstruction. At the feature modeling level of the Transformer, we propose a distanceguided self-attention mechanism to achieve effective information fusion of anatomical structures that balances both global and local contexts. Then, we consider a more general approach to verify the anatomical consistency at the output level of the whole synthesis network. We develop an anatomy-aware discriminative loss that encourages anatomical consistency between the input and output modalities. HAAG was validated on a public brain MRI dataset for reconstruction of tissue microstructure from T1-weighted MRI. The results demonstrate that our method significantly improves the quality of tissue microstructure reconstruction.", "filename": "2025_0599.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Yuxing Li", "Chuyang Ye"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0600", "x": 3.2, "y": 3.39, "title": "Hierarchical Part-Based Generative Model for Realistic 3D Blood Vessel", "abstract": "Advancements in 3D vision have increased the impact of blood vessel modeling on medical applications. However, accurately representing the complex geometry and topology of blood vessels remains a challenge due to their intricate branching patterns, curvatures, and irregular shapes. In this study, we propose a hierarchical part-based framework for 3D vessel generation that separates the global binary tree-like topology from local geometric details. Our approach proceeds in three stages: (1) key graph generation to model the overall hierarchical structure, (2) vessel segment generation conditioned on geometric properties, and (3) hierarchical vessel assembly by integrating the local segments according to the global key graph. We validate our framework on realworld datasets, demonstrating superior performance over existing methods in modeling complex vascular networks. This work marks the first successful application of a part-based generative approach for 3D vessel modeling, setting a new benchmark for vascular data generation. The code is available at: https://github.com/CybercatChen/PartVessel.git.", "filename": "2025_0600.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Siqi Chen", "Guoqing Zhang", "Jiahao Lai", "Bingzhi Shen", "Sihong Zhang", "Caixia Dong", "Xuejin Chen", "Yang Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0601", "x": 4.407, "y": 3.44, "title": "Hierarchical Spatio-Temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos", "abstract": "Automated segmentation of the left ventricular endocardium in echocardiography videos is a key research area in cardiology. It aims to provide accurate assessment of cardiac structure and function through Ejection Fraction (EF) estimation. Although existing studies have achieved good segmentation performance, their results do not perform well in EF estimation. In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network (HSS-Net) for echocardiography video, aiming to improve EF estimation accuracy by synergizing local detail modeling with global dynamic perception. The network employs a hierarchical design, with low-level stages using convolutional networks to process single-frame images and preserve details, while high-level stages utilize the Mamba architecture to capture spatio-temporal relationships. The hierarchical design balances single-frame and multi-frame processing, avoiding issues such as local error accumulation when relying solely on single frames or neglecting details when using only multi-frame data. To overcome local spatio-temporal limitations, we propose the Spatiotemporal Cross Scan (STCS) module, which integrates long-range context through skip scanning across frames and positions. This approach helps mitigate EF calculation biases caused by ultrasound image noise and other factors. We achieved state-of-the-art results on three datasets. Our code is available at https://github.com/DF-W/HSS-Net.", "filename": "2025_0601.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Dongfang Wang", "Jian Yang", "Yizhe Zhang", "Tao Zhou"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0602", "x": 0.704, "y": 3.756, "title": "HybridMamba: A Dual-Domain Mamba for 3D Medical Image Segmentation", "abstract": "In the domain of 3D biomedical image segmentation, Mamba exhibits superior performance as it addresses the limitations in modeling long-range dependencies inherent to CNNs and mitigates the abundant computational overhead associated with Transformer-based frameworks when processing high-resolution medical volumes. However, attaching undue importance to global context modeling may inadvertently compromise critical local structural information, thus leading to boundary ambiguity and regional distortion in segmentation outputs. Therefore, we propose the HybridMamba, an architecture employing dual complementary mechanisms: 1) a feature scanning strategy that progressively integrates representations both axial-traversal and local-adaptive pathways to harmonize the relationship between local and global representations, and 2) a gated module combining spatial-frequency analysis for comprehensive contextual modeling. Besides, we collect a multi-center CT dataset related to lung cancer. Experiments on MRI and CT datasets demonstrate that HybridMamba significantly outperforms the state-ofthe-art methods in 3D medical image segmentation.", "filename": "2025_0602.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Weitong Wu", "Zhaohu Xing", "Jing Gong", "Qin Peng", "Lei Zhu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0603", "x": 3.193, "y": 1.67, "title": "IFRFNet: Iterative Frequency Restoration-Fusion Network for Fast System Matrix Calibration on Magnetic Particle Image", "abstract": "Magnetic Particle Imaging (MPI), an emerging technique with high sensitivity and resolution, requires time-consuming calibration for System Matrix (SM)-based reconstruction.Due to the strong locality and redundancy in the frequency domain, sparse sampling can capture sufficient information for rapid SM calibration without full-size SMs. However, it often leads to low-frequency energy leakage due to nonlinear magnetization of nanoparticles, causing the loss of low-frequency components. These components are essential for maintaining the SM's shape, and their absence leads to structural degradation and visible artifacts. Current methods tend to overemphasize high-frequency features, neglecting these low-frequency ones. Besides, single-step upsampling leads to error accumulation, especially with large scaling ratios, degrading reconstruction quality. To address these issues, we propose the Iterative Frequency Restoration-Fusion Network (IFRFNet), which uses an iterative frequency-domain restoration-fusion module. Unlike single-step upsampling, our approach refines, fuses, and upsamples high-and low-frequency features in stages, ensuring continuous optimization. This prevents error accumulation, preserves fine details, and maintains structural integrity. By iteratively recovering low-frequency components and refining highfrequency details, IFRFNet minimizes artifacts and retains crucial information. The Effective Upsampler further enhances the quality of the features, ensuring clear and realistic final SM volumes. Experiments on the OpenMPI dataset show that IFRFNet achieves SOTA performance.", "filename": "2025_0603.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Weixin Xu", "Penghua Zhai", "Jie Tian", "Wei Mu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0604", "x": -0.097, "y": 2.178, "title": "Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting", "abstract": "Real2Sim is becoming increasingly important with the rapid development of surgical artificial intelligence (AI) and autonomy. In this work, we propose a novel Real2Sim methodology, Instrument-Splatting, that leverages 3D Gaussian Splatting to provide fully controllable 3D reconstruction of surgical instruments from monocular surgical videos. To maintain both high visual fidelity and manipulability, we introduce a geometry pre-training to bind Gaussian point clouds on part mesh with accurate geometric priors and define a forward kinematics to control the Gaussians as flexible as real instruments. Afterward, to handle unposed videos, we design a novel instrument pose tracking method leveraging semantics-embedded Gaussians to robustly refine per-frame instrument poses and joint states in a render-and-compare manner, which allows our instrument Gaussian to accurately learn textures and reach photorealistic rendering. We validated our method on 2 publicly released surgical videos and 4 videos collected on ex vivo tissues and green screens. Quantitative and qualitative evaluations demonstrate the effectiveness and superiority of the proposed method. Our code is available at https:// github.com/jinlab-imvr/Instrument-Splatting.", "filename": "2025_0604.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Shuojue Yang", "Zijian Wu", "Mingxuan Hong", "Qian Li", "Daiyun Shen", "Septimiu E Salcudean", "Yueming Jin"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0605", "x": 2.811, "y": 5.016, "title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?", "abstract": "Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at https:// hyperbolic-anomalies.github.io.", "filename": "2025_0605.pdf", "year": 2025, "institution": "Lucerne University of Applied Sciences and Arts", "country": "Switzerland", "authors": ["Alvaro Gonzalez-Jimenez", "Simone Lionetti", "Ludovic Amruthalingam", "Philippe Gottfrois", "Fabian Gröger", "Marc Pouly", "Alexander A Navarini"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0606", "x": 3.535, "y": 2.41, "title": "LDDMEm: Large Deformation Diffeomorphic Metric Embedding", "abstract": "We present a method, open-source software, and experiments which embed arbitrary deformation vector fields produced by any method (e.g., ANTs or VoxelMorph) in the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework. This decouples formal diffeomorphic shape analysis from image registration, which has many practical benefits. Shape analysis can be added to study designs without modification to already chosen image registration methods and existing databases of deformation fields can be reanalyzed within the LDDMM framework without repeating image registrations. Pairwise time series studies can be extended to full time series regression with minimal added computing. The diffeomorphic rigor of image registration methods can be compared by embedding deformation fields and comparing projection distances. Finally, the added value of formal diffeomorphic shape analysis can be more fairly evaluated when it is derived from and compared to a baseline set of deformation fields. In brief, the method is a straightforward use of geodesic shooting in diffeomorphisms with a deformation field as the target, rather than an image. This is simpler than the image registration case which leads to a faster implementation that requires fewer user derived parameters.", "filename": "2025_0606.pdf", "year": 2025, "institution": "HHMI", "country": "USA", "authors": ["Greg M Fleishman", "P Thomas Fletcher"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0607", "x": 2.619, "y": 1.398, "title": "MDAA-Diff: CT-Guided Multi-dose Adaptive Attention Diffusion Model for PET Denoising", "abstract": "Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single lowdose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a doseconditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18 F-FDG and 68 Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available at https://github.com/Long0121/MDAA-Diff.", "filename": "2025_0607.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Xiaolong Niu", "Zanting Ye", "Xu Han", "Yanchao Huang", "Hao Sun", "Hubing Wu", "Lijun Lu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0608", "x": 3.634, "y": 6.686, "title": "Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound", "abstract": "Fetal abdominal malformations are serious congenital anomalies that require accurate diagnosis to guide pregnancy management and reduce mortality. Although AI has demonstrated significant potential in medical diagnosis, its application to prenatal abdominal anomalies remains limited. Most existing studies focus on image-level classification and rely on standard plane localization, placing less emphasis on case-level diagnosis. In this paper, we develop a case-level multiple instance learning (MIL)-based method, free of standard plane localization, for classifying fetal abdominal anomalies in prenatal ultrasound. Our contribution is three-fold. First, we adopt a mixture-of-attentionexperts module (MoAE) to weight different attention heads for various planes. Secondly, we propose a medical-knowledge-driven feature selection module (MFS) to align image features with medical knowledge, performing self-supervised image token selection at the case-level. Finally, we propose a prompt-based prototype learning (PPL) to enhance the MFS. Extensively validated on a large prenatal abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748 images and", "filename": "2025_0608.pdf", "year": 2025, "institution": "Medical School Shenzhen University", "country": "China", "authors": ["Huanwen Liang", "Jingxian Xu", "Yuanji Zhang", "Yuhao Huang", "Yuhan Zhang", "Xin Yang", "Ran Li", "Xuedong Deng", "Yanjun Liu", "Guowei Tao", "Yun Wu", "Sheng Zhao", "Xinru Gao", "Dong Ni"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0609", "x": 4.873, "y": 5.294, "title": "MGG-Net: A Multi-modal Feature Extraction and Global-Aware Feature Graph-Based Deep Learning Network for MGMT Status Classification in Glioma", "abstract": "Gliomas, especially high-grade gliomas, have a high mortality rate. O6-Methylguanine-DNA Methyltransferase (MGMT) status is crucial for gliomas treatment and prognosis. Traditional diagnosis relies on invasive tissue analysis, which is often infeasible for high-risk patients. While machine learning and deep learning methods using multi-sequence Magnetic Resonance Imaging (MRI) images and radiomics provides a non-invasive alternative, existing methods suffer from low accuracy and poor generalization due to challenges in extracting features from the integrated multi-sequence representation. To address this issue, we propose a Multi-modal feature extraction and Global-aware feature Graphbased deep learning network (MGG-Net), integrating convolutional neural network (CNN) and graph convolutional network (GCN) for multimodal and multi-scale feature learning. Specifically, MGG-Net consists of multiple CNN-GCN stages, responsible for processing MRI image features and radiomic features at different scales. CNN blocks are used to extract fine-grained and sequence-specific local features from each MRI sequence. These features are then fed into a GCN, which models long-range dependencies and extracts high-level global representations. Finally, the fused multi-scale features extracted are used for classification. Experimental results demonstrate that MGG-Net outperforms previous approaches, effectively leveraging multi-scale and multi-modal information for improved MGMT status classification.", "filename": "2025_0609.pdf", "year": 2025, "institution": "Tohoku University", "country": "Japan", "authors": ["Haoyang Liu", "Yuwen Zeng", "Xiaoyong Zhang", "Wentong Zhou", "Arata Nagai", "Masayuki Kanamori", "Hidenori Endo", "Noriyasu Homma"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0610", "x": 3.103, "y": 4.295, "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "abstract": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging technique with great potential for diagnosing anterior uveitis, a vision-threatening condition. This condition is characterized by the presence of inflammatory cells in the eye's anterior chamber (AC). Automatic detection of these cells on AS-OCT images has attracted great attention. However, this task is challenging since each cell is minuscule (extremely small), representing less than 0.005% of the high-resolution image. Moreover, pixel-level noise introduced by OCT can be misclassified as cells, leading to false positive detections. These challenges make both traditional image processing algorithms and stateof-the-art (SOTA) deep learning object detection methods ineffective for this task. To that end, we propose a minuscule cell detection framework that progressively refines the field-of-view from the whole image to the AC region, and further to minuscule regions potentially containing individual cells. Our framework consists of: (1) a Field-of-Focus module that uses a vision foundation model to zero-shot segment the AC region, and (2) a Fine-grained Object Detection module that introduces Minuscule Region Proposal followed by our Cell Mamba to distinguish individual cells from noise. Experimental results demonstrate that our framework outperforms SOTA methods, improving F1 by around 7% over the best baseline and offering a more reliable alternative for cell detection. Our code is available at: https://github.com/joeybyc/MCD.", "filename": "2025_0610.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Boyu Chen", "Ameenat Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0611", "x": 2.114, "y": 4.624, "title": "MixStyleFlow: Domain Generalization in Medical Image Segmentation Using Normalizing Flows", "abstract": "Despite the success of deep learning in medical image segmentation, domain shifts caused by variations in scanners and imaging protocols often degrade performance, limiting real-world clinical deployment. Domain generalization (DG) aims to address this issue by learning robust models that generalize well across different domains. While existing DG methods based on featurespace domain randomization have shown promise, they suffer from a limited and unordered search space of feature styles. In this work, we propose MixStyleFlow, a novel DG approach that utilizes normalizing flows to explicitly model the distribution of domain feature styles. By sampling domain feature styles from the learned normalizing flows and mixing them with original feature statistics along the feature channel dimension, our method effectively expands and diversifies domain features in a controllable manner. We evaluate MixStyleFlow on two medical segmentation tasks-prostate MRI and fundus imaging-demonstrating superior generalization performance on unseen target-domain data. Our results highlight the potential of normalizing flows for improving domain generalization in medical image segmentation, paving the way for more robust deep learning models capable of handling diverse clinical scenarios. The code is available at https://git hub.com/Reza-Safdari/MixStyleFlow.", "filename": "2025_0611.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Reza Safdari", "Mohammad-Ali Nikouei Mahani", "Mohamad Koohi-Moghadam", "Kyongtae Tyler Bae"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0612", "x": 2.574, "y": 3.331, "title": "MoDiff: A Morphology-Emphasized Diffusion Model for Ambiguous Medical Image Segmentation", "abstract": "MoDiff is a morphology-emphasized diffusion model designed for ambiguous medical image segmentation. It replaces traditional onehot encoding with probability-based label maps to capture inherent uncertainties and ensure consistent segmentation results. By determining the presence of individual radiologist labels, MoDiff enables diverse sampling that provides richer insights into ambiguous areas. Its Learnable Discrete Frequency Filter (LDF) extracts high-frequency details for improved boundary precision, and when integrated with the Morphologybased Cross Attention Network (MCA), it enhances feature synthesis for more accurate anatomical segmentation. Evaluations on the LIDC-IDRI and MS-MRI datasets confirm its superior accuracy, boundary precision, and consistency.", "filename": "2025_0612.pdf", "year": 2025, "institution": "Yonsei University Mirae Campus", "country": "Republic of Korea", "authors": ["Jung Su Ahn", "Ki Hoon Kwak", "Jung Woo Seo", "Young-Rae Cho"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0613", "x": 1.428, "y": 4.022, "title": "MOST: MR Reconstruction Optimization for Multiple DownStream Tasks via Continual Learning", "abstract": "Deep learning-based Magnetic Resonance (MR) reconstruction methods have focused on generating high-quality images but often overlook the impact on downstream tasks (e.g., segmentation) that utilize the reconstructed images. Cascading separately trained reconstruction network and downstream task network has been shown to introduce performance degradation due to error propagation and the domain gaps between training datasets. To mitigate this issue, downstream taskoriented reconstruction optimization has been proposed for a single downstream task. In this work, we extend the optimization to handle multiple downstream tasks that are introduced sequentially via continual learning. The proposed method integrates techniques from replay-based continual learning and image-guided loss to overcome catastrophic forgetting. Comparative experiments demonstrated that our method outperformed a reconstruction network without finetuning, a reconstruction network with naïve finetuning, and conventional continual learning methods. The source code is available at: https://github.com/SNU-LIST/ MOST.", "filename": "2025_0613.pdf", "year": 2025, "institution": "Seoul National University", "country": "Korea", "authors": ["Hwihun Jeong", "Se Young Chun", "Jongho Lee"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0614", "x": 4.166, "y": 3.199, "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentation in 4D Ultrasound", "abstract": "Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per-and interphase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75 mm). Both the code and the dataset are available at https://github.com/crs524/MTCNet.", "filename": "2025_0614.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0615", "x": 3.594, "y": 1.652, "title": "Multifrequency Neural Network-Based Wave Inversion in MR Elastography", "abstract": "Magnetic Resonance Elastography (MRE) is a non-invasive imaging modality quantifying soft tissue stiffness. The reconstruction of stiffness maps is based on solutions of an inverse problem, which poses challenges in balancing accuracy, computational resources, and robustness. To stabilize the reconstruction, many inversion techniques, and most recently neural network-based inversion techniques, have explored multifrequency acquisition and reconstruction. However, these techniques typically perform separate single-frequency inversions followed by multifrequency aggregation. In this work, we propose a fully multifrequency neural network-based inversion trained on synthetically generated data that directly incorporates the relationship between multifrequency acquisitions, assuming a viscoelastic material model. Our proposed approach provides flexibility with respect to the acquisition frequencies, ensuring its practical applicability in the clinical and research setting. We evaluated our method using finite element simulations and in vivo abdominal MRE datasets, achieving increased accuracy and providing a more reliable and effective solution for MRE-based tissue characterization than standard reconstruction approaches.", "filename": "2025_0615.pdf", "year": 2025, "institution": "Deutsches Herzzentrum der Charité", "country": "Germany", "authors": ["Héloïse Bustin", "Tom Meyer", "Jakob Jordan", "Lars Walczak", "Heiko Tzschätzsch", "Ingolf Sack", "Anja Hennemuth"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0616", "x": 4.473, "y": 4.088, "title": "Multi-level Gated U-Net for Denoising TMR Sensor-Based MCG Signals", "abstract": "Tunnel magnetoresistance (TMR) sensors have been recognized as a cost-effective alternative for measuring magnetocardiography (MCG) signals. However, their relatively high noise levels and susceptibility to contamination limit their practical clinical applications. To address these challenges, we propose a novel Multi-Level Gated U-Net (MGU-Net) model specifically designed for denoising long sequential MCG signals obtained from TMR sensors. The MGU-Net leverages the U-Net architecture to learn hierarchical representations, integrated with a novel Gated Linear Unit (GLU) module to capture the periodic pattern of Q, R, and S wave complex (QRS complex) from MCG. This design enhances periodic cardiac signatures and suppresses irregular noise components through adaptive gating mechanisms. We have developed a TMR-based MCG system and collected both simulated and real MCG data in a magnetically shielded environment. The results show that our method improve signal-to-noise ratio (SNR) from -2.142 dB to 10.505 dB on the simulated MCG dataset and from 3.958 dB to 14.514 dB on the real dataset, surpassing other state-of-the-art methods. Our model successfully recovers subtle P-wave and T-wave features from the noisy signals, illustrating a promising direction of using TMR-based systems for potential practical clinical applications.", "filename": "2025_0616.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Zeyu Xing", "Hao Li", "Hao Dou", "Zhong Zheng", "Jingguo Dai", "Chen Wang", "Jian Cui", "Xin Zhang", "Tianzi Jiang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0617", "x": 3.901, "y": 3.618, "title": "Multiscale Graph and Multi-step Cross-Frame Mamba for Myocarditis Lesion Segmentation", "abstract": "Myocarditis, an acute cardiac disorder progressing rapidly to life-threatening heart failure, requires precise lesion segmentation from Cine Magnetic Resonance Imaging (Cine-MRI) for timely intervention. Current segmentation accuracy is limited by two key challenges: 1) spatiotemporal discordance between myocardial motion patterns and evolving pathological features and 2) morphological complexity (irregular borders, scattered lesions). In this paper, we propose the MG-Mamba, a framework integrating deep state space models with graph-based spatiotemporal analysis. The architecture employs Mamba blocks to establish initial intra-/inter-frame dependencies in Cine-MRI sequences. For Challenge 1, we improve the detection of subtle abnormal motions through multi-step cross-frame analysis, extending beyond conventional adjacent-frame analysis. For Challenge 2, we further implement multiscale patch division and constructs inter-patch graphs to concurrently capture global lesion distribution and local geometric patterns. Extensive evaluations on SYC-QC and SYC-SX clinical datasets demonstrate MG-Mamba's superior segmentation accuracy over ten state-of-the-art benchmarks, significantly advancing myocarditis diagnostic precision. The code is available at https://github.com/userZ-CY/MICCAI.", "filename": "2025_0617.pdf", "year": 2025, "institution": "Anhui University", "country": "China", "authors": ["Chengjin Yu", "Hao Zhang", "Yuanting Yan", "Dong Zhang", "Sangyin Lv", "Cailing Pu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0618", "x": 1.24, "y": 2.014, "title": "OsteoOpt: A Bayesian Optimization Framework for Enhancing Bone Union Likelihood in Mandibular Reconstruction Surgery", "abstract": "Mandibular reconstruction is crucial after oral tumor resection, yet current methods rely on premorbid geometric approximations and struggle with achieving reliable donor-native bone union. We propose a Bayesian optimization framework that enhances predicted bone union likelihood and facilitates computer-aided intervention by systematically varying key surgical parameters-resection plane orientation, donor bone positioning, and graft length-across three mandibular regions. Reconstruction performance is evaluated using two cost functions, coupled with a sensitivity analysis on modeling parameters. We validated the model using longitudinal patient-specific data from 5-day and 1-year postoperative CT and MRI scans. Our results show that optimization significantly enhances the predicted likelihood of bone union, with a relative improvement of up to 329% compared to the standard surgical practice. Additionally, validation shows a Dice coefficient of up to 0.76 between union prediction and actual postoperative imaging data. This study suggests that modifying the standard surgical plan can significantly improve bone union, underscoring the need for advanced optimization frameworks in surgical planning. The open-source code is available on GitHub.", "filename": "2025_0618.pdf", "year": 2025, "institution": "University of British Columbia", "country": "Canada", "authors": ["Hamidreza Aftabi", "John E Lloyd", "Amanda Ding", "Benedikt Sagl", "Eitan Prisman", "Antony Hodgson", "Sidney Fels"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0619", "x": 3.277, "y": 1.211, "title": "PD-INR: Prior-Driven Implicit Neural Representations for TOF-PET Reconstruction", "abstract": "Positron emission tomography (PET) reconstruction is a challenging inverse problem, where projection data often contain low statistics. While current supervised learning methods offer strong noise suppression abilities, they may suffer from generalization issues and are in many cases not accurate quantitatively. To overcome these challenges, we propose a novel self-supervised Time-of-Flight PET (TOF-PET) reconstruction framework that utilizes Implicit Neural Representations (INR) to model PET images. Specifically, we introduce a differentiable forward projection model based on the imaging mechanism for TOF-PET and reformulate TOF-PET reconstruction problem using INR. To enhance image smoothness, we develop a ray-based total variation (TV) regularization term, distinct from the traditional TV. For the internal structure of our INR, we integrate a multi-resolution hash encoder with our designed prior-image encoder, where the latter provides sufficient image prior and always delivers reliable initial reconstructions for arbitrary network depth. Experiments on brain and chest datasets show that our method outperforms traditional iterative algorithms and self-supervised approaches in noise suppression and contrast recovery. Compared to conventional NeRF-based architectures, our model is more compact and converges faster, providing an efficient solution for TOF-PET reconstruction. The source code repository is hosted on GitHub: https://github. com/zyl123300/PD-INR.git.", "filename": "2025_0619.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Yuxuan Long", "Yulin Zhang", "Hong Wang", "Xiaodong Kuang", "Hailiang Huang", "Fan Rao", "Huafeng Liu", "Yefeng Zheng", "Wentao Zhu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0620", "x": 0.885, "y": 4.362, "title": "PFESA: FFT-Based Parameter-Free Edge and Structure Attention for Medical Image Segmentation", "abstract": "The U-Net architecture remains pivotal in medical image segmentation, yet its skip connections often propagate redundant noise and compromise edge information. We propose a Parameter-Free Edge and Structure Attention (PFESA) based on Fast Fourier Transform (FFT) to address these limitations. PFESA employs frequency-domain feature decoupling to separate high-frequency (edge details) and lowfrequency (structural components) representations. Leveraging feature Signal-to-Noise Ratio(SNR) analysis, we devise dual attention paths: a High-frequency Edge Attention (EA) enhances gradient-sensitive regions to preserve anatomical contours, while a Low-frequency Structure Attention (SA) suppresses noise through energy redistribution. This frequencyaware attention mechanism enables adaptive feature refinement in skip connections without introducing trainable parameters. The parameterfree design ensures robustness against overfitting in medical datasets with scarce data. Extensive experiments on multi modal 2D/3D medical image datasets demonstrate PFESA's superiority over existing attention methods, achieving SOTA performance with statistically significant improvements in Dice Similarity Coefficient (DSC: +3.3% vs. baseline) and Hausdorff Distance metrics. Code is available at: https://github. com/59-lmq/PFESA.", "filename": "2025_0620.pdf", "year": 2025, "institution": "South China Normal University", "country": "China", "authors": ["Mingqian Li", "Zhiqian Yan", "Miaoning Yan", "Yaodong Liang", "Qingmao Zhang", "Qiongxiong Ma"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0621", "x": 3.549, "y": 1.423, "title": "Physics-Informed Implicit Neural Representations for Joint B0 Estimation and Echo Planar Imaging", "abstract": "Echo Planar Imaging (EPI) is widely used for its rapid acquisition but suffers from severe geometric distortions due to B0 inhomogeneities, particularly along the phase encoding direction. Existing methods follow a two-step process: reconstructing blip-up/down EPI images, then estimating B0, which can introduce error accumulation and reduce correction accuracy. This is especially problematic in high B0 regions, where distortions align along the same axis, making them harder to disentangle. In this work, we propose a novel approach that integrates Implicit Neural Representations (INRs) with a physics-informed correction model to jointly estimate B0 inhomogeneities and reconstruct distortion-free images from rotated-view EPI acquisitions. INRs offer a flexible, continuous representation that inherently captures complex spatial variations without requiring predefined grid-based field maps. By leveraging this property, our method dynamically adapts to subject-specific B0 variations and improves robustness across different imaging conditions. Experimental results on 180 slices of brain images from three subjects demonstrate that our approach outperforms traditional methods in terms of reconstruction quality and field estimation accuracy (Code available: https://github.com/wenqihuang/PINR).", "filename": "2025_0621.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Wenqi Huang", "Nan Wang", "Congyu Liao", "Yimeng Lin", "Mengze Gao", "Daniel Rueckert", "Kawin Setsompop"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0622", "x": 3.38, "y": 1.123, "title": "Physiological Neural Representation for Personalised Tracer Kinetic Parameter Estimation from Dynamic PET", "abstract": "Dynamic positron emission tomography (PET) with tracer [ 18 F]FDG enables non-invasive quantification of glucose metabolism by means of kinetic analysis, often modelled by the two-tissue compartment model (TCKM). However, voxel-wise kinetic parameter estimation using conventional methods is computationally intensive and limited by spatial resolution. Deep neural networks (DNNs) offer an alternative but require large training datasets and significant computational resources. To address these limitations, we propose a physiological neural representation based on implicit neural representations (INRs) for personalized kinetic parameter estimation. INRs, which learn continuous functions, allow for efficient, high-resolution parametric imaging with reduced data requirements. Our method also integrates anatomical priors from a 3D CT foundation model to enhance robustness and precision in kinetic modelling. We evaluate our approach on an [ 18 F]FDG dynamic PET/CT dataset and compare it to state-of-the-art DNNs. Results demonstrate superior spatial resolution, lower mean-squared error, and improved anatomical consistency, particularly in tumour and highly vascularized regions. Our findings highlight the potential of INRs for personalized, data-efficient tracer kinetic modelling, enabling applications in tumour characterization, segmentation, and prognostic assessment. The code is available at: https://github.com/tkartikay/PhysNRPET.", "filename": "2025_0622.pdf", "year": 2025, "institution": "University Hospital Augsburg (UKA)", "country": "Germany", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0623", "x": 2.946, "y": 2.159, "title": "Q-Space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis", "abstract": "This study, we propose a novel Q-space Guided Collaborative Attention Translation Networks (Q-CATN) for multi-shell, highangular resolution DWI (MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly acquired structural MRI data. Q-CATN employs a collaborative attention mechanism to effectively extract complementary information from multiple modalities and dynamically adjust its internal representations based on flexible q-space information, eliminating the need for fixed sampling schemes. Additionally, we introduce a range of task-specific constraints to preserve anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic relationships between directional DWI signal distributions and q-space. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD, and QGAN, in estimating parameter maps and fiber tracts both quantitatively and qualitatively, while preserving fine-grained details. Notably, its ability to accommodate flexible q -space sampling highlights its potential as a promising toolkit for clinical and research applications. Our code is available at https://github.com/ Idea89560041/Q-CATN.", "filename": "2025_0623.pdf", "year": 2025, "institution": "Hong Kong Polytechnic University", "country": "China", "authors": ["Pengli Zhu", "Yingji Fu", "Nanguang Chen", "Anqi Qiu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0624", "x": 1.895, "y": 1.973, "title": "RDMR: Recursive Inference and Representation Disentanglement for Multimodal Large Deformation Registration", "abstract": "Multimodal large deformation image registration is a challenging task in medical imaging, primarily due to significant modality differences and large tissue deformations. Current methods typically employ dual-branch multiscale pyramid registration networks. However, the dual-branch structure fails to explicitly enforce that the model learns modality-invariant image registration features. Furthermore, in the multiscale registration process, only the deformation field is propagated, which restricts the model's capacity to accommodate more complex deformations. To enhance the model's ability to learn features from different modalities, we propose a modality representation disentanglement method, incorporating Multi-layer Contrastive Loss(MCL) to enforce the learning of modality-invariant features. To address the challenge of complex large deformations, we introduce a Multi-Scale Feature fusion Registration module(MSFR), which integrates features and deformation fields from different scales during the registration process. To explore the registration potential of the trained model, we propose a Recursive Inference enhancement strategy that further improves registration performance. This model is referred to as RDMR. Based on experimental results from both private and public datasets, the RDMR model outperforms other SOTA models. Compared to the baseline registration model (Voxel Morph), the RDMR model achieved improvements of 1.4 and 4.5% points in the DSC metric, respectively. Our code is publicly available at:https://github.com/ybby2020/RDMR.", "filename": "2025_0624.pdf", "year": 2025, "institution": "Shanghai JiaoTong University", "country": "China", "authors": ["Yibo Hu", "Ziqi Zhao", "Qi Zhang", "Lisa X Xu", "Jianqi Sun"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0625", "x": 1.681, "y": 2.574, "title": "Real Super-Resolution for Proximal Femur: Enhanced Computation of Structural Bone Metrics from Clinical CTs", "abstract": "Fracture risk due to osteoporosis is a highly prevalent disease with costs in the European Union alone of 56 billion p.a. Accurate assessment of the microarchitecture of the proximal femur (e.g., trabecular thickness, trabecular spacing, bone volume fraction) is essential for assessing bone strength and predicting fracture risk. High resolution (HR) CT provides the necessary spatial resolution. However, for best hip fracture risk assessment HR-CT imaging should be performed at the proximal femur but this would require an unacceptably high level of radiation dose. Therefore, we aimed to investigate whether deep learning based super-resolution (SR) models applied to low-resolution (LR) clinical CT images permit improved assessment of structural parameters.In this study we adapted and optimized state-of-the-art model architectures to compare them in the context of CT-SR of the proximal femur. The dataset used consisted of pairs of clinical LR-CTs and HR-CTs of 50 individuals. This represents clinical reality and avoids bias of downsampling HR images to mimic LR images. Using automated preprocessing data is prepared for model training. We used three-stage template matching of point clouds to automatically extract the relevant regions of interest, from which metrics for bone microarchitecture were determined. We compared SRGAN, Real-ESRGAN+, LDM, and ResShift regarding improvement in structural assessment. We also tested whether 2.5D approaches -using multiple slices of the CT-are superior to 2D approaches. In terms of perceptual reconstruction, the ResShift 2.5D model outperforms the other SR models and achieves comparable results to the Real-ESRGAN+ architectures in the derivation of biomechanical properties.", "filename": "2025_0625.pdf", "year": 2025, "institution": "Kiel University", "country": "Germany", "authors": ["Niklas C Koser", "Marten J Finck", "Felix N Von Brackel", "Benjamin Ondruschka", "Sören Pirk", "Null- C Glüer"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0626", "x": 0.6, "y": 5.119, "title": "Region-Based Text-Consistent Augmentation for Multimodal Medical Segmentation", "abstract": "Medical image segmentation is crucial for various clinical applications, and deep learning has significantly advanced this field. To further enhance performance, recent research explores multimodal data integration, combining medical images and textual reports. However, a critical challenge lies in image data augmentation for multimodal medical data, specifically in maintaining text-image consistency. Traditional augmentation techniques, designed for unimodal images, can introduce mismatches between augmented images and text, hindering effective multimodal learning. To address this, we introduce Region-Based Text-Consistent Augmentation (RBTCA), a novel framework for coherent multimodal augmentation. Our approach performs region-based image augmentation by first identifying image regions described in associated text reports and then extracting textual cues grounded in these regions. These cues are integrated into the image, and augmentation is subsequently performed on this modality-aware representation, ensuring inherent text-cue consistency. Notably, the RBTCA's plug-and-play design allows for straightforward integration into existing medical image analysis pipelines, enhancing its practical utility. We demonstrate the efficacy of our framework on the QaTa-Covid19 and our in-house Lung Tumor CT Segmentation (LTCT) datasets, achieving substantial gains, with a Dice coefficient improvement of up to 7.24% when integrated into baseline segmentation models. Our code will be released on https://github. com/KunyanCAI/RBTCA.", "filename": "2025_0626.pdf", "year": 2025, "institution": "Macao Polytechnic University", "country": "China", "authors": ["Kunyan Cai", "Chenggang Yan", "Min He", "Liangqiong Qu", "Shuai Wang", "Tao Tan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0627", "x": 0.974, "y": 4.315, "title": "ReSeg-UNet: A Reconstruction-Guided Optimization Framework for Enhanced Medical Image Segmentation", "abstract": "Medical image segmentation is critical for accurate diagnosis; however, the task remains challenging due to the inherent ambiguities in low-contrast anatomical boundaries and the presence of extensive redundant features in the skip connections of segmentation models. To address these limitations, we propose ReSeg-UNet, a novel two-stage framework that synergizes image reconstruction with segmentation optimization. In the first stage, a composite reconstruction loss-combining Mean Squared Error (MSE) and L1 regularization is applied to a standard segmentation network, generating stable reconstruction weights that encode multi-scale feature representations. These weights explicitly capture both global anatomical context and local boundary details. In the second stage, a three-level cross-feature alignment mechanism is introduced: the encoder of the reconstruction model is aligned with the decoder of the segmentation model, the decoder of the former is aligned with the encoder of the latter, and the intermediate features of both models are also aligned. This strategy ensures multi-level feature consistency during downsampling, intermediate layers, and upsampling, effectively mitigating information loss in blurred regions. Extensive experiments on the Synapse (abdominal CT) and ACDC (cardiac MRI) datasets demonstrate significant improvements. Our code is available at https://github. com/Li-gzhu/ReSeg-UNet.git.", "filename": "2025_0627.pdf", "year": 2025, "institution": "Guangzhou University", "country": "China", "authors": ["Lin Li", "Dong Tang", "Xiaowen Chu", "Xiaofei Yang", "Fei Yu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0628", "x": 3.561, "y": 2.199, "title": "Reverse Imaging for Wide-Spectrum Generalization of Cardiac MRI Segmentation", "abstract": "Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem.Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this \"spin prior\" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable \"latent variable\" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.", "filename": "2025_0628.pdf", "year": 2025, "institution": "Delft University of Technology", "country": "The Netherlands", "authors": ["Yidong Zhao", "Peter Kellman", "Hui Xue", "Tongyun Yang", "Yi Zhang", "Yuchi Han", "Orlando Simonetti", "Qian Tao"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0629", "x": 2.458, "y": 1.457, "title": "Self is the Best Learner: CT-Free Ultra-low-Dose PET Organ Segmentation via Collaborating Denoising and Segmentation Learning", "abstract": "Organ segmentation in Positron Emission Tomography (PET) plays a vital role in cancer quantification. Low-dose PET (LDPET) provides a safer alternative by reducing radiation exposure. However, the inherent noise and blurred boundaries make organ segmentation more challenging. Additionally, existing PET organ segmentation methods rely on co-registered Computed Tomography (CT) annotations, overlooking the problem of modality mismatch. In this study, we propose LDOS, a novel CT-free ultra-LDPET organ segmentation pipeline. Inspired by Masked Autoencoders (MAE), we reinterpret LDPET as a naturally masked version of Full-Dose PET (FDPET). LDOS adopts a simple yet effective architecture: a shared encoder extracts generalized features, while task-specific decoders independently refine outputs for denoising and segmentation. By integrating CT-derived organ annotations into the denoising process, LDOS improves anatomical boundary recognition and alleviates the PET/CT misalignments. Experiments demonstrate that LDOS achieves state-of-the-art performance with mean Dice scores of 73.11% ( 18 F-FDG) and 73.97% ( 68 Ga-FAPI) across 18 organs in 5% dose PET. Our code will be available at https://github. com/yezanting/LDOS.", "filename": "2025_0629.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Zanting Ye", "Xiaolong Niu", "Xu Han", "Xuanbin Wu", "Wantong Lu", "Yijun Lu", "Hao Sun", "Yanchao Huang", "Hubing Wu", "Lijun Lu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0630", "x": 2.109, "y": 2.448, "title": "Semi-Supervised Deformation-Free Image-to-Image Translation for Realistic CT Synthesis from CBCT", "abstract": "Cone-Beam Computed Tomography (CBCT) is widely used for diagnostics and treatment planning in oral and maxillofacial field due to its low radiation dose and high spatial resolution. Still, its clinical utility is limited by low contrast and incorrect Hounsfield Unit (HU) values. In contrast, multi-detector CT (CT) provides high contrast and reliable HU measurements, with a higher radiation dose. In this work, we present a novel two-stage framework for unpaired CBCTto-CT synthesis that ensures the exact preservation of anatomical structure, maintains high resolution, and achieves accurate HU value. In the first stage, we generate pseudo-paired CT images. In the second stage, we utilize a UNet++ generator enhanced with Interpolation and Convolution Upsampling (ICUP), Edge-Conditioned Skip Connections (ECSC), and a dual discriminator strategy for a semi-supervised approach. Consequently, we generate realistic CT images using pseudopaired CT images. Extensive quantitative and qualitative evaluations demonstrate that our method outperforms existing unpaired translation techniques, producing realistic CT images that closely match CT images in both HU accuracy and exactly preserve anatomical structure of the CBCT. The code is available at https://github.com/HANJIYONG/ Semi-Supervised-Deformation-Free-I2I.", "filename": "2025_0630.pdf", "year": 2025, "institution": "Seoul National University", "country": "Republic of Korea", "authors": ["Ji Yong Han", "Su Yang", "Sujeong Kim", "Sunjung Kim", "Sang-Heon Lim", "Heejin Yun", "Dahee Kim", "Won-Jin Yi"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0631", "x": 1.072, "y": 2.159, "title": "Sim-to-Real Transformer-Based Shape Reconstruction for Automated Orthopedic Fracture Reduction Planning", "abstract": "Accurate orthopedic fracture reduction planning is essential for ensuring successful postoperative recovery and improving patient outcomes. However, current methods are challenged by the complex and irregular fracture geometries and the scarcity of annotated training data. To address these challenges, we propose a novel approach that integrates learning-based shape restoration and fracture simulation. A transformerbased model is developed, which utilizes patch-to-patch restoration and recursive fragment registration to iteratively refine fracture reduction poses. To generate diverse and anatomically realistic fractured datasets for model training, we develop a fracture data simulation approach that combines statistical shape modeling with clinically representative fracture patterns, reducing reliance on annotated samples. Tested on extensive clinical data with hipbone and sacrum fractures, the proposed method achieved mean translational and rotational errors of 2.34 mm and 4.54 • , respectively, outperforming both template-based and existing learning-based methods. Our approach enhances learning and generalization for automated fracture reduction by connecting synthetic and real-world fracture data.", "filename": "2025_0631.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Sutuke Yibulayimu", "Yanzhen Liu", "Yudi Sang", "Gang Zhu", "Hui Li", "Hao Lu", "Chunpeng Zhao", "Xinbao Wu", "Yu Wang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0632", "x": 4.102, "y": 4.136, "title": "SMF-Net: Unlocking Multimodal Insights for Enhanced Stroke Lesion Segmentation", "abstract": "Stroke is a leading cause of death and disability worldwide, necessitating accurate lesion segmentation for effective diagnosis and treatment. Multimodal images provide complementary insights into stroke detection and progression. However, existing segmentation methods often struggle to fully leverage the distinct and dynamic sensitivities of these modalities. Current approaches, including encoder-decoder networks and SAM-based models, are either limited to single-modality data or rely on suboptimal fusion techniques, hindering their ability to adapt to the distinct nature of stroke lesions. To address these challenges, we propose SAM-driven Multimodal Fusion Network (SMF-Net) for enhanced stroke lesion segmentation. SMF-Net incorporates a multimodal Siamese image encoder based on the Swin Transformer to extract modality-specific features, alongside two novel fusion strategies: (1) Complementary dynamic fusion module, which uses pairwise co-attention and dynamic learnable weights to model interdependencies and adaptively combine multimodal features; and (2) Context-aware intermediatelayer fusion module, a lightweight, multi-layer fusion mechanism that captures multiscale features while preserving modality-specific information. Extensive experiments on an open benchmark dataset demonstrate that SMF-Net outperforms previous stroke lesion segmentation methods through effective multimodal integration.", "filename": "2025_0632.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Meklit Atlaw", "Geng Chen", "Haotian Jiang", "Xuyun Wen", "Hengfei Cui", "Yong Xia"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0633", "x": 6.127, "y": 3.727, "title": "Spatio-Temporal Pre-Trained Foundation Model for Neural Decoding with Fine-Grained Optimization", "abstract": "Traditional neural decoding methods are heavily based on fully annotated brain data, which are both expensive to produce and scarce in availability. This limitation hinders the development of accurate and generalizable decoding models. Drawing inspiration from the success of foundational AI models in reducing dependency on annotated data in fields such as natural language processing, we introduce a novel foundation model that leverages the inherent spatiotemporal covariation of functional brain networks, which enables effective neural decoding with minimal annotation requirements. Our framework incorporates three key innovations: 1) A spatiotemporal importance-guided augmentation strategy is designed to capture the synergistic relationships between brain regions and their dynamic changes; 2) A progressive spatiotemporal-aware encoder is proposed to learn local-to-global brain interaction information; 3) A fine-grained consistency optimization technique is developed to enhance the representations of overall brain function. Evaluations of publicly available fMRI datasets demonstrate that our proposed framework not only achieves superior decoding performance, but also exhibits strong generalizability and reveals patterns of nervous activity. Our research advances brain representation learning and provides an innovative solution for universal neural decoding models.", "filename": "2025_0633.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Ziyu Li", "Zhiyuan Zhu", "Yang Bai", "Qing Li", "Xia Wu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0634", "x": 3.331, "y": 2.355, "title": "Speech Audio Generation from Dynamic MRI via a Knowledge Enhanced Conditional Variational Autoencoder", "abstract": "Dynamic Magnetic Resonance Imaging (MRI) of the vocal tract has become an increasingly adopted imaging modality for speech motor studies. Beyond image signals, systematic data loss, noise pollution, and audio file corruption can occur due to the unpredictability of the MRI acquisition environment. In such cases, generating audio from images is critical for data recovery in both clinical and research applications. However, this remains challenging due to hardware constraints, acoustic interference, and data corruption. Existing solutions, such as denoising and multi-stage synthesis methods, face limitations in audio fidelity and generalizability. To address these challenges, we propose a Knowledge Enhanced Conditional Variational Autoencoder (KE-CVAE), a novel two-step \"knowledge enhancement + variational inference\" framework for generating speech audio signals from cine dynamic MRI sequences. This approach introduces two key innovations: (1) integration of unlabeled MRI data for knowledge enhancement, and (2) a variational inference architecture to improve generative modeling capacity. To the best of our knowledge, this is one of the first attempts at synthesizing speech audio directly from dynamic MRI video sequences. The proposed method was trained and evaluated on an open-source dynamic vocal tract MRI dataset recorded during speech. Experimental results demonstrate its effectiveness in generating natural speech waveforms while addressing MRI-specific acoustic challenges, outperforming conventional deep learning-based synthesis approaches (https://github.", "filename": "2025_0634.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "Hong Kong, China", "authors": ["Yaxuan Li", "Han Jiang", "Yifei Ma", "Shihua Qin", "Jonghye Woo", "Fangxu Xing"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0635", "x": 3.715, "y": 2.816, "title": "Steerable Anatomical Shape Synthesis with Implicit Neural Representations", "abstract": "Generative modeling of anatomical structures plays a crucial role in virtual imaging trials, which allow researchers to perform studies without the costs and constraints inherent to in vivo and phantom studies. For clinical relevance, generative models should allow targeted control to simulate specific patient populations rather than relying on purely random sampling. In this work, we propose a steerable generative model based on implicit neural representations. Implicit neural representations naturally support topology changes, making them well-suited for anatomical structures with varying topology, such as the thyroid. Our model learns a disentangled latent representation, enabling finegrained control over shape variations. Evaluation includes reconstruction accuracy and anatomical plausibility. Our results demonstrate that the proposed model achieves high-quality shape generation while enabling targeted anatomical modifications.", "filename": "2025_0635.pdf", "year": 2025, "institution": "University of Twente", "country": "The Netherlands", "authors": ["Bram De Wilde", "Max T Rietberg", "Guillaume Lajoinie", "Jelmer M Wolterink"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0636", "x": 3.627, "y": 2.166, "title": "T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models", "abstract": "Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation leveldependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a timedependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification. The code is available at repository.", "filename": "2025_0636.pdf", "year": 2025, "institution": "Monash University Malaysia", "country": "Malaysia", "authors": ["Hwa Hui Tew", "Junn Yong Loo", "Yee-Fan Tan", "Xinyu Tang", "Hernando Ombao", "Fuad Noman", "Null- W Phan", "Chee-Ming Ting"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0637", "x": 0.48, "y": 5.049, "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "abstract": "Common prototype-based medical image few-shot segmentation (FSS) methods model foreground and background classes using class-specific prototypes. However, given the high variability of the background, a more promising direction is to focus solely on foreground modeling, treating the background as an anomaly-an approach introduced by ADNet. Yet, ADNet faces three key limitations: dependence on a single prototype per class, a focus on binary classification, and fixed thresholds that fail to adapt to patient and organ variability. To address these shortcomings, we propose the Tied Prototype Model (TPM), a principled reformulation of ADNet with tied prototype locations for foreground and background distributions. Building on its probabilistic foundation, TPM naturally extends to multiple prototypes and multi-class segmentation while effectively separating non-typical background features. Notably, both extensions lead to improved segmentation accuracy. Finally, we leverage naturally occurring class priors to define an ideal target for adaptive thresholds, boosting segmentation performance. Taken together, TPM provides a fresh perspective on prototype-based FSS for medical image segmentation. The code can be found at https://github. com/hjk92g/TPM-FSS.", "filename": "2025_0637.pdf", "year": 2025, "institution": "The Arctic University of Norway", "country": "Norway", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0638", "x": 2.949, "y": 5.486, "title": "Towards Accurate Tumor Budding Detection: A Benchmark Dataset and A Detection Approach Based on Implicit Annotation Standardization and Positive-Negative Feature Coupling", "abstract": "The detection of tumor budding on histopathological images provides vital information for treatment planning and prognosis prediction. As manual identification of tumor budding is labor-intensive, automated tumor budding detection is desired. However, unlike other tumor cell detection tasks, tumor budding involves clusters of multiple tumor cells, which is more likely to be confused with other clusters of cells with similar appearances. It becomes challenging for existing cell detection methods to discriminate tumor budding from other cells. Additionally, the lack of public datasets for tumor budding detection hinders further development of accurate tumor budding detection methods. To address these challenges, to the best of our knowledge, we introduce the first publicly available benchmark dataset for tumor budding detection. The dataset consists of 410 images with H&E staining and the corresponding bounding box annotations of 3,968 cases of tumor budding made by experts. Moreover, based on this dataset, we propose a designated approach Tumor Budding Detection Network (TBDNet) for tumor budding detection with improved detection performance. On top of standard objection detection backbones, we develop two major components in TBDNet, Iteratively Distilled Annotation Relocation (IDAR) and Rotational Feature Decoupling And Recoupling (RFDAR). First, as different experts have different standards for budding boundaries in the annotation, the detection model may receive inconsistent knowledge during model training. Therefore, we introduce the IDAR module that implicitly standardizes the annotations. IDAR relocates the annotations via iterative model distillation so that the relocated annotations are consistent for training the detection model. Second, to reduce the interference from cells with similar features, i.e., negative samples, to tumor budding, i.e., positive samples, we develop the RFDAR module. RFDAR enhances feature extraction via positive-negative feature R.-Q. Sun and Z. Fan-These authors contributed equally.", "filename": "2025_0638.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Rui-Qing Sun", "Zeng Fan", "Boyang Dai", "Yiyan Su", "Qun Hao", "Chuyang Ye", "Shaohui Zhang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0639", "x": 2.943, "y": 2.503, "title": "Unisyn: A Generative Foundation Model for Universal Medical Image Synthesis Across MRI, CT and PET", "abstract": "Multi-modal brain imaging with MRI, CT, and PET has significantly advanced our understanding of cognition and neurodisease by providing complementary information. However, constraints on scan time and cost often result in missing critical high-quality sequences. Existing cross-modality synthesis methods are typically task-or modality-specific, leading to performance degradation when applied to heterogeneous realworld imaging data. Here, we propose UniSyn, a unified framework capable of synthesizing target imaging modalities with specific acquisition parameters from any available ones, guided by metadata. UniSyn first learns robust metadata representations through image-text alignment on large-scale multimodal neuroimaging datasets. We then introduce a cross-modality synthesis framework that leverages learned metadata representations to guide the generation of metadata-specified target images. To enhance interpretable metadata-driven control over image synthesis across diverse protocols, we design a dual-parameter arithmetic operation that explicitly integrates source and target metadata into the image translation process. Extensive experiments on multi-institutional brain imaging datasets demonstrate that UniSyn surpasses the existing crossmodality synthesis approaches in both quantitative fidelity and clinical relevance, enabling the generation of missing imaging counterparts tailored to specific clinical and research needs.", "filename": "2025_0639.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Yulin Wang", "Honglin Xiong", "Kaicong Sun", "Jiameng Liu", "Xin Lin", "Ziyi Chen", "Yuanzhe He", "Qian Wang", "Dinggang Shen"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0640", "x": 3.171, "y": 2.202, "title": "Unpaired Multi-site Brain MRI Harmonization with Image Style-Guided Latent Diffusion", "abstract": "Multi-site brain MRI heterogeneity caused by differences in scanner field strengths, acquisition protocols, and software versions poses a significant challenge for consistent analysis. Image-level harmonization, leveraging advanced learning methods, has attracted increasing attention. However, existing methods often rely on paired data (e.g., human traveling phantoms) for training, which are not always available. Some methods perform MRI harmonization by transferring target-style features to source images but require explicitly learning disentangled image styles (e.g., contrast) via encoder-decoder networks, which increases computational complexity. This paper presents an unpaired MRI harmonization (UMH) framework based on a new image style-guided diffusion model. UMH operates in two stages: (1) a coarse harmonizer that aligns multi-site MRIs to a unified domain via a conditional latent diffusion model while preserving anatomical content; and (2) a fine harmonizer that adapts coarsely harmonized images to a specific target using style embeddings derived from a pre-trained Contrastive Language-Image Pre-training (CLIP) encoder, which captures semantic style differences between the original MRIs and their coarsely-aligned counterparts, eliminating the need for paired data. By leveraging rich semantic style representations of CLIP, UMH avoids learning image styles explicitly, thereby reducing computation costs. We evaluate UMH on 4, 123 MRIs from three distinct multi-site datasets, with results suggesting its superiority over several state-of-the-art (SOTA) methods across image-level comparison, downstream classification, and brain tissue segmentation tasks.", "filename": "2025_0640.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill", "country": "USA", "authors": ["Mengqi Wu", "Minhui Yu", "Weili Lin", "Pew-Thian Yap", "Mingxia Liu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0641", "x": 4.199, "y": 4.615, "title": "A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma Triage", "abstract": "Recent advancements in AI and medical imaging offer transformative potential in emergency head CT interpretation for reducing assessment times and improving accuracy in the face of an increasing request of such scans and a global shortage in radiologists. This study introduces a 3D foundation model for detecting diverse neuro-trauma findings with high accuracy and efficiency. Using large language models (LLMs) for automatic labeling, we generated comprehensive multilabel annotations for critical conditions. Our approach involved pretraining neural networks for hemorrhage subtype segmentation and brain anatomy parcellation, which were integrated into a pretrained comprehensive neuro-trauma detection network through multimodal finetuning. Performance evaluation against expert annotations and comparison with CT-CLIP demonstrated strong triage accuracy across major neuro-trauma findings, such as hemorrhage and midline shift, as well as less frequent critical conditions such as cerebral edema and arterial hyperdensity. The integration of neuro-specific features significantly enhanced diagnostic capabilities, achieving an average AUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation models in medical imaging, serving as a benchmark for future AI-assisted neurotrauma diagnostics in emergency radiology.", "filename": "2025_0641.pdf", "year": 2025, "institution": "Siemens Healthineers", "country": "USA", "authors": ["Youngjin Yoo", "Bogdan Georgescu", "Yanbo Zhang", "Sasa Grbic", "Han Liu", "Gabriela D Aldea", "Thomas J Re", "Jyotipriya Das", "Poikavila Ullaskrishnan", "Eva Eibenberger", "Andrei Chekkoury", "Uttam K Bodanapally", "Savvas Nicolaou", "Pina C Sanelli", "Thomas J Schroeppel", "Yvonne W Lui", "Eli Gibson"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0642", "x": 1.317, "y": 6.153, "title": "Abnormality-Driven Representation Learning for Radiology Imaging", "abstract": "Radiology deep learning pipelines predominantly employ end-to-end 3D networks based on models pre-trained on other tasks, which are then fine-tuned on the task at hand. In contrast, adjacent medical fields such as pathology, which focus on 2D images, have effectively adopted task-agnostic foundational models based on self-supervised learning (SSL), combined with weakly-supervised deep learning (DL). However, the field of radiology still lacks task-agnostic representation models due to the computational and data demands of 3D imaging and the anatomical complexity inherent to radiology scans. To address this gap, we propose Clear, a framework for 3D radiology images that uses extracted embeddings from 2D slices along with attention-based aggregation to efficiently predict clinical endpoints. As part of this framework, we introduce Lecl, a novel approach to obtain visual representations driven by abnormalities in 2D axial slices across different locations of the CT scans. Specifically, we trained single-domain contrastive learning approaches using three different architectures: Vision Transformers, Vision State Space Models and Gated Convolutional Neural Networks. We evaluate our approach across three clinical tasks: tumor lesion location, lung disease detection, and patient staging, benchmarking against four state-of-the-art foundation models, including BiomedCLIP. Our findings demonstrate that Clear, using representations learned through Lecl, outperforms existing foundation models, while being substantially more compute-and data-efficient. The code is available at https:// github.com/KatherLab/CLEAR.", "filename": "2025_0642.pdf", "year": 2025, "institution": "EKFZ for Digital Health TU Dresden", "country": "Germany", "authors": ["Marta Ligero", "Tim Lenz", "Georg Wölflein", "Omar S M El Nahhas", "Daniel Truhn", "Jakob Nikolas Kather"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0643", "x": 2.471, "y": 3.637, "title": "Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge", "abstract": "Accurate segmentation of medical images is challenging due to unclear lesion boundaries and mask variability. We introduce Segmentation Schödinger Bridge (SSB), the first application of Schödinger Bridge for ambiguous medical image segmentation, modelling joint image-mask dynamics to enhance performance. SSB preserves structural integrity, delineates unclear boundaries without additional guidance, and maintains diversity using a novel loss function. We further propose the Diversity Divergence Index (DDDI ) to quantify inter-rater variability, capturing both diversity and consensus. SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER (in-house) datasets.", "filename": "2025_0643.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Lalith Bharadwaj Baru", "Kamalaker Dadi", "Tapabrata Chakraborti", "Raju S Bapi"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0644", "x": 4.144, "y": 4.572, "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications", "abstract": "Multimodal large language models (MLLMs) can process and integrate information from multimodality sources, such as text and images. However, interrelationship among input modalities, uncertainties due to individual uni-modal data and potential clinical applications following such an uncertainty decomposition are yet fully understood in the context of large-scale MLLMs. In this work, we propose a multimodal uncertainty propagation model (MUPM) based on uncertainty propagation, to characterise the relationship among the uncertainties arising from image-only, text-only, and joint image-text variations in MLLM inputs. Using real clinical data consisting of cardiac MR scans and digital health records, we describe that MUPMs can be optimised robustly with a few samples. We then show that the fitted MUPMs are generalisable across different input data distributions and, perhaps surprisingly, across different downstream tasks. Such a transferability may be explained by the shared pretraining, comparatively light MLLM fine-tuning, along with the lowdimensional nature of the MUPMs. More importantly, this learned transferability, quantifying the relationship between these uncertainties, led to direct clinical applications in which uncertainties may be estimated and thus analysed robustly for varying data or even a novel set of cardiac disease prediction tasks. In addition, we show experimentally the efficiency in multimodal data required for estimating the overall uncertainty and its ability to identify redundant factors, both of which are considered practical yet clinically useful applications with the proposed MUPMs. Codes are available at https://github.com/yucheng722/MUPM.", "filename": "2025_0644.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Yucheng Tang", "Yunguan Fu", "Weixi Yi", "Yipei Wang", "Daniel C Alexander", "Rhodri Davies", "Yipeng Hu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0645", "x": 2.025, "y": 2.507, "title": "Anatomy-Conserving Unpaired CBCT-to-CT Translation via Schrödinger Bridge", "abstract": "Unpaired Cone-beam CT (CBCT)-to-CT translation is pivotal for radiotherapy planning, aiming to synergize CBCT's clinical practicality with CT's dosimetric precision. Existing methods, limited by scarce paired data and registration errors, struggle to preserve anatomical fidelity-a critical requirement to avoid incorrect diagnosis and inadequate treatments. Current CycleGAN-derived approaches risk structural distortions, while diffusion models oversmooth highfrequency details vital for dose calculation in the reverse diffusion. In this paper, we propose the Anatomy-Conserving Schrödinger Bridge (ACSB), a novel unpaired medical image translation framework leveraging entropy-regularized optimal transport to disentangle modalityspecific artifacts from anatomy. We incorporate a carefully designed generator, Anatomy-Conserving vision transformer (AC-ViT) to integrate multi-scale anatomical priors via attention-guided feature fusion. We further adopt frequency-aware optimization targeting radiotherapy-critical spectral components. Extensive experiments on the dataset demonstrate the superiority of the proposed ACSB, showcasing excellent generalization over different anatomically distinct regions. Code: https://github. com/Lalala-iks/ACSB.", "filename": "2025_0645.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Ke Shi", "Song Ouyang", "Gang Liu", "Yong Luo", "Kehua Su", "Zhiwen Liang", "Bo Du"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0646", "x": 2.996, "y": 4.683, "title": "Class-Conditioned Image Synthesis with Diffusion for Imbalanced Diabetic Retinopathy Grading", "abstract": "Diabetic retinopathy (DR) is a major cause of vision impairment, with early detection playing a crucial role in preventing irreversible blindness. While deep learning-based automated DR grading has improved diagnostic efficiency, class imbalance in public datasets hinders reliable performance evaluation, particularly for underrepresented DR stages. Current state-of-the-art classifiers achieve high overall accuracy but suffer from poor balanced accuracy, limiting their real-world applicability. Inspired by recent advancements in diffusion models, we propose to mitigate class imbalance by generating synthetic fundus images. Unlike prior methods prioritizing visual quality, we introduce a semantic quality metric based on classifier-predicted likelihood to selectively filter synthetic samples that enhance classification performance. Furthermore, we incorporate explicit class constraint during diffusion model finetuning to generate more semantically relevant data. Experimental results demonstrate a significant improvement in balanced classification accuracy from 66.84% to 74.20%, highlighting the effectiveness of our approach in improving DR diagnosis. Our code is available at: https:// github.com/AlanZhang1995/ECC_DM_for_DR.git.", "filename": "2025_0646.pdf", "year": 2025, "institution": "UC San Diego", "country": "USA", "authors": ["Haochen Zhang", "Anna Heinke", "Ines D Nagel", "Dirk-Uwe G. Bartsch", "William R Freeman", "Truong Q Nguyen", "Cheolhong An"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0647", "x": 3.412, "y": 3.394, "title": "C-NCA: Chained Neural Cellular Automata for Fast and Accurate Thermal Ablation Estimation", "abstract": "Thermal ablation is an increasingly utilized treatment modality for both secondary and primary hepatic tumors. However, it presents significant challenges in treatment planning, particularly when employing multiple applicators. Numerical methods for evaluating the effectiveness of an ablation procedure plan can assist in this task, but they are often computationally intensive or too simplistic, making them impractical for interaction or fast optimization loops in automatic planning. This paper introduces Chained Neural Cellular Automata (C-NCA), a deep learning approach that allows to quickly estimate cell death in thermal ablation procedures. The C-NCA model is trained on a dataset generated by a numerical simulation. When compared to existing methods, the C-NCA achieves comparable accuracy with substantially reduced computation time, thereby making it suitable for interactive planning, instant visualisation, fast automatic planning or even real-time surgical replanning, and potentially enhancing clinical workflows.", "filename": "2025_0647.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Jonas Mehtali", "Juan Manuel Verde", "Caroline Essert"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0648", "x": 1.49, "y": 3.613, "title": "Conformal Prediction for Image Segmentation Using Morphological Prediction Sets", "abstract": "Image segmentation is a challenging task influenced by multiple sources of uncertainty, such as the data labeling process or the sampling of training data. In this paper we focus on binary segmentation and address these challenges using conformal prediction, a family of model-and data-agnostic methods for uncertainty quantification that provide finite-sample theoretical guarantees and applicable to any pretrained predictor. Our approach involves computing nonconformity scores, a type of prediction residual, on held-out calibration data not used during training. We use dilation, one of the fundamental operations in mathematical morphology, to construct a margin added to the borders of predicted segmentation masks. At inference, the predicted set formed by the mask and its margin contains the ground-truth mask with high probability, at a confidence level specified by the user. The size of the margin serves as an indicator of predictive uncertainty for a given model and dataset. We work in a regime of minimal information as we do not require any feedback from the predictor: only the predicted masks are needed for computing the prediction sets. Hence, our method is applicable to any segmentation model, including those based on deep learning; we evaluate our approach on several medical imaging applications. Our code is available at https://github.com/deel-ai-papers/consema.", "filename": "2025_0648.pdf", "year": 2025, "institution": "IRT Saint Exupéry", "country": "France", "authors": ["Luca Mossina", "Corentin Friedrich"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0649", "x": 3.012, "y": 2.433, "title": "Contrast Flow Pattern and Cross-Phase Specificity-Aware Diffusion Model for NCCT-to-Multiphase CECT Synthesis", "abstract": "Multiphase contrast-enhanced computed tomography (CT) is clinically significant in providing vascular structure and lesion phasespecific enhancements. Yet, its clinical utility is constrained by intrinsic contrast agent-associated risks (e.g., nephrotoxicity, allergic reactions) and multiphase cumulative radiation exposure. To tackle this, synthesizing contrast-enhanced CT (CECT) using non-contrast CT (NCCT) offers a potential alternative. However, achieving a high-quality synthesis of multiphase CECT remains challenging due to the contrast agent (CA)-induced complex contrast flow dynamics and the specific variations across phases. Therefore, this paper proposes a contrast flow pattern and cross-phase specificity-aware diffusion model for NCCT-to-multiphase CECT synthesis. Specifically, a contrast flow pattern learning mechanism is integrated into the conditional diffusion model, which enables orderly phase transitions while ensuring anatomically and temporally coherent enhancement synthesis. Furthermore, a phase distinction network is introduced to align cross-phase specificity features with the contrast features in synthesized CECT images. Experimental results on multicenter abdomen CT datasets have demonstrated the superiority of our method compared to state-of-the-art methods.", "filename": "2025_0649.pdf", "year": 2025, "institution": "Southern Medical University", "country": "China", "authors": ["Kaiyi Zheng", "Mu Huang", "Xinming Li", "Jianhua Ma", "Qianjin Feng", "Wei Yang", "Liming Zhong"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0650", "x": 3.595, "y": 3.872, "title": "Coronary Artery Calcification Segmentation by Using Cross-Frequency Conditioner and Geometric Priors Learning", "abstract": "Coronary artery calcification (CAC) is a powerful indicator of cardiovascular disease. Cardiac CT angiography (CCTA) has significant advantages in detecting CAC. However, since the image quality of CCTA can be compromised by cardiac motion or imaging equipment, and the contrast between CAC and surrounding tissue is low, accurate assessment of CAC remains a significant challenge. To address this issue, we propose a model (CAC-Net) for the comprehensive evaluation of CAC to fully exploit the characteristics of clinician annotations. First, inspired by the clinical annotation process, where doctors determine the subject based on boundaries, we propose a cross-frequency regulator module. This module models the interaction between high and low frequencies to distinguish the CAC body and its edges, thereby enhancing edge perception. Then, building on clinicians' anatomical prior knowledge that CAC is confined within coronary arteries, we introduce a geometric prior module to encode their topological relationship, effectively reducing false positives. In experiments, our proposed method is compared with existing state-of-the-art methods on two CAC datasets. The results demonstrate that: (1) our method significantly improves CAC segmentation performance, as evidenced by a higher Dice score compared to U-Net (0.731 vs. 0.659); and (2) it ensures consistency in clinically relevant indicators, including calcium scores.", "filename": "2025_0650.pdf", "year": 2025, "institution": "Southwest Jiaotong University", "country": "People's Republic of China", "authors": ["Weili Jiang", "Yiming Li", "Gadeng Luosang", "Gang Peng", "Yong Peng", "Yijun Yao", "Zhang Yi", "Jianyong Wang", "Mao Chen"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0651", "x": 3.228, "y": 4.044, "title": "Cross-Modal Graph Learning for Perivascular Spaces Segmentation", "abstract": "Perivascular spaces (PVS), also known as Virchow-Robin spaces, are critical biomarkers for diagnosing cerebral small vessel disease (CSVD). Quantifying PVS visible in magnetic resonance imaging (MRI) is essential for understanding their relationship with various neurological disorders. Traditional methods for assessing PVS rely on visual scoring of MRI images, which is time-consuming, subjective, and unsuitable for large-scale studies. Additionally, due to their small size, scattered distribution, and complex morphology, PVS can easily be confused with neighboring structures, posing significant challenges for their accurate extraction. In this paper, we propose a novel graph interaction-enhanced model based on vision-language modeling (VLM) technology for accurate PVS extraction from MRI. Our approach leverages textual information to guide image feature extraction and employs a graph structure to enhance cross-modal interactions, facilitating the reasoning of relationships between different modalities. Furthermore, we introduce a cross-modal attention mechanism for global feature alignment and an attention-based dynamic fusion module to effectively integrate multimodal information, improving the accuracy of PVS segmentation. Validated on an independent T1-weighted dataset, our model demonstrates superior performance in capturing both global and local information, addressing the limitations of traditional image-only models and providing a robust solution for PVS segmentation in complex clinical scenarios.", "filename": "2025_0651.pdf", "year": 2025, "institution": "Eindhoven University of Technology", "country": "Netherlands", "authors": ["Tao Chen", "Dan Zhang", "Xi Long", "Marcel Breeuwer", "Sveta Zinger", "Peiyu Huang", "Jiong Zhang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0652", "x": 1.495, "y": 2.642, "title": "Deep Learning-Based Alignment Measurement in Knee Radiographs", "abstract": "Radiographic knee alignment (KA) measurement is important for predicting joint health and surgical outcomes after total knee replacement. Traditional methods for KA measurements are manual, time-consuming and require long-leg radiographs. This study proposes a deep learning-based method to measure KA in anteroposterior knee radiographs via automatically localized knee anatomical landmarks. Our method builds on hourglass networks and incorporates an attention gate structure to enhance robustness and focus on key anatomical features. To our knowledge, this is the first deep learning-based method to localize over 100 knee anatomical landmarks to fully outline the knee shape while integrating KA measurements on both pre-operative and post-operative images. It provides highly accurate and reliable anatomical varus/valgus KA measurements using the anatomical tibiofemoral angle, achieving mean absolute differences ∼1° when compared to clinical ground truth measurements. Agreement between automated and clinical measurements was excellent pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can be automated with high accuracy, creating opportunities for digitally enhanced clinical workflows.", "filename": "2025_0652.pdf", "year": 2025, "institution": "The University of Manchester", "country": "UK", "authors": ["Zhisen Hu", "Dominic Cullen", "Peter Thompson", "David Johnson", "Chang Bian", "Aleksei Tiulpin", "Timothy Cootes", "Claudia Lindner"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0653", "x": 0.255, "y": 1.894, "title": "DIGS: Dynamic CBCT Reconstruction Using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as Hex-Plane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6× speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.", "filename": "2025_0653.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R Mcclelland"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0654", "x": 3.646, "y": 1.568, "title": "Direct Inversion Formula of the Multi-coil MR Operator Under Arbitrary Trajectories", "abstract": "This study introduces a novel inversion formula for the multicoil MRI forward operator applicable to arbitrary sampling trajectories. Traditional MRI reconstruction leverages fast Fourier transforms (FFTs) for Cartesian sampling and nonuniform FFTs for non-Cartesian patterns. However, subsampled k-space reconstruction typically relies on iterative least-squares (LS) solutions, which are computationally intensive due to the complex structure introduced by multiple coil sensitivities. We hypothesize that the MRI multi-coil forward operator exhibits the low displacement rank (LDR) property, enabling an efficient inversion using triangular Toeplitz operators with a computational complexity of O(αN log 2 N ), with α being a small integer. The hypothesis is supported through numerical simulations. For demonstration of the feasibility of such inversion formula, we propose a learning-based approach to determine the necessary LDR parameters, demonstrating successful forward and inverse operator representations across various sampling patterns, including Cartesian and radial trajectories. The proposed inversion formula offers a significant acceleration in MR reconstruction, reducing computational complexity by a factor of approximately 26 compared to conventional conjugate gradient methods. The proposed inversion formula will greatly enhance reconstruction speed and simplify reconstruction pipelines, including iterative reconstructions and deep learning solutions incorporating data-consistency layers. Future work will focus on deriving the LDR parameters analytically to further streamline the inversion process. The code is available at https://github.com/mikecjz/structured- nets.", "filename": "2025_0654.pdf", "year": 2025, "institution": "University of Southern California", "country": "USA", "authors": ["Junzhou Chen", "Anthony G Christodoulou", "Zhaoyang Fan"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0655", "x": 2.833, "y": 1.311, "title": "Distribution-Guided Multi-tracer Brain PET Synthesis from Structural MRI with Class-Conditioned Weighted Diffusion", "abstract": "Multi-tracer positron emission tomography (PET), which assesses key neurological biomarkers such as tau pathology, neuroinflammatory, β-amyloid deposition, and glucose metabolism, plays a vital role in diagnosing neurological disorders by providing complementary insights into the brain's molecular and functional state. Acquiring multitracer PET scans remains challenging due to high costs, radiation exposure, and limited tracer availability. Recent studies have attempted to synthesize multi-tracer PET images from structural MRI. However, these approaches typically either rely on direct mappings to individual tracers or lack distributional constraints, leading to inconsistencies in image quality across tracers. To this end, we propose a normalized diffusion framework (NDF) to generate high-quality multi-tracer PET images from a single MRI through a distribution-guided classconditioned weighted diffusion model. Specifically, a diffusion model conditioned on MRI and tracer-specific class labels is trained to synthesize PET images of multiple tracers, and a pre-trained normalizing flow model refines these outputs by mapping them into a shared distribution space. This mapping ensures that the subject-specific high-level features across different PET tracers are preserved, resulting in more consistent and accurate synthesis. Experiments on a total of 425 subjects with multitracer PET scans demonstrate that our NDF outperforms current stateof-the-art methods, indicating its potential for advancing multi-tracer PET synthesis.", "filename": "2025_0655.pdf", "year": 2025, "institution": "University of North Carolina at Chapel Hill (UNC-CH)", "country": "USA", "authors": ["Minhui Yu", "David S Lalush", "Derek C Monroe", "Kelly S Giovanello", "Weili Lin", "Pew-Thian Yap", "Jason P Mihalik", "Mingxia Liu"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0656", "x": 2.932, "y": 3.103, "title": "Domain-Agnostic Stroke Lesion Segmentation Using Physics-Constrained Synthetic Data", "abstract": "Segmenting stroke lesions in MRI is challenging due to diverse acquisition protocols that limit model generalisability. In this work, we introduce two physics-constrained approaches to generate synthetic quantitative MRI (qMRI) images that improve segmentation robustness across heterogeneous domains. Our first method, qATLAS, trains a neural network to estimate qMRI maps from standard MPRAGE images, enabling the simulation of varied MRI sequences with realistic tissue contrasts. The second method, qSynth, synthesises qMRI maps directly from tissue labels using label-conditioned Gaussian mixture models, ensuring physical plausibility. Extensive experiments on multiple out-of-domain datasets show that both methods outperform a baseline UNet, with qSynth notably surpassing previous synthetic data approaches. These results highlight the promise of integrating MRI physics into synthetic data generation for robust, generalisable stroke lesion segmentation. Code is available at https://github.com/ liamchalcroft/qsynth.", "filename": "2025_0656.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Liam Chalcroft", "Jenny Crinion", "Cathy J Price", "John Ashburner"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0657", "x": 3.521, "y": 1.459, "title": "Dynamic-Aware Spatio-Temporal Representation Learning for Dynamic MRI Reconstruction", "abstract": "Dynamic MRI reconstruction, one of inverse problems, has seen a surge by the use of deep learning techniques. Especially, the practical difficulty of obtaining ground truth data has led to the emergence of unsupervised learning approaches. A recent promising method among them is implicit neural representation (INR), which defines the data as a continuous function that maps coordinate values to the corresponding signal values. This allows for filling in missing information only with incomplete measurements and solving the inverse problem effectively. Nevertheless, previous works incorporating this method have faced drawbacks such as long optimization time and the need for extensive hyperparameter tuning. To address these issues, we propose Dynamic-Aware INR (DA-INR), an INR-based model for dynamic MRI reconstruction that captures the spatial and temporal continuity of dynamic MRI data in the image domain and explicitly incorporates the temporal redundancy of the data into the model structure. As a result, DA-INR outperforms other models in reconstruction quality even at extreme undersampling ratios while significantly reducing optimization time and requiring minimal hyperparameter tuning. Our code is available at https://github. com/9B8DY6/DA_INR.", "filename": "2025_0657.pdf", "year": 2025, "institution": "Ulsan National Institute of Science and Technology", "country": "Republic of Korea", "authors": ["Dayoung Baik", "Jaejun Yoo"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0658", "x": 1.354, "y": 3.845, "title": "Edge-Aware Token Halting for Efficient and Accurate Medical Image Segmentation", "abstract": "The effectiveness of Vision Transformer (ViT)-based feature encoding network has been demonstrated in medical image analysis tasks. However, the complexity growing quadratically with the token number limits its application in dense prediction. To accelerate ViT, we propose an efficient and accurate token halting and reconstruction encoder framework, termed HRViT, designed for precise medical image semantic segmentation. Our approach is motivated by the observation that background and internal tokens can be easily identified and halted in early layers, while complex and ambiguous edge regions require deeper computational processing for accurate segmentation. HRViT leverages this insight by incorporating an edge-aware token halting module, which dynamically identifies edge patches and halts non-edge tokens. The preserved edge tokens are propagated to deeper layers and further refined through edge reinforcement. After encoding, all tokens are restored to their original positions, and auxiliary supervision is also introduced to strengthen the encoder's representation power. We evaluate the segmentation performance of our method using two public medical image datasets and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. Our code is released at https://github.com/guoyh6/hrvit.", "filename": "2025_0658.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Yuhao Guo", "Bo Song", "Heng Fan", "Erkang Cheng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0659", "x": 0.854, "y": 4.175, "title": "EfficientMedNeXt: Multi-receptive Dilated Convolutions for Medical Image Segmentation", "abstract": "In this work, we introduce EfficientMedNeXt-a lightweight, high-performance segmentation architecture developed through a twophase optimization process applied to the MedNeXt architecture. To this end, we first optimize the decoder by reducing the high-resolution redundancy and unifying the decoder channels across stages for improved efficiency. Then, we introduce a new Dilated Multi-Receptive Field Block (DMRFB) to capture the multi-scale spatial context efficiently without increasing the kernel sizes and relying on the channel expansion convolutions. Extensive evaluations on BTCV, FeTA, and MSD show that EfficientMedNeXt-L achieves 87.0% DICE score on BTCV (+1.04% over MedNeXt-L) with 96.5% fewer parameters and 77.03% lower FLOPs. In addition, EfficientMedNeXt-S offers comparable DICE score, improved HD95, and 78.1% higher throughput while reducing parameters by 98.5% and FLOPs by 95%. These results demonstrate EfficientMed-NeXt's efficiency and accuracy, making it well-suited for real-world clinical applications. Our implementation is available at https://github.com/ SLDGroup/EfficientMedNeXt.", "filename": "2025_0659.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Md Mostafijur Rahman", "Mustafa Munir", "Radu Marculescu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0660", "x": 1.935, "y": 4.993, "title": "Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification", "abstract": "Masked Autoencoders (MAEs) have emerged as a dominant strategy for self-supervised representation learning in natural images, where models are pre-trained to reconstruct masked patches with a pixelwise mean squared error (MSE) between original and reconstructed RGB values as the loss. We observe that MSE encourages blurred image reconstruction, but still works for natural images as it preserves dominant edges. However, in medical imaging, when the texture cues are more important for classification of a visual abnormality, the strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM) feature in Radiomics studies, we propose a novel MAE based pre-training framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM captures intensity and spatial relationships in an image, hence proposed loss helps preserve morphological features. Further, we propose a novel formulation to convert matching GLCM matrices into a differentiable loss function. We demonstrate that unsupervised pre-training on medical images with the proposed GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms the current state-of-the-art across four tasks -gallbladder cancer detection from ultrasound images by 2.1%, breast cancer detection from ultrasound by 3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by 0.6%. Source code and pre-trained models are available at: https://github.com/ ChetanMadan/GLCM-MAE.", "filename": "2025_0660.pdf", "year": 2025, "institution": "Indian Institute of Technology", "country": "India", "authors": ["Chetan Madan", "Aarjav Satia", "Soumen Basu", "Pankaj Gupta", "Usha Dutta", "Chetan Arora"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0661", "x": 1.661, "y": 1.992, "title": "Gaussian Primitive Optimized Deformable Retinal Image Registration", "abstract": "Deformable retinal image registration is notoriously difficult due to large homogeneous regions and sparse but critical vascular features, which cause limited gradient signals in standard learning-based frameworks. In this paper, we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework that performs structured message passing to overcome these challenges. After an initial coarse alignment, we extract keypoints at salient anatomical structures (e.g., major vessels) to serve as a minimal set of descriptor-based control nodes (DCN). Each node is modelled as a Gaussian primitive with trainable position, displacement, and radius, thus adapting its spatial influence to local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation then blends and propagates displacement signals from these information-rich nodes to construct a globally coherent displacement field; focusing interpolation on the top (K) neighbors reduces computational overhead while preserving local detail. By strategically anchoring nodes in high-gradient regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal in textureless areas. The framework is optimized end-to-end via a multi-term loss that enforces both keypoint consistency and intensity alignment. Experiments on the FIRE dataset show that GPO reduces the target registration error from 6.2 px to 2.4 px and increases the AUC at 25 px from 0.770 to 0.938, substantially outperforming existing methods. The source code can be accessed via https:// github.com/xintian-99/GPOreg.", "filename": "2025_0661.pdf", "year": 2025, "institution": "University of Oxford", "country": "UK", "authors": ["Xin Tian", "Jiazheng Wang", "Yuxi Zhang", "Xiang Chen", "Renjiu Hu", "Gaolei Li", "Min Liu", "Hang Zhang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0662", "x": 1.288, "y": 5.601, "title": "Graph-Based Neighbor-Aware Network for Gaze-Supervised Medical Image Segmentation", "abstract": "Creating fully annotated labels for medical image segmentation is time-consuming and expensive, underscoring the need for efficient labeling schemes to alleviate the workload. Eye tracking presents a cost-effective solution, seamlessly integrating into radiologists' workflows while offering task-relevant eye gaze supervision. However, due to the inaccuracy and ambiguity of gaze, it may introduce erroneous supervision and hinder the model's ability to learn robust features. To address these challenges, we propose the graph-based neighbor-aware network (GNAN). The network constructs a graph structure from the image, separating different categories of nodes by simulating the attention distribution during the diagnostic process, to learn image segmentation based on the radiologist's gaze information. The GNAN comprises neighbor-aware pseudo supervision (NAP) and graph contrastive decoupling (GCD). NAP utilizes the neighbor features of graph nodes to infer pseudo-labels for uncertain regions, effectively compensating for the inaccuracy in gaze supervision and further refining the supervisory signal. GCD decouples the graph structure by maximizing the inter-class node feature differences to distinguish between different categories, thereby improving segmentation performance. Experimental results on the public dataset demonstrate that GNAN outperforms state-of-the-art methods. Our code is available at https://github.com/IPMI-NWU/GNAN.", "filename": "2025_0662.pdf", "year": 2025, "institution": "Northwest University", "country": "China", "authors": ["Shaoxuan Wu", "Jingkun Chen", "Zhuo Jin", "Peilin Zhang", "Zhizezhang Gao", "Jun Feng", "Xiao Zhang", "Dinggang Shen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0663", "x": 2.519, "y": 2.427, "title": "Guiding Registration with Emergent Similarity from Pre-trained Diffusion Models", "abstract": "Diffusion models, while trained for image generation, have emerged as powerful foundational feature extractors for downstream tasks. We find that off-the-shelf diffusion models, trained exclusively to generate natural RGB images, can identify semantically meaningful correspondences in medical images. Building on this observation, we propose to leverage diffusion model features as a similarity measure to guide deformable image registration networks. We show that common intensity-based similarity losses often fail in challenging scenarios, such as when certain anatomies are visible in one image but absent in another, leading to anatomically inaccurate alignments. In contrast, our method identifies true semantic correspondences, aligning meaningful structures while disregarding those not present across images. We demonstrate superior performance of our approach on two tasks: multimodal 2D registration (DXA to X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted MRI). Code: https:// github.com/uncbiag/dgir.", "filename": "2025_0663.pdf", "year": 2025, "institution": "UNC Chapel Hill", "country": "USA", "authors": ["Nurislam Tursynbek", "Hastings Greer", "Başar Demir", "Marc Niethammer"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0664", "x": 6.157, "y": 4.591, "title": "Improving Autism Detection with Multimodal Behavioral Analysis", "abstract": "Due to the complex and resource-intensive nature of diagnosing Autism Spectrum Condition (ASC), several computer-aided diagnostic support methods have been proposed to detect autism by analyzing behavioral cues in patient video data. While these models show promising results on some datasets, they struggle with poor gaze feature performance and lack of real-world generalizability. To tackle these challenges, we analyze a standardized video dataset comprising 168 participants with ASC (46% female) and 157 non-autistic participants (46% female), making it, to our knowledge, the largest and most balanced dataset available. We conduct a multimodal analysis with a primary focus on gaze behaviour, complemented by facial expressions, voice prosody, head motion, heart rate variability (HRV). Addressing previous limitations in gaze modeling, we introduce novel statistical descriptors that quantify variability in eye gaze angles, improving gaze-based classification accuracy from 64% to 69% and aligning computational findings with clinical research on gaze aversion in ASC. Using late fusion, we achieve a classification accuracy of 74%, demonstrating the effectiveness of integrating behavioral markers across multiple modalities. Our findings highlight the potential for scalable, video-based screening tools to support autism assessment. To facilitate reproducibility, we share our code on GitHub: https://github.com/mbp-lab/miccai25_sit_autism_classification.", "filename": "2025_0664.pdf", "year": 2025, "institution": "Bielefeld University", "country": "Germany", "authors": ["William Saakyan", "Matthias Norden", "Lola Eversmann", "Simon Kirsch", "Muyu Lin", "Simon Guendelman", "Isabel Dziobek", "Hanna Drimalla"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0665", "x": 3.909, "y": 5.479, "title": "Indepth Integration of Multi-granularity Features from Dual-modal for Disease Classification", "abstract": "Multi-granularity features can be extracted from multiple modal medical images and how to effectively analyze these features is a challenging and critical issue for computer-aided diagnosis (CAD). However, most existing multi-modal classification methods have not fully explored the interactions among the intra-and inter-granularity features across multi-modal. To address this limitation, we propose a novel Indepth Integration of Multi-Granularity Features Network (IIMGF-Net) for a typical multi-modal task, i.e., a dual-modal based CAD. Specifically, the proposed IIMGF-Net consists of two types of key modules, i.e., Cross-Modal Intra-Granularity Fusion (CMIGF) and Multi-Granularity Collaboration (MGC). The CMIGF module enhances the attentive interactions between the same granularity features from dual-modals and derive an integrated representation at each granularity. Based on these representations, the MGC module captures inter-granularity interactions among the resulting representations of CMIGF through the coarse-to-fine and fine-to-coarse collaborative learning mechanism.Extensive experiments on two dual-modal datasets validate the effectiveness of the proposed method, demonstrating its superiority in dual-modal CAD tasks by integrating multi-granularity information.", "filename": "2025_0665.pdf", "year": 2025, "institution": "Shenzhen Campus of Sun Yat-sen University", "country": "China", "authors": ["Yeli Wu", "Xiaocai Zhang", "Weiwen Wu", "Haiteng Jiang", "Chao An", "Jianjia Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0666", "x": 3.095, "y": 5.993, "title": "Language of Stains: Tokenization Enhances Multiplex Immunofluorescence and Histology Image Synthesis", "abstract": "Multiplex tissue imaging (MTI) is a powerful tool in cancer research, allowing spatially resolved, single-cell phenotype analysis. However, MTI platforms face challenges such as high costs, tissue loss, lengthy acquisition times, and complex analysis of large, multichannel images with batch effects. To address these challenges, we propose a novel computational method to model the interactions between dozens of panel markers and Hematoxylin & Eosin (H&E) staining, enabling in-silico generation of marker stains. This approach reduces the reliance on experimentally measured markers, bridging low-cost H&E data with MTI's high-content information. Our approach uses a two-stage framework for channel-wise bioimage synthesis: first, vector quantization learns a visual token vocabulary, then a bidirectional transformer infers missing markers through masked language modeling. Comprehensive benchmarking across different MTI platforms and tissue types demonstrates the effectiveness of our method in improving marker prediction while maintaining biological relevance. This advance makes high-dimensional multiplex tissue imaging more accessible and scalable, supporting deeper insights and potential clinical applications in cancer research.", "filename": "2025_0666.pdf", "year": 2025, "institution": "Oregon Health and Science University", "country": "USA", "authors": ["Zachary Sims", "Sandhya Govindarajan", "Gordon B Mills", "Ece Eksi", "Young Hwan Chang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0667", "x": 2.778, "y": 5.322, "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "abstract": "Accurate lesion tracking in temporal mammograms is essential for monitoring breast cancer progression and facilitating early diagnosis. However, automated lesion correspondence across exams remains a challenges in computer-aided diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker, a mask-guided lesion tracking framework that automates lesion localization across consecutively exams. Our approach follows a coarse-to-fine strategy incorporating three key modules: global search, local search, and score refinement. To support large-scale training and evaluation, we introduce a new dataset with curated prior-exam annotations for 730 mass and calcification cases from the public EMBED mammogram dataset, yielding over 20000 lesion pairs, making it the largest known resource for temporal lesion tracking in mammograms. Experimental results demonstrate that MammoTracker achieves 0.455 average overlap and 0.509 accuracy, surpassing baseline models by 8%, highlighting its potential to enhance CAD-based lesion progression analysis. Our dataset will be available at https://gitlab.oit. duke.edu/railabs/LoGroup/mammotracker.", "filename": "2025_0667.pdf", "year": 2025, "institution": "Duke University", "country": "USA", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D Ryser", "Lars J Grimm", "Joseph Y Lo"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0668", "x": 1.035, "y": 4.408, "title": "MARSeg: Enhancing Medical Image Segmentation with MAR and Adaptive Feature Fusion", "abstract": "Recent advances in Masked Autoregressive (MAR) models highlight their ability to preserve fine-grained details through continuous vector representations, making them highly suitable for tasks requiring precise pixel-level delineation. Motivated by these strengths, we introduce MARSeg, a novel segmentation framework tailored for medical images. Our method first pre-trains a MAR model on large-scale CT scans, capturing both global structures and local details without relying on vector quantization. We then propose a Generative Parallel Adaptive Feature Fusion (GPAF) module that effectively unifies spatial and channel-wise attention, thereby combining latent features from the pre-trained MAE encoder and decoder. This approach preserves essential boundary information while enhancing the robustness of organ and tumor segmentation. Experimental results on multiple CT datasets from the Medical Segmentation Decathlon (MSD) demonstrate that MARSeg outperforms existing state-of-the-art methods in terms of Dice Similarity Coefficient (DSC) and Intersection over Union (IoU), confirming its efficacy in handling complex anatomical and pathological variations. The code is available at https://github.com/ Ewha-AI/MARSeg.", "filename": "2025_0668.pdf", "year": 2025, "institution": "Ewha Womans University", "country": "Korea", "authors": ["Jeonghyun Hwang", "Seungyeon Rhee", "Minjeong Kim", "Thanaporn Viriyasaranon", "Jang-Hwan Choi"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0669", "x": 2.634, "y": 3.37, "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-Tuning with Structural Guidance for Controllable Medical Image Synthesis", "abstract": "Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data. While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT , a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets, MedDiff-FT 's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at https://github.com/JianhaoXie1/MedDiff-FT.", "filename": "2025_0669.pdf", "year": 2025, "institution": "Peking University Shenzhen Graduate School", "country": "China", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0670", "x": 2.1, "y": 3.006, "title": "MSDG-StyleNet: Multi-source Unsupervised Domain-Generalized CBCT-to-CT Translation with Style-Consistent Disentangled Representations", "abstract": "Cone-beam computed tomography (CBCT) is gaining prominence in clinical radiology, particularly for intraoperative guidance, owing to its lower radiation dose and faster acquisition speed compared to computed tomography (CT). However, CBCT images often exhibit compromised quality, characterized by increased noise, artifacts, and diminished soft-tissue contrast, which can hinder their direct clinical application. While CBCT-to-CT translation presents a promising solution, this task faces significant challenges in multi-institutional settings where diverse imaging protocols introduce substantial domain shifts, especially when paired CBCT-CT data is scarce. Current unsupervised domain generalization (UDG) techniques often struggle to simultaneously maintain robust anatomical accuracy and preserve domainspecific characteristics-both crucial for clinical reliability. To address these limitations, we propose a novel disentangled representation learning framework for UDG-based CBCT-to-CT translation. Our method uniquely separates domain-invariant anatomical content from domainspecific styles, while leveraging learnable domain-style prototypes to dynamically capture key stylistic characteristics. To ensure high-quality translation, we implement a dual-level consistency mechanism that guarantees both anatomical fidelity and style alignment. By utilizing unpaired data for training and enabling flexible content-prototype combinations, our framework effectively generalizes to new institutions without requiring paired data. Extensive validation across three distinct institutional domains demonstrates that our method achieves superior anatomical accuracy and style fidelity compared to state-of-the-art approaches, establishing a clinically practical UDG paradigm with inherent crossinstitutional interoperability.", "filename": "2025_0670.pdf", "year": 2025, "institution": "Nanchang University", "country": "China", "authors": ["Xin Long", "Xinrui Liu", "Fan Gan"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0671", "x": 0.979, "y": 3.726, "title": "MT-WilmsNet: A Multi-level Transformer Fusion Network for Wilms’ Tumor Segmentation and Metastasis Prediction", "abstract": "Wilms' tumor (WT) is a prevalent cancer affecting the kidneys of children, and accurate segmentation and prediction of metastasis are vital for treatment planning and prognosis. Current methods for assessing metastasis, such as invasive biopsies and expensive PET-CT scans, hinder their widespread use in clinical settings. Deep learning, especially classification models for 3D data, is currently widely used in tumor metastasis prediction. However, existing models may not have fully accounted for the global significance of cross-sectional slices, and segment-assisted classification frameworks tailored for low-cost clinical CT imaging protocols remain understudied, with systematic validation in clinical settings yet to be comprehensively established. In this study, we propose MT-WilmsNet, a slice-guided multi-task multi-level Transformer fusion network featuring three synergistic components. First, a Wide Reinforced Transformer Feature Pyramid Network integrates multi-scale features to boost preoperative metastasis prediction accuracy. Second, a dedicated UNet-like architecture performs tumor segmentation while providing anatomical context for metastasis analysis. Finally, a global slice attention mechanism combined with multi-level self-distilling transformers emulates radiologists' cross-slice diagnostic reasoning. Our MT-WilmsNet outperforms many typical classification models for WT metastasis prediction. The source code is available at: https://github.com/ wenjing-gg/MT-WilmsNet.", "filename": "2025_0671.pdf", "year": 2025, "institution": "Zhejiang University School of Medicine", "country": "China", "authors": ["Zhu Zhu", "Wenjing Yu", "Xiaohui Ma", "Shuai Liu", "Jie Dong", "Yuxin Du", "Changmiao Wang", "Gang Yu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0672", "x": 0.524, "y": 5.333, "title": "Multimodal Prompt Sequence Learning for Interactive Segmentation of Vascular Structures", "abstract": "Interactive segmentation tools are necessary to achieve the desired segmentation accuracy for complex target structures, such as vessels in medical images. But existing interactive methods-including those pre-trained on large internet-scale datasets-offer limited mechanisms for users to provide prompts that effectively control segmentation outcomes. In particular, one-at-a-time point or text prompts are often insufficient for correcting errors in vascular segmentation masks. To address these limitations, we propose a novel interactive medical image segmentation method tailored for complex vascular structures. Our approach learns to interpret sequences of multimodal prompts-combining both text and point inputs. By enabling dual mode prompting, the method allows users to add semantic meaning to point-based interactions. Furthermore, by learning from aggregated sequences of prompts, the method captures inter-prompt relationships, enhancing its understanding and response to user input. Quantitative evaluations on six vascular datasets demonstrate that our method outperforms existing approaches. Additionally, it avoids critical failure cases and consistently generates improved segmentation masks across diverse imaging modalities and vascular anatomies.", "filename": "2025_0672.pdf", "year": 2025, "institution": "Kookmin University", "country": "Korea", "authors": ["Jongsoo Lim", "Soochahn Lee"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0673", "x": 7.171, "y": 4.456, "title": "Oblique Genomics Mixture of Experts: Prediction of Brain Disorder with Aging-Related Changes of Brain’s Structural Connectivity Under Genomic Influences", "abstract": "During the process of brain aging, the changes of white matter structural connectivity are closely correlated with the cognitive traits and brain function. Genes have strong controls over this transition of structural connectivity-altering, which influences brain health and may lead to severe dementia disease, e.g., Alzheimer's disease. In this work, we introduce a novel deep-learning diagram, an oblique genomics mixture of experts (OG-MoE), designed to address the prediction of brain disease diagnosis, with awareness of the structural connectivity changes over time, and coupled with the genomics influences. By integrating genomics features into the dynamic gating router system of MoE layers, the model specializes in representing the structural connectivity components in separate parameter spaces. We pretrained the model on the self-regression task of brain connectivity predictions and then implemented multi-task supervised learning on brain disorder predictions and brain aging prediction. Compared to traditional associations analysis, this work provided a new way of discovering the soft but intricate inter-play between brain connectome phenotypes and genomic traits. It revealed the significant divergence of this correlation between the normal brain aging process and neurodegeneration. Keywords: Structural Connectivity • Mixture of Experts • Genomics • Alzheimer's Disease • Mild Cognitive Impairment Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).", "filename": "2025_0673.pdf", "year": 2025, "institution": "University of Texas at Arlington", "country": "USA", "authors": ["Yanjun Lyu", "Jing Zhang", "Lu Zhang", "Wei Ruan", "Tianming Liu", "Dajiang Zhu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0674", "x": 1.75, "y": 4.826, "title": "ODES: Online Domain Adaptation with Expert Guidance for Medical Image Segmentation", "abstract": "Unsupervised domain adaptive segmentation typically relies on self-training using pseudo labels predicted by a pre-trained network on an unlabeled target dataset. However, noisy pseudo-labels present a major bottleneck in adapting a network to distribution shifts between source and target domains, particularly when data is coming in an online manner and adaptation is constrained to exactly one round of forward and backward passes. In this scenario, relying solely on inaccurate pseudo-labels can degrade segmentation quality, which is detrimental to medical image segmentation where accuracy and precision are of utmost priority. In this paper, we propose an approach to address this issue by incorporating expert guided active learning to enhance online domain adaptation, even without dedicated training data. We call our method ODES: Online Domain Adaptation with Expert Guidance for Medical Image Segmentation that adapts to each incoming batch of data in an online setup. However, acquiring annotations through active learning for all images in a batch often results in redundant data annotation and increases temporal overhead in online adaptation. We address this issue by proposing a novel image-pruning strategy that selects the most informative subset of images from the current batch for active learning. We also propose a novel acquisition function that enhances diversity of the selected samples for annotating. Our approach outperforms existing online adaptation approaches and produces competitive results compared to offline domain adaptive active learning methods. The code can be found at https://github.com/ShazidAraf/ODES.", "filename": "2025_0674.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Md Shazid Islam", "Sayak Nag", "Arindam Dutta", "Sk Miraj Ahmed", "Fahim Faisal Niloy", "Shreyangshu Bera", "Amit K Roy-Chowdhury"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0675", "x": 2.683, "y": 3.798, "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "abstract": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.", "filename": "2025_0675.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0676", "x": 2.288, "y": 2.891, "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks", "abstract": "We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at https://github. com/xiaovhua/tenvoo", "filename": "2025_0676.pdf", "year": 2025, "institution": "Tokyo University of Agriculture and Technology", "country": "Japan", "authors": ["Binghua Li", "Ziqing Chang", "Tong Liang", "Chao Li", "Toshihisa Tanaka", "Shigeki Aoki", "Qibin Zhao", "Zhe Sun"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0677", "x": 1.65, "y": 4.963, "title": "PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation", "abstract": "Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic Resonance Images (MRI) is crucial for more precise prognosis assessment and the development of personalized treatment plans. However, automated segmentation remains challenging due to factors such as complex organ morphologies and confusing context. To address these challenges, we propose a novel Pattern Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to use different network modules to \"divide\" various local and global patterns and, through flexible feature selection, to \"conquer\" the Regions of Interest (ROI) during the decoding phase. Specifically, considering that our ROI often manifests as strip-like or circular-like structures in MR slices, we introduce a Multi-Direction Aggregation (MDA) module. This module enhances the model's ability to fit the shape of the organ by applying strip convolutions in four distinct directions. Additionally, to mitigate the challenge of confusing context, we propose a Memory-Guided Context (MGC) module. This module explicitly maintains a memory parameter to track cross-image patterns at the dataset level, thereby enhancing the distinction between global patterns associated with the positive and negative classes. Finally, we design an Adaptive Fusion Decoder (AFD) that dynamically selects features from different patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating the final segmentation results. We evaluate our method on the first large-scale X. Xiong, W. Cao, Z. Wu-Equal contribution.", "filename": "2025_0677.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Xinyu Xiong", "Wuteng Cao", "Zihuang Wu", "Lei Zhang", "Chong Gao", "Guanbin Li", "Qiyuan Qin"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0678", "x": 3.132, "y": 1.967, "title": "Physics-Driven Signal Regularization in Diffusion Models for Multi-contrast MR Image Synthesis", "abstract": "To achieve accurate diagnostic outcomes, it is often necessary to acquire multiple series of magnetic resonance imaging (MRI) with varying contrasts. However, this process is time-consuming and imposes a significant burden on patients and healthcare providers. While diffusion models have emerged as a highly effective tool for image synthesis, they face challenges in handling the complexities of real-world clinical data and may distort vital information during medical image synthesis. To address these issues, we propose MRDiff, a novel diffusion model for multi-contrast MR image synthesis. MRDiff leverages the intrinsic relationship between different contrast images to derive shared anatomical information based on MR physics equations. Our approach integrates MR physics-based signal regularization for proper content feature generation and employs self-content consistency training to capture accurate anatomical structures. Experimental results demonstrate that MRDiff outperforms existing methods by generating diagnostically valuable images, highlighting its potential for clinical applications in MR image synthesis.", "filename": "2025_0678.pdf", "year": 2025, "institution": "Yonsei University", "country": "Republic of Korea", "authors": ["Yejee Shin", "Yunsu Byeon", "Geonhui Son", "Hanbyol Jang", "Dosik Hwang", "Sewon Kim"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0679", "x": 1.584, "y": 2.04, "title": "Probabilistic Inverse Consistent Image Registration Using Sparse Bayesian Network", "abstract": "This paper proposes a probabilistic inverse consistency image registration network using a sparse BNN for cardiac motion estimation, aiming to simultaneously measure aleatoric and epistemic uncertainty. We construct a sparse BNN to predict the distribution parameters of the inverse consistency transformations between two images. Two symmetric Variational Autoencoders (VAEs) are constructed to predict the distribution parameters of latent variables in deformation space. The posterior distribution parameters of network weights are estimated during optimization, and only important weights are updated. Our sparse BNNs significantly reduce the computational cost and improve the registration accuracy by Bayesian model averaging (BMA). Experiments on a public cardiac MR dataset show that our sparse BNNs significantly improve the accuracy of the bidirectional registration for small datasets. It also provides aleatoric and epistemic uncertainty of registration results.", "filename": "2025_0679.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Shenglong Yang", "Kangrong Xu", "Zefeng He", "Tianchao Feng", "Xuan Yang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0680", "x": 3.278, "y": 1.881, "title": "Probabilistic Prior-Guided Anatomical Alignment for MRI Super-Resolution", "abstract": "High-resolution (HR) magnetic resonance imaging (MRI) offers exceptional visualization of human tissue but is often limited by hardware constraints. While recent super-resolution (SR) methods leveraging learned codebooks have shown promise, they often overlook the rich anatomical priors inherent in MRI data. To address this, we propose a probabilistic prior-guided anatomical alignment for MRI super-resolution (PGASR) method that incorporates anatomical knowledge into the SR process. Specifically, we first introduce an anatomical-conditioned codebook generation (ACG) module that generates rough anatomical structure maps by extracting the regions of interest from MRI slices. These maps are used as anatomical conditions for the discrete codebook generation. Then, to better exploit information between MRI slices, we propose a prior matching alignment (PMA) module that aligns the codebook index matching probabilities between adjacent slices, as well as across low-resolution (LR) and high-resolution (HR) domains, thereby reducing the loss of image details. We validate the effectiveness of the proposed PGASR method with the public MRI dataset IXI. The experimental results demonstrate that PGASR outperforms state-of-the-art methods.", "filename": "2025_0680.pdf", "year": 2025, "institution": "City University of Hong Kong", "country": "China", "authors": ["Yiwen Luo", "Xiaoying Tang", "Yixuan Yuan"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0681", "x": -0.176, "y": 5.073, "title": "RadSAM: Segmenting 3D Radiological Images with a 2D Promptable Model", "abstract": "Medical image segmentation is a crucial and time-consuming task in clinical care, where precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, providing an interactive interface based on visual prompting and edition. However, this model and adaptations for medical images are built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. This requires one prompt per slice, making the segmentation process tedious. We propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt, based on an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art 2D and 3D models using the AMOS abdominal organ segmentation dataset.", "filename": "2025_0681.pdf", "year": 2025, "institution": "1 Raidium , Paris , France\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\tRaidium Paris\n\t\t\t\t\t\t\t\t\tFrance", "country": "France", "authors": ["Julien Khlaut", "Elodie Ferreres", "Daniel Tordjman", "Helene Philippe", "Tom Boeken", "Pierre Manceron", "Corentin Dancette"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0682", "x": 2.466, "y": 5.335, "title": "RedDino: A Foundation Model for Red Blood Cell Analysis", "abstract": "Red blood cells (RBCs) are fundamental to human health, and precise morphological analysis is critical for diagnosing hematological disorders. Despite the potential of foundation models for medical diagnostics, comprehensive AI solutions for RBC analysis remain limited. We introduce RedDino, a self-supervised foundation model specifically designed for RBC image analysis. Leveraging a RBC-tailored version of the DINOv2 self-supervised learning framework, RedDino is trained on an extensive, meticulously curated dataset comprising over 1.25 million RBC images from diverse acquisition modalities and sources. Comprehensive evaluations demonstrate that RedDino significantly outperforms existing state-of-the-art models in the RBC shape classification. Through systematic assessments, including linear probing and nearest neighbor classification, we validate the model's robust feature representation and strong generalization capabilities. Our key contributions are (1) a dedicated foundation model tailored for RBC analysis, (2) detailed ablation studies exploring DINOv2 configurations for RBC modeling, and (3) comprehensive generalization performance evaluation. RedDino captures nuanced morphological characteristics and represents a substantial advancement in developing reliable diagnostic tools. Source code and pretrained models for RedDino are available at https://github.com/Snarci/ RedDino.", "filename": "2025_0682.pdf", "year": 2025, "institution": "University of Cagliari", "country": "Italy", "authors": ["Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Carsten Marr"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0683", "x": 3.583, "y": 2.83, "title": "Reflect: Rectified Flows for Efficient Brain Anomaly Correction Transport", "abstract": "Unsupervised anomaly detection (UAD) in brain imaging is crucial for identifying pathologies without the need for labeled data. However, accurately localizing anomalies remains challenging due to the intricate structure of brain anatomy and the scarcity of abnormal examples. In this work, we introduce Reflect, a novel framework that leverages rectified flows to establish a direct, linear trajectory for correcting abnormal MR images toward a normal distribution. By learning a straight, one-step correction transport map, our method efficiently corrects brain anomalies and can precisely localize anomalies by detecting discrepancies between anomalous input and corrected counterpart. In contrast to the diffusion-based UAD models, which require iterative stochastic sampling, rectified flows provide a direct transport map, enabling single-step inference. Extensive experiments on popular UAD brain segmentation benchmarks demonstrate that Reflect significantly outperforms state-of-the-art unsupervised anomaly detection methods. The code is available at https://github.com/farzad-bz/REFLECT.", "filename": "2025_0683.pdf", "year": 2025, "institution": "University of Montreal", "country": "Canada", "authors": ["Farzad Beizaee", "Sina Hajimiri", "Ismail Ben Ayed", "Gregory Lodygensky", "Christian Desrosiers", "Jose Dolz"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0684", "x": 1.393, "y": 2.325, "title": "Register Anything: Estimating “Corresponding Prompts” for Segment Anything Model", "abstract": "Establishing pixel/voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving finegrained capability at pixel/voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly \"searching for corresponding prompts\", using extensively pre-trained segmentation models (e.g., SAM) for a trainingfree registration approach, PromptReg. Firstly, we introduce the \"corresponding prompt problem\", which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an \"inverse prompt\" solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learningbased DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data.", "filename": "2025_0684.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Shiqi Huang", "Tingfa Xu", "Wen Yan", "Dean Barratt", "Yipeng Hu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0685", "x": 1.72, "y": 6.577, "title": "RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation Using Vision-Language Foundation Models", "abstract": "Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating highresolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions, a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM (e.g. Stable Diffusion) provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. Experiments on the public ISIC2019 skin lesion dataset demonstrate that the proposed method improves (a) the quality of the generated images, and (b) the alignment with the text prompt over the original fine-tuned Stable Diffusion baseline. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation. Our code is accessible through the project website (https://parhamsaremi.github. io/rl4med-ddpo).", "filename": "2025_0685.pdf", "year": 2025, "institution": "McGill University", "country": "Canada", "authors": ["Parham Saremi", "Amar Kumar", "Mohamed Mohamed", "Zahra Tehraninasab", "Tal Arbel"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0686", "x": 0.09, "y": 5.243, "title": "SafeClick: Error-Tolerant Interactive Segmentation of Any Medical Volumes via Hierarchical Expert Consensus", "abstract": "Foundation models for volumetric medical image segmentation have emerged as powerful tools in clinical workflows, enabling radiologists to delineate regions of interest through intuitive clicks. While these models demonstrate promising capabilities in segmenting previously unseen anatomical structures, their performance is strongly influenced by prompt quality. In clinical settings, radiologists often provide suboptimal prompts, which affects segmentation reliability and accuracy. To address this limitation, we present SafeClick, an error-tolerant interactive segmentation approach for medical volumes based on hierarchical expert consensus. SafeClick operates as a plug-and-play module compatible with foundation models including SAM 2 and MedSAM 2. The framework consists of two key components: a collaborative expert layer (CEL) that generates diverse feature representations through specialized transformer modules, and a consensus reasoning layer (CRL) that performs cross-referencing and adaptive integration of these features. This architecture transforms the segmentation process from a promptdependent operation to a robust framework capable of producing accurate results despite imperfect user inputs. Extensive experiments across 15 public datasets demonstrate that our plug-and-play approach consistently improves the performance of base foundation models, with particularly significant gains when working with imperfect prompts. The source code is available at https://github.com/yifangao112/SafeClick.", "filename": "2025_0686.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Yifan Gao", "Jiaxi Sheng", "Wenbin Wu", "Haoyue Li", "Yaoxian Dong", "Chaoyang Ge", "Feng Yuan", "Xin Gao"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0687", "x": 4.287, "y": 3.243, "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation", "abstract": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with catheter ablation procedures, but procedural outcomes are highly variable. Evaluating and improving ablation efficacy is challenging due to the complex interaction between patient-specific tissue and procedural factors. This paper asks two questions: Can AF recurrence be predicted by simulating the effects of procedural parameters? How should we ablate to reduce AF recurrence? We propose SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel deeplearning framework that addresses these questions. SOFA first simulates the outcome of an ablation strategy by generating a post-ablation image depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and the specific procedural parameters used (e.g., ablation locations, duration, temperature, power, and force). During this simulation, it predicts AF recurrence risk. Critically, SOFA then introduces an optimization scheme that refines these procedural parameters to minimize the predicted risk. Our method leverages a multi-modal, multi-view generator that processes 2.5D representations of the atrium. Quantitative evaluations show that SOFA accurately synthesizes post-ablation images and that our optimization scheme leads to a 22.18% reduction in the model-predicted recurrence risk. To the best of our knowledge, SOFA is the first framework to integrate the simulation of procedural effects, recurrence prediction, and parameter optimization, offering a novel tool for personalizing AF ablation. The code is available at our repository: link.", "filename": "2025_0687.pdf", "year": 2025, "institution": "Tulane University", "country": "USA", "authors": ["Yunsung Chung", "Chanho Lim", "Ghassan Bidaoui", "Christian Massad", "Nassir Marrouche", "Jihun Hamm"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0688", "x": 2.599, "y": 2.65, "title": "Sparse3Diff: A Diffusion Framework for 3D Reconstruction from Sparse 2D Slices in Volumetric Optical Imaging", "abstract": "Volumetric optical imaging is an essential tool for understanding various biological processes. However, due to the inherent limitations, such as long imaging time, volume scanning techniques reduce volumetric information into sparse 2D slices. Although many deep learning methods attempt to reconstruct 3D volumes from sparse slices, they struggle with out-of-distribution (OOD) data, which arises from the diversity of biological structures and the limited structural information in sparse slices. To overcome these challenges, we propose Sparse3Diff, a novel diffusion-based framework that reconstructs high-fidelity 3D volumes from sparse 2D slices. Sparse3Diff incorporates a sparse slice-guided position-aware diffusion process that utilizes sparse slices as guidance and conditions on z-position to maintain structural coherence along the z-axis. Additionally, to achieve stable reconstruction under sparse OOD data, we propose a self-alignment strategy that enables the model to be gradually fine-tuned by leveraging its own inferred slices as selfguidance. Experimental results demonstrate that even with sparse OOD data, the Sparse3Diff achieves accurate 3D reconstruction and remains robust across various scanning datasets.", "filename": "2025_0688.pdf", "year": 2025, "institution": "Korea University", "country": "South Korea", "authors": ["Hyun Jung Lee", "Eunjung Jo", "Minjoo Lim", "Young-Han Son", "Bogyeong Kang", "Hyeonyeong Nam", "Ji-Hoon Jeong", "Dong-Hee Shin", "Tae-Eui Kam"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0689", "x": 1.546, "y": 4.032, "title": "Spatial Prior-Guided Boundary and Region-Aware 2D Lesion Segmentation in Neonatal Hypoxic Ischemic Encephalopathy", "abstract": "Segmenting acute and hyper-acute brain lesions in neonatal hypoxic ischemic encephalopathy (HIE) from diffusion-weighted MRI (DWI) is critical for prognosis and treatment planning but remains challenging due to severe class imbalance and lesion variability. We propose a computationally efficient 2D segmentation framework leveraging ADC and ZADC maps as a three-channel input to UNet++ with an Inception-v4 encoder and scSE attention for enhanced spatial-channel recalibration. To address critical class imbalance and lack of volumetric context in 2D methods, we introduce a novel boundary-and-region-aware weighted loss integrating Tversky, Log-Hausdorff, and Focal losses. Our method surpasses state-of-the-art 2D approaches and achieves competitive performance against computationally intensive 3D architectures, securing a DSC of 0.6060, MASD of 2.6484, and NSD of 0.7477. These results establish a new benchmark for neonatal HIE lesion segmentation, demonstrating superior detection of both acute and hyper-acute lesions while mitigating the challenge of loss collapse. The code is available at https://github.com/BONBID-HIE/Neonatal-HIE-SPARSeg.", "filename": "2025_0689.pdf", "year": 2025, "institution": "Plaksha University", "country": "India", "authors": ["Amog Rao", "Ananya Shukla", "Jia Bhargava", "Yangming Ou", "Rina Bao"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0690", "x": 6.563, "y": 4.365, "title": "SSPNet: Towards Feasible Spatio-Spectral Portraits-Based Deep Learning Framework for Neurodegenerative Disease Multi-classification", "abstract": "Early diagnosis of neurodegenerative diseases is crucial for effective intervention and treatment planning. However, conventional screening tests such as Mini-Mental State Examination (MMSE) often produce false-negative issues. While electroencephalogram (EEG) signals contain valuable neurophysiological information, multi-class classification remains challenging due to subtle differences between conditions, with existing methods achieving around 50-60% accuracy. Therefore, we propose SSPNet, a novel deep learning framework for multi-class classification of neurodegenerative diseases using spatio-spectral portraits derived from EEG signals. Our approach extracts spatio-spectral images that maximize neurophysiological differences between Alzheimer's disease, frontotemporal dementia (FTD), and cognitively normal subjects, utilizing minimal frequency bands encoded through specialized asymmetric convolutional blocks and attention mechanisms. To our knowledge, this represents the first attempt to use EEG spatio-spectral portraits for multi-class classification of neurodegenerative diseases. The proposed SSPNet significantly improves accuracy to 72.22% compared to existing EEG-based methods for multi-class classification. It also demonstrates notably lower false-negative rates for FTD patients compared to MMSE, thus accelerating practical clinical application.", "filename": "2025_0690.pdf", "year": 2025, "institution": "Hallym University", "country": "Republic of Korea", "authors": ["Ho-Jung Kim", "Dogeun Park", "Jeong-Woo Jang", "Young-Gi Ju", "Dong-Ok Won"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0691", "x": 1.515, "y": 2.031, "title": "Structure-Preserve Expansion for Medical Image Registration with Minimal Overlap", "abstract": "Medical image registration relies on the overlapping regions between two images to calculate transformation parameters, thus posing a significant challenge for image registration with limited overlap. To overcome this challenge, this study proposes an image expansion solution by generating more overlapping regions to improve the registration performance between images with minimal overlap. As this is the first study to expand images for registration, we trained a generative network from scratch to avoid chaotic structures in the expanded regions. We proposed the Sequential Structure-Preserve Expansion (SSPE) framework to realize the expansion-based registration, where each image is present by a sliding scope and its expansion can be observed by sliding the scope. When given the current image and a sliding step, SSPE utilizes a generative network to predict the scope content of the next sliding position. Specially, we also bring in the gradient matching to maintain anatomical structures in the predicted scope. The performance of SSPE is evaluated on a public dataset of total-body CT images, which proves that our SSPE is significantly efficient in solving the registration difficulties caused by insufficient overlapping regions. The codes of our framework are made available at https://github.com/YongshengPan/ Structure-Preserve-Expansion, and we will also publish software for userfriendly access and testing.", "filename": "2025_0691.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Zaiyuan Liu", "Yongsheng Pan", "Yong Xia"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0692", "x": 3.7, "y": 2.631, "title": "Super-Resolution and Segmentation of 4D Flow MRI Using Deep Learning and Weighted Mean Frequencies", "abstract": "4D Flow MRI is a promising imaging sequence that provides 3D anatomy and velocity along the cardiac cycle. However, hemodynamic biomarkers are susceptible to degradation due to the low resolution of the imaging modality, which can compromise vessel segmentation. In this study, we propose a novel deep-learning approach, named SURFR-Net, that combines both super-resolution and segmentation tasks, leading to a super-resolved segmentation. SURFR-Net is based on the RCAN superresolution network, modified to handle a multi-task problem. A novel handcraft feature, named Weighted Mean Frequencies (WMF), has been introduced with the objective of assisting the network in differentiating between pulsatile and non-pulsatile fluid regions. Moreover, we demonstrate the use of WMF feature as input to enhance super-resolution and provide a more relevant segmentation on 4D Flow MRI images. The proposed solution has been shown to outperform the state-of-the-art solution, SRFlow, in terms of direction and quantification error on systolic and diastolic times with a maximum gain of 4.1% in relative error. Furthermore, this study demonstrates the benefit of combining the superresolution with the segmentation in a multi-task framework on both outcomes. In conclusion, the proposed solution has the capacity to facilitate a super-resolved segmentation of the aorta, thereby potentially addressing the primary concern regarding 4D Flow MRI parietal biomarkers, such as wall shear stress.", "filename": "2025_0692.pdf", "year": 2025, "institution": "Nantes Université", "country": "France", "authors": ["Simon Perrin", "Sébastien Levilly", "Harold Mouchère", "Jean-Michel Serfaty"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0693", "x": 2.799, "y": 1.326, "title": "Supervised Diffusion-Model-Based PET Image Reconstruction", "abstract": "Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PET's Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [ 18 F]FDG brain PET data.", "filename": "2025_0693.pdf", "year": 2025, "institution": "King's College London", "country": "UK", "authors": ["George Webber", "Alexander Hammers", "Andrew P King", "Andrew J Reader"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0694", "x": 2.386, "y": 2.391, "title": "SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model", "abstract": "X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multiview X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at https://github.com/xiechun298/SV-DRR.", "filename": "2025_0694.pdf", "year": 2025, "institution": "University of Tsukuba", "country": "Japan", "authors": ["Chun Xie", "Yuichi Yoshii", "Itaru Kitahara"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0695", "x": 3.4, "y": 2.257, "title": "Synthesizing Delayed-Phase Contrast-Enhanced Breast MR Images from Early-Phase Images Using an Iterative Deep Network", "abstract": "Acquisition of dynamic contrast-enhanced MR imaging with gadolinium-based contrast agents at multiple time points provides valuable diagnostic information. In breast MRI, dynamics of enhancement serve as key indicators for differentiating malignant from benign tumors. However, acquiring delayed-phase images requires extended scan times and could lead to patient discomfort and increased costs. Furthermore, some protocols acquire only early-phase images, limiting the ability to capture dynamics of enhancement over time. In this study, we propose an iterative deep neural network that sequentially generates post-contrast images using prior outputs. By synthesizing delayed-phase images at multiple time points from early acquisitions, the proposed network enables the temporal prediction of enhancement. We evaluate our approach using a breast MRI dataset consisting of images acquired at six time points, including the pre-contrast phase. The results indicate that the proposed method can approximate delayed-phase images from early-phase images, suggesting its potential to support abbreviated scan protocols in dynamic contrast-enhanced MRI. Our code is available at: https://github.com/ goglxych97/iterU-Net.git.", "filename": "2025_0695.pdf", "year": 2025, "institution": "Hankuk University of Foreign Studies", "country": "Korea", "authors": ["Woojin Chung", "Junghwa Kang", "Ga Eun Park", "Sung Hun Kim", "Yoonho Nam"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0696", "x": 2.947, "y": 1.927, "title": "Tackling Hallucination from Conditional Models for Medical Image Reconstruction with DynamicDPS", "abstract": "Hallucinations are spurious structures not present in the ground truth, posing a critical challenge in medical image reconstruction, especially for data-driven conditional models. We hypothesize that combining an unconditional diffusion model with data consistency, trained on a diverse dataset, can reduce these hallucinations. Based on this, we propose DynamicDPS, a diffusion-based framework that integrates conditional and unconditional diffusion models to enhance low-quality medical images while systematically reducing hallucinations. Our approach first generates an initial reconstruction using a conditional model, then refines it with an adaptive diffusion-based inverse problem solver. DynamicDPS skips early stage in the reverse process by selecting an optimal starting time point per sample and applies Wolfe's line search for adaptive step sizes, improving both efficiency and image fidelity. Using diffusion priors and data consistency, our method effectively reduces hallucinations from any conditional model output. We validate its effectiveness in Image Quality Transfer for low-field MRI enhancement. Extensive evaluations on synthetic and real MR scans, including a downstream task for tissue volume estimation, show that DynamicDPS reduces hallucinations, improving relative volume estimation by over 15% for critical tissues while using only 5% of the sampling steps required by baseline diffusion models. As a model-agnostic and fine-tuning-free approach, DynamicDPS offers a robust solution for hallucination reduction in medical imaging. Code is available at https://github.com/edshkim98/ DynamicDPS.", "filename": "2025_0696.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Seunghoi Kim", "Henry F J Tregidgo", "Matteo Figini", "Chen Jin", "Sarang Joshi", "Daniel C Alexander"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0697", "x": 3.357, "y": 2.457, "title": "Temporal Neural Cellular Automata: Application to Modeling of Contrast Enhancement in Breast MRI", "abstract": "Synthetic contrast enhancement offers fast image acquisition and eliminates the need for intravenous injection of contrast agent. This is particularly beneficial for breast imaging, where long acquisition times and high cost are significantly limiting the applicability of (MRI) as a widespread screening modality. Recent studies have demonstrated the feasibility of synthetic contrast generation. However, current (SOTA) methods lack sufficient measures for consistent temporal evolution. (NCA) offer a robust and lightweight architecture to model evolving patterns between neighboring cells or pixels. In this work we introduce TeNCA (Temporal Neural Cellular Automata), which extends and further refines NCAs to effectively model temporally sparse, nonuniformly sampled imaging data. To achieve this, we advance the training strategy by enabling adaptive loss computation and define the iterative nature of the method to resemble a physical progression in time. This conditions the model to learn a physiologically plausible evolution of contrast enhancement. We rigorously train and test TeNCA on a diverse breast MRI dataset and demonstrate its effectiveness, surpassing the performance of existing methods in generation of images that align with ground truth post-contrast sequences. Code: https://github.com/ LangDaniel/TeNCA.", "filename": "2025_0697.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Daniel M Lang", "Richard Osuala", "Veronika Spieker", "Karim Lekadir", "Rickmer Braren", "Julia A Schnabel"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0698", "x": 1.261, "y": 5.155, "title": "The Missing Piece: A Case for Pre-training in 3D Medical Object Detection", "abstract": "Large-scale pre-training holds the promise to advance 3D medical object detection, a crucial component of accurate computeraided diagnosis. Yet, it remains underexplored compared to segmentation, where pre-training has already demonstrated significant benefits. Existing pre-training approaches for 3D object detection rely on 2D medical data or natural image pre-training, failing to fully leverage 3D volumetric information. In this work, we present the first systematic study of how existing pre-training methods can be integrated into state-of-the-art detection architectures, covering both CNNs and Transformers. Our results show that pre-training consistently improves detection performance across various tasks and datasets. Notably, reconstruction-based self-supervised pre-training outperforms supervised pre-training, while contrastive pre-training provides no clear benefit for 3D medical object detection. Our code is publicly available at: https://github.com/MIC-DKFZ/nnDetection-finetuning.", "filename": "2025_0698.pdf", "year": 2025, "institution": "Heidelberg University", "country": "Germany", "authors": ["Katharina Eckstein", "Constantin Ulrich", "Michael Baumgartner", "Jessica Kächele", "Dimitrios Bounias", "Tassilo Wald", "Ralf Floca", "Klaus H Maier-Hein"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0699", "x": 3.014, "y": 2.674, "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "abstract": "3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodalconditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code", "filename": "2025_0699.pdf", "year": 2025, "institution": "Durham University", "country": "UK", "authors": ["Minye Shao", "Xingyu Miao", "Haoran Duan", "Zeyu Wang", "Jingkun Chen", "Yawen Huang", "Xian Wu", "Jingjing Deng", "Yang Long", "Yefeng Zheng"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0700", "x": 2.966, "y": 1.849, "title": "UFO-3: Unsupervised Three-Compartment Learning for Fiber Orientation Distribution Function Estimation", "abstract": "Fiber orientation distribution function (fODF) estimation from diffusion MRI is crucial for mapping brain connectivity but often requires extensive multi-shell acquisitions and complex computational methods. While supervised deep learning approaches have shown promise in accelerating this process, they typically require large training datasets and face challenges with domain shifts and interpretability. We present UFO-3, an unsupervised framework that combines a three-compartment biophysical model with deep learning for fODF estimation from singleshell data. The method leverages a U-Net architecture to simultaneously estimate fiber orientations and tissue microstructure parameters while maintaining physical constraints through an optimization-based reconstruction. Evaluated on synthetic data across 2500 test cases, UFO-3 achieves superior angular accuracy (MAE < 10 • at infinite SNR) and correlation (ACC > 91%) compared to existing methods, particularly in resolving challenging fiber crossings. On in vivo human brain data, UFO-3 produces fODF reconstructions comparable to multi-shell reference methods while providing interpretable tissue parameter estimates. The framework requires a one-time, subject-specific training of about 30 min on a single consumer GPU and enables fast inference (< 10 s per subject), improving throughput compared to other unsupervised approaches that require hours or days of training. Our results demonstrate that UFO-3 effectively balances reconstruction accuracy, biological interpretability, and computational performance without requiring extensive training data or multi-shell acquisitions.", "filename": "2025_0700.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Xueqing Gao", "Rizhong Lin", "Jianhui Feng", "Yonggang Shi", "Yuchuan Qiao"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0701", "x": 2.516, "y": 4.187, "title": "Uncertainty-Aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "abstract": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https:// github.com/yuhoo0302/CUA-US.", "filename": "2025_0701.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0702", "x": 1.987, "y": 2.223, "title": "Unsupervised OCT Image Interpolation Using Deformable Registration and generative models", "abstract": "Optical coherence tomography (OCT) images are often acquired as highly anisotropic volumes, where the scanning step is dense along the fast axis but sparse along the slow axis. This affects image analysis, such as image registration for longitudinal alignment. To create more isotropic volumes, bicubic interpolation can be used along the slow axis, but it generally produces blurry features. Registration-based interpolation can reduce blurriness, but often fails to generate realistic OCT images. Deep generative models can sample realistic images, but lack the structural consistency constraints required for interpolation. In this paper, we propose an unsupervised image interpolation method that combines registration-based interpolation with a deep generative model to overcome their individual limitations and improve the structural accuracy and realism of interpolated OCT images. We compare the proposed method with both bicubic and registration-based interpolation on real OCT datasets, and show that it achieves the best interpolation performance.", "filename": "2025_0702.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Shuwen Wei", "Samuel W Remedios", "Zhangxing Bian", "Shimeng Wang", "Junyu Chen", "Yihao A Liu", "Bruno Jedynak", "Tin Y A Liu", "Shiv Saidha", "Peter A Calabresi", "Jerry L Prince", "Aaron Carass"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0703", "x": 1.481, "y": 1.966, "title": "VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration", "abstract": "Recent developments in neural networks have improved deformable image registration (DIR) by amortizing iterative optimization, enabling fast and accurate DIR results. However, learning-based methods often face challenges with limited training data, large deformations, and tend to underperform compared to iterative approaches when label supervision is unavailable. While iterative methods can achieve higher accuracy in such scenarios, they are considerably slower than learning-based methods. To address these limitations, we propose Vox-elOpt, a discrete optimization-based DIR framework that combines the strengths of learning-based and iterative methods to achieve a better balance between registration accuracy and runtime. VoxelOpt uses displacement entropy from local cost volumes to measure displacement signal strength at each voxel, which differs from earlier approaches in three key aspects. First, it introduces voxel-wise adaptive message passing, where voxels with lower entropy receive less influence from their neighbors. Second, it employs a multi-level image pyramid with 27-neighbor cost volumes at each level, avoiding exponential complexity growth. Third, it replaces hand-crafted features or contrastive learning with a pretrained foundational segmentation model for feature extraction. In abdominal CT registration, these changes allow VoxelOpt to outperform leading iterative methods in both efficiency and accuracy, while matching state-of-the-art learning-based methods trained with label supervision. The source code will be available at https://github.com/tinymilky/ VoxelOpt.", "filename": "2025_0703.pdf", "year": 2025, "institution": "Cornell University", "country": "USA", "authors": ["Hang Zhang", "Yuxi Zhang", "Jiazheng Wang", "Xiang Chen", "Renjiu Hu", "Xin Tian", "Gaolei Li", "Min Liu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0704", "x": 1.257, "y": 3.752, "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation", "abstract": "Transformer-based architectures have advanced medical image analysis by effectively modeling long-range dependencies, yet they often struggle in 3D settings due to substantial memory overhead and insufficient capture of fine-grained local features. We address these limitations with WaveFormer, a novel 3D-transformer that: i) leverages the fundamental frequency-domain properties of features for contextual representation, and ii) is inspired by the top-down mechanism of the human visual recognition system, making it a biologically motivated architecture. By employing discrete wavelet transformations (DWT) at multiple scales, WaveFormer preserves both global context and high-frequency details while replacing heavy upsampling layers with efficient waveletbased summarization and reconstruction. This significantly reduces the number of parameters, which is critical for real-world deployment where computational resources and training times are constrained. Furthermore, the model is generic and easily adaptable to diverse applications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with state-of-the-art methods while offering substantially lower computational complexity. The code for WaveFormer is publicly available at: https://github.com/mahfuzalhasan/WaveFormer.", "filename": "2025_0704.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Md Mahfuz Al Hasan", "Mahdi Zaman", "Abdul Jawad", "Alberto Santamaria-Pang", "Ho Hin Lee", "Ivan Tarapov", "Kyle B See", "Md Shah Imran", "Antika Roy", "Yaser Pourmohammadi Fallah", "Navid Asadizanjani", "Reza Forghani"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0705", "x": 3.019, "y": 4.312, "title": "XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-scale Feature Learning", "abstract": "Optical Coherence Tomography Angiography (OCTA) and its derived en-face projections provide high-resolution visualization of the retinal and choroidal vasculature, which is critical for the rapid and accurate diagnosis of retinal diseases. However, acquiring high-quality OCTA images is challenging due to motion sensitivity and the high costs associated with software modifications for conventional OCT devices. Moreover, current deep learning methods for OCT-to-OCTA translation often overlook the vascular differences across retinal layers and struggle to reconstruct the intricate, dense vascular details necessary for reliable diagnosis. To overcome these limitations, we propose XOCT, a novel deep learning framework that integrates Cross-Dimensional Supervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for layeraware vascular reconstruction. Our CDS module leverages 2D layerwise en-face projections, generated via segmentation-weighted z-axis averaging, as supervisory signals to compel the network to learn distinct representations for each retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF module enhances vessel delineation through multi-scale feature extraction combined with a channel reweighting strategy, effectively capturing vascular details at multiple spatial scales. Our experiments on the OCTA-500 dataset demonstrate XOCT's improvements, especially for the en-face projections which are significant for clinical evaluation of retinal pathologies, underscoring its potential to enhance OCTA accessibility, reliability, and diagnostic value for ophthalmic disease detection and monitoring. The code is available at https://github.com/uci-cbcl/XOCT.", "filename": "2025_0705.pdf", "year": 2025, "institution": "University of California", "country": "USA Usha", "authors": ["Pooya Khosravi", "Kun Han", "Anthony T Wu", "Arghavan Rezvani", "Zexin Feng", "Xiaohui Xie"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0706", "x": 0.654, "y": 6.756, "title": "$${\\mu }^2$$ Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "abstract": "Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose µ 2 LLM, a multiscale multimodal large language models for RRG tasks. The novel µ 2 Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our finetuned µ 2 LLMs on limited data for RRG tasks. All code, data, and models will be publicly available in our official repository: https://github.com/ Siyou-Li/u2Tokenizer.", "filename": "2025_0706.pdf", "year": 2025, "institution": "Queen Mary University of London", "country": "UK", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J Thirunavukarasu", "Juntao Yu", "Le Zhang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0707", "x": 2.74, "y": 3.927, "title": "A Multi-Branch Framework for Cross-Domain Vessel Segmentation via the Few-Shot Paradigm", "abstract": "In recent years, deep learning-based vessel segmentation methods have made significant progress. However, the diversity of image modalities and the high-cost of acquiring sufficient annotated data constrain the performance of existing approaches. Given that the primary objective of segmenting various types of vessels is to extract highfrequency tubular structures, leveraging existing annotated datasets for training and fast generalizing to novel vessel segmentation tasks is an ideal solution to the above challenges, which can be achieved by the fewshot segmentation (FSS) paradigm. Unfortunately, the significant differences in texture and thickness among different types of vessels leave unsolved challenges. To address this issue, we propose a novel framework that incorporates FSS into cross-domain vessel segmentation. In particular, we construct high-frequency auxiliary modalities to guide the model in focusing on high-frequency features, which are highly correlated with vessel regions, thereby bridging the texture gap between images o f various vessel types. Furthermore, we design a Dual-Modal Feature Extraction and Fusion (DM-FEF) module to extract modality-specific features. Finally, addressing the thickness variations between different vessels, we designed a Multi-Branch Feature Extractor (MBFE) module to capture the diverse characteristics of vessels with different thickness, enabling the model to perceive the thickness differences between distinct vessels. Experimental results on six public datasets demonstrate the effectiveness of our method. Source code: https://github.com/ZiH-Huang/FSS_ Cross.", "filename": "2025_0707.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Zihang Huang", "Tianyu Zhao", "Liang Zhang", "Xin Yang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0708", "x": 1.651, "y": 4.575, "title": "Adapting Vision Foundation Models for Real-Time Ultrasound Image Segmentation", "abstract": "We propose a novel approach that adapts hierarchical vision foundation models for real-time ultrasound image segmentation. Existing ultrasound segmentation methods often struggle with adaptability to new tasks, relying on costly manual annotations, while real-time approaches generally fail to match state-of-the-art performance. To overcome these limitations, we introduce an adaptive framework that leverages the vision foundation model Hiera to extract multi-scale features, interleaved with DINOv2 representations to enhance visual expressiveness. These enriched features are then decoded to produce precise and robust segmentation. We conduct extensive evaluations on six public datasets and one in-house dataset, covering both cardiac and thyroid ultrasound segmentation. Experiments show that our approach outperforms state-of-the-art methods across multiple datasets and excels with limited supervision, surpassing nnUNet by over 20% on average in the 1% and 10% data settings. Our method achieves ∼77 FPS inference speed with TensorRT on a single GPU, enabling real-time clinical applications.", "filename": "2025_0708.pdf", "year": 2025, "institution": "Yale University", "country": "USA", "authors": ["Xiaoran Zhang", "Eric Z Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S Duncan", "Terrence Chen", "Shanhui Sun"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0709", "x": 2.457, "y": 4.692, "title": "Anatomical Structure Few-Shot Detection Utilizing Enhanced Human Anatomy Knowledge in Ultrasound Images", "abstract": "Deep learning-based models have significantly advanced clinical ultrasound tasks by detecting anatomical structures within vast ultrasound image datasets. However, their remarkable performance inherently requires extensive training of annotated medical datasets. Few-shot learning addresses the challenge of limited labeled data for model training. Currently, few-shot learning in the field of medical image analysis mainly focuses on classification and semantic segmentation, with relatively fewer studies on object detection. In this paper, we propose a novel few-shot anatomical structure detection method in ultrasound images called TRR-CCM, which consists of Circular Channel Mamba (CCM) and Topological Relationship Reasoning (TRR) based on human anatomy knowledge. CCM, as a new Mamba variant, performs contextual modeling of anatomical structures and captures long-and short-term dependencies. TRR learns spatial topological relationships between human anatomical structures to further improve the accuracy of detection and localization. Experimental results on two fetal ultrasound datasets demonstrate that TRR-CCM outperforms 9 state-of-theart baseline methods.", "filename": "2025_0709.pdf", "year": 2025, "institution": "Yunnan University", "country": "China", "authors": ["Ying Zhu", "Bocheng Liang", "Ningshu Li", "Lei Zhao", "Xi Li", "Hao Li", "Fengwei Yang", "Bin Pu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0710", "x": 4.528, "y": 3.5, "title": "CardiacCLIP: Video-Based CLIP Adaptation for LVEF Prediction in a Few-Shot Manner", "abstract": "Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing i nformative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at https://github.com/xmed-lab/CardiacCLIP.", "filename": "2025_0710.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Yao Du", "Jiarong Guo", "Xiaomeng Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0711", "x": 6.0, "y": 4.729, "title": "CARE-VL: A Domain-Specialized Vision-Language Model for Early ASD Screening", "abstract": "We propose an autism spectrum disorder (ASD) screening framework that integrates an expert vision-language model (VLM), CARE-VL, with a large language model (LLM)-based aggregation module to assess children's social interactions and derive subject-level ASD/typical development (TD) classifications. Our framework processes video data collected using social interaction-inducing content, where medical experts annotated predefined query-response (Q-R) intervals based on key social indicators-such as response to name, eye contact, imitation behavior, social smiling, and pointing-by marking correct responses and assigning subject-level ASD/TD classifications. To adapt the generalpurpose VLM to the ASD screening domain, we constructed a synthetic instruction-tuning dataset using a label-guided reasoning method on these clinical tags, fine-tuning the model to generate detailed captions and multiple-choice question-answer (MC-QA) pairs, capturing children's critical social behaviors. CARE-VL processes Q-R intervals t o produce clip-level MC-QA results and descriptive captions, which are then aggregated by an LLM to derive final ASD/TD classification and clinical reasoning. Our end-to-end framework combines visual understanding and linguistic reasoning, achieving 84.6% accuracy for clip-level response prediction and 75.8% accuracy for subject-level ASD/TD classification. These results demonstrate the potential of our framework as a practical and interpretable tool for early ASD screening and behavioral assessment. The code is publicly available at https://github.com/etri/AI4ASD.", "filename": "2025_0711.pdf", "year": 2025, "institution": "ETRI", "country": "Republic of Korea", "authors": ["Cheol-Hwan Yoo", "Jang-Hee Yoo", "Jaeyoon Jang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0712", "x": 2.191, "y": 4.841, "title": "CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer", "abstract": "Cell microscopy data are abundant; however, corresponding segmentation annotations remain scarce. Moreover, variations in cell types, imaging devices, and staining techniques introduce significant domain gaps between datasets. As a result, even large, pretrained segmentation models trained on diverse datasets (source datasets) struggle to generalize to unseen datasets (target datasets). To overcome this generalization problem, we propose CellStyle, which improves the segmentation quality of such models without requiring labels for the target dataset, thereby enabling zero-shot adaptation. CellStyle transfers the attributes of an unannotated target dataset, such as texture, color, and noise, to the annotated source dataset. This transfer is performed while preserving the cell shapes of the source images, ensuring that the existing source annotations can still be used while maintaining the visual characteristics of the target dataset. The styled synthetic images with the existing annotations enable the finetuning of a generalist segmentation model for application to the unannotated target data. We demonstrate that CellStyle significantly improves the cell segmentation performance across diverse datasets by finetuning multiple segmentation models on the style-transferred data. The source code for CellStyle is publicly available at https://github.com/ruveydayilmaz0/cellStyle.", "filename": "2025_0712.pdf", "year": 2025, "institution": "RWTH Aachen U niversity", "country": "Germany", "authors": ["Rüveyda Yilmaz", "Zhu Chen", "Yuli Wu", "Johannes Stegmaier"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0713", "x": 0.522, "y": 6.704, "title": "Cervical-RG: Automated Cervical Cancer Report Generation from 3D Multi-sequence MRI via CoT-Guided Hierarchical Experts", "abstract": "Cervical cancer remains a leading cause of cancer-related mortality among females globally, with diagnosis primarily relying on multi-sequence magnetic resonance imaging (MRI). However, existing Multi-modal Large Language Models (MLLMs) struggle with processing 3D multi-sequence inputs due to high computational complexity and inefficient long-sequence modeling. To this end, we present Cervical-RG, which, to the best of our knowledge, this is the first framework that utilizes 3D multi-sequence MRI images for automated report generation. International Federation of Gynecology and Obstetrics (FIGO) staging, which plays a critical role in cervical cancer management, is also incorporated into the report. The workflow consists of (1) image diagnosis generation. (2) Chain of Thought (CoT)-guided FIGO staging with rationale, and (3) cross-stage consistency v erification. Meanwhile, the entire pipeline simulates the collaborative diagnostic process of multi-disciplinary experts in clinical practice. Besides, we propose a novel model to handle multi-sequence inputs, comprising a volumetric multisequence encoder and a Mamba-Transformer hybrid decoder, which integrates global attention with selective state-space modeling to effectively handle long-range dependencies and spatial relationships. To validate H. Zhang, Y. Long, Y. Fan, and Y. Wang-Equal con tribution.", "filename": "2025_0713.pdf", "year": 2025, "institution": "Shenzhen MSU-BIT University", "country": "China", "authors": ["Hanwen Zhang", "Yu Long", "Yimeng Fan", "Yu Wang", "Zhaoyi Zhan", "Sen Wang", "Yuncheng Jiang", "Rui Sun", "Zheng Xing", "Zhen Li", "Xiaohui Duan", "Weibing Zhao"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0714", "x": 0.834, "y": 6.671, "title": "Configurable Platform for Biomedical Literature Mining via Multimodal-Driven Extraction", "abstract": "Biomedical literature serves as a critical repository for cutting-edge research achievements, encompassing substantial statistically validated biological knowledge. However, the dispersed storage and unstructured characteristics of such literature significantly hinder manual acquisition efficiency while increasing error susceptibility. To address these challenges, this study proposes an intelligent literature knowledge mining platform. Three core innovations distinguish this research: (1) The development of an extensible literature collectionparsing-structuring framework based on a \"literature tree\" architecture (ECPS-LitTree), which facilitates HTML dynamic report generation and full-cycle data management, offering a novel solution for cross-source heterogeneous literature knowledge aggregation; (2) The design of a configurable requirement customization framework (CRC ) that combines named entity recognition (NER) technology with user-configurable mining templates to enable personalized knowledge extraction; (3) The implementation of an integrated online platform, providing comprehensive services including visual analytics, interactive search, and batch data export functionalities. Experimental validation demonstrates that the platform surpasses existing mainstream tools in literature retrieval success rate, processing efficiency, and knowledge extraction volume. The platform's flexible configurability exhibits broad applicability across multiple biomedical domains, offering researchers a reliable intelligent tool for knowledge discovery. The Configurable Platform is publicly and freely accessible at https://medseeker.genemed.tech/", "filename": "2025_0714.pdf", "year": 2025, "institution": "Hunan University of Technology", "country": "China", "authors": ["Xinpan Yuan", "Bozhao Li", "Guihu Zhao", "Yueming Wang", "Liujie Hua", "Junhua Kuang", "Jianguo Chen", "Shaomin Xie", "Gan Li"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0715", "x": 1.993, "y": 4.674, "title": "ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation", "abstract": "Medical images are usually collected from multiple domains, leading to domain shifts that impair the performance of medical image segmentation models. Domain Generalization (DG) aims to address this issue by training a robust model with strong generalizability. Recently, numerous domain randomization-based DG methods have been proposed. However, these methods suffer from the following limitations: 1) constrained efficiency of domain randomization due to their exclusive dependence on image style perturbation, and 2) neglect of the adverse effects of over-augmented images on model training. To address these issues, we propose a novel domain randomization-based DG method, called content style augmentation (ConStyX), for generalizable medical image segmentation. Specifically, ConStyX 1) augments the content and style of training data, allowing the augmented training data to better cover a wider range of data domains, and 2) leverages well-augmented features while mitigating the negative effects of over-augmented features during model training. Extensive experiments across multiple domains demonstrate that our ConStyX achieves superior generalization performance. The code is available at https://github.com/jwxsp1/ConStyX.", "filename": "2025_0715.pdf", "year": 2025, "institution": "Northeastern University", "country": "China", "authors": ["Xi Chen", "Zhiqiang Shen", "Peng Cao", "Jinzhu Yang", "Osmar R Zaiane"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0716", "x": 2.635, "y": 5.069, "title": "Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities", "abstract": "Traditional fundus image analysis models focus on singlemodal tasks, ignoring fundus modality complementarity, which limits their versatility. Recently, retinal foundation models have emerged, but most still remain modality-specific. Integrating multiple fundus imaging modalities into a single foundation model is valuable. However, in dynamic environments, data from different modalities often arrive incrementally, necessitating continual pre-training. To address this, we propose RetCoP, the first continual vision-language pre-training framework in the fundus domain, which incrementally integrates image and text features from different imaging modalities into a single unified foundation model. To mitigate catastrophic forgetting in continual pre-training, we introduce a rehearsal strategy utilizing representative image-text pairs and an off-diagonal information distillation approach. The former allows the model to revisit knowledge from previous stages, while the latter explicitly preserves the alignment between image and text representations. Experiments show that RetCoP outperforms all the compared methods, achieving the best generalization and lowest forgetting rate. The code can be found at https://github.com/Yuang-Yao/RetCoP.", "filename": "2025_0716.pdf", "year": 2025, "institution": "Southeast U niversity", "country": "China", "authors": ["Yuang Yao", "Ruiqi Wu", "Yi Zhou", "Tao Zhou"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0717", "x": 1.4, "y": 4.776, "title": "D-CAM: Learning Generalizable Weakly-Supervised Medical Image Segmentation from Domain-Invariant CAM", "abstract": "Weakly-supervised medical image segmentation with only image-level annotation is particularly challenging to infer precise pixelwise predictions. Existing works are usually highly restricted by the assumption that the medical images for training and testing are under the same distribution. However, a robust weakly-supervised segmentation model needs to show accurate inference on medical images from unseen distributions. Different feature distributions can lead to a dramatic shift in the feature activation and class activation map (CAM), which in turn leads to the degradation of pseudo labels. In this paper, we aim to learn generalizable weakly-supervised medical image segmentation by focusing on enhancing the domain invariance for pseudo labels. A novel domain-in variant CAM learning scheme (D-CAM) is proposed, in which the content and style are decoupled during training. By inferring domain-invariant pseudo labels, the supervision of a segmentation model is more generalizable to different target domains. Extensive experiments under multiple generalized medical image segmentation settings show the state-of-the-art performance of our D-CAM. Source code is available at https://github.com/JingjunYi/D-CAM.", "filename": "2025_0717.pdf", "year": 2025, "institution": "Tencent Jarvis Lab", "country": "China", "authors": ["Jingjun Yi", "Qi Bi", "Hao Zheng", "Haolan Zhan", "Wei Ji", "Huimin Huang", "Yuexiang Li", "Shaoxin Li", "Xian Wu", "Yefeng Zheng", "Feiyue Huang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0718", "x": 2.021, "y": 6.506, "title": "Delving Into Out-of-Distribution Detection with Medical Vision-Language Models", "abstract": "Recent advances in medical vision-language models (VLMs) demonstrate impressive performance in image classification tasks, driven by their strong zero-shot generalization capabilities. However, given the high variability and complexity inherent in medical imaging data, the ability of these models to detect out-of-distribution (OOD) data in this domain remains underexplored. In this work, we conduct the first systematic investigation into the OOD detection potential of medical VLMs. We evaluate state-of-the-art VLM-based OOD detection methods across a diverse set of medical VLMs, including both general and domain-specific purposes. To accurately reflect real-world challenges, we introduce a cross-modality evaluation p ipeline for benchmarking full-spectrum OOD detection, rigorously assessing model robustness against both semantic shifts and covariate shifts. Furthermore, we propose a novel hierarchical prompt-based method that significantly enhances OOD detection performance. Extensive experiments are conducted to validate the effectiveness of our approach. The codes are available at https://github.com/PyJulie/ Medical-VLMs-OOD-Detection", "filename": "2025_0718.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Lie Ju", "Sijin Zhou", "Yukun Zhou", "Huimin Lu", "Zhuoting Zhu", "Pearse A Keane", "Zongyuan Ge"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0719", "x": 0.817, "y": 1.052, "title": "DentEval: Fine-tuning-Free Expert-Aligned Assessment in Dental Education via LLM Agents", "abstract": "Large language models (LLMs) have demonstrated considerable potential in automating assignment scoring within higher education, providing efficient and consistent evaluations. However, existing systems encounter substantial challenges when assessing students' responses to open-ended short-answer questions. These challenges include the need for large, annotated datasets for fine-tuning or additional training, as well as inconsistencies between model outputs and human-level evaluations. This issue is particularly pronounced in domains requiring specialized knowledge, such as dentistry. To address these limitations, we propose DentEval, an LLM-based automated assignment assessment system supporting multimodal inputs (e.g., text and clinical images) that is tailored for dental curricula. This framework integrates role-playing prompting and Self-refining Retrieval-Augmented Generation (SR-RAG) to assess student responses and ensure that the system's outputs closely align with human grading standards. W e further utilized a dataset annotated by dental professors, dividing it into few-shot learning and testing sets to evaluate the DentEval framework. Results demonstrate that DentEval exhibits a stronger correlation with human grading compared to representative baselines. Finally, comprehensive ablation studies validate the effectiveness of the individual components incorporated in DentEval. Our code is available on GitHub at: https://github.com/DXY0711/DentEval.", "filename": "2025_0719.pdf", "year": 2025, "institution": "The University of Sydney", "country": "Australia", "authors": ["Xinyu Deng", "Vesna Miletic", "Elvis Trinh", "Jinlong Gao", "Chang Xu", "Daochang Liu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0720", "x": 2.931, "y": 3.794, "title": "DetectDiffuse: Aggregation- and Attention-Driven Universal Lesion Detection with Multi-scale Diffusion Model", "abstract": "Automated Universal Lesion Detection (ULD) based on computed tomography (CT) images provides physicians with rapid and objective information regarding lesion locations and shapes. However, it is difficult to detect universal lesions in various regions because of the disparity in lesion sizes and the grayscale variation present in CT images. In this paper, we propose DetectDiffuse, a multi-scale diffusion model driven by feature aggregation and 3D attention. First, we utilize the diffusion model to generate noisy detection boxes, incorporating a scale factor to simulate lesions at different scales and mitigate detection errors. Second, we develop a Neighborhood Aggregation (NA) module to enhance the model's capability to distinguish between lesioned and normal tissues. This module aggregates features within and around detection boxes, reducing false detections caused by significant grayscale differences i n lesions. Third, we propose a 3D Stripe Attention (SA) module leveraging dimensional disambiguation. This module uses an attention mechanism to extract information across different dimensions of CT images more effectively. We performed comparison experiments on five datasets, the results show that the proposed method outperforms the 12 compared state-of-the-art methods, and improves the performance by 5.82% compared with the best method.", "filename": "2025_0720.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Xinyu Li", "Danni Ai", "Jingfan Fan", "Tianyu Fu", "Hong Song", "Deqiang Xiao", "Jian Yang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0721", "x": 1.496, "y": 5.228, "title": "Dynamic Gradient Sparsification Training for Few-Shot Fine-Tuning of CT Lymph Node Segmentation Foundation Model", "abstract": "Accurate lymph node (LN) segmentation is critical in radiotherapy treatment and prognosis analysis, but is limited by the need for large annotated datasets. While deep learning-based segmentation foundation models show potential in developing high-performing models with fewer samples, their medical adaptation faces domain-specific prior deficiencies in the LN domain and inefficient few-shot fine-tuning for complex clinical practices, highlighting the necessity of an LN segmentation foundation model. In this work, we annotated 36,106 visible LNs from 3,346 publicly available head-and-neck CT scans to establish a robust LN segmentation model (nnUNetv2). Building on this, we propose Dynamic Gradient Sparsification Training (DGST), a few-shot fine-tuning approach that preserves foundational kno wledge while dynamically updating the most critical parameters of the LN segmentation model with few annotations. We validate it on two publicly available LN segmentation datasets: SegRap2023 and LNQ2023. The results show that DGST outperforms existing few-shot fine-tuning methods, achieving satisfactory performance with limited labeled data. We release the dataset, models and all implementations to facilitate relevant research: https://github. com/HiLab-git/LN-Seg-FM.", "filename": "2025_0721.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Zihao Luo", "Zijun Gao", "Wenjun Liao", "Shichuan Zhang", "Guotai Wang", "Xiangde Luo"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0722", "x": 4.71, "y": 3.686, "title": "EchoingECG: An Electrocardiogram Cross-Modal Model for Echocardiogram Tasks", "abstract": "Electrocardiogram (ECG) is a widely used tool for assessing cardiac function due to its low cost and accessibility. Emergent research shows that ECGs can help make predictions on key outcomes traditionally derived from more complex modalities such as echocardiograms (ECHO), enabling the use of ECGs as a more accessible method to predict broader measurements of cardiac function. ECHO, in particular, are of great importance because they require considerable hospital resources while playing a key role in clinical cardiac assessment. To aid this use case, we introduce EchoingECG, a probabilistic student-teacher model that leverages uncertainty-aware ECG embeddings and ECHO supervision to improve ECG-based cardiac function prediction. Our approach integrates Probabilistic Cross-Modal Embeddings (PCME++), a probabilistic contrastive framework, with ECHO-CLIP, a vision-language pretrained m odel trained on ECHO-text pairs, to distill ECHO knowledge into ECG representations. Through experiments and external validation, we showed that EchoingECG outperforms state-of-the-art foundation ECG models in zero-shot, few-shot, and fine-tune settings for ECHO predictions based on ECG. We also highlighted that variance estimation (enabled through our method) enhanced our understanding of model performance by identifying underlying regions of uncertainty within ECGs. The code is available: https://github.com/mcintoshML/EchoingECG.", "filename": "2025_0722.pdf", "year": 2025, "institution": "University Health Network (UHN)", "country": "Canada", "authors": ["Yuan Gao", "Sangwook Kim", "Chris Mcintosh"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0723", "x": 2.081, "y": 6.887, "title": "Fair-MoE: Medical Fairness-Oriented Mixture of Experts in Vision-Language Models", "abstract": "Fairness is an important principle in medical ethics. Vision Language Models (VLMs) have shown significant potential in the medical field due to their ability to leverage both visual and linguistic contexts, reducing the need for large datasets and enabling the performance of complex tasks. However, the exploration of fairness within VLM applications remains limited. Applying VLMs without a comprehensive analysis of fairness could lead to concerns about equal treatment opportunities and diminish public trust in medical deep learning models. To build trust in medical VLMs, we propose Fair-MoE, a model specifically designed to ensure both fairness and effectiveness. Fair-MoE comprises two key components: the Fairness-Oriented Mixture of Experts (FO-MoE) and the Fairness-Oriented Loss (FOL). FO-MoE is designed to leverage the expertise of various specialists to filter out biased patch embeddings a nd use an ensemble approach to extract more equitable information relevant to specific tasks. FOL is a novel fairness-oriented loss function that not only minimizes the distances between different attributes but also optimizes the differences in the dispersion of various attributes' distributions. Extended experiments show that Fair-MoE improves both fairness and accuracy across all four attributes. Code is made publicly available at https://github.com/LinjieT/Fair-MoE-Medical-Fairness-Oriented-Mixt ure-of-Experts-in-Vision-Language-Models.", "filename": "2025_0723.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Peiran Wang", "Linjie Tong", "Jian Wu", "Jiaxiang Liu", "Zuozhu Liu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0724", "x": 0.938, "y": 1.152, "title": "Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model", "abstract": "Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to interobserver variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in humancentric vision tasks, and (2) a novel geometric loss function that improves the model's capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: https://github.com/xmed-lab/GeoSapiens.", "filename": "2025_0724.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Anbang Wang", "Marawan Elbatel", "Keyuan Liu", "Lizhuo Lin", "Meng Lan", "Yanqi Yang", "Xiaomeng Li"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0725", "x": 1.995, "y": 6.466, "title": "Global and Local Vision-Language Alignment for Few-Shot Learning and Few-Shot OOD Detection", "abstract": "Training data in the medical domain is often limited due to privacy concerns and data scarcity. In such few-shot settings, neural network models are prone to overfitting, resulting in poor performance on new in-distribution (ID) data and misclassification of out-ofdistribution (OOD) data as learned ID diseases. Existing research treats these two tasks (few-shot learning and few-shot OOD detection) separately, and no prior work has explored a unified approach to simultaneously improving the performance of both tasks. To bridge this gap, we propose a novel framework based on CLIP that jointly enhances ID classification accuracy and OOD detection performance. Our framework consists of three key components: (1) a visually-guided text refinement module, which refines text representations of each disease utilizing diseaserelevant visual information; (2) a local version of supervised cont rastive learning, which enhances local representation consistency among diseaserelevant regions while improving ID-OOD separability; and (3) a global and local image-text alignment strategy, which adaptively combines the global and local similarity measurements for better image-text alignment. Extensive experiments demonstrate that our method outperforms the best methods specifically-tailored for both tasks, achieving new stateof-the-art performance. The source code is available at https://openi.pcl. ac.cn/OpenMedIA/GLAli.", "filename": "2025_0725.pdf", "year": 2025, "institution": "Sun Yat-sen Univerisity", "country": "China", "authors": ["Jie Yan", "Xiaoyuan Guan", "Wei-Shi Zheng", "Hao Chen", "Ruixuan Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0726", "x": 2.36, "y": 4.573, "title": "GrInAdapt: Source-Free Multi-target Domain Adaptation for Retinal Vessel Segmentation", "abstract": "Retinal vessel segmentation is critical for diagnosing ocular conditions, yet current deep learning methods are limited by modalityspecific challenges and significant distribution shifts across imaging devices, resolutions, and anatomical regions. In this paper, we propose GrInAdapt, a novel framework for source-free multi-target domain adaptation that leverages multi-view images to refine segmentation labels and enhance model generalizability for optical coherence tomography angiography (OCTA) of the fundus of the eye. GrInAdapt follows an intuitive three-step approach: (i) grounding images to a common anchor space via registration, (ii) integrating predictions from multiple views to achieve improved label consensus, and (iii) adapting the source model to diverse target domains. Furthermore, GrInAdapt is flexible enough to incorporate auxiliary modalities-such as color fundus photograph y-to provide complementary cues for robust vessel segmentation. Extensive experiments on a multi-device, multi-site, and multi-modal retinal dataset demonstrate that GrInAdapt significantly outperforms existing domain adaptation methods, achieving higher segmentation accuracy and robustness across multiple domains. These results highlight the potential of GrInAdapt to advance automated retinal vessel analysis and support robust clinical decision-making. Our code is here.", "filename": "2025_0726.pdf", "year": 2025, "institution": "Allen School of Computer Science and Engineering", "country": "USA", "authors": ["Zixuan Liu", "Aaron Honjaya", "Yuekai Xu", "Yi Zhang", "Hefu Pan", "Xin Wang", "Linda G Shapiro", "Sheng Wang", "Ruikang K Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0727", "x": 1.988, "y": 6.54, "title": "Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection", "abstract": "In trustworthy medical diagnosis systems, integrating outof-distribution (OOD) detection aims to identify unknown diseases in samples, thereby mitigating the risk of misdiagnosis. In this study, we propose a novel OOD detection framework based on vision-language models (VLMs), which integrates hierarchical visual information to cope with challenging unknown diseases that resemble known diseases. Specifically, a cross-scale visual fusion strategy is proposed to couple visual embeddings from multiple scales. This enriches the detailed representation of medical images and thus improves the discrimination of unknown diseases. M oreover, a cross-scale hard pseudo-OOD sample generation strategy is proposed to benefit OOD detection maximally. Experimental evaluations on three public medical datasets support that the proposed framework achieves superior OOD detection performance compared to existing methods. The source code is available at https://openi.pcl.ac. cn/OpenMedIA/HVL", "filename": "2025_0727.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Runhe Lai", "Xinhua Lu", "Kanghao Chen", "Qichao Chen", "Wei-Shi Zheng", "Ruixuan Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0728", "x": 4.295, "y": 6.197, "title": "HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction", "abstract": "Survival prediction using whole-slide images (WSIs) is crucial in cancer research. Despite notable success, existing approaches are limited by their reliance on sparse slide-level labels, which hinders the learning of discriminative representations from gigapixel WSIs. Recently, vision language (VL) models, which incorporate additional language supervision, have emerged as a promising solution. However, VL-based survival prediction remains largely unexplored due to two key challenges. First, current methods often rely on only one simple language prompt and basic cosine similarity, which fails to learn fine-grained associations between multi-faceted linguistic information and visual features within WSI, resulting in inadequate vision-language alignment. Second, these methods primarily exploit patch-level information, overlooking the intrinsic hierarchy of WSIs and their interactions, causing ineffective modeling of hierarchical interactions. To tackle these problems, we propose a novel Hierarchical vision-Language collaboration (HiLa) framework for improved survival prediction. Specifically, HiLa employs pretrained feature extractors to generate hierarchical visual features from WSIs at both patch and region levels. At each level, a series of language prompts describing various survival-related attributes are constructed and aligned with visual features via Optimal Prompt Learning (OPL). This approach enables the comprehensive learning of discriminative visual features corresponding to different survival-related attributes from prompts, thereby improving visionlanguage alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical cooperation by promoting interactions and consistency between patch and region levels. Experiments on The Cancer Genome Atlas (TCGA) datasets demonstrate our state-of-the-art performance..", "filename": "2025_0728.pdf", "year": 2025, "institution": "Sichuan University", "country": "China", "authors": ["Jiaqi Cui", "Lu Wen", "Yuchen Fei", "Bo Liu", "Luping Zhou", "Dinggang Shen", "Yan Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0729", "x": 1.953, "y": 4.811, "title": "HyDA: Hypernetworks for Test Time Domain Adaptation in Medical Imaging Analysis", "abstract": "Medical imaging datasets often vary due to differences in acquisition protocols, patient demographics, and imaging devices. These variations in data distribution, known as domain shift, present a significant challenge in adapting imaging analysis models for practical healthcare applications. Most current domain adaptation (DA) approaches aim either to align the distributions between the source and target domains or to learn an invariant feature space that generalizes well across all domains. However, both strategies require access to a sufficient number of examples, though not necessarily annotated, from the test domain during training. This limitation hinders the widespread deployment of models in clinical settings, where target domain data may only be accessible in real time.In this work, we introduce HyDA, a novel hypernetwork framework that leverages domain-specific characteristics rather than suppressing them, enabling dynamic adaptation at inference time. Specifically, HyDA learns implicit domain representations and uses them to adjust model parameters on-the-fly, allowing effective interpolation to unseen domains. We validate HyDA on two clinically relevant applications-MRI-based brain age prediction and chest X-ray pathology classification-demonstrating its ability to generalize across tasks and imaging modalities. Our code is available at: https://github.com/ doronser/hyda.", "filename": "2025_0729.pdf", "year": 2025, "institution": "Ben-Gurion University of The Negev", "country": "Israel", "authors": ["Doron Serebro", "Tammy Riklin-Raviv"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0730", "x": 3.214, "y": 6.567, "title": "HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis", "abstract": "Pathology is essential for cancer diagnosis, with multiple instance learning (MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural hierarchy-patches, regions, and slides-with distinct semantic associations. While some methods attempt to leverage this hierarchy for improved representation, they predominantly rely on Euclidean embeddings, which struggle to fully capture semantic hierarchies. To address this limitation, we propose HyperPath, a novel method that integrates knowledge from textual descriptions to guide the modeling of semantic hierarchies of WSIs in hyperbolic space, thereby enhancing WSI classification. Our approach adapts both visual and textual features extracted by pathology vision-language foundation models to the hyperbolic space. We design an Angular Modality Alignment Loss to ensure robust cross-modal alignment, while a Semantic Hierarchy Consistency Loss further refines feature hierarchies through entailment and contradiction relationships and thus enhance seman tic coherence. The classification is performed with geodesic distance, which measures the similarity between entities in the hyperbolic semantic hierarchy. This eliminates the need for linear classifiers and enables a geometry-aware approach to WSI analysis. Extensive experiments show that our method achieves superior performance across tasks compared to existing methods, highlighting the potential of hyperbolic embeddings for WSI analysis. The source code is available at https://github.com/HKU-MedAI/ HyperPath.", "filename": "2025_0730.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Peixiang Huang", "Yanyan Huang", "Weiqin Zhao", "Junjun He", "Lequan Yu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0731", "x": 0.552, "y": 6.21, "title": "Interactive Segmentation and Report Generation for CT Images", "abstract": "Automated CT report generation plays a crucial role in improving diagnostic accuracy and clinical workflow efficiency. However, existing methods lack interpretability and impede patient-clinician understanding, while their static nature restricts radiologists from dynamically adjusting assessments during image review. Inspired by interactive segmentation techniques, we propose a novel interactive framework for 3D lesion morphology reporting that seamlessly generates segmentation masks with comprehensive attribute descriptions, enabling clinicians to generate detailed lesion profiles for enhanced diagnostic assessment. To our best knowledge, we are the first to in tegrate the interactive segmentation and structured reports in 3D CT medical images. Experimental results across 15 lesion types demonstrate the effectiveness of our approach in providing a more comprehensive and reliable reporting system for lesion segmentation and capturing. The source code is publicly available at https://github.com/yanniangu/ISRG-CT-MICCAI2025.", "filename": "2025_0731.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Yannian Gu", "Wenhui Lei", "Hanyu Chen", "Shaoting Zhang", "Xiaofan Zhang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0732", "x": 3.481, "y": 4.749, "title": "Learning Disease State from Noisy Ordinal Disease Progression Labels", "abstract": "Learning from noisy ordinal labels is a key challenge in medical imaging. In this work, we ask whether ordinal disease progression labels (better, worse, or stable) can be used to learn a representation allowing to classify disease state. For neovascular age-related macular degeneration (nAMD), we cast the problem of modeling disease progression between medical visits as a classification task with ordinal ranks. To enhance generalization, we tailor our model to the problem setting by (1) independent image encoding, (2) antisymmetric logit space equivariance, and (3) ordinal scale a wareness. In addition, we address label noise by learning an uncertainty estimate for loss re-weighting. Our approach learns an interpretable disease representation enabling strong fewshot performance for the related task of nAMD activity classification from single images, despite being trained only on image pairs with ordinal disease progression labels (https://github.com/berenslab/Learning- Disease-State.", "filename": "2025_0732.pdf", "year": 2025, "institution": "University of Tübingen", "country": "Germany", "authors": ["Gustav Schmidt", "Holger Heidrich", "Philipp Berens", "Sarah Müller"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0733", "x": 1.925, "y": 5.491, "title": "Learning Foundation Models from Multi-organ Medical Images by Capturing Consistency and Diversity of Anatomical Structures", "abstract": "Medical images span a wide range of imaging protocols and anatomical regions, exhibiting two fundamental properties: inter-organ diversity-where different organs exhibit distinct structural patterns (e.g., hand vs. chest)-and intra-organ consistency-where each organ retains a coherent structure with subtle variations across patient (e.g., left vs. right hand). While existing foundation models typically focus on a single organ or combine organs across heterogeneous modalities-often failing to jointly capture both properties-we envision that a model purposefully built on these fundamental properties would yield representations with greater generalizability, robustness, and interpretability. To this end, we introduce a general-purpose and scalable framework for learning foundation models from diverse organs within a given imaging modality. We call our framework Coda, as it is explicitly designed to jointly capture both the consistency and diversity of anatomical structures, encoding high-level semantic relationships across distinct organs and fine-grained anatomical details within each organ. Our experiments in zero-shot, fewshot transfer, and full-transfer settings show that Coda, pretrained on 23 diverse organs, learns semantically rich representations that not only yield strong inter-organ and intra-organ discrimination capabilities but also offer superior generalizability and robustness on diverse tasks.", "filename": "2025_0733.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Mohammad Reza Hosseinzadeh Taher", "Junpyo Hong", "Ravi Soni", "Gopal Avinash"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0734", "x": 0.592, "y": 6.244, "title": "Learning Segmentation from Radiology Reports", "abstract": "Tumor segmentation in CT scans is key for diagnosis, surgery, and prognosis, yet segmentation masks are scarce because their creation requires time and expertise. Public abdominal CT datasets have from dozens to a couple thousand tumor masks, but hospitals have hundreds of thousands of tumor CTs with radiology reports. Thus, leveraging reports to improve segmentation is key for scaling. In this paper, we propose a report-supervision loss (R-Super) that converts radiology reports into voxel-wise supervision for tumor segmentation AI. We created a dataset with 6,718 CT-Report pairs (from the UCSF Hospital), and merged it with public CT-Mask datasets (from AbdomenAtlas 2.0). We used our R-Super to train with these masks and reports, and strongly improved tumor segmentation in internal and external validation-F1 Score increased by up to 16% with respect to training with masks only. By leveraging readily available radiology reports to supplement scarce segmentation masks, R-Super strongly improves AI performance both when very few training masks are available (e.g., 50), and when many masks were available (e.g., 1.7K", "filename": "2025_0734.pdf", "year": 2025, "institution": "John Hopkins University", "country": "USA", "authors": ["Pedro R A S Bassi", "Wenxuan Li", "Jieneng Chen", "Zheren Zhu", "Tianyu Lin", "Sergio Decherchi", "Andrea Cavalli", "Kang Wang", "Yang Yang", "Alan L Yuille", "Zongwei Zhou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0735", "x": 0.874, "y": 6.317, "title": "LEAVS: An LLM-Based Labeler for Abdominal CT Supervision", "abstract": "Extracting structured labels from radiology reports has been employed to create vision models that detect several types of abnormalities simultaneously. However, existing works focus mainly on the chest region. Few works have investigated abdominal radiology reports due to the more complex anatomy and a wider range of pathologies in the abdomen. We propose LEAVS (Large language model Extractor for Abdominal Vision Supervision). This labeler can annotate the certainty of presence and the urgency of seven types of abnormalities for nine abdominal organs on CT radiology reports. To ensure broad coverage, we chose abnormalities that encompass most of the finding types from CT reports. Our approach employs a specialized chain-ofthought prompting strategy for a locally run LLM using sentence extraction and multiple-choice questions in a tree-based decision system. We demonstrate that the LLM can extract several abnormality types across abdominal organs w ith an average F1 score of 0.89, significantly outperforming competing labelers and humans. Additionally, we show that the extraction of urgency labels achieves performance comparable to that of human annotations. Finally, we demonstrate that the abnormality labels contain valuable information for training a vision model that classifies several organs as normal or abnormal. We release our code and structured annotations for a publicly available dataset containing over 1,000 CT volumes.", "filename": "2025_0735.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Ricardo Bigolin Lanfredi", "Yan Zhuang", "Mark Finkelstein", "Praveen Thoppey Srinivasan Balamuralikrishn", "Luke Krembs", "Brandon Khoury", "Arthi Reddy", "Pritam Mukherjee", "Neil M Rofsky", "Ronald M Summers"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0736", "x": 2.703, "y": 4.011, "title": "LesionDiffusion: Towards Text-Controlled General Lesion Synthesis", "abstract": "Fully-supervised lesion recognition methods in medical imaging face challenges due to the reliance on large annotated datasets, which are expensive and difficult to collect. To address this, synthetic lesion generation has become a promising approach. However, existing models struggle with scalability, fine-grained control over lesion attributes, and the generation of complex structures. We propose LesionDiffusion, a text-controllable lesion synthesis framework for 3D CT imaging that generates both lesions and corresponding masks. By utilizing a structured lesion report template, our model provides greater control over lesion attributes and supports a wider variety of lesion types. We introduce a dataset of 1,505 annotated CT scans with paired lesion masks and structured reports, covering 14 lesion types across 8 organs. Lesion-Diffusion consists of two components: a lesion mask synthesis network (LMNet) and a lesion inpainting network (LINet), both guided by lesion attributes and image features. Extensive experiments demonstrate that LesionDiffusion significantly improves segmentation performance, with strong generalization to unseen lesion types and organs, outperforming current state-of-the-art models. Code is available at here.", "filename": "2025_0736.pdf", "year": 2025, "institution": "Shanghai Jiaotong University", "country": "China", "authors": ["Wenhui Lei", "Hengrui Tian", "Linrui Dai", "Hanyu Chen", "Xiaofan Zhang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0737", "x": 2.445, "y": 4.51, "title": "Leveraging Diffusion Models for Continual Test-Time Adaptation in Fundus Image Classification", "abstract": "Continual Test-Time Adaptation (CTA) aims to improve model generalization under distribution shifts by adapting to incoming test data. However, conventional CTA methods, such as pseudolabel refinement and entropy minimization, face challenges in fundus image classification due to the limited number of training samples and class categories, which lead to overconfident yet miscalibrated predictions, making traditional adaptation methods ineffective. To address these issues, we propose a novel diffusion-based CTA framework, Dif-fCTA, which leverages the generative capabilities of diffusion models to refine test samples and align them with the source domain distribution without modifying the source model. DiffCTA enhances test-time adaptation using diffusion guidance while preserving diagnostic features. Specifically, we integrate content guidance to retain anatomical structures, consistency guidance to stabilize predictions via en tropy minimization, style guidance for CLIP-based domain alignment, and a sampling optimization module that dynamically adjusts guidance strength across diffusion timesteps. We conducted experiments on glaucoma classification and diabetic retinopathy grading tasks. In the glaucoma classification task, our method outperformed the best existing approach by 2.6%, demonstrating its effectiveness in handling domain shifts without modifying the source model. The code is available at: https://github.com/ mingsiliu557/DiffCTA.", "filename": "2025_0737.pdf", "year": 2025, "institution": "University o f Electronic Science and Technology of China", "country": "China", "authors": ["Mingsi Liu", "Xiang Li", "Mengxiang Guo", "Lixin Duan", "Huihui Fang", "Yanwu Xu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0738", "x": 1.013, "y": 5.843, "title": "Location-Guided Automated Lesion Captioning in Whole-Body PET/CT Images", "abstract": "Whole-body PET/CT imaging provides detailed metabolic and anatomical information, which is critical for accurate cancer staging, treatment evaluation, and radiotherapy planning. Automated lesion captioning for whole-body PET/CT is essential for reducing radiologists' workload and assisting personalized treatment decisions. Unlike previous works that focus on captioning body-part images, we propose a novel automated lesion captioning framework for whole-body PET/CT images, which usually have large volume and high anatomical variability. Our framework first leverages CLIP for lesion localization, upon which we introduce two location-guided strategies: Confidence-Guided Location Prompts (CGLP), which select top-1 or top-3 anatomical location prompts based on confidence scores to guide captioning, and Dynamic Window Setting (DWS), which applies appropriate intensity windowing to enhance visual representation of the localized regions. To our kno wledge, our work is the first to achieve whole-body PET/CT lesion captioning. Experimental results on a large dataset comprising 1867 subjects from Siemens, GE, and United Imaging show that our method not only yields higher BLEU scores compared to state-of-the-art methods, but also produces consistent improvements across multiple scanner makers. This advancement has the potential to streamline radiology reporting and enhance clinical decision-making using whole-body PET/CT images.", "filename": "2025_0738.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Mingyang Yu", "Yaozong Gao", "Yiran Shu", "Yanbo Chen", "Jingyu Liu", "Caiwen Jiang", "Kaicong Sun", "Zhiming Cui", "Weifang Zhang", "Yiqiang Zhan", "Xiang Sean Zhou", "Shaonan Zhong", "Xinlu Wang", "Meixin Zhao", "Dinggang Shen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0739", "x": 1.138, "y": 6.221, "title": "MAARTA:Multi-agentic Adaptive Radiology Teaching Assistant", "abstract": "Radiology students often struggle to develop perceptual expertise due to limited time for expert mentorship, leading to errors in visual search patterns and diagnostic interpretation. These perceptual errors-such as missed fixations, brief dwell times, or misinterpretations-are not adequately addressed by existing AI systems, which focus on diagnostic accuracy but fail to explain how and why errors occur. To bridge this gap, we propose MAARTA (Multi-Agentic Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes gaze patterns and radiology reports to provide personalized feedback. Unlike single-agent models, MAARTA dynamically recruits agents based on error complexity, ensuring adaptive a nd efficient reasoning. By leveraging thought graphs to compare expert and student gaze behavior, the system identifies missed findings and assigns Perceptual Error Teacher (PET) agents to analyze discrepancies. Using Chainof-Thought (CoT) prompting, MAARTA generates meaningful insights, helping students understand their errors and refine their diagnostic reasoning, ultimately enhancing AI-driven radiology education.", "filename": "2025_0739.pdf", "year": 2025, "institution": "University of Houston", "country": "USA", "authors": ["Akash Awasthi", "Brandon V Chung", "Anh M Vu", "Ngan Le", "Rishi Agrawal", "Zhigang Deng", "Carol Wu", "Hien V Nguyen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0740", "x": 1.725, "y": 6.717, "title": "MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-Shot Dermatological Assessment", "abstract": "Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced subtexts through large language models, (2) a fine-grained alignment mechanism that connects subtexts with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different subtexts based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from the internet, MAKE significantly outperforms state-of-the-art VLP models on seven datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code is available at https://github.com/SiyuanYan1/MAKE.", "filename": "2025_0740.pdf", "year": 2025, "institution": "Monash University", "country": "Australia", "authors": ["Siyuan Yan", "Xieji Li", "Ming Hu", "Yiwen Jiang", "Zhen Yu", "Zongyuan Ge"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0741", "x": 0.751, "y": 6.803, "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation", "abstract": "Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.", "filename": "2025_0741.pdf", "year": 2025, "institution": "Huazhong University of Science and T echnology", "country": "China", "authors": ["Qilong Xing", "Zikai Song", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0742", "x": 1.365, "y": 6.729, "title": "MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization", "abstract": "Medical Image Grounding (MIG), which involves localizing specific regions in medical images based on textual descriptions, requires models to not only perceive regions but also deduce spatial relationships of these regions. Existing Vision-Language Models (VLMs) for MIG often rely on Supervised Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning annotations, which are expensive and time-consuming to acquire. Recently, DeepSeek-R1 demonstrated that Large Language Models (LLMs) can acquire reasoning abilities through Group Relative Policy Optimization (GRPO) without requiring CoT annotations. In this paper, we adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding. We propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations. Specifically, we introduce Spatial-Semantic Rewards, which combine spatial accuracy reward and semantic consistency reward to provide nuanced feedback for both spatially positive and negative completions. Additionally, w e propose to use the Chain-of-Box template, which integrates visual information of referring bounding boxes into the <think> reasoning process, enabling the model to explicitly reason about spatial regions during intermediate steps. Experiments on three datasets MS-CXR, ChestX-ray8, and M3D-RefSeg-demonstrate that our method achieves stateof-the-art performance in Medical Image Grounding. Ablation studies further validate the effectiveness of each component in our approach. Code, checkpoints, and datasets are available at https://github.com/ bio-mlhui/MedGround-R1. H. Xu and Y. N ie-Equal contribution.", "filename": "2025_0742.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Huihui Xu", "Yuanpeng Nie", "Hualiang Wang", "Ying Chen", "Wei Li", "Junzhi Ning", "Lihao Liu", "Hongqiu Wang", "Lei Zhu", "Jiyao Liu", "Xiaomeng Li", "Junjun He"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0743", "x": 1.446, "y": 6.931, "title": "Medical Large Vision Language Models with Multi-image Visual Ability", "abstract": "Medical large vision-language models (LVLMs) have demonstrated promising performance across various single-image question answering (QA) benchmarks, yet their capability in processing multiimage clinical scenarios remains underexplored. Unlike single image based tasks, medical tasks involving multiple images often demand sophisticated visual understanding capabilities, such as temporal reasoning and cross-modal analysis, which are poorly supported by current medical LVLMs. To bridge this critical gap, we present the Med-MIM instruction dataset, comprising 83.2K medical multi-image QA pairs that span four types of multi-image visual abilities (temporal understanding, reasoning, comparison, co-reference). Using this dataset, we fine-tune Mantis and LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and Med-Mantis, both optimized for multiimage analysis. Additionally, we develop the Med-MIM benchmark to comprehensively evaluate the medical multi-image understanding capabilities of LVLMs. W e assess eight popular LVLMs, including our two models, on the Med-MIM benchmark. Experimental results show that both Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM instruction dataset effectively enhances LVLMs' multi-image understanding capabilities in the medical domain. The Med-MIM instruction dataset, benchmark, and fine-tuned models are available at Med-MIM.", "filename": "2025_0743.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Xikai Yang", "Juzheng Miao", "Yuchen Yuan", "Jiaze Wang", "Qi Dou", "Jinpeng Li", "Pheng-Ann Heng"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0744", "x": 2.306, "y": 5.004, "title": "MedPro-DG: Domain-Aware Masked Contrastive Prompt Learning of Institution Generalization for Outcome Prediction", "abstract": "Accurate outcome prediction for head and neck cancer is critical but remains challenging due to domain shifts across multi-institutional imaging datasets. Existing domain generalization (DG) methods focus on visual features while overlooking clinical domain-invariant information. To address this gap, we propose MedPro-DG, a novel prompt learning framework that integrates CT imaging with clinical variables using domain-aware masked contrastive prompt learning. Our method can effectively mitigate domain shifts by aligning cross-modal features with domain-invariant clinical semantics. Extensive experiments conducted across six medical centers demonstrate the superiority of MedPro-DG, which outperforms state-of-the-art DG methods b y 1.35% in AUC and 4.06% in ACC on average. Ablation studies further reveal that our prompt learning can capture clinically domain-invariant features, highlighting their diagnostic relevance. This work pioneers domain-invariant vision-language fusion for medical domain generalization, providing an available and effective solution for multi-center collaborative diagnosis.", "filename": "2025_0744.pdf", "year": 2025, "institution": "Xidian University", "country": "China", "authors": ["Rongfang Wang", "Jiasheng Chen", "Xinlong Zhang", "Jing Wang", "Hui Liu", "Zhiguo Zhou", "Kai Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0745", "x": 3.586, "y": 6.636, "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification", "abstract": "Recent advances in histopathology vision-language foundation models (VLFMs) have shown promise in addressing data scarcity for whole slide image (WSI) classification via zero-shot adaptation. However, these methods remain outperformed by conventional multiple instance learning (MIL) approaches trained on large datasets, motivating recent efforts to enhance VLFM-based WSI classification through few-shot learning paradigms. While existing few-shot methods improve diagnostic accuracy with limited annotations, their reliance on conventional classifier designs introduces critical vulnerabilities to data scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC) comprising two core components: (1) a meta-learner that automatically optimizes a classifier configuration from a mixture of candidate classifiers and (2) a classifier bank housing diverse candidate classifiers to enable a holistic pathological interpretation. Extensive experiments demonstrate that MOC outperforms prior arts in multiple few-shot benchmarks. Notably, on the TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions, offering a critical advancement for clinical deployments where diagnostic training data is severely limited. Code is available at https://github.com/xmed-lab/MOC.", "filename": "2025_0745.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Tianqi Xiang", "Yi Li", "Qixiang Zhang", "Xiaomeng Li"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0746", "x": 2.037, "y": 1.91, "title": "Mono-Modalizing Extremely Heterogeneous Multi-modal Medical Image Registration", "abstract": "In clinical practice, imaging modalities with functional characteristics, such as positron emission tomography (PET) and fractional anisotropy (FA), are often aligned with a structural reference (e.g., MRI, CT) for accurate interpretation or group analysis, necessitating multimodal deformable image registration (DIR). However, due to the extreme heterogeneity of these modalities compared to standard structural scans, conventional unsupervised DIR methods struggle to learn reliable spatial mappings and often distort images. We find that the similarity metrics guiding these models fail to capture alignment between highly disparate modalities. To address this, we propose M2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal DIR models using only mono-modal similarity while preserving the established architectural paradigm for seamless integration into existing models. We also introduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training scheme to promote diffeomorphism. Furthermore, our framework naturally extends to a semi-supervised setting, integrating pre-aligned and unaligned pairs only, without requiring ground-truth transformations or segmentation masks. Experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset demonstrate that M2M-Reg achieves up to 2× higher DSC than prior methods for PET-MRI and FA-MRI registration, highlighting its effectiveness in handling highly heterogeneous multi-modal DIR. Our code is available at https://github.com/ MICV-yonsei/M2M-Reg.", "filename": "2025_0746.pdf", "year": 2025, "institution": "Yonsei University", "country": "Republic of Korea", "authors": ["Kyobin Choo", "Hyunkyung Han", "Jinyeong Kim", "Chanyong Yoon", "Seong Jae Hwang"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0747", "x": 1.986, "y": 2.438, "title": "Multi-Tube-Voltage vBMD Measurement via Dual-Branch Frequency Balancing and Asymmetric Channel Attention", "abstract": "Phantom-less volumetric bone mineral density (vBMD) measurement using computed tomography (CT) presents a cost-effective alternative to conventional phantom-based approaches, yet faces accuracy challenges across varying tube voltages. Current deep learningbased phantom-less solutions frequently overlook the critical role of frequency variance-a crucial factor for precise BMD measurement and crossvoltage generalization. We present a lightweight CT-based phantomfree vBMD measurement framework that addresses critical limitations in cross-voltage generalization. Core innovations include: (1) Frequencybalancing feature modulation with multi-band fusion, preserving spectral measurement cues; (2) A dual-branch architecture combining domainspecific convolutions with cross-frequency interaction; and (3) Asymmetric channel attention, which allocates attention weights based on frequency characteristics, enabling adaptive emphasis on critical low-and high-frequency components. Comprehensiv e evaluations across 80, 100, and 120 kVp tube voltages demonstrate the proposed method's superior measurement accuracy and reliability, achieving overall mean absolute errors of 5.990 mg/cm 3 and 7.175 mg/cm 3 on internal (1,614 images) and external (2,245 images) testing sets from two centers, respectively. These results suggest that our method offers a promising solution for clinical PL vBMD measurement across varying CT protocols.", "filename": "2025_0747.pdf", "year": 2025, "institution": "United Imaging Intelligence (Beijing) Co., Ltd", "country": "China", "authors": ["Mengze Zhang", "Yali Li", "Huishu Yuan", "Zhen Qian"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0748", "x": 1.056, "y": 6.747, "title": "MVP-LLMs: Optimizing Intervention Timing and Subsequent Decision Support for Mechanical Ventilation Parameter Control Using Large Language Models", "abstract": "Since the COVID-19 outbreak, global health systems have faced unprecedented challenges, with mechanical ventilation playing a critical role in supporting patients in ICUs. However, precise adjustment of ventilation parameters remains complex, requiring continuous monitoring and personalized interventions by clinicians. This paper introduces a novel formulation of ventilator parameter adjustment as a composite problem involving optimal stopping and subsequent decision optimization, supported by a domain-specific dataset reflecting real-world scenarios. We propose a framework utilizing Large Language Models (LLMs) to enhance interactivity and interpretability, leveraging their extensive clinical knowledge from large text corpora for informed decision-making. The framework addresses two key tasks: developing scheduled prompts for optimal stopping to replicate clinical observation processes and implementing Best Action Imitation Learning for robust ventilator parameter optimization. Experimental results show significant improvements in LLMs' ability to predict optimal stopping points and optimize decisionmaking, advancing clinical ventilator control. To our knowledge, this is the first application of LLMs to this dual-task paradigm.", "filename": "2025_0748.pdf", "year": 2025, "institution": "Shanghai University of Engineering Science", "country": "China", "authors": ["Teqi Hao", "Xiaoyu Tan", "Bin Li", "Xuemin Wang", "Chao Qu", "Yinghui Xu", "Xihe Qiu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0749", "x": 2.077, "y": 4.32, "title": "Neighborhood-Consistent Binary Transformation for Domain-Invariant Chest X-Ray Diagnosis", "abstract": "Chest X-ray diagnosis models face domain generalization challenges due to cross-institutional variations in imaging protocols and scanner specifications, which degrade diagnostic accuracy on unseen domains. To address this, we propose a domain-invariant learning framework leveraging the inherent anatomical consistency of medical imaging. Our method first applies a Neighborhood-Consistent Binarization Transformation (NCBT) to convert grayscale images into topologypreserving high-dimensional binary tensors, encoding pixel intensity relationships within local neighborhoods to strip device-specific textures while retaining anatomical structures. These tensors are then reconstructed into an intermediate domain via an Intermediate Domain Style-preserving Autoencoder (IDSP-AE), decoupling structural information from domain-specific features. Crucially, our framework aligns domains without requiring target domain data during training, leveraging anatomical consistency. Experiments on four public datasets show superior generalization and improved diagnostic accuracy compared to state-of-the-art methods. The source code is available at https://github.com/LZL501/NCBT.", "filename": "2025_0749.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Zelong Liu", "Huachao Zhu", "Zhichao Sun", "Yuda Zou", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0750", "x": -0.408, "y": 4.706, "title": "Noise-Robust Tuning of SAM for Domain Generalized Ultrasound Image Segmentation", "abstract": "The Segment Anything Model (SAM) has achieved outstanding performance in both natural and medical image segmentation with extensive research validation. When applied to ultrasound images, which involve low contrast, indistinct boundaries and complex shapes, large models still suffer from significant performance degradation and limited generalization ability. We explore these challenges from a new perspective with the help of the segmentation foundation model SAM. In this paper, we propose Nora, a noise-robust fine-tuning framework for SAM to address domain generalized ultrasound image segmentation. Specifically, we introduce a feature-adaptive perturbation module, which applies welldesigned noise to the fine-tuned features. We stimulate the model to segment the correct regions even under severe interference, thereby improving its robustness. Moreover, to further optimize SAM with prompts, we present an instance-aw are prompt generation module. We introduce a set of tokens linked to distinct instances and then design a token-based augmentation strategy to prevent overcoupling and encourage tokens to capture more diverse information. Our Nora achieves state-of-the-art performance across extensive cross-domain experiments with three ultrasound image segmentation tasks, fully demonstrating its effectiveness and generalizability. The code is available at https://github.com/wkklavis/Nora.", "filename": "2025_0750.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Zhikai Wei", "Chao Wu", "Hanyu Du", "Rui Yu", "Bo Du", "Yongchao Xu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0751", "x": 1.522, "y": 6.331, "title": "PedCLIP: A Vision-Language Model for Pediatric X-Rays with Mixture of Body Part Experts", "abstract": "Vision-language models have demonstrated remarkable success in general medical image analysis, yet their application in pediatric imaging remains significantly underexplored. These models show limited performance on pediatric datasets, primarily due to domain gaps stemming from anatomical differences, lower radiation doses, and pediatric-specific diseases. To this end, we present the first pediatric vision-language pre-training framework, dubbed PedCLIP, trained on a comprehensive pediatric imaging dataset comprising 404,670 X-rays of pediatric patients across diverse anatomical regions. To address anatomical diversity, we introduce a Mixture of Body part Experts design, with each expert sp ecializing in learning features from distinct anatomical regions. Experimental evaluation across eleven downstream tasks demonstrates that our model significantly outperforms current stateof-the-art vision-language models, achieving superior diagnostic accuracy in challenging pediatric conditions, including rare diseases such as pediatric inflammatory arthritis. Code is available: https://github.com/ tadeephuy/PedCLIP", "filename": "2025_0751.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Ta Duc Huy", "Abin Shoby", "Sen Tran", "Yutong Xie", "Qi Chen", "Phi Le Nguyen", "Akshay Gole", "Lingqiao Liu", "Antonios Perperidis", "Mark Friswell", "Rebecca Linke", "Andrea Glynn", "Minh-Son To", "Anton Van den Hengel", "Johan Verjans", "Zhibin Liao", "Minh Hieu Phan"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0752", "x": 1.552, "y": 2.144, "title": "PromptReg: Universal Medical Image Registration via Task Prompt Learning and Domain Knowledge Transfer", "abstract": "Most existing deep learning-based registration methods are typically constrained to dataset-specific optimization, requiring separate models for different data characteristics. In contrast, training a single model across diverse datasets presents an opportunity to create a universal registration framework capable of handling multiple domains simultaneously. However, key challenges remain in achieving effective crossdataset adaptation while maintaining robust generalization capabilities, particularly for zero-shot registration tasks. In this work, we propose PromptReg, a universal image registration framework that incorporates prompt learning to guide the model in effectively adapting to different registration scenarios through explicit task prompts. The core of PromptReg is a Registration Prompt Generator (RPG) that generates domain-specific task prompts based on the domains of input images. Specifically, we first introduce a Static Knowledge Base (SKB) to store domain prompts and a dynamic prompt generation mechanism that projects different inputs into a shared prompt space. Then, we propose an adaptive prompt fusion strategy that combines stored domain knowledge based on the similarity between the generated dynamic prompt and the prompts in SKB, creating transferable knowledge for unseen domains. Finally, we optimize the prompt generator using domain orthogonality and task similarity losses. Our experiments show that PromptReg achieves competitive performance in universal registration and offers stronger zero-shot generalization. The code is available at https://github. com/xiehousheng/PromptReg.", "filename": "2025_0752.pdf", "year": 2025, "institution": "Shanghai Jiao Tong Universit y", "country": "China", "authors": ["Housheng Xie", "Xiaoru Gao", "Guoyan Zheng"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0753", "x": 0.749, "y": 6.564, "title": "RadIR: A Scalable Framework for Multi-grained Medical Image Retrieval via Radiology Report Mining", "abstract": "Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of 'similar images' across different medical contexts. This challenge is compounded by the lack of largescale, high-quality medical imaging retrieval datasets and benchmarks. In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner. Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR and RadIR-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks. These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described in text, achieving state-ofthe-art results on 77 out of 78 metrics.", "filename": "2025_0753.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Tengfei Zhang", "Ziheng Zhao", "Chaoyi Wu", "Xiao Zhou", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0754", "x": 3.343, "y": 6.057, "title": "Refining Cervical Cell Classification with Cytological Knowledge and Optimal Attribute Descriptor Matching", "abstract": "Cervical cancer remains a significant global health concern, emphasizing the need for effective diagnostic methods. Despite advancements in Vision Language Models, challenges persist in incorporating cytological knowledge, ensuring data relevance, and maintaining accuracy when aggregating visual information. Current methods often struggle to handle fine-grained morphological details and the complex relationships between images and textual knowledge. In this paper, we present a novel framework for cervical cell classification that combines attribute descriptors with cytological knowledge for enhanced morphology recognition. Our approach leverages the Vision Large Language Model to generate descriptions for each cervical image and pretrain image and text encoders, improving both image understanding and cytological context. We introduce Attribute Descriptors Extraction using LLMs and Retriev al-Augmented Generation to generate detailed descriptors that capture important cytological features while minimizing irrelevant information. Additionally, we propose Optimal Attribute Descriptors Matching to dynamically align textual descriptors with image features, enhancing prediction accuracy, interpretability, and cytological relevance. Experimental results demonstrate the superior performance and generalizability of our method with varying amounts of labeled data. The code is publicly available at https://github.com/feimanman/ CervicalCellClassifier", "filename": "2025_0754.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Manman Fei", "Zhenrong Shen", "Mengjun Liu", "Zhiyun Song", "Yusong Sun", "Xu Han", "Zelin Liu", "Haotian Jiang", "Lu Bai", "Qian Wang", "Lichi Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0755", "x": 2.502, "y": 7.513, "title": "Restyled, Tuning, and Alignment: Taming VLMs for Federated Non-IID Medical Image Analysis", "abstract": "Adapting pretrained Vision Language Models like CLIP, for medical image analysis in federated learning (FL) offers cross-modal insights while preserving privacy. However, effective cross-domain federated adaptation requires intensive fine-tuning and knowledge sharing, challenging in low-resource medical practice due to the divergence between pretrained natural image and medical imagery. Moreover, the significant statistical heterogeneity (non-IID) of medical data exacerbates these challenges. To address these issues, this paper introduces a framework that tames CLIP for non-IID federated medical image classification. This develops client-specific personalized models by reinforcement and constrain local cross-mo dal alignment, enabling the models to integrate client-specific and globally common knowledge. This approach not only addresses non-IID challenges but also optimizes the trade-off between performance and efficiency. Extensive experiments on real-world medical image datasets confirm the effectiveness and superiority of our FedTCA.", "filename": "2025_0755.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Shengchao Chen", "Ting Shu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0756", "x": 2.863, "y": 5.15, "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models", "abstract": "The rise of imaging techniques such as optical coherence tomography (OCT) and advances in deep learning (DL) have enabled clinicians and researchers to streamline the assessment of retinal diseases. A popular DL approach is self-supervised learning (SSL), where models learn from vast amounts of unlabeled data, avoiding costly annotation. SSL has allowed the development of foundation models (FMs), large models that can be used for a variety of downstream tasks. However, existing FMs for OCT, trained solely on image data, lack a comprehensive and robust semantic understanding of images, as evidenced by their downstream performance (especially for complex tasks), and thus require supervised fine-tuning (which may be unfeasible) to better adapt to specific applications and populations. To address this, we propose RetFiner, an SSL vision-language refinement scheme that improves the representations of existing FMs and enables their efficient and direct adaptation to specific populations for improved downstream performance. Our method uses a diverse set of training objectives which take advantage of the rich supervisory signal found in textual data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM, showing significant improvements in linear probing performance on seven highly diverse OCT classification tasks, with an average increase of 5.7, 3.9, and 2.1% points over their baselines, respectively.", "filename": "2025_0756.pdf", "year": 2025, "institution": "Medical University o f Vienna", "country": "Austria", "authors": ["Ronald Fecso", "José Morano", "Ursula Schmidt-Erfurth", "Hrvoje Bogunović"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0757", "x": 0.574, "y": 6.877, "title": "RRG-DPO: Direct Preference Optimization for Clinically Accurate Radiology Report Generation", "abstract": "Automated radiology report generation (RRG) of routine 2D and 3D radiographs, such as X-ray and computed tomography (CT), has great potential in reducing the workload, variations, and errors of report writing and facilitating patient care. Despite significant advancements in linguistic quality, existing methods may generate reports with hallucinated type I and II (false positive and false negative) errors, which limit clinical efficiency. To mitigate the hallucinations, we propose RRG-DPO, an innovative direct preference optimization procedure with a new loss term, both tailored for effective alignment with the preference for clinically accurate RRG. RRG-DPO retrieves a set of highly relevant reports closest to the preferred response (i.e., the ground truth (GT) report) in a biomedical CLIP embedding space, and selects the one with the most significant abnormality conflicts with the GT as the dispreferred response. Besides being clinically relevant and abnormally aware, this preference data curation process is cost-effective and scalable compared to using large language models for response sampling or evaluation. In addition, we note that except for the abnormality-conflicting sentences, other sentences of the dispreferred report can legibly describe the radiograph of the preferred in a clinically equivalent manner, despite variations in expression. Thus, RRG-DPO creates a sub-preferred report from the dispreferred by deleting the abnormality-conflicting sentences, and promotes its likelihood with a new loss term. RRG-DPO is evaluated on both 2D X-ray and 3D CT data to align a wide range of RRG", "filename": "2025_0757.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Hong Liu", "Dong Wei", "Zhe Xu", "Xian Wu", "Yefeng Zheng", "Liansheng Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0758", "x": 0.808, "y": 6.542, "title": "SimCroP: Radiograph Representation Learning with Similarity-Driven Cross-Granularity Pre-training", "abstract": "Medical vision-language pre-training shows great potential in learning representative features from massive paired radiographs and reports. However, in computed tomography (CT) scans, the distribution of lesions which contain intricate structures is characterized by spatial sparsity. Besides, the complex and implicit relationships between different pathological descriptions in each sentence of the report and their corresponding sub-regions in radiographs pose additional challenges. In this paper, we propose a Similarity-Driven Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines similarity-driven alignment and cross-granularity fusion to improve radiograph interpretation. We first leverage multi-modal masked modeling to optimize the encoder for understanding precise low-level semantics from radiographs. Then, similarity-driven alignment is designed to pre-train the encoder to adaptively select and align the correct patches corresponding to each sentence in reports. The cross-granularity fusion module integrates multimodal information across instance level and word-patch level, which helps the model better capture key pathology structures in sparse radiographs, resulting in improved performance for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale paired CT-reports dataset and validated on image classification and segmentation tasks across five public datasets. Exp erimental results demonstrate that SimCroP outperforms both cutting-edge medical self-supervised learning methods and medical vision-language pre-training methods. Codes and models are available at https://github.com/ToniChopp/SimCroP. R. Wang and F. T ang-Equal con tribution.", "filename": "2025_0758.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "China", "authors": ["Rongsheng Wang", "Fenghe Tang", "Qingsong Yao", "Rui Yan", "Xu Zhang", "Zhen Huang", "Haoran Lai", "Zhiyang He", "Xiaodong Tao", "Zihang Jiang", "S Kevin Zhou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0759", "x": 1.823, "y": 4.774, "title": "Source-Free Domain Adaptation for Cross-Modality Cardiac Image Segmentation with Contrastive Class Relationship Consistency", "abstract": "This paper investigates source-free domain adaptation for cross-modality cardiac image segmentation. Source-free domain adaptation (SFDA) leverages a pretrained model from source domain knowledge and adapts it using target domain data to predict target image labels. While existing SFDA methods have demonstrated strong performance in various medical segmentation tasks, cross-modality cardiac segmentation remains challenging due to significant domain discrepancies between MRI and CT modalities, hindering effective knowledge transfer. Current SFDA approaches primarily focus on pseudo-label denoising through image-level and feature-level alignment, often overlooking classlevel information derived from classifier outputs. This paper proposes a novel framework that constructs two class relationship matrices using predictions from a teacher-student model. These matrices are integrated into a con trastive learning framework through intra-view and inter-view pairs. The teacher-student architecture processes both original samples and their augmented counterparts, enforcing prediction consistency for robust adaptation. Simultaneously, our class-aware contrastive learning enhances discriminative capability for cardiac structures. Experimental results demonstrate that our method outperforms state-of-the-art approaches by significant margins, particularly on the challenging CT → MR adaptation task.", "filename": "2025_0759.pdf", "year": 2025, "institution": "Southwestern University of Finance and Economics", "country": "China", "authors": ["Ao Ma", "Qingpeng Zhu", "Jingjing Li", "Mads Nielsen", "Xu Chen"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0760", "x": 0.141, "y": 4.962, "title": "SPENet: Self-guided Prototype Enhancement Network for Few-Shot Medical Image Segmentation", "abstract": "Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intraclass variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies be tween the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing stateof-the-art methods, achieving superior performance.", "filename": "2025_0760.pdf", "year": 2025, "institution": "Beijing University of Technology", "country": "China", "authors": ["Chao Fan", "Xibin Jia", "Anqi Xiao", "Hongyuan Yu", "Zhenghan Yang", "Dawei Yang", "Hui Xu", "Yan Huang", "Liang Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0761", "x": -0.098, "y": 4.971, "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "abstract": "The advent of Large Vision Models (LVMs) offers new opportunities for few-shot medical image segmentation. However, existing training-free methods based on LVMs fail to effectively utilize negative prompts, leading to poor performance on medical images with lowcontrast. To address this issue, we propose SynPo, a training-free fewshot method based on LVMs (e.g., SAM), with the core insight: improving the quality of negative prompts. To select point prompts in a more reliable confidence map, we design a novel Confidence Map Synergy Module by combining the strengths of DINOv2 and SAM. Based on the confidence map, we select the top-k pixels as the positive points set and choo se the negative points set using a Gaussian distribution, followed by independent K-means clustering for both sets. Then, these selected points are leveraged as high-quality prompts for SAM to get the segmentation results. Extensive experiments demonstrate that SynPo achieves performance comparable to state-of-the-art training-based fewshot methods. Project page: https://liu-yufei.github.io/synpo-project- page/.", "filename": "2025_0761.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0762", "x": 0.549, "y": 4.974, "title": "Text-Driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation", "abstract": "Semi-supervised medical image segmentation plays a critical method in mitigating the high cost of data annotation. When labeled data is limited, textual information can provide additional context to enhance visual semantic understanding. However, research exploring the use of textual data to enhance visual semantic embeddings in 3D medical imaging tasks remains scarce. In this paper, we propose a novel text-driven multiplanar visual interaction framework for semi-supervised medical image segmentation (termed Text-SemiSeg), which consists of three main modules: Text-enhanced Multiplanar Representation (TMR), Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation (DCA). Specifically, TMR facilitates text-visual interaction through planar mapping, thereby enhancing the category awareness of visual features. CSA performs cross-modal semantic alignment be tween the text features with introduced learnable variables and the intermediate layer of visual features. DCA reduces the distribution discrepancy between labeled and unlabeled data through their interaction, thus improving the model's robustness. Finally, experiments on three public datasets demonstrate that our model effectively enhances visual features with textual information and outperforms other methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.", "filename": "2025_0762.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Kaiwen Huang", "Yi Zhou", "Huazhu Fu", "Yizhe Zhang", "Chen Gong", "Tao Zhou"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0763", "x": 3.646, "y": 5.45, "title": "Treat: A Unified Text-Guided Conditioned Deep Learning Model for Generalized Radiotherapy Treatment Planning", "abstract": "Deep learning has shown potential to enable automated personalized cancer treatment by automating radiotherapy treatment (RT) planning. However, generalizing RT planning across multiple protocols with deep learning remains a critical challenge due to the diversity of clinical requirements. This paper introduces Treat: a unified Text-guided Radiotherapy for dose prEdiction in Automated Treatment planning to address these complexities. By leveraging conditional text embeddings using the CLIP text-encoder, the model dynamically adapts to protocol-specific requirements, enabling the generation of high-quality per-protocol dose distributions. We propose an efficient text-conditioning method, graph prompts pooling (GPP), to effectively leverage multiple protocol-specific prompts, and dynamic batch weighting to balance the model training using multiple datasets. We validated Treat on five datasets-two early-stage prostate, left and right partial breast, and headand-neck-using clinically relevant metrics: mean absolute error (MAE) of homogeneity index (HI) and dose-volume histogram (DVH). Compared to the protocol-specific model with the MAE-HI of 0.274 and the MAE-DVH of 7.46, Treat achieves a superior performance of 0.062 and 2.87 for MAE-HI and MAE-DVH score, respectively. When compared to baseline one-hot conditioning with the MAE-HI of 0.085 and the MAE-DVH of 3.35, GPP demonstrates its efficiency in adapting prompt-based conditioning for predicting dose distributions for diverse protocols. The code is available: https://github.com/mcintoshML/TextGuided_RT", "filename": "2025_0763.pdf", "year": 2025, "institution": "University Health Network (UHN)", "country": "Canada", "authors": ["Sangwook Kim", "Yuan Gao", "Thomas G Purdie", "Chris Mcintosh"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0764", "x": 2.413, "y": 4.975, "title": "UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot CLIP Adaptation", "abstract": "Precise anomaly detection in medical images is critical for clinical decision-making. While recent unsupervised or semi-supervised anomaly detection methods trained on large-scale normal data show promising results, they lack fine-grained differentiation, such as benign vs. malignant tumors. Additionally, ultrasound (US) imaging is highly sensitive to devices and acquisition parameter variations, creating significant domain gaps in the resulting US images. To address these challenges, we propose UltraAD, a vision-language model (VLM)-based approach that leverages few-shot US examples for generalized anomaly localization and fine-grained classification. To enhance localization performance, the image-level token of query visual prototypes is first fused with learnable text embeddings. This image-informed prompt feature is then further integrated with patch-level tokens, refining local representations for improved accuracy. For fine-grained classification, a memory bank is constructed from few-shot image samples and corresponding text descriptions that capture anatomical and abnormality-specific features. During training, the stored text embeddings remain frozen, while image features are adapted to better align with medical data. UltraAD has been extensively evaluated on three breast US datasets, outperforming stateof-the-art methods in both lesion localization and fine-grained medical classification. Project page: https://karolinezhy.github.io/UltraAD/.", "filename": "2025_0764.pdf", "year": 2025, "institution": "TU Munich", "country": "Germany", "authors": ["Yue Zhou", "Yuan Bi", "Wenjuan Tong", "Wei Wang", "Nassir Navab", "Zhongliang Jiang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0765", "x": 0.926, "y": 6.747, "title": "UniMRG: Refining Medical Semantic Understanding Across Modalities via LLM-Orchestrated Synergistic Evolution", "abstract": "Current medical report generation (MRG) methods remain limited by cross-modal associations, particularly when handling complex medical terminology across different modalities. In this work, we propose the Universal Medical Report Generation (UniMRG) framework to enhance Vision-Language foundation models (VLFMs) through coordinated data augmentation and architecture optimization. Specifically, we introduce Universal Semantics-Synergistic Multimodal Augmentation to enhance model adaptability to diverse medical scenarios while preserving critical diagnostic features. We further design a Medical Content Learner to capture both fine-grained pathological variations and specialized diagnostic contexts for robust cross-modal alignment. To achieve robust medical understanding against real-world variations, we develop a Dynamic Synergistic Evolution strategy guided by Large Language Model (LLM) that enables joint optimization of augmentation policies and a rchitectural configurations. To address the existing gap in public VL datasets for skin diseases, we release a large-scale Skin-Path dataset, consisting of 277,761 patches covering 10 distinct skin diseases. Extensive experiments on PatchGastric22, IU-Xray, and Skin-Path demonstrate that UniMRG achieves state-of-the-art performance, surpassing Clinical-BERT by 2.6% in BLEU-4 and 3.9% in Rouge-L on IU-Xray.", "filename": "2025_0765.pdf", "year": 2025, "institution": "University of New South Wales", "country": "Australia", "authors": ["Hongyan Xu", "Arcot Sowmya", "Ian Katz", "Dadong Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0766", "x": 3.567, "y": 3.802, "title": "Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion", "abstract": "Accurate coronary artery segmentation is critical for computer-aided diagnosis of coronary artery disease (CAD), yet it remains challenging due to the small size, complex morphology, and low contrast with surrounding tissues. To address these challenges, we propose a novel segmentation framework that leverages the power of vision foundation models (VFMs) through a parallel encoding architecture. Specifically, a vision transformer (ViT) encoder within the VFM captures global structural features, enhanced by the activation of the final two ViT blocks and the integration of an attention-guided enhancement (AGE) module, while a convolutional neural network (CNN) encoder extracts local details. These complementary features are adaptively fused using a cross-branch variational fusion (CVF) module, which models latent distributions and applies variational attention to assign modalityspecific weights. Additionally, we introduce an evidential-learning uncertainty refinement (EUR) m odule, which quantifies uncertainty using evidence theory and refines uncertain regions by incorporating multiscale feature aggregation and attention mechanisms, further enhancing segmentation accuracy. Extensive evaluations on one in-house and two public datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods, achieving superior performance in accurate coronary artery segmentation and showcasing strong generalization across multiple datasets. The code is available at https://github.com/d1c2x3/CAseg.", "filename": "2025_0766.pdf", "year": 2025, "institution": "The Second Affiliated Hospital of Xi'an Jiaotong University", "country": "China", "authors": ["Caixia Dong", "Duwei Dai", "Xinyi Han", "Fan Liu", "Xu Yang", "Zongfang Li", "Songhua Xu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0767", "x": 1.521, "y": 7.127, "title": "V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis", "abstract": "Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.", "filename": "2025_0767.pdf", "year": 2025, "institution": "Zhejiang University-University of Illinois Urbana-Champaign Institute", "country": "China", "authors": ["Yuan Wang", "Jiaxiang Liu", "Shujian Gao", "Bin Feng", "Zhihang Tang", "Xiaotang Gai", "Jian Wu", "Zuozhu Liu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0768", "x": 1.622, "y": 6.911, "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical Visual Question Answering", "abstract": "Multimodal large language models (MLLMs) have demonstrated significant potential in medical Visual Question Answering (VQA). Yet, they remain prone to hallucinations-incorrect responses that contradict input images, posing substantial risks in clinical decisionmaking. Detecting these hallucinations is essential for establishing trust in MLLMs among clinicians and patients, thereby enabling their realworld adoption. Current hallucination detection methods, especially semantic entropy (SE), have demonstrated promising hallucination detection capacity for LLMs. However, adapting SE to medical MLLMs by incorporating visual perturbations presents a dilemma. Weak perturbations preserve image content and ensure clinical validity, but may be overlooked by medical MLLMs, which tend to over-rely on language priors. In contrast, strong perturbations can distort essential diagnostic features, compromising clinical interpretation. To address this issue, we propose Vision Amplified Semantic Entropy (VASE), which incorporates weak image transformations and amplifies the impact of v isual input, to improve hallucination detection in medical VQA. We first estimate the semantic predictive distribution under weak visual transformations to preserve clinical validity, and then amplify visual influence by contrasting this distribution with that derived from a distorted image. The entropy of the resulting distribution is estimated as VASE. Experiments on two medical open-ended VQA datasets demonstrate that VASE consistently outperforms existing hallucination detection methods. The code will be available at https://github.com/Merrical/VASE.", "filename": "2025_0768.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Zehui Liao", "Shishuai Hu", "Ke Zou", "Huazhu Fu", "Liangli Zhen", "Yong Xia"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0769", "x": 3.425, "y": 6.8, "title": "A cartoon robot holding a digital screen displaying abstract purple patterns and a triangular shape. The robot has a friendly appearance with glowing blue eyes and metallic features. The image combines elements of technology and art, emphasizing a futuristic theme. WSI-Agents: A Collaborative Multi-agent System for Multi-modal Whole Slide Image Analysis", "abstract": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multiagent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both taskspecific accuracy and multi-task versatility through three components:(1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs and medical agent frameworks across diverse tasks. Source code is available at https://github.com/CVI-SZU/WSI-Agents.", "filename": "2025_0769.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0770", "x": 1.52, "y": 6.854, "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "abstract": "Clinical decision-making relies heavily on understanding relative positions of anatomical structures and anomalies. Therefore, for Vision-Language Models (VLMs) to be applicable in clinical practice, the ability to accurately determine relative positions on medical images is a fundamental prerequisite. Despite its importance, this capability remains highly underexplored. To address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o, Llama3.2, Pixtral, and JanusPro, and find that all models fail at this fundamental task. Inspired by successful approaches in computer vision, we investigate whether visual prompts, such as alphanumeric or colored markers placed on anatomical structures, can enhance performance. While these markers provide moderate improvements, results remain significantly lower on medical images compared to observations made on natural images. Our evaluations suggest that, in medical imaging, VLMs rely more on prior anatomical knowledge than on actual image content for answering relative position questions, often leading to incorrect conclusions. To facilitate further research in this area, we introduce the MIRP -Medical Imaging Relative Positioning -benchmark dataset, designed to systematically evaluate the capability to identify relative positions in medical images. Dataset and code are available on https://wolfda95.github.io/your_other_left/.", "filename": "2025_0770.pdf", "year": 2025, "institution": "Ulm University", "country": "Germany", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex Bäuerle", "Meinrad Beer", "Michael Götz", "Timo Ropinski"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0771", "x": 1.688, "y": 4.794, "title": "Active Source-Free Cross-Domain and Cross-Modality Adaptation for Volumetric Medical Image Segmentation by Image Sensitivity and Organ Heterogeneity Sampling", "abstract": "Deep learning (DL) methods have achieved great success in medical image segmentation, but they are challenged to demonstrate robust performance across different datasets due to domain and modality gaps. The Source-Free Domain Adaptation techniques adapt DL models to generalize across domains without access to source data, and active learning is implemented to actively query informative target samples to fine-tune models, thus improving their generalization. However, only a few Active Source-Free Domain Adaptation methods have been proposed. Additionally, existing methods focus on same-modality adaptation and lack mechanisms to address modality gaps, thus limiting their applicability. To address these limitations, we propose a novel Active Source-Free Cross-Domain and Cross-Modality Adaptation method for medical image segmentation. This method adapts models across different domains and modalities by employing a novel Active Test Time Sample Query strategy to jointly implement Image Sensitivity Query (ISQ) and Organ Heterogeneity Query (OHQ). ISQ is designed to evaluate samples' imagelevel modality agnostic informativeness, thus querying informative samples from different domains and modalities. OHQ is prop osed to query samples with large foreground diversity by measuring the uncertaintyweighted organ boundary discontinuity and uncertainty-weighted organ interior abnormality, thus avoiding the influence of modality-specific background noise. A Dynamic Image-to-Organ Scaling mechanism is proposed to dynamically fuse the results of ISQ and OHQ for sample querying. We evaluated our method on cross-domain and cross-modality volumetric pancreas segmentation tasks. Our method outperformed other state-of-the-art methods on adaptation from a CT domain to another larger CT domain, T1-weighted MR and T2-weighted MR domains.", "filename": "2025_0771.pdf", "year": 2025, "institution": "Washington University School of Medicine in St. Louis", "country": "USA", "authors": ["Jin Yang", "Xiaobing Yu", "Peijie Qiu", "Daniel Marcus", "Aristeidis Sotiras"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0772", "x": 2.0, "y": 6.6, "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-Rays", "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive exp eriments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIPbased medical diagnostic models, particularly for CXR analysis.", "filename": "2025_0772.pdf", "year": 2025, "institution": "Texas A&M University", "country": "USA", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak J Mortazavi", "Tianbao Yang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0773", "x": 0.383, "y": 5.264, "title": "Bio2Vol: Adapting 2D Biomedical Foundation Models for Volumetric Medical Image Segmentation", "abstract": "2D biomedical foundation models (FM) have demonstrated remarkable capabilities in 2D medical image segmentation across various modalities, with text-prompted approaches offering scalable analysis that facilitate integration with LLMs and clinical application. Adapting these models for 3D medical image segmentation can leverage their rich visual features while enabling text-prompted volumetric image segmentation. However, efficient adaptation poses significant challenges due to the substantial disparity between 2D and 3D medical images and the necessity to establish text-volume alignment. To address these limitations, we propose Bio2Vol, a novel adaptation framework that enables text-prompted 2D biomedical FMs to effectively handle volumetric data. Specifically, (1) To bridge the dimensional disparity, we propose a Dual-Rate Sampling strategy (DRS) that processes inter slices within a volume at both sparse and dense intervals, capturing global contexts and local details; (2) To enhance volumetric feature representation, a Crossslice Dual-head Attention (CSDHA) is built upon the intra-slice features by repurposing existing pre-trained attention m odules for parameterefficient inter-slice information fusion; and (3) To establish text-volume understanding, a Semantic Text-Visual Alignment loss (SAT) is used to extend the existing 2D text-visual alignment to the volumetric domain. Using BiomedParse as a demonstration case, extensive evaluation across 11 medical datasets across diverse anatomical regions and modalities shows that Bio2Vol significantly improves 3D medical image segmentation performance, enhancing DSC by 4.72% on Amos22 dataset with substantial improvements across MSD tasks. Code will be available https:// github.com/JiaxinZhuang/Bio2Vol.", "filename": "2025_0773.pdf", "year": 2025, "institution": "Hong Kong University of Science and Technology", "country": "China", "authors": ["Jiaxin Zhuang", "Linshan Wu", "Xuefeng Ni", "Xi Wang", "Liansheng Wang", "Hao Chen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0774", "x": 3.604, "y": 5.485, "title": "Bridging Radiological Images and Factors with Vision-Language Model for Accurate Diagnosis of Proliferative Hepatocellular Carcinoma", "abstract": "The integration of multimodal data, particularly medical images and tabular data encompassing physician-assessed radiological factors, holds significant promise for enhancing clinical decision-making. However, effective fusion of these heterogeneous data modalities remains challenging due to their disparate feature spaces and the limitations of current independent encoding approaches. We introduce FM-Bridge, a novel methodology leveraging vision-language foundation model (VLM) to address this challenge. Our approach capitalizes on the intrinsic imagetext embedding space alignment within VLMs to achieve robust multimodal fusion. We propose transforming clinical expertise-rich tabular data into semantically coherent textual descriptions, subsequently utilizing the VLM's text encoder to generate textual features explicitly aligned with image features. This method facilitates a more semantically congruent and effective fusion of medical image and tabular data, demonstrating potential for improved performance in downstream medical image analysis tasks compared to conventional methods. Code is available at https:// github.com/HKU-MedAI/FM-Bridge.", "filename": "2025_0774.pdf", "year": 2025, "institution": "The University of Hong Kong", "country": "China", "authors": ["Yanyan Huang", "Wanli Zhang", "Peixiang Huang", "Yu Fu", "Ruimeng Yang", "Lequan Yu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0775", "x": 2.222, "y": 4.321, "title": "CAUDA-MI: Cross Attention-Guided Unsupervised Domain Adaptation with Mutual Information for Cardiac MRI Segmentation", "abstract": "Late Gadolinium Enhancement (LGE) imaging has emerged as the gold standard for cardiovascular disease diagnosis due to its ability to clearly delineate myocardial pathology. Professional interpretation of LGE images is usually difficult since their annotations are scarce, often necessitating the reliance on domain adaptation methods. Nevertheless, significant distribution discrepancy between datasets of different modalities usually results in poor transfer learning performances. To address this issue, we propose a general framework for cardiac MRI segmentation, called Cross Attention-Guided Unsupervised Domain Adaptation with Mutual Information (CAUDA-MI). This model leverages attention mechanisms on two data streams from the source and target domains, cleverly fusing the Query from the source domain with the Key and Value from the target domain, thereby aligning the implicit features of the target d omain towards the source domain in the latent space. Additionally, we introduce single-domain mutual information as a supplementary means to further enhance the accuracy of myocardial segmentation. The proposed CAUDA-MI is tested on the MS-CMRSeg 2019 and MyoPS 2020 datasets, which achieves an average Dice score of 0.847 and 0.797 respectively. Comprehensive experimental results demonstrate that our proposed method surpasses previous state-of-the-art algorithms.", "filename": "2025_0775.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Dianrong Du", "Hengfei Cui", "Jiatong Li", "Fan Zheng", "Yong Xia"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0776", "x": 2.42, "y": 4.984, "title": "Clinical Prior-Guided Tumor Generation for Breast Ultrasound with Cross Domain Adaptation", "abstract": "Computer-aided diagnosis (CAD) has become an essential solution for breast ultrasound (BUS) image analysis; however, the development of CAD systems is hindered by high-quality data scarcity and annotation challenges. We propose a novel clinical prior-guided tumor generation method that allows precise control over tumor characteristics, such as size, shape, and texture, using clinical knowledge from textual descriptions and structural masks. Additionally, our method enables cross-domain data generation, enhancing the adaptability of the synthetic data across different imaging conditions. Experiments on three public BUS d atasets demonstrate the favorable generation quality and effective cross-domain adaptation of our method. Moreover, the improved accuracy in downstream classification and segmentation tasks further show the clinical utility and practical effectiveness of our synthetic images in supporting breast cancer diagnosis. The code is available at https:// github.com/Violetphy/Clinical-Prior-Tumor-Generation.", "filename": "2025_0776.pdf", "year": 2025, "institution": "Shenzhen University Medical Scho ol", "country": "China", "authors": ["Haoyu Pan", "Junyang Mo", "Hongxin Lin", "Chu Zhang", "Zijian Wu", "Yi Wang", "Qingqing Zheng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0777", "x": 3.446, "y": 4.135, "title": "CLIP-DSA: Textual Knowledge-Guided Cerebrovascular Diseases Recognition in Multi-view Digital Subtraction Angiography", "abstract": "Digital Subtraction Angiography (DSA) sequences are the gold standard for diagnosing most Cerebrovascular diseases (CVDs). Rapid and accurate recognition of CVDs in DSA sequences helps clinicians make the right decisions, which is important in clinical practice. However, the pathological characteristics of CVDs are numerous and complex, and the spatiotemporal complexity of DSA sequences is high, making the diagnosis of CVDs challenging. Therefore, in this paper, we propose a novel CVDs classification framework CLIP-DSA based on CLIP, a pre-trained vision language model. We aim to utilize textual knowledge to guide the robust classification of common CVDs in multi-view DSA sequences. Specifically, our CLIP-DSA comprises a dualbranch vision encoder and a text encoder. The vision encoder is used to extract features from multi-view sequences, while the text encoder is used to obtain textual knowledge. To optimally harness the temporal information in DSA sequences, we introduce a temporal pooling module that dynamically compresses image features in the time dimension. Additionally, we design a multi-view contrastive loss to enhance the network's image-text representation ability by constraining the image features between two views. In a large dataset with 2,026 patients, the proposed CLIP-DSA achieved an AUC of 90.8% in the CVDs classification. The code is available at this website (https://github.com/jiongzhang- john/CLIP-DSA).", "filename": "2025_0777.pdf", "year": 2025, "institution": "Ningbo Institute of Materials Technology and Engineering", "country": "China", "authors": ["Qihang Xie", "Dan Zhang", "Mengting Liu", "Jianwei Zhang", "Ruisheng Su", "Caifeng Shan", "Jiong Zhang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0778", "x": 0.974, "y": 6.533, "title": "CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding", "abstract": "Vision-language models have proven to be of great benefit for medical image analysis since they learn rich semantics from both images and reports. Prior efforts have focused on better alignment of image and text representations to enhance image understanding. However, though explicit reference to a prior image is common in Chest X-Ray (CXR) reports, aligning progression descriptions with the semantics differences in image pairs remains under-explored. In this work, we propose two components to address this issue. (1) A CXR report processing pipeline to extract temporal structure. It processes reports with a large language model (LLM) to separate the description and comparison contexts, and extracts fine-grained annotations from reports. (2) A contrastive captioner model for CXR, namely CoCa-CXR, to learn how to both describe images and their temporal progressions. CoCa-CXR incorporates a novel regional cross-attention module to identify local differences between paired CXR images. Extensive experiments show the superiority of CoCa-CXR on both progression analysis and report generation compared to previous methods. Notably, on MS-CXR-T progression classification, CoCa-CXR obtains 65.0% average testing accuracy on five pulmonary conditions, outperforming the previous state-of-theart (SOTA) model BioViL-T by 4.8%. It also achieves a RadGraph F1 of 24.2% on MIMIC-CXR, which is comparable to the Med-Gemini foundation model.", "filename": "2025_0778.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Yixiong Chen", "Shawn Xu", "Andrew Sellergren", "Yossi Matias", "Avinatan Hassidim", "Shravya Shetty", "Daniel Golden", "Alan L Yuille", "Lin Yang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0779", "x": 1.813, "y": 7.082, "title": "Confidence Calibration for Multimodal LLMs: An Empirical Study Through Medical VQA", "abstract": "Multimodal Large Language Models (MLLMs) show great potential in medical tasks, but their elicited confidence often misaligns with actual accuracy, potentially leading to misdiagnosis or overlooking correct advice. This study presents the first comprehensive analysis of the relationship between accuracy and confidence in medical MLLMs. It proposes a novel method that combines Multi-Strategy Fusion-Based Interrogation (MS-FBI) with auxiliary expert LLM assessment, aiming to improve confidence calibration in Medical Visual Question Answering (VQA). Experiments demonstrate that our method reduces the Expected Calibration Error (ECE) by an average of 40% across three Medical VQA datasets, significantly enhancing MLLMs' reliability. The findings highlight the importance of domain-specific calibration for MLLMs in healthcare, offering a more trustworthy solution for AI-assisted diagnosis.", "filename": "2025_0779.pdf", "year": 2025, "institution": "Zhejiang U niversity", "country": "China", "authors": ["Yuetian Du", "Yucheng Wang", "Ming Kong", "Tian Liang", "Qiang Long", "Bingdi Chen", "Qiang Zhu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0780", "x": 3.418, "y": 2.281, "title": "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method", "abstract": "Magnetic resonance imaging (MRI) is an invaluable tool for clinical and research applications. Yet, variations in scanners and acquisition parameters cause inconsistencies in image contrast, hindering data comparability and reproducibility across datasets and clinical studies. Existing scanner harmonization methods, designed to address this challenge face limitations, such as requiring traveling subjects or struggling to generalize to unseen domains. We propose a novel approach using a conditioned diffusion autoencoder with a contrastive loss and domainagnostic contrast augmentation to harmonize MR images across scanners while preserving subject-specific anatomy. Our method enables brain MRI synthesis from a single reference image. It outperforms baseline techniques, achieving a +7% PSNR improvement on a tra veling subjects dataset and +18% improvement on age regression in unseen scanners. Our model provides robust, effective harmonization of brain MRIs to target scanners without requiring fine-tuning. This advancement promises to enhance comparability, reproducibility, and generalizability in multisite and longitudinal clinical studies, ultimately contributing to improved healthcare outcomes. (The code is publicly available at: https://github. com/daniel-scholz/cacd).", "filename": "2025_0780.pdf", "year": 2025, "institution": "Technical University of Munich (TUM)", "country": "Germany", "authors": ["Daniel Scholz", "Ayhan Can Erdur", "Robbie Holland", "Viktoria Ehm", "Jan C Peeken", "Benedikt Wiestler", "Daniel Rueckert"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0781", "x": 0.879, "y": 6.908, "title": "Contrastive Knowledge-Guided Large Language Models for Medical Report Generation", "abstract": "Automatic medical report generation (MRG) holds considerable research value and has the potential to significantly alleviate the workload of radiologists. Recently, the rapid development of large language models (LLMs) has improved the performance of MRG. However, numerous challenges still need to be addressed to achieve highly accurate medical reports. For instance, most existing methods struggle to interpret image details, lack relevant medical knowledge, and overlook fine-grained cross-modality alignment. To overcome these limitations, we propose a knowledge-guided vision-language alignment framework with contrastive learning and LLMs for medical report generation. The designed method leverages visual representations, relevant medical knowledge, and enhanced features to generate accurate reports via the LLMs-based decoder. To improve the integration of medical-related information, we introduce the Knowledge Injection Module, which enhances the model's feature representation capabilities while unlocking medical domain knowledge in LLMs. Inspired by the contrastive learning scheme, we introduce the Contrastive Alignment Module to align the visual features and textual information effectively. Additionally, the Cross-Modality Enhancement Module can retrieve similar reports for the input images to boost diagnostic accuracy. We conduct extensive experiments on two popular benchmark datasets, including IU X-Ray and MIMIC-CXR. The results demonstrate that our proposed method achieves promising performance compared with state-of-the-art frameworks.", "filename": "2025_0781.pdf", "year": 2025, "institution": "Macao Polytechnic U niversity", "country": "China", "authors": ["Yuyang Sha", "Hongxin Pan", "Weiyu Meng", "Kefeng Li"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0782", "x": 2.006, "y": 4.367, "title": "Cross-Modality Supervised Prostate Segmentation on CBCT for Adaptive Radiotherapy", "abstract": "Accurate organ segmentation is crucial for prostate cancer radiotherapy, but cone-beam computer tomography (CBCT) based models are hindered by low image quality and annotation scarcity. Existing approaches rely on deformable registration, which struggles with softtissue deformations, or direct CBCT training, which suffers from domain shifts and low-quality labels. We propose a domain adaptation framework that enables robust prostate segmentation on CBCT using crossmodality supervision from planning CT (pCT). A cycle-consistent generative adversarial network translates pCT i nto synthetic CBCT, enabling segmentation models to train on high-quality pCT-derived annotations while adapting to CBCT characteristics. Additionally, anatomy-aware augmentation enhances robustness to organ deformations across diverse patient anatomies. Using a multi-center dataset, our approach achieves B. Kovacs and G. Stanic-Equal contribution; each may list themselves as lead co-first author. R. Floca and K. Giske-Shared last authorship.", "filename": "2025_0782.pdf", "year": 2025, "institution": "Heidelberg University", "country": "Germany", "authors": ["Balint Kovacs", "Goran Stanic", "Fabian Weykamp", "Florian Ebert", "Dimitrios Bounias", "Bouchra Tawk", "Martin Niklas", "Jakob Liermann", "Oliver Jäkel", "Klaus H Maier-Hein", "Ralf Floca", "Kristina Giske"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0783", "x": 4.442, "y": 6.589, "title": "Deep Association Multimodal Learning for Zero-Shot Spatial Transcriptomics Prediction", "abstract": "Spatial transcriptomics enables localized gene expression profiling within histological regions. Current supervised methods struggle to infer patterns for novel gene types beyond their training scope, while existing zero-shot frameworks partially address this by incorporating gene semantics, the \"independent learning\" paradigms hamper their usage in zero-shot gene expression prediction. Specifically, they learn tissue morphology and gene semantics (inter-modality) independently, and treat gene functions (intra-modality) as independent entities. In this paper, we present a deep association multimodal framework which bridges pathological image with gene functionality semantics for zeroshot expression prediction. Concretely, our framework achieves generalized expression prediction by integrating nuclei-aware spatial modeling that preserves tissue microarchitecture, cross-modal alignment of pathological features with gene functionality semantics via iterative visionlanguage prompt learning, and gene interaction modeling that dynamically captures relationships across gene descriptions. On standard benchmark datasets, we demonstrate competitive zero-shot performance compared to other competitors (e.g., outperforms 16.3% in mean Pearson Correlation Coefficient on cSCC dataset), and we show clinical interpretability of our method. Codes is publicly available at https://github. com/DeepMed-Lab-ECNU/ALIGN-ST.", "filename": "2025_0783.pdf", "year": 2025, "institution": "N ormal University", "country": "East China China", "authors": ["Yijing Zhou", "Yadong Lu", "Qingli Li", "Xinxing Li", "Yan Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0784", "x": 2.142, "y": 6.341, "title": "DGHFA: Dynamic Gradient and Hierarchical Feature Alignment for Robust Distillation of Medical VLMs", "abstract": "Recent advancements in Medical Vision-Language Models (VLMs) have significantly improved medical cross-modal task performance through large-scale contrastive pre-training. However, deploying these large models in clinical settings is hindered by their computational complexity and vulnerability to adversarial attacks. While knowledge distillation offers a solution by transferring knowledge to efficient student models, traditional methods usually ignore the robustness problem, leaving models susceptible to adversarial attacks. To address these challenges, we propose a novel Dynamic Gradient and Hierarchical Feature Alignment framework (DGHFA) for robust knowledge distillation. Our approach introduces a dynamic gradient calibration mechanism for balanced knowledge transfer and a hierarchical a dversarial feature alignment framework to enhance robustness under adversarial attacks. Extensive experiments on two medical VLMs and downstream pathology and X-Ray datasets demonstrate that our method outperforms state-of-the-art approaches across multiple attack scenarios, achieving improvements of 2.3 and 1.7% points in robust accuracy, respectively.", "filename": "2025_0784.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Boyi Xiao", "Jianghao Wu", "Lanfeng Zhong", "Xiaoguang Zou", "Yuanquan Wu", "Guotai Wang", "Shaoting Zhang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0785", "x": 2.069, "y": 5.008, "title": "Domain Generalization for Pulmonary Nodule Detection via Distributionally-Regularized Mamba", "abstract": "Extending deep learning models to out-of-distribution (o.o.d.) data remains a persistent challenge, especially in domains like medical imaging with restricted data availability and limited data sharing. This challenge is particularly evident in pulmonary nodule detection, as the model struggles to distinguish nodules from the surrounding normal tissues across different data distributions. To address this issue, we propose a Distributionally Regularized Mamba Network (DRM-Net). Inspired by Mamba, we propose a Feature-Augmented State-Space module that unifies pulmonary nodule features to effectively distinguish nodules from surrounding confounding tissues. Furthermore, a Region-Aware Distribution Alignment module is elaborately introduced to reduce disparities in feature distributions between domains. We construct a pulmonary nodule detection dataset, named Generalization for Pulmonary Nodule Detection (GPND), comprising diverse domains, including private and well-known public datasets. Extensive experiments conducted on GPND demonstrate that DRMNet outperforms state-ofthe-art domain generalization methods. The code is available at https:// github.com/TzhongBoyyy97/DRMNet.", "filename": "2025_0785.pdf", "year": 2025, "institution": "Sichuan Unive rsity", "country": "China", "authors": ["Tianzhong Lan", "Nan Chen", "Zhang Yi", "Xiuyuan Xu", "Min Zhu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0786", "x": 1.676, "y": 5.146, "title": "DpDNet: An Dual-Prompt-Driven Network for Universal PET-CT Segmentation", "abstract": "PET-CT lesion segmentation is challenging due to noise sensitivity, small and variable lesion morphology, and interference from physiological high-metabolic signals. Current mainstream approaches follow the practice of one network solving the segmentation of multiple cancer lesions by treating all cancers as a single task. However, this overlooks the unique characteristics of different cancer types. Considering the specificity and similarity of different cancers in terms of metastatic patterns, organ preferences, and FDG uptake intensity, we propose DpDNet, a Dual-Prompt-Driven network that incorporates specific prompts to capture cancer-specific features and common prompts to retain shared knowledge. Additionally, to mitigate information forgetting caused by the early introduction of prompts, prompt-aware heads are employed after the decoder to adaptively handle multiple segmentation tasks. Experiments on a PET-CT dataset with four cancer types show that DpDNet outperforms state-of-the-art models. Finally, based on the segmentation results, we calculated MTV, TLG, and SUVmax for breast cancer survival analysis. The results suggest that DpDNet has the potential to serve as a valuable tool for personalized risk stratification, supporting clinicians in optimizing treatment strategies and improving outcomes.", "filename": "2025_0786.pdf", "year": 2025, "institution": "Radboud University Medical Centre", "country": "The Netherlands", "authors": ["Xinglong Liang", "Jiaju Huang", "Luyi Han", "Tianyu Zhang", "Xin Wang", "Yuan Gao", "Chunyao Lu", "Lishan Cai", "Tao Tan", "Ritse Mann"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0787", "x": 1.207, "y": 4.772, "title": "DSFC: Deformation-Aware Learning Strategy via Self-sustaining Feedback Cycle for Medical Vision Foundation Model Domain Adaptation", "abstract": "Vision foundation model, despite strong segmentation capabilities enabled by pretraining on large-scale data, remain underexplored in specific medical visual concept segmentation tasks. Medical imaging presents unique challenges: pixel intensity differences between target regions and surrounding structures are often subtle, and significant variations in the shape, size, and location of anatomical structures limit the effectiveness of traditional pixel-similarity-based alignment strategies. This paper proposes a Deformation-Aware Learning Strategy via Self-sustaining Feedback Cycle (DSFC) for medical image segmentation. The framework introduces a dual-deformation perturbation mechanism, combining global gaussian-distributed deformations and target-focused local deformations, to preserve anatomical patterns while capturing nonrigid variations. Hard Example Adaptive (HEA) loss is proposed to enhance training stability and mask accuracy . DSFC establishes a closedloop training process, alternately optimizing the segmentation model and destroyer to improve anatomical understanding. Our extensive experiments on public datasets with various dimensions, organs demonstrate that DSFC significantly enhances model performance in fully supervised training settings without the need for increasing the samples. and its components are effective. Our code is available at: https://github.com/ jaylinio/DSFC.", "filename": "2025_0787.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Jie Lin", "Hengyi Jiang", "Hong Liu", "Liansheng Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0788", "x": 1.837, "y": 4.778, "title": "Dual Knowledge-Aware Guidance for Source-Free Domain Adaptive Fundus Image Segmentation", "abstract": "Source-free domain adaptation (SFDA), where only a pretrained source model is available to adapt to the target domain, has gained widespread application in the medical field. Most existing methods overlook low-quality pseudo-labels, i.e., pseudo-labels with boundary semantic confusion, when learning target domain-specific knowledge, leading to the loss of crucial boundary information. Furthermore, focusing solely on the specific knowledge can drive the model shifts in an uncontrollable direction, resulting in model degradation. To address these issues, we propose Dual Knowledge-aware Guidance (DKG), a novel SFDA method that integrates domain-specific knowledge with domain-invariant knowledge to improve transfer performance. Specifically, the pseudo-label calibration scheme is proposed to reduce semantic bias in high-uncertainty pixels, preserving the boundary information o f target domain-specific knowledge. To ensure stable training, we propose a domain-invariant knowledge-based loss strategy, leveraging a confidenceguided mechanism and a consistency constraint. Additionally, we also introduce a dynamic balancing loss to address class imbalance. Extensive experiments on cross-domain fundus image segmentation show that DKG achieves state-of-the-art performance. Code is available at https:// github.com/Hanshuqian/DKG", "filename": "2025_0788.pdf", "year": 2025, "institution": "East China Normal University", "country": "China", "authors": ["Yu Chen", "Hailing Wang", "Chunwei Wu", "Guitao Cao"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0789", "x": 1.478, "y": 7.04, "title": "Eliminating Language Bias for Medical Visual Question Answering with Counterfactual Contrastive Training", "abstract": "Medical Visual Question Answering (Med-VQA) aims to assist in clinical diagnosis, but still faces challenges with language bias. Current approaches oversimplify the causal relationship between clinical terms and answers by treating it as a binary positive/negative effect. This can lead to the persistence of bias or reduced sensitivity to questions. To address this limitation, we propose a novel approach named DeCoCT (Debiasing Med-VQA via Counterfactual Contrastive Training). We decompose the causal relationship between clinical terms and answers into two components: (1) concept localization in medical images, and (2) prior knowledge from training data. We introduce a Key Region Capture Module (KRCM), trained with counterfactual strategies. It can enhance the model's ability to capture critical information through clinical terms. Furthermore, we employ counterfactual contrastive training to eliminate spurious correlations introduced by clinical terms while enhancing the model's focus on relevant visual regions. In addition, we construct a new conditional prior dataset based on VQA-RAD, named VQA-RAD-CP. Extensive experiments demonstrate that our approach significantly mitigates language bias in Med-VQA. Our codes and VQA-RAD-CP dataset are available at https://github.com/YX542/DeCoCT.", "filename": "2025_0789.pdf", "year": 2025, "institution": "Jiangsu University", "country": "China", "authors": ["Xingyu Wan", "Qiaoying Teng", "Jun Chen", "Yonghan Lu", "Deqi Yuan", "Zhe Liu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0790", "x": 2.504, "y": 5.249, "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and Beyond", "abstract": "Detecting abnormalities in medical images poses unique challenges due to differences in feature representations and the intricate relationship between anatomical structures and abnormalities. This is especially evident in mammography, where dense breast tissue can obscure lesions, complicating radiological interpretation. Despite leveraging anatomical and semantic context, existing detection methods struggle to learn effective class-specific features, limiting their applicability across different tasks and imaging modalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal contrastive detector that enables feature-based detection. It employs cross-attention with inherently derived, intuitive class-specific exemplar features and is trained with an iterative strategy. We achieve state-of-the-art performance across three distinct imaging modalities from four public datasets. On Vietnamese dense breast mammograms, we attain an mAP50 of 0.7 for mass detection and 0.55 for calcifications, yielding an absolute improvement of 16% po ints from previous state-of-the-art. Additionally, a radiologistsupported evaluation of 100 mammograms from an out-of-distribution Chinese cohort demonstrates a twofold gain in lesion detection performance. For chest X-rays and angiography, we achieve an mAP50 of 0.25 for mass and 0.37 for stenosis detection, improving results by 4% and 7% points, respectively. These results highlight the potential of our approach to advance robust and generalizable detection systems for medical imaging.", "filename": "2025_0790.pdf", "year": 2025, "institution": "Friedrich-Alexander-Universität", "country": "Germany", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0791", "x": 2.828, "y": 6.405, "title": "Explain Any Pathological Concept: Discovering Hierarchical Explanations for Pathology Foundation Models", "abstract": "Foundation models have demonstrated significant promise in medical image analysis, particularly in pathology. However, their black-box nature makes it challenging for clinicians to understand their decision-making processes. In this paper, we evaluate the explainability of existing pathology foundation models based on visual concepts. Considering the hierarchical structure of pathological anatomy, comprising of regions, units, and cells, we introduce a novel Hierarchical Concept-based Explanation (HCE) method to illuminate how concepts at different levels influence the model's predictions. Specifically, our approach begins with the utilization of a specialist-generalist collaborative segmentation model to perform instance segmentation across various levels. We then employ a surrogate model to approximate the target foundation model and compute the Shapley values for each concept. Finally, we visualize these con tributions through a comprehensive global ShapMap. We evaluate several state-of-the-art pathology foundation models, including CONCH, UNI, and Virchow, on an adenoma classification task. The findings reveal that the explanations provided by CONCH and UNI show better composability, suggesting they draw from a wider contextual understand demonstrate great separability, reflecting a reliance on specific regions. Additionally, we explore the consistency of concept explanations across different foundation models.", "filename": "2025_0791.pdf", "year": 2025, "institution": "Nankai University", "country": "China", "authors": ["Shuting Xu", "Junlin Hou", "Hao Chen"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0792", "x": 0.565, "y": 5.039, "title": "Exploring Text-Enhanced Mixture-of-Experts for Semi-supervised Medical Image Segmentation with Composite Data", "abstract": "Semi-supervised learning (SSL) has emerged as an effective approach to reduce reliance on expensive labeled data by leveraging large amounts of unlabeled data. However, existing SSL methods predominantly focus on visual data in isolation. Although text-enhanced SSL approaches integrate supplementary textual information, they still treat image-text pairs independently. In this paper, we explore the potential of jointly learning from related text-image datasets to further advance the capabilities of SSL. To this end, we introduce a novel text-enhanced Mixture-of-Experts (MoE) model, augmented with textual information, for semi-supervised medical image segmentation (TextMoE). TextMoE incorporates a universal vision encoder and a text-assisted MoE (TMoE) decoder, enabling it to simultaneously process CT-text and X-Ray-text data within a unified framework. To achieve effective knowledge integration from heterogeneous unlabeled data, a content regularization with frequency space exchange is designed, guiding TextMoE to learn modality-invariant representations. Additionally, the proposed TMoE decoder is enhanced by modality indicators, securing the effective fusion of visual and textual features. Finally, a differential loss is introduced to diversify the semantic understanding between visual experts, ensuring complementary insights to the overall interpretation. Experiments conducted on two public datasets indicate that TextMoE outperforms SSL and text-assisted SSL methods, achieving superior performance.", "filename": "2025_0792.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Qingjie Zeng", "Huan Luo", "Xinke Ma", "Zilin Lu", "Yang Hu", "Yong Xia"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0793", "x": 0.715, "y": 6.69, "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation", "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at https:// github.com/bowang-lab/AMOS-MM-Solution.", "filename": "2025_0793.pdf", "year": 2025, "institution": "Harvard University", "country": "Canada", "authors": ["Mohammed Baharoon", "Jun Ma", "Congyu Fang", "Augustin Toma", "Bo Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0794", "x": 2.523, "y": 7.75, "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "abstract": "Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model's effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates client-adaptive momentum terms derived from each client's loss reduction during local training, as well as a personalized dampening factor to curb overfitting. We further introduce a no vel intensity alignment loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cuttingedge methods in medical segmentation tasks, underscoring its efficacy. The code is available at https://github.com/siomvas/FedCLAM.", "filename": "2025_0794.pdf", "year": 2025, "institution": "University of London", "country": "UK", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0795", "x": 1.874, "y": 3.502, "title": "Frequency Strikes Back: Boosting Parameter-Efficient Foundation Model Adaptation for Medical Imaging", "abstract": "Adapting vision transformer (ViT) foundation models with parameter-efficient fine-tuning (PEFT) has become increasingly popular in medical imaging, enabling efficient adaptation while updating only a small subset of parameters. However, existing PEFT methods process tokens independently, overlooking cross-token dependencies and limiting their ability to capture global contextual information. To address these challenges, we propose FreqFiT, a novel Frequency-based Fine-Tuning module inserted between ViT blocks to enhance model adaptability. Fre-qFiT is effective and seamlessly integrates with existing PEFT methods to improve their performance. We evaluate FreqFiT across 2D and 3D medical imaging datasets, such as PAPILA, HAM10000, ADNI-1.5T, and COVID-CT-MD. It improves accuracy 9% and AUC 10%, surpassing the original PEFT methods on both MedMAE and DINOv2 backbones. Despite using only ≤ 1.2% of full fine-tuning parameters, Freq-FiT achieves state-of-the-art medical imaging adaptation efficiently. The source code is available here.", "filename": "2025_0795.pdf", "year": 2025, "institution": "Universit y of Houston", "country": "USA", "authors": ["Son T Ly", "Hien V Nguyen"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0796", "x": 0.493, "y": 6.231, "title": "From Slices to Volumes: Multi-scale Fusion of 2D and 3D Features for CT Scan Report Generation", "abstract": "The increasing complexity of medical imaging data underscores the necessity for multimodal intelligent systems capable of integrating diverse data representations for comprehensive and precise analysis. In the domain of 3D CT scans, the generation of accurate and clinically meaningful medical reports requires both volumetric contextual information and the fine-grained spatial details inherent in 2D slices. To address this challenge, we propose a framework that employs a pretrained 2D self-supervised learning encoder, initially trained on CT scan slices integrated with a 3D aggregator. By combining the rich, high-resolution information from 2D slices with the spatial coherence of 3D volumetric data, our approach maximizes the complementary strengths of both representations. Experimental results demonstrate that our method o utperforms existing baseline approaches in both report generation and multiple-choice question answering, highlighting the critical role of multidimensional feature integration. This work underscores the transformative potential of multimodal intelligent systems in bridging complex imaging data with practical clinical insights, ultimately improving radiological diagnostics and patient care (Our code is now available at github.com/serag-ai/SAMF).", "filename": "2025_0796.pdf", "year": 2025, "institution": "Weill Cornell Medicine-Qatar", "country": "Qatar", "authors": ["Abdullah Hosseini", "Ahmed Ibrahim", "Ahmed Serag"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0797", "x": 1.638, "y": 6.865, "title": "FunBench: Benchmarking Fundus Reading Skills of MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) have shown significant potential in medical image analysis. However, their capabilities in interpreting fundus images, a critical skill for ophthalmology, remain under-evaluated. Existing benchmarks lack fine-grained task divisions and fail to provide modular analysis of its two key modules, i.e., large language model (LLM) and vision encoder (VE). This paper introduces FunBench, a novel visual question answering (VQA) benchmark designed to comprehensively evaluate MLLMs' fundus reading skills. FunBench features a hierarchical task organization across four levels (modality perception, anatomy perception, lesion analysis, and disease diagnosis). It also offers three targeted evaluation modes: linear-probe based VE evaluation, kno wledge-prompted LLM evaluation, and holistic evaluation. Experiments on nine open-source MLLMs plus GPT-4o reveal significant deficiencies in fundus reading skills, particularly in basic tasks such as laterality recognition. The results highlight the limitations of current MLLMs and emphasize the need for domain-specific training and improved LLMs and VEs.", "filename": "2025_0797.pdf", "year": 2025, "institution": "Renmin University of China", "country": "China", "authors": ["Qijie Wei", "Kaiheng Qian", "Xirong Li"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0798", "x": 1.47, "y": 4.752, "title": "Fusing Dual Encoders: Single-Source Domain Generalization with Extremely Few Annotations", "abstract": "Considering the commonly existing domain shifts and label scarcity, single-source domain generalization (SDG) is a crucial and promising topic in medical image segmentation. SDG trains the model on one source domain and aims for generalization on the unseen target domain. However, previous methods rely on the quantity of training samples and perform poorly when only a few labeled training volumes are available, limiting the effective applicability in clinical practice. Thus, we concentrate on the challenging SDG setting with extremely few annotated samples and propose a Medical Dual-encoder framework (MEDU). A dual-encoder U-shaped network incorporates two different encoders and fuses features via simple yet effective layers for learning representative features. We integrate pretrained SAM2 encoder with semantic knowledge for a proper initialization and resisting overfitting, proving effective in training with limited supervision. F urthermore, we introduce a perturbation consistency training strategy with perturbation operations and hierarchical consistency to learn domain-invariant features and alleviate discrepancies between training and inference. MEDU exceeds existing advanced methods in three challenging cross-domain settings concerning SDG with extremely few annotations. For example, on Abdominal MRI-CT, MEDU attains a Dice score of 81.75% with only three labeled training volumes, achieving an improvement of 12.60%. Our source code is available at https://github.com/wrf-nj/MEDU.", "filename": "2025_0798.pdf", "year": 2025, "institution": "Nanjing University", "country": "China", "authors": ["Ruofan Wang", "Jintao Guo", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0799", "x": 1.821, "y": 6.343, "title": "Geometry-Guided Local Alignment for Multi-view Visual Language Pre-training in Mammography", "abstract": "Mammography screening is an essential tool for early detection of breast cancer. The speed and accuracy of mammography interpretation has the potential to be improved with deep learning methods. However, the development of a foundation visual language model (VLM) is hindered by limited data and domain differences between natural and medical images. Existing mammography VLMs, adapted from natural images, often ignore domain-specific characteristics, such as multi-view relationships in mammography. Unlike radiologists who analyze both views together to process ipsilateral correspondence, current methods treat them as independent images or do not properly model the multi-view correspondence learning, losing critical geometric context and resulting in suboptimal p rediction. We propose GLAM: Global and Local Alignment for Multi-view mammography for VLM pretraining using geometry guidance. By leveraging the prior knowledge about the multi-view imaging process of mammograms, our model learns local cross-view alignments and fine-grained local features through joint global and local, visual-visual, and visual-language contrastive learning. Pretrained on EMBED [14], one of the largest open mammography datasets, our model outperforms baselines across multiple datasets under differen t settings (The code is available at https://github.com/XYPB/GLAM).", "filename": "2025_0799.pdf", "year": 2025, "institution": "Yale University", "country": "USA", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C Dvornek"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0800", "x": 1.35, "y": 4.613, "title": "HARP: Harmonization and Adaptive Refinement of Pseudo-labels for Cross-Domain Medical Image Segmentation", "abstract": "Medical image segmentation is crucial for accurate diagnosis and effective treatment planning. However, in cross-domain semisupervised segmentation, the scarcity of labeled data often leads to suboptimal performance and poor generalization across diverse medical imaging domains. Moreover, pseudo-labels generated from unlabeled data are inherently noisy, introducing confirmation bias that destabilizes training and hinders the models ability to accurately capture complex anatomical structures. To address these challenges, we propose HARP: Harmonization and Adaptive Refinement of Pseudo-Labels for Cross-Domain Medical Image Segmentation, a framework designed to enhance segmentation performance by integrating two novel modules: the Adaptive Pseudo-label Selection (APS) module and the Cross-Domain Harmonization (CDH) module. The APS module ensures the quality and reliability of pseudo-labels by using a confidence-based filtering mechanism and a refinement strategy. The CDH module uses m atrix decomposition to harmonize differences across medical imaging modalities, enhancing data diversity while preserving domain-specific features and improving the model's adaptability to varying imaging protocols for robust performance across diverse medical datasets. Extensive experiments on three medical datasets demonstrate the effectiveness of HARP, achieving significant improvements across multiple evaluation metrics. The source code is available at https://github.com/lbllyl/HARP.", "filename": "2025_0800.pdf", "year": 2025, "institution": "University of Science and Tec hnology of China", "country": "People's Republic of China", "authors": ["Yulong Liu", "Wenqing Ye", "Hui Liu", "Ziyi Chen", "Peilin Li", "Ronald X Xu", "Mingzhai Sun"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0801", "x": -0.394, "y": 4.676, "title": "HA-SAM: Hierarchically Adapting SAM for Nerve Segmentation in Ultrasound Images", "abstract": "Automatic ultrasound nerve localization algorithm is crucial in nerve block procedures and neuropathy detection. However, the performance of existing approaches is typically constrained by the limited scale of ultrasound image datasets. While adapting from large scale models such as Segment Anything Model (SAM) has demonstrated remarkable performance on medical images, its effectiveness heavily relies on extensive datasets and substantial computational resources. This presents significant challenges for adapting SAM to ultrasound image segmentation. To address these challenges, we propose a novel parameter-and data-efficient adaptation method called Hierarchical Adapter. Specifically, the Hierarchical Adapter can flexibly adjust the number of finetuning parameters to optimize the exploitation of data and computational resources. In addition, we observe the depth-dependent difficulty for adapting different Transformer blocks of SAM. Therefore, we insert Hierarchical Adapters with varying sizes into transformer layers at different depths of the SAM encoder, optimizing the distribution of trainable parameters. This design significantly improves the parameter-efficiency during adaptation while simultaneously enhancing segmentation performance. Compared to state-of-the-art methods, our model reduces training parameter requirements by more than half while still achieving an approximately 1.5% improvement in Dice score on two ultrasound nerve datasets.", "filename": "2025_0801.pdf", "year": 2025, "institution": "Huazhong University of S cience and Technology", "country": "China", "authors": ["Zihao Peng", "Susu Kang", "Xuping Huang", "Xucheng Xiang", "Gengyu He", "Tianzhu Liu", "Wei Mei", "Shan Tan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0802", "x": 3.111, "y": 6.148, "title": "HASD: Hierarchical Adaption for Pathology Slide-Level Domain-Shift", "abstract": "Domain shift is a critical problem for artificial intelligence (AI) in pathology as it is heavily influenced by center-specific conditions. Current pathology domain adaptation methods focus on image patches rather than whole-slide images (WSI), thus failing to capture global WSI features required in typical clinical scenarios. In this work, we address the challenges of slide-level domain shift by proposing a Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD achieves multi-scale feature consistency and computationally efficient slide-level domain adaptation through two key components: (1) a hierarchical adaptation framework that integrates a Domain-level Alignment Solver for feature alignment, a Slide-level Geometric Invariance Regularization to preserve the morphological structure, and a Patch-level Attention Consistency Regularization to maintain local critical diagnostic cues; and (2) a prototy pe selection mechanism that reduces computational overhead. We validate our method on two slide-level tasks across five datasets, achieving a 4.1% AUROC improvement in a Breast Cancer HER2 Grading cohort and a 3.9% C-index gain in a UCEC survival prediction cohort. Our method provides a practical and reliable slide-level domain adaption solution for pathology institutions, minimizing both computational and annotation costs. Code is available at https://github.com/TumVink/HASD.", "filename": "2025_0802.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J Schüffler"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0803", "x": 2.906, "y": 6.691, "title": "Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation", "abstract": "Automated pathology report generation from Whole Slide Images (WSIs) faces two key challenges: (1) lack of semantic content in visual features and (2) inherent information redundancy in WSIs. To address these issues, we propose a novel Historical Report Guided Bi-modal Concurrent Learning Framework for Pathology Report Generation (BiGen) emulating pathologists' diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to provide rich semantic content, which retrieves WSI-relevant knowledge from pre-built medical knowledge bank by matching high-attention patches and (2) A bi-modal concurrent learning strategy instantiated via a learnable visual token and a learnable textual token to dynamically extract key visual features and retrieved knowledge, where weight-shared layers enable crossmodal alignment between visual features and knowledge features. Our multi-modal decoder integrates both modals for comprehensive diagnostic reports generation. Experiments on the PathText (BRCA) dataset demonstrate our framework's superiority, achieving state-of-the-art performance with 7.4% relative improvement in NLP metrics and 19.1% enhancement in classification metrics for Her-2 prediction versus existing methods. Ablation studies validate the necessity of our proposed modules, highlighting our method's ability to provide WSI-relevant rich semantic content and suppress information redundancy in WSIs. Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.", "filename": "2025_0803.pdf", "year": 2025, "institution": "East China Normal University", "country": "China", "authors": ["Ling Zhang", "Boxiang Yun", "Qingli Li", "Yan Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0804", "x": 0.824, "y": 6.724, "title": "ITAdaptor: Image-Tag Adapter Framework with Knowledge Enhancement for Radiology Report Generation", "abstract": "Automated radiology report generation holds significant research value as it has the potential to alleviate the heavy burden of report writing for radiologists. Previous studies have incorporated diagnostic information through multi-label classification to assist in report generation. However, these methods treate visual and diagnostic information equally, which overlooks the difference in the importance of both when generating different types of words. This can lead to errors in report generation. We propose the Image-Tag Adapter framework (ITAdaptor), which dynamically balances the contributions of visual and diagnostic information in the decoder, ensuring both are fully utilized during the report generation process. The model introduces two novel modules: Cross-Modal Knowledge Enhancement (CMKE) and Image-Tag Adapter (ITA). CMKE leverages pre-trained CLIP to retrieve similar reports from a database, assisting in the diagnosis of query images by providing relevant disease information. ITA adaptively fuses the visual information from the input images with the diagnostic information from the disease tags to generate more accurate reports. For training, we propose a strategy combining reinforcement learning and knowledge distillation, optimizing iteratively to extract knowledge into the ITAdaptor. Extensive comparative experiments on the IU-Xray and MIMIC-CXR benchmark datasets demonstrate the effectiveness of our proposed approach.", "filename": "2025_0804.pdf", "year": 2025, "institution": "Chongqing Normal University", "country": "China", "authors": ["Shuaipeng Ding", "Mengnan Fan", "Mingyong Li", "Chao Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0805", "x": 2.061, "y": 6.366, "title": "Iterative Deployment Exposure for Unsupervised Out-of-Distribution Detection", "abstract": "Deep learning models are vulnerable to performance degradation when encountering out-of-distribution (OOD) images, potentially leading to misdiagnoses and compromised patient care. These shortcomings have led to great interest in the field of OOD detection. Existing unsupervised OOD (U-OOD) detection methods typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution, neglecting the reality that deployed models passively accumulate task-specific OOD samples over time. To better reflect this real-world scenario, we introduce Iterative Deployment Exposure (IDE), a novel and more realistic setting for U-OOD detection. We propose CSO, a method for IDE that starts from a U-OOD detector that is agnostic to the OOD distribution and slowly refines it during deployment using observed unlabeled data. CSO uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearestneighbor approach, along with a novel confidence-scaled few-shot OOD detector to effectively learn from limited OOD examples. We validate our approach on a dedicated benchmark, showing that our method greatly improves upon strong baselines on three medical imaging modalities.", "filename": "2025_0805.pdf", "year": 2025, "institution": "University of Bern", "country": "Switzerland", "authors": ["Lars Doorenbos", "Raphael Sznitman", "Pablo Márquez-Neila"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0806", "x": 5.602, "y": 4.59, "title": "Knowledge Tree Driven Contextualized Instruction Tuning of Foundation Models for Epilepsy Drug Recommendation", "abstract": "Epilepsy affects over 50 million people worldwide, with antiseizure medications (ASMs) as the primary treatment for seizure control. However, ASM selection remains a \"trial and error\" process due to the lack of reliable predictors of effectiveness and tolerability. While machine learning approaches have been explored, existing models are limited to predicting outcomes only for ASMs encountered during training and have not leveraged recent biomedical foundation models for this task. This work investigates ASM outcome prediction using only patient MRI scans and reports. Specifically, we leverage biomedical vision-language foundation models and introduce a novel contextualized instruction-tuning framework that integrates expert-built knowledge trees of MRI entities to enhance their performance. Additionally, by training only on the four most commonly prescribed ASMs, our framework enables generalization to predicting outcomes and effectiveness for unseen ASMs not present during t raining. We evaluate our instruction-tuning framework on two retrospective epilepsy patient datasets, achieving an average AUC of 71.39 and 63.03 in predicting outcomes for four primary ASMs and three completely unseen ASMs, respectively. Our approach improves the AUC by 5.53 and 3.51 compared to standard report-based instruction tuning for seen and unseen ASMs, respectively. Our code, MRI knowledge tree, prompting templates, and TREE-TUNE generated instructionanswer tuning dataset are available at the link.", "filename": "2025_0806.pdf", "year": 2025, "institution": "Swinburne University of Technology", "country": "Australia", "authors": ["Duy Khoa Pham", "Deval Mehta", "Yiwen Jiang", "Daniel Thom", "Richard Shek-Kwan Chang", "Mohammad Nazem-Zadeh", "Emma Foster", "Timothy Fazio", "Sarah Holper", "Karin Verspoor", "Jiahe Liu", "Duong Nhu", "Sarah Barnard", "Terence O’brien", "Zhibin Chen", "Jacqueline French", "Patrick Kwan", "Zongyuan Ge"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0807", "x": 2.709, "y": 3.521, "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation", "abstract": "Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient. Project page: https://anonymous.4open.science/r/LEAF-F669.", "filename": "2025_0807.pdf", "year": 2025, "institution": "Sun Yat-sen University", "country": "China", "authors": ["Qilin Huang", "Tianyu Lin", "Zhiguang Chen", "Fudan Zheng"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0808", "x": 1.688, "y": 5.125, "title": "LKA: Large Kernel Adapter for Enhanced Medical Image Classification", "abstract": "Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensiv e experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets. The code is available at: https://github.com/misswayguy/LKA.", "filename": "2025_0808.pdf", "year": 2025, "institution": "University of Leicester", "country": "UK", "authors": ["Ziquan Zhu", "Si-Yuan Lu", "Tianjin Huang", "Lu Liu", "Zhe Liu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0809", "x": 0.703, "y": 5.227, "title": "LTSE: Language-Guided Tissue Referring Segmentation in Pathology Images with Adaptive Expert Mixture", "abstract": "Tissue segmentation is essential for pathology image analysis. Conventional deep learning based segmentation methods require large amounts of annotated data and are constrained by the predefined classes, making them less flexible in adapting to diverse clinical requirements and user-specific queries. The language-guided referring segmentation (LGRS) model can help identify and segment specific objects based on user-provided descriptions. However, the existing LGRS models lack the capability to explicitly reject nonexistent targets, and struggle in effectively segmenting multiple target regions. Based on the above considerations, we propose LTSE, a language-guided tissue referring segmentation assistant for pathology images, which inherits the powerful multi-modal alignment capabilities of Multi-modal Large Language Models (MLLMs) to implement tissue segmentation according to the instructions. Specifically, we expand the original vocabulary with multiple [SEG] tokens to support multiple mask references and a [REJ] token to reject the empty targets. In addition, we enhance the adaptability and accuracy in multi-target segmentation by developing an Adaptive Expert Mixture (AEM) module that can dynamically select specialized expert decoders based on the textual and visual characteristics of the input data. We for the first time curate a vision-language pathology dataset BCSS-Ref for the tissue referring segmentation task with matched images, masks and textual information, and the experimental results demonstrate the superiority of our method in comparison with the existing studies.", "filename": "2025_0809.pdf", "year": 2025, "institution": "Nanjing University of Aeronautics and Astronautics", "country": "China", "authors": ["Jiao Tang", "Bo Qian", "Peng Wan", "Wei Shao", "Daoqiang Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0810", "x": 2.058, "y": 6.195, "title": "MadCLIP: Few-Shot Medical Anomaly Detection with CLIP", "abstract": "An innovative few-shot anomaly detection approach is presented, leveraging the pre-trained CLIP model for medical data, and adapting it for both image-level anomaly classification (AC) and pixellevel anomaly segmentation (AS). A dual-branch design is proposed to separately capture normal and abnormal features through learnable adapters in the CLIP vision encoder. To improve semantic alignment, learnable text prompts are employed to link visual features. Furthermore, SigLIP loss is applied to effectively handle the many-to-one relationship between images and unpaired text prompts, showcasing its adaptation in the medical field for the first time. Our approach is validated on multiple m odalities, demonstrating superior performance over existing methods for AC and AS, in both same-dataset and cross-dataset evaluations. Unlike prior work, it does not rely on synthetic data or memory banks, and an ablation study confirms the contribution of each component. The code is available at https://github.com/mahshid1998/MadCLIP.", "filename": "2025_0810.pdf", "year": 2025, "institution": "University of V erona", "country": "Italy", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0811", "x": 2.335, "y": 5.583, "title": "MedGCD: Generalized Category Discovery in Medical Imaging", "abstract": "Existing semi-supervised learning methods in medical imaging assume that unlabeled and labeled data share identical classes. However, in real-world medical scenarios, unlabeled datasets often contain novel categories not present in the labeled data. To address this problem, we propose MedGCD (Generalized Category Discovery for Medical Images), a method that identifies seen categories in labeled data and clusters novel categories in unlabeled data. Specifically, MedGCD introduces a dual stream of strong views in a weak-to-strong framework coupled with a confidence-aware pairwise objective for discovering novel categories. This dual view approach enhances feature extraction from unlabeled data, while the confidence-aware pairwise objective ensures the selection of reliable samples, e nabling effective clustering of novel categories. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed model in discovering novel categories while maintaining consistent performance on seen categories, with improvements in novel category ranging from 4% to 15%, leading to an overall accuracy improvement of 2% to 8% (https://github.com/Chandan-IITI/ MedGCD).", "filename": "2025_0811.pdf", "year": 2025, "institution": "Institute of High Performance Computing (IHPC)", "country": "Singapore", "authors": ["Ankit Das", "Chandan Gautam", "Pritee Agrawal", "Feng Yang", "Yong Liu", "Ramasamy Savitha"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0812", "x": 1.954, "y": 5.538, "title": "Med-LEGO: Editing and Adapting Toward Generalist Medical Image Diagnosis", "abstract": "The adoption of visual foundation models has become a common practice in computer-aided diagnosis (CAD). While these foundation models provide a viable solution for creating generalist medical AI, privacy concerns make it difficult to pre-train or continuously update such models across multiple domains and datasets, leading many studies to focus on specialist models. To address this challenge, we propose Med-LEGO, a training-free framework that enables the seamless integration or updating of a generalist CAD model by combining multiple specialist models, similar to assembling LEGO bricks. Med-LEGO enhances LoRA (low-rank adaptation) by incorporating singular value decomposition (SVD) to efficiently capture the domain expertise of each specialist model with minimal additional parameters. By combining these adapted weights through simple operations, Med-LEGO allows for the easy integration or modification of specific diagnostic capabilities without the need for original data or retraining. Finally, the combined model can be further adapted to new diagnostic tasks, making it a versatile generalist model. Our extensive experiments demonstrate that Med-LEGO outperforms existing methods in both cross-domain and in-domain medical tasks while using only 0.18% of full model parameters. These merged models show better convergence and generalization to new tasks, providing an effective path toward generalist medical AI.", "filename": "2025_0812.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Yitao Zhu", "Yuan Yin", "Jiaming Li", "Mengjie Xu", "Zihao Zhao", "Honglin Xiong", "Sheng Wang", "Qian Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0813", "x": 1.329, "y": 4.412, "title": "MedNNS: Supernet-Based Medical Task-Adaptive Neural Network Search", "abstract": "Deep learning (DL) has achieved remarkable progress in the field of medical imaging. However, adapting DL models to medical tasks remains a significant challenge, primarily due to two key factors: (1) architecture selection, as different tasks necessitate specialized model designs, and (2) weight initialization, which directly impacts the convergence speed and final performance of the models. Although transfer learning from ImageNet is a widely adopted strategy, its effectiveness is constrained by the substantial differences between natural and medical images. To address these challenges, we introduce Medical Neural Network Search (MedNNS), the first Neural Network Search framework for medical imaging applications. MedNNS jointly optimizes architecture selection and weight initialization by constructing a meta-space that encodes datasets and models based on how well they perform together. We build this space using a Supernetwork-based approach, expanding the model zoo size by 51× times over previous state-of-the-art (SOTA) methods. Moreover, we introduce rank l oss and Fréchet Inception Distance (FID) loss into the construction of the space to capture inter-model and inter-dataset relationships, thereby achieving more accurate alignment in the meta-space. Experimental results across multiple datasets demonstrate that MedNNS significantly outperforms both ImageNet pretrained DL models and SOTA Neural Architecture Search (NAS) methods, achieving an average accuracy improvement of 1.7% across datasets while converging substantially faster. The code and the processed metaspace is available at https://github.com/BioMedIA-MBZUAI/MedNNS.", "filename": "2025_0813.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Almakky", "Martin Takac", "Mohammad Yaqub"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0814", "x": 1.385, "y": 7.2, "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "abstract": "Medical visual question answering (MedVQA) plays a vital role in clinical decision-making by providing contextually rich answers to image-based queries. Although vision-language models (VLMs) are widely used for this task, they often generate factually incorrect answers. Retrieval-augmented generation addresses this challenge by providing information from external sources, but risks retrieving irrelevant context, which can degrade the reasoning capabilities of VLMs. Re-ranking retrievals, as introduced in existing approaches, enhances retrieval relevance by focusing on query-text alignment. However, these approaches neglect the visual or multimodal context, which is particularly crucial for medical diagnosis. We propose MOTOR, a novel multimodal retrieval and re-ranking approach that leverages grounded captions and optimal transport. It captures the underlying r elationships between the query and the retrieved context based on textual and visual information. Consequently, our approach identifies more clinically relevant contexts to augment the VLM input. Empirical analysis and human expert evaluation demonstrate that MOTOR achieves higher accuracy on MedVQA datasets, outperforming state-of-the-art methods by an average of 6.45%.", "filename": "2025_0814.pdf", "year": 2025, "institution": "University of Artificial In telligence", "country": "UAE", "authors": ["Mai A Shaaban", "Tausifa Jan Saleem", "Vijay Ram Kumar Papineni", "Mohammad Yaqub"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0815", "x": 2.943, "y": 3.837, "title": "One-Shot Active Learning for Vessel Segmentation", "abstract": "Vessel segmentation is crucial for analyzing brain vasculature and understanding cerebral functions and disease mechanisms. Current deep-learning models for segmenting blood vessels within brain images are supervised and depend on extensive labeled data, which requires expert annotation and is both time-consuming and resource-intensive. To address these challenges, we propose Vessel-Dictionary Selection Net (V-DiSNet), a one-shot active learning (OSAL) framework specifically designed for vessels that can be used to select a small, representative set of informative and diverse samples for expert annotation and training, given an unlabeled dataset in a single iteration. The selection process involves sampling from a latent space designed by leveraging the recurrent properties of brain vessel patterns. Specifically, we combine dictionary learning with k-means clustering to learn a latent r epresentation integrating fundamental basis elements representing recurrent vessel features such as shape, connectivity, and structures. We experimentally demonstrate the effectiveness of our method on three publicly available 3D Magnetic Resonance Angiography datasets, showing that V-DisNet consistently outperforms random sampling and other state-of-the-art OSAL methods in terms of standard vessel segmentation metrics. Our code is available at github.com/i-vesseg/V-DiSNet.", "filename": "2025_0815.pdf", "year": 2025, "institution": "EURECOM", "country": "France", "authors": ["Daniele Falcetta", "Hava Chaptoukaev", "Francesco Galati", "Maria A Zuluaga"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0816", "x": 2.634, "y": 6.357, "title": "OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models", "abstract": "Pathology image classification plays a crucial role in accurate medical diagnosis and treatment planning. Training high-performance models for this task typically requires large-scale annotated datasets, which are both expensive and time-consuming to acquire. Active Learning (AL) offers a solution by iteratively selecting the most informative samples for annotation, thereby reducing the labeling effort. However, most AL methods are designed under the assumption of a closed-set scenario, where all the unannotated images belong to target classes. In real-world clinical environments, the unlabeled pool often contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low efficiency of annotation in traditional AL methods. Furthermore, most existing AL methods start with random selection in the first query round, leading to a significant waste of labeling costs in open-set scenarios. To address these challenges, we propose OpenPath, a novel open-set active learning approach for pathological image classification leveraging a pre-trained Vision-Language Model (VLM). In the first query, we propose task-specific prompts that combine target and relevant non-target class prompts to effectively select In-Distribution (ID) and informative samples from the unlabeled pool. In subsequent queries, Diverse Informative ID Sampling (DIS) that includes Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic Sampling (EGSS) is proposed to ensure both purity and informativeness in a query, avoiding the selection of OOD samples. Experiments on two public pathology image datasets show that OpenPath significantly enhances the model's performance due to its high purity of selected samples, and outperforms several state-of-the-art open-set AL methods. The code is available at https://github.com/HiLab-git/OpenPath.", "filename": "2025_0816.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Lanfeng Zhong", "Xin Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0817", "x": 2.159, "y": 6.145, "title": "PATE: Enhancing Few-Shot Pathological Image Classification via Prompt-Based Text-Image Embedding Adaptation", "abstract": "Automated pathological image classification remains a critical challenge, particularly due to the scarcity of annotated data and the complexity of disease-specific features. Existing methods, such as CLIP-based prompt tuning, struggle with limited few-shot learning and poor integration of multimodal information in medical contexts. In this study, we introduce PATE (Prompt-based Adaptation for Text-Image Embedding), a novel framework to enhance CLIP's adaptability for fewshot pathological image classification. Our approach incorporates deep learnable prompts in both vision and language encoders, enabling effective use of visual and textual information. We also propose a dynamic bridging function for bidirectional information exchange and a Gaussianweighted Prompt Integration (GPI) strategy to adjust prompt contributions across epochs, enhancing generalization and reducing overfitting. Extensive experiments on the PatchGastric dataset, which includes 179,285 histopathological patches across three gastric adenocarcinoma subtypes, demonstrate that PATE consistently outperforms state-of-theart methods, achieving superior performance in both low-data and fulldata settings. Ablation studies validate the effectiveness of each component, marking a significant advancement in few-shot medical image analysis, particularly in rare disease diagnosis and digital pathology workflows.", "filename": "2025_0817.pdf", "year": 2025, "institution": "University of Science and Technology of China (USTC)", "country": "China", "authors": ["Shenghao Chen", "Zhen Huang", "Xiaoqian Zhou", "Han Li", "Chunjiang Wang", "Shaohua Kevin Zhou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0818", "x": 2.077, "y": 6.733, "title": "Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions", "abstract": "Millions of melanocytic skin lesions are examined by pathologists each year, the majority of which concern common nevi (i.e., ordinary moles). While most of these lesions can be diagnosed in seconds, writing the corresponding pathology report is much more timeconsuming. Automating part of the report writing could, therefore, alleviate the increasing workload of pathologists. In this work, we develop a vision-language model specifically for the pathology domain of cutaneous melanocytic lesions. The model follows the Contrastive Captioner framework and was trained and evaluated using a melanocytic lesion dataset of 42,512 H&E-stained whole slide images and 19,645 corresponding pathology reports. Our results show t hat the quality scores of model-generated reports were on par with pathologist-written reports for common nevi, assessed by an expert pathologist in a reader study. While report generation revealed to be more difficult for rare melanocytic lesion subtypes, the cross-modal retrieval performance for these cases was considerably better.", "filename": "2025_0818.pdf", "year": 2025, "institution": "University Medical Center Utrecht", "country": "The Netherlands", "authors": ["Ruben T Lucassen", "Sander P J Moonemans", "Tijn Van De Luijtgaarden", "Gerben E Breimer", "Willeke A M Blokx", "Mitko Veta"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0819", "x": 0.063, "y": 5.143, "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images", "abstract": "Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points o n partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.", "filename": "2025_0819.pdf", "year": 2025, "institution": "Huaqiao U niversity", "country": "China", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0820", "x": 4.705, "y": 5.972, "title": "Prototype-Guided Cross-Modal Knowledge Enhancement for Adaptive Survival Prediction", "abstract": "Histo-genomic multimodal survival prediction has garnered growing attention for its remarkable model performance and potential contributions to precision medicine. However, a significant challenge in clinical practice arises when only unimodal data is available, limiting the usability of these advanced multimodal methods. To address this issue, this study proposes a prototype-guided cross-modal knowledge enhancement (ProSurv) framework, which eliminates the dependency on paired data and enables robust learning and adaptive survival prediction. Specifically, we first introduce an intra-modal updating mechanism to construct modality-specific prototype banks that encapsulate the statistics of the whole training set and preserve the modality-specific risk-relevant features/prototypes across intervals. Subsequently, the proposed cross-modal translation module utilizes the learned prototypes to enhance knowledge representation for multimodal inputs and generate features for missing modalities, ensuring robust and adaptive survival prediction across diverse scenarios. Extensive experiments on four public datasets demonstrate the superiority of ProSurv over state-of-theart methods using either unimodal or multimodal input, and the ablation study underscores its feasibility for broad applicability. Overall, this study addresses a critical practical challenge in computational pathology, offering substantial significance and potential impact in the field. Codes are available at https://github.com/cyclexfy/ProSurv.", "filename": "2025_0820.pdf", "year": 2025, "institution": "Harbin Institute of Technology (Shenzhen)", "country": "China", "authors": ["Fengchun Liu", "Linghan Cai", "Zhikang Wang", "Zhiyuan Fan", "Jin-Gang Yu", "Hao Chen", "Yongbing Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0821", "x": 0.924, "y": 6.54, "title": "Radar-Based Imaging for Sign Language Recognition in Medical Communication", "abstract": "Ensuring equitable access to medical communication is crucial for deaf and hard-of-hearing individuals, especially in clinical settings where effective patient-doctor interaction is essential. In this work, we present a novel radar-based imaging framework for Sign Language recognition (with a focus on the Italian Sign Language, LIS), specifically designed for medical communication. Our method leverages 60 GHz mm-wave radar to capture motion features while ensuring anonymity by avoiding the use of personally identifiable visual data. Our approach performs sign language classification through a two-stage pipeline: first, a residual autoencoder processes Range Doppler Maps (RDM) and movingtarget indications (MTI), compressing them into compact latent representations; then, a Transformer-based classifier learns temporal dependencies to recognize signs across varying durations. By relying on radarderived motion imaging, our method not only preserves privacy but also establishes radar as a viable tool for analyzing human motion in medical applications be yond sign language, including neurological disorders and other movement-related conditions. We carried out experiments on a new large-scale dataset containing 126 LIS signs-100 medical terms and 26 alphabet letters. Our method achieves 93.6% accuracy, 87.9% sensitivity, 99.3% specificity, and an 87.7% F1 score, surpassing existing approaches, including an RGB-based baseline. These results underscore the potential of radar imaging for real-time human motion monitoring, paving the way for scalable, privacy-compliant solutions in both sign language recognition and broader clinical applications. The code is available at https://github. com/IngRaffaeleMineo/SignRadarClassification_MICCAI2025 and the dataset will b e released publicly .", "filename": "2025_0821.pdf", "year": 2025, "institution": "University of Catania", "country": "Italy", "authors": ["Raffaele Mineo", "Gaia Caligiore", "Federica Proietto Salanitri", "Isaak Kavasidis", "Senya Polikovsky", "Sabina Fontana", "Egidio Ragonese", "Concetto Spampinato", "Simone Palazzo"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0822", "x": 0.764, "y": 6.701, "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports", "abstract": "Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at https://github.com/AB-Story/RetSTA-7B.", "filename": "2025_0822.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Jiushen Cai", "Weihang Zhang", "Hanruo Liu", "Ningli Wang", "Huiqi Li"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0823", "x": 2.996, "y": 6.336, "title": "Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology", "abstract": "Vision foundation models (FMs) are accelerating the development of digital pathology algorithms and transforming biomedical research. These models learn, in a self-supervised manner, to represent histological features in highly heterogeneous tiles extracted from wholeslide images (WSIs) of real-world patient samples. The performance of these FMs is significantly influenced by the size, diversity, and balance of the pre-training data. Yet, data selection has been primarily guided by expert knowledge at the WSI level, focusing on factors such as disease classification and tissue types, while largely overlooking the granular details available at the tile level. In this paper, we investigate the potential of unsupervised automatic data curation at the tile-level, taking in to account 350 million tiles. Specifically, we apply hierarchical clustering trees to pre-extracted tile embeddings, allowing us to sample balanced datasets uniformly across the embedding space of the pretrained FM. We further show that these datasets are subject to a trade-off between size and balance, potentially compromising the quality of representations learned by FMs. We propose tailored batch sampling strategies", "filename": "2025_0823.pdf", "year": 2025, "institution": "ETH Zurich", "country": "Switzerland", "authors": ["Boqi Chen", "Cédric Vincent-Cuaz", "Lydia A Schoenpflug", "Manuel Madeira", "Lisa Fournier", "Vaishnavi Subramanian", "Sonali Andani", "Samuel Ruiperez-Campillo", "Julia E Vogt", "Raphaëlle Luisier", "Dorina Thanou", "Viktor H Koelzer", "Pascal Frossard", "Gabriele Campanella", "Gunnar Rätsch"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0824", "x": 3.471, "y": 6.577, "title": "Robust Sensitivity Control in Digital Pathology via Tile Score Distribution Matching", "abstract": "Deploying digital pathology models across medical centers is challenging due to distribution shifts. Recent advances in domain generalization improve model transferability in terms of aggregated performance measured by the Area Under Curve (AUC). However, clinical regulations often require to control the transferability of other metrics, such as prescribed sensitivity levels. We introduce a novel approach to control the sensitivity of whole slide image (WSI) classification models, based on optimal transport and Multiple Instance Learning (MIL). Validated across multiple cohorts and tasks, our method enables robust sensitivity control with only a handful of calibration samples, providing a practical solution for reliable deployment of computational pathology systems.", "filename": "2025_0824.pdf", "year": 2025, "institution": "Owkin, Inc", "country": "USA", "authors": ["Arthur Pignet", "John Klein", "Geneviève Robin", "Antoine Olivier"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0825", "x": 5.409, "y": 4.238, "title": "Robust Sleep Stage Prediction from Electroencephalogram with Label Noise Using Multimodal Large Language Models", "abstract": "Sleep stage prediction is a critical task in medical diagnostics, such as for sleep disorders like Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS). Traditionally, this task involves analyzing Electroencephalogram (EEG) signals and classifying the stages based on general features, often relying on medical expertise. However, this process is prone to bias and variance, as clinicians incorporate subjective experience into their predictions. In recent years, multimodal large language models (MLLMs) have demonstrated significant advancements, particularly in medical applications, outperforming traditional methods in many domains. Despite their promising potential, MLLMs are sensitive to high memorization effects and require high-quality, well-labeled data for fine-tuning. Label noise, commonly present in real-world datasets, can severely hinder their performance and robustness. Consequently, directly applying MLLMs to sleep stage prediction using noisy EEG labels presents a challenge. In this paper, we introduce a novel framework for sleep stage prediction using EEG data under label noise, leveraging the p ower of MLLMs. Our approach integrates multi-perspective agreement techniques to identify high-quality samples based on the prior knowledge embedded in MLLMs. We then employ a self-training method to enhance prediction accuracy despite the presence of label noise. We validate our framework using real patient EEG data in sleep stage prediction tasks, and the results demonstrate that our approach is both robust and accurate under label noise, outperforming other state-of-theart robust learning methods. Our code will be made publicly available at https://github.com/Leonard-zc/MICCAI2025-RSSP.", "filename": "2025_0825.pdf", "year": 2025, "institution": "Shanghai University of Engineering Science", "country": "China", "authors": ["Xihe Qiu", "Chen Zhan", "Gengchen Ma", "Jingjing Huang", "Xiaoyu Tan"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0826", "x": -0.067, "y": 4.748, "title": "SAM2-ProMem: Enhancing Zero-Shot 3D Segmentation with Stochastic Propagation and Memory Search", "abstract": "Although the SAM2 foundational segmentation model excels in natural images, its direct adaptation to 3D medical imaging (e.g., CT/MR) remains underexplored, particularly for zero-shot generalization. We identify two critical barriers when treating medical volumes as pseudo-video sequences: (1) the non-convexity of anatomical structures leading to slice-wise mask discontinuities; (2) difficulty in effectively generalizing the dependencies between long-term and short-term memory.To address these problems, we propose a stochastic connected component propagation strategy for handling mask discontinuities during training, coupled with a dynamic memory window search mechanism during inference. Extensive experiments demonstrate the effectiveness of our method, achieving a 16% Dice score improvement over conventional fine-tuning in the unseen classes of TotalSegmentator dataset. Furthermore, our approach generalizes well across modalities (CT/MR) and lesion types, and it performs comparably to or outperforms previous methods on the ULS23 and CHAOS benchmarks.", "filename": "2025_0826.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Yujie Wang", "Juntao Huang", "Dazhu Liang", "Fangzhou Liao", "Jie Chen", "Boan Chen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0827", "x": 2.133, "y": 5.082, "title": "Selective Alignment Transfer for Domain Adaptation in Skin Lesion Analysis", "abstract": "Domain adaptation is crucial for deep learning in skin lesion analysis because models trained on dermoscopic images often struggle to generalise to clinical images, which exhibit variations in lighting, resolution, and background conditions. We propose Selective Alignment Transfer for Domain Adaptation (SAT-DA), a fully supervised framework that significantly reduces this domain gap by dynamically assigning feature importance weights based on statistical moments from both source and target domains. SAT-DA emphasises domain-invariant features and suppresses domain-specific noise to preserve crucial diagnostic cues. Our multi-loss strategy combines classification, alignment, and diversity losses to optimise feature selection and prevent feature collapse onto a narrow set. SAT-DA was evaluated on six public datasets comprising dermoscopic and clinical images and consistently outperformed state-of-the-art supervised and unsupervised methods. On Derm7pt-Derm to Derm7pt-Clinic, SAT-DA a chieves 82.46% AUROC, surpassing the strongest baseline by over 6%. Notably, SAT-DA also maintains high performance on completely unseen datasets not used as source or target, demonstrating robust cross-domain generalisation. Overall, these results highlight SAT-DA's ability to address practical clinical deployment challenges, offering a reliable, fully supervised solution for cross-domain skin lesion analysis. The complete implementation of the SAT-DA method is available at our GitHub repository.", "filename": "2025_0827.pdf", "year": 2025, "institution": "Manchester Metropolitan University", "country": "UK", "authors": ["Nurjahan Sultana", "Wenqi Lu", "Xinqi Fan", "Moi Hoon Yap"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0828", "x": 0.717, "y": 6.58, "title": "Semantic-Aware Chest X-ray Report Generation with Domain-Specific Lexicon and Diversity-Controlled Retrieval", "abstract": "Image-to-text radiology report generation aims to produce comprehensive diagnostic reports by leveraging both X-ray images and historical textual data. Existing retrieval-based methods focus on maximizing similarity scores, leading to redundant content and limited diversity in generated reports. Additionally, they lack sensitivity to medical domain-specific information, failing to emphasize critical anatomical structures and disease characteristics essential for accurate diagnosis. To address these limitations, we propose a novel retrieval-augmented framework that integrates exemplar radiology reports with X-ray images to enhance report generation. First, we introduce a diversity-controlled retrieval strategy to improve information diversity and reduce redundancy, ensuring broader clinical knowledge coverage. Second, we develop a comprehensive medical lexicon covering chest anatomy, diseases, radiological descriptors, treatments, and related concepts. This lexicon is integrated into a weight ed cross-entropy loss function to improve the model's sensitivity to critical medical terms. Third, we introduce a sentencelevel semantic loss to enhance clinical semantic accuracy. Evaluated on the MIMIC-CXR dataset, our method achieves superior performance on clinical consistency metrics and competitive results on linguistic quality metrics, demonstrating its effectiveness in enhancing report accuracy and clinical relevance. The code is publicly available at github.com/DrLS.", "filename": "2025_0828.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Baochang Zhang", "Chen Jia", "Shuting Liu", "Heribert Schunkert", "Nassir Navab"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0829", "x": 1.947, "y": 4.726, "title": "Single Image Test-Time Adaptation via Multi-View Co-Training", "abstract": "Test-time adaptation enables a trained model to adjust to a new domain during inference, making it particularly valuable in clinical settings where such on-the-fly adaptation is required. However, existing techniques depend on large target domain datasets, which are often impractical and unavailable in medical scenarios that demand perpatient, real-time inference. Moreover, current methods commonly focus on two-dimensional images, failing to leverage the volumetric richness of medical imaging data. Bridging this gap, we propose a Patch-Based Multi-View Co-Training method for Single Image Test-Time adaptation. Our method enforces feature and prediction consistency through uncertainty-guided self-training, enabling effective volumetric segmentation in the target domain with only a single test-time image. Validated on three publicly available breast magnetic resonance imaging datasets for tumor segmentation, our method achieves performance close to the upper bound supervised benchmark while also outperforming all existing state-of-the-art methods, on average by a Dice Similarity Coefficient of 3.75%. We will publicly share our accessible codebase, readily integrable with the popular nnUNet framework, at https://github.com/ smriti-joshi/muvi.git.", "filename": "2025_0829.pdf", "year": 2025, "institution": "Universitat de Barcelona", "country": "Spain", "authors": ["Smriti Joshi", "Richard Osuala", "Lidia Garrucho", "Kaisar Kushibar", "Dimitri Kessler", "Oliver Diaz", "Karim Lekadir"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0830", "x": 1.977, "y": 4.641, "title": "TEGDA: Test-Time Evaluation-Guided Dynamic Adaptation for Medical Image Segmentation", "abstract": "Distribution shifts of medical images seriously limit the performance of segmentation models when applied in real-world scenarios. Test-Time Adaptation (TTA) has emerged as a promising solution for ensuring robustness on images from different institutions by tuning the parameters at test time without additional labeled training data. However, existing TTA methods are limited by unreliable supervision due to a lack of effective methods to monitor the adaptation performance without ground-truth, which makes it hard to adaptively adjust model parameters in the stream of testing samples. To address these limitations, we propose a novel Test-Time Evaluation-Guided Dynamic Adaptation (TEGDA) framework for TTA of segmentation models. In the absence of ground-truth, we propose a novel prediction quality evaluation metric based on Agreement with Dropout Inferences calibrated by Confidence (ADIC). Then it is used to guide adaptive feature fusion with those in a feature bank with high ADIC values to obtain refined predictions for supervision, which is combined with an ADIC-adaptive teacher model and loss weighting for robust adaptation. Experimental results on multidomain cardiac structure and brain tumor segmentation demonstrate that our ADIC can accurately estimate segmentation quality on the fly, and our TEGDA obtained the highest average Dice and lowest average HD95, significantly outperforming several state-of-the-art TTA methods. The code is available at https://github.com/HiLab-git/TEGDA.", "filename": "2025_0830.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Yubo Zhou", "Jianghao Wu", "Wenjun Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0831", "x": 0.671, "y": 4.667, "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration", "abstract": "Deep learning has achieved great success in medical image segmentation and computer-aided diagnosis, with many advanced methods reaching state-of-the-art performance in brain tumor segmentation from MRI. While studies in other medical domains show that integrating textual reports with images can enhance segmentation, there is no comprehensive brain tumor dataset pairing radiological images with textual annotations. This gap has limited the development of multimodal approaches. To address this, we introduce TextBraTS, the first publicly available, volume-level multimodal dataset with paired MRI volumes and textual annotations, derived from the BraTS2020 benchmark. Based on this dataset, w e propose a baseline framework and a sequential crossattention method for text-guided volumetric segmentation. Extensive experiments with various text-image fusion strategies and templated text demonstrate clear improvements in segmentation accuracy and provide insights into effective multimodal integration. The dataset and model are available at https://github.com/Jupitern52/TextBraTS.", "filename": "2025_0831.pdf", "year": 2025, "institution": "Ritsumeikan University", "country": "Japan", "authors": ["Xiaoyu Shi", "Rahul Kumar Jain", "Yinhao Li", "Ruibo Hou", "Jingliang Cheng", "Jie Bai", "Guohua Zhao", "Lanfen Lin", "Rui Xu", "Yen-Wei Chen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0832", "x": 3.291, "y": 6.493, "title": "Text-Guided Multi-instance Learning for Scoliosis Screening via Gait Video Analysis", "abstract": "Early-stage scoliosis is often difficult to detect, particularly in adolescents, where delayed diagnosis can lead to serious health issues. Traditional X-ray-based methods carry radiation risks and rely heavily on clinical expertise, limiting their use in large-scale screenings. To overcome these challenges, we propose a Text-Guided Multi-Instance Learning Network (TG-MILNet) for non-invasive scoliosis detection using gait videos. To handle temporal misalignment in gait sequences, we employ Dynamic Time Warping (DTW) clustering to segment videos into key gait phases. To focus on the most relevant diagnostic features, we introduce an Inter-Bag Temporal Attention (IBTA) mechanism that highlights critical gait phases. Recognizing the difficulty in identifying borderline cases, we design a Boundary-Aware Model (BAM) to improve sensitivity to subtle spinal deviations. Additionally, we incorporate textual guidance from domain experts and large language models (LLM) to enhance feature representation and improve model interpretability. Experiments on the large-scale Scoliosis1K gait dataset show that TG-MILNet achieves state-of-the-art performance, particularly excelling in handling class imbalance and accurately detecting challenging borderline cases. The code is available at https://github.com/lhqqq/TG-MILNet.", "filename": "2025_0832.pdf", "year": 2025, "institution": "The University of T exas at Arlington", "country": "USA", "authors": ["Haiqing Li", "Yuzhi Guo", "Feng Jiang", "Thao M Dang", "Hehuan Ma", "Qifeng Zhou", "Jean Gao", "Junzhou Huang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0833", "x": 4.279, "y": 5.911, "title": "Tumor Microenvironment-Guided Fine-Tuning of Pathology Foundation Models for Esophageal Squamous Cell Carcinoma Immunotherapy Response Prediction", "abstract": "Esophageal squamous cell carcinoma (ESCC) has high incidence and mortality rates. While immunotherapy shows promise for some ESCC patients, others can experience severe side effects. Accurate prescreening of individual patients' immunotherapy response to ESCC is a crucial but difficult task. Subtle differences in pre-treatment biomarkers hinder physicians' judgment in pathological diagnosis. While pathological foundation models (PFMs) have shown potential in pathology image analysis, traditional PFMs focused on image-level features still struggle to capture nuanced preoperative characteristic differences. To address this, we propose a fine-tuning framework for PFMs based on the tumor microenvironment (TME). First, morphological and topological attributes are extracted from larger field-of-view patches to better analyze TME interactions. Next, we utilize PFMs which are typically constrained to small inputs to extract image features. To address this limitation, larger patches are subdivided to prevent p recision loss, with trainable position encodings maintaining relative spatial positional relationships to guide the re-aggregation of large patch-level representations. Finally, a TME-guided learning algorithm trains all trainable layers to understand ESCC-specific characteristics. Our framework demonstrates superior performance in the downstream task of predicting ESCC immunotherapy response compared to those fine-tuned using self-supervised learning methods. By allowing flexibility in patch sizes, our approach captures more contextual information. Code is available at https://github.com/stoney03/ESCC.", "filename": "2025_0833.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Yixuan Lin", "Weiping Lin", "Chenxu Guo", "Xinxin Yang", "Hongxue Meng", "Liansheng Wang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0834", "x": -0.13, "y": 4.996, "title": "Unleashing SAM for Few-Shot Medical Image Segmentation with Dual-Encoder and Automated Prompting", "abstract": "Deep learning has made significant progress in natural image segmentation but faces challenges in medical imaging due to the limited availability of annotated data. Few-shot learning offers a solution by enabling segmentation with only a few labeled samples, yet generalization remains a challenge when data is scarce. In this work, we investigate the potential of the Segment Anything Model (SAM), a foundation model trained on over one billion annotated images, for few-shot medical image segmentation. However, SAM faces two key challenges: (1) the domain gap between natural and medical images, leading to suboptimal performance, and (2) prompt dependency, as SAM requires userdefined prompts, limiting automation. To address these issues, we propose a novel framework, named AM-SAM, that adapts SAM for few-shot medical image segmen tation. Our approach introduces a medical imagespecific augmentation strategy and a dual-encoder architecture to bridge the domain gap. Additionally, we develop an automated dual-prompt mechanism to eliminate prompt dependency, generating point and mask prompts from support images. Extensive experiments show that AM-SAM outperforms existing approaches by up to 3.8% on ABD-MRI and 4.0% on ABD-30 in terms of dice score metric.", "filename": "2025_0834.pdf", "year": 2025, "institution": "Hanoi University of Science and Tec hnology", "country": "Vietnam", "authors": ["Cuong M Pham", "Phi Le Nguyen", "Thanh Trung Nguyen", "Vu Minh Hieu Phan", "Binh P Nguyen"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0835", "x": 3.728, "y": 3.991, "title": "ViTAL-CT: Vision Transformers for High-Risk Plaque Classification in Coronary CTA", "abstract": "High-risk plaque (HRP) detected by coronary CT angiography (CTA) is associated with increased risks of major adverse cardiovascular events such as heart attack. Current identification of HRP characteristics involves labor-intensive segmentation of plaques, requiring substantial time and expert knowledge. In this work, we propose a novel coronary cross-sectional Vision Transformer (ViT) framework that bypasses the need for explicit segmentation by directly predicting the presence of HRP. Our approach extracts cross-sectional slices along the coronary centerline, ensuring that the model focuses on the artery. By leveraging the standard patch-based input of ViT, we capture not only the coronary cross-section itself but also surrounding contextual information (e.g., adipose tissue). Furthermore, we incorporate multiple levels of detail by combining the cross-sections from proximal and distal p ositions with their corresponding CTA axial planes, forming a comprehensive cross-sectional representation. We also embedded the actual 3D position of each cross-section into the positional encoding of the Transformer to enhance spatial awareness. Experimental results of 3,068 coronary arteries demonstrate that our method outperforms conventional approaches, highlighting its potential to optimize clinical decision-making in the care of coronary artery diseases. The code is available at https://github.com/ JZCambridge/ViTAL-CT-MICCAI25.A. Le and J. Z heng-Equal contribution.", "filename": "2025_0835.pdf", "year": 2025, "institution": "University of Cambridge", "country": "UK", "authors": ["Anjie Le", "Jin Zheng", "Tan Gong", "Quanlin Sun", "Jonathan Weir-Mccall", "Declan P O’regan", "Michelle C Williams", "David E Newby", "James H F Rudd", "Yuan Huang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0836", "x": 3.908, "y": 4.274, "title": "A Multimodal Contrastive Learning for Detecting Aortic Dissection on 3D Non-contrast CT with Anatomy Simplification", "abstract": "Accurate detection of aortic dissection (AD) in emergency settings is of significant importance, as misdiagnosis can significantly delay subsequent treatments and even endanger patients' lives. Currently, non-contrast CT scans are standard protocols in emergency departments for patients with chest pain, yet their ability to detect AD remains limited. We introduce a novel multimodal contrastive learning framework designed to learn discriminative features from both contrastenhanced CT and corresponding diagnostic reports. These features are then aligned with non-contrast CT scans through a multimodal contrastive learning approach. Specifically, we first segment and straighten the aorta to effectively apply attention to the aortic area. Finally, the pre-trained encoder is fine-tuned for the tasks of AD detection and lumen segmentation using non-contrast CT scans. Our experiments, conducted on a test dataset comprising 239 subjects (127 with AD and 112 without), demonstrated that the proposed framework achieves an accuracy of 0.958, an F1-score of 0.969, and an AUC of 0.983 in AD detection. These results surpass those of six state-of-the-art classification models. In lumen segmentation experiments, the framework achieves an average DSC of 0.705, outperforming others. These findings indicate that our proposed framework not only outperforms existing AD detection methods but also holds the potential to accurately localize false lumen using non-contrast CT scans alone.", "filename": "2025_0836.pdf", "year": 2025, "institution": "Zhejiang Lab", "country": "China", "authors": ["Duoer Zhang", "Wenbo Xiao", "Chen Jiang", "Yuxuan Qiu", "Zhan Feng", "Hong Wang", "Yefeng Zheng", "Wentao Zhu"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0837", "x": 7.033, "y": 4.857, "title": "Adaptive Adversarial Data Augmentation with Trajectory Constraint for Alzheimer’s Disease Conversion Prediction", "abstract": "Distinguishing progressive mild cognitive impairment (pMCI) from stable MCI (sMCI) is crucial for timely treatment of Alzheimer's disease (AD), yet it is challenging due to inherent class imbalance and limited data. While recent data synthesis methods have shown successful results, they often disregard distributional differences between groups and individual heterogeneity in disease progression. Also, they treat the whole-brain as a unified entity, overlooking region-specific features despite their varying associations with AD. To address this, we propose a novel end-to-end framework that augments MCI data and predicts their future conversion to AD. This is realized by using adversarial attacks that directly control data points in the feature space considering group differences. The attacks are adaptively applied with region-wise learnable attack intensities and subject-specific attack steps, which are flexibly adjusted based on each subject's observation interval. Moreover, we introduce a trajectory constraint that ensures the attacked (i.e., augmented) data follow plausible disease progressions and preserve realistic neurodegeneration patterns. Extensive validations on two AD biomarkers across three classifiers show our method's superiority over six baselines.", "filename": "2025_0837.pdf", "year": 2025, "institution": "Pohang University of Science and Technology (POSTECH)", "country": "South Korea", "authors": ["Hyuna Cho", "Hayoung Ahn", "Guorong Wu", "Won Hwa Kim"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0838", "x": 2.859, "y": 5.802, "title": "Adaptive Stain Normalization for Cross-Domain Medical Histology", "abstract": "Deep learning advances have revolutionized automated digital pathology analysis. However, differences in staining protocols and imaging conditions can introduce significant color variability. In deep learning, such color inconsistency often reduces performance when deploying models on data acquired under different conditions from the training data, a challenge known as domain shift. Many existing methods attempt to address this problem via color normalization but suffer from several notable drawbacks such as introducing artifacts or requiring careful choice of a template image for stain mapping. To address these limitations, we propose a trainable color normalization model that can be integrated with any backbone network for downstream tasks such as object detection and classification. Based on the physics of the imaging process per the Beer-Lambert law, our model architecture is derived via algorithmic unrolling of a nonnegative matrix factorization (NMF) model to extract stain-invariant structural information from the original pathology images, which serves as input for further processing. Experimentally, we evaluate the method on publicly available pathology datasets and an internally curated collection of malaria blood smears for cross-domain object detection and classification, where our method outperforms many state-of-the-art stain normalization methods. Our code is available at https://github.com/xutianyue/BeerLaNet.", "filename": "2025_0838.pdf", "year": 2025, "institution": "Johns Hopkins Univ ersity", "country": "USA", "authors": ["Tianyue Xu", "Yanlin Wu", "Abhai K Tripathi", "Matthew M Ippolito", "Benjamin D Haeffele"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0839", "x": 1.201, "y": 4.644, "title": "Addressing Label Scarcity and Domain Shift in Medical Image Segmentation", "abstract": "Limited labeled data and domain shifts present significant challenges for accurate medical image segmentation. Semi-supervised learning (SSL) and unsupervised domain adaptation (UDA) methods address these challenges individually. Existing SSL methods do not perform well in UDA scenarios, and vice versa. We observe that excelling in SSL requires effective learning from limited labeled data while avoiding overfitting, whereas in UDA, the domain gap must be effectively reduced. To design a novel unified framework that tackles both the scarcity of labeled data and domain shift, it is essential to address both objectives. To accomplish this, we introduce Wavelet Frequency Exchange (WFE), which decomposes encoder features into low and high-frequency components and exchanges high-frequency features between labeled and unlabeled data. WFE provides two key benefits: it disrupts overfitting by preventing the model from memorizing details from limited labeled data in SSL, and it reduces the domain gap in UDA. To improve the representation of exchanged features, we propose a Learnable Parametric Feature Network (LPFN), which includes downsampling and upsampling blocks. These blocks include Parametric Spline (PS) layers, which map the relationships between the exchanged features using a spline function. Evaluations on two publicly available medical datasets demonstrate the effectiveness of our method.", "filename": "2025_0839.pdf", "year": 2025, "institution": "Indian Institute of Technology", "country": "India", "authors": ["Suruchi Kumari", "Pravendra Singh"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0840", "x": 3.842, "y": 6.813, "title": "AEM: Attention Entropy Maximization for Multiple Instance Learning Based Whole Slide Image Classification", "abstract": "Multiple Instance Learning (MIL) effectively analyzes whole slide images but faces overfitting due to attention over-concentration. While existing solutions rely on complex architectural modifications or additional processing steps, we introduce Attention Entropy Maximization (AEM), a simple yet effective regularization technique. Our investigation reveals the positive correlation between attention entropy and model performance. Building on this insight, we integrate AEM regularization into the MIL framework to penalize excessive attention concentration. To address sensitivity to the AEM weight parameter, we implement Cosine Weight Annealing, reducing parameter dependency. Extensive evaluations demonstrate AEM's superior performance across diverse feature extractors, MIL frameworks, attention mechanisms, and augmentation techniques. Here is our anonymous code: https://github. com/dazhangyu123/AEM", "filename": "2025_0840.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Yunlong Zhang", "Honglin Li", "Yuxuan Sun", "Zhongyi Shui", "Jingxiong Li", "Chenglu Zhu", "Lin Yang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0841", "x": 3.197, "y": 5.507, "title": "Automated Detection of Abnormalities in Zebrafish Development", "abstract": "Zebrafish embryos are a valuable model for drug discovery due to their optical transparency and genetic similarity to humans. However, current evaluations rely on manual inspection, which is costly and labor-intensive. While machine learning offers automation potential, progress is limited by the lack of comprehensive datasets. To address this, we introduce a large-scale dataset of high-resolution microscopic image sequences capturing zebrafish embryonic development under both control conditions and exposure to compounds (3,4-dichloroaniline). This dataset, with expert annotations at fine-grained temporal levels, supports two benchmarking tasks: (1) fertility classification, assessing zebrafish egg viability (130,368 images), and (2) toxicity assessment, detecting malformations induced by toxic exposure over time (55,296 images). Alongside the d ataset, we present the first transformer-based baseline model that integrates spatiotemporal features to predict developmental abnormalities at early stages. Experimental results present the model's effectiveness, achieving 98% accuracy in fertility classification and 92% in toxicity assessment. These findings underscore the potential of automated approaches to enhance zebrafish-based toxicity analysis.", "filename": "2025_0841.pdf", "year": 2025, "institution": "CISPA Helmholtz Center for Information Security", "country": "Germany", "authors": ["Sarath Sivaprasad", "Hui-Po Wang", "Anna-Lisa Jäckel", "Jonas Baumann", "Carole Baumann", "Jennifer Herrmann", "Mario Fritz"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0842", "x": 2.441, "y": 5.758, "title": "Automatic Dataset Shift Identification to Support Safe Deployment of Medical Imaging AI", "abstract": "Shifts in data distribution can substantially harm the performance of clinical AI models and lead to misdiagnosis. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and the choice of shift mitigation strategies highly depends on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework, effectively distinguishing between prevalence shift , covariate shift and mixed shifts. We show the effectiveness of the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts, using five large publicly available datasets. Code is publicly available at https:// github.com/biomedia-mira/shift_identification.", "filename": "2025_0842.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Mélanie Roschewitz", "Raghav Mehta", "Charles Jones", "Ben Glocker"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0843", "x": 1.473, "y": 7.155, "title": "BaMCo: Balanced Multimodal Contrastive Learning for Knowledge-Driven Medical VQA", "abstract": "Medical Visual Question Answering enables large language models to answer questions related to clinical images. While domainspecific LLMs are capable of strong reasoning, their development can be costly. In contrast, general-purpose models are more efficient, but often lack deep understanding. Previous research has shown that integrating external knowledge enhances the performance of general-purpose LLMs, particularly for questions that involve complex medical terminology. To improve the utilization of external knowledge, we introduce a novel multimodal knowledge space pretraining method trained with the proposed Balanced Multimodal Contrastive Learning Loss. Our approach optimizes knowledge spaces through balanced contrastive learning across modalities, together with the auxiliary classification task. Additionally, we developed a novel framework to improve knowledge-driven Medical VQA for LLMs by integrating the pretrained knowledge space. Experiments on the Slake, VQA-RAD, and PathVQA datasets demonstrate that our approach outperforms state-of-the-art Medical VQA methods with an average accuracy of 85.8%, 76.7%, and 60.0%, respectively. The source code is available at https://github.com/yaziciz/BaMCo.", "filename": "2025_0843.pdf", "year": 2025, "institution": "Istanbul Technical University", "country": "Türkiye", "authors": ["Ziya Ata Yazici", "Hazim Kemal Ekenel"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0844", "x": 2.138, "y": 6.926, "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models", "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of demonstration examples affects VLM performance in two medical imaging tasks: skin lesion malignancy prediction and pneumothorax detection from chest radiographs. Our analysis reveals that ICL influences model predictions through multiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease base rates from prompts and (2) ICL leads VLMs to make predictions that perform differently across demographic groups, ev en after controlling for subgroup-specific disease base rates. Our empirical results inform best-practices for prompting current VLMs (specifically examining demographic subgroup performance, and matching base rates of labels to target distribution at a bulk level and within subgroups), while also suggesting next steps for improving our theoretical understanding of these models https://github.com/DaneshjouLab/BiasICL.", "filename": "2025_0844.pdf", "year": 2025, "institution": "Stanford University", "country": "USA", "authors": ["Sonnet Xu", "Joseph D Janizek", "Yixing Jiang", "Roxana Daneshjou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0845", "x": 6.133, "y": 3.684, "title": "BrainAlign: EEG-Vision Alignment via Frequency-Aware Temporal Encoder and Differentiable Cluster Assigner", "abstract": "While understanding visual processing in the human brain is fundamental for computational neuroscience, decoding objects from electroencephalography (EEG) remains challenging due to noisy neural dynamics during rapid image presentation and semantic misalignment in zero-shot settings. We propose BrainAlign, a novel framework leveraging contrastive learning to align EEG features with visual-language models (VLM). Our approach addresses three fundamental challenges: (1) We introduce a Frequency-Aware Temporal Encoder (FATE) using real Fast Fourier Transform with tunable bandpass filters to compress noisy signals while preserving temporal fidelity. (2) We develop a Differentiable Cluster Assigner (DCA) that dynamically optimizes channel grouping through cross-attention mechanisms, adaptively suppressing noise and enhancing task-relevant features. (3) We implement a self-supervised framework aligning EEG features with VLMs through contrastive learning. Extensive experiments demonstrate state-of-the-art performance on large-scale datasets, improving zero-shot retrieval accuracy by 5.85% and classification by 3.3%. Our work establishes new possibilities for brain-computer interfaces.", "filename": "2025_0845.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Enze Shi", "Huawen Hu", "Qilong Yuan", "Kui Zhao", "Sigang Yu", "Shu Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0846", "x": 1.023, "y": 1.352, "title": "Contour Makes it Stronger: Cross-Domain Cephalometric Landmark Detection Based on Contour Priors", "abstract": "The detection of cephalometric landmarks is crucial for orthodontic diagnosis. Current methods mainly focus on utilizing contextual information to detect landmarks while overlooking the challenges posed by domain gaps. In this paper, we propose a contour-guided framework that leverages cranial soft/hard tissue contours as domaininvariant anatomical priors. The method introduces a joint attention module to fuse the topological features corresponding to the contours with contextual features, ensuring the accuracy of landmark positioning. Additionally, we address anisotropic prediction uncertainty in unseen domains through a direction-aware regression module, which incorporates contour geometry to regularize error distributions. Evaluated on the multi-domain datasets with five source and three unseen target domains, our framework demonstrates superior robustness to domain shifts while maintaining anatomical plausibility, achieving state-of-theart cross-domain localization accuracy.", "filename": "2025_0846.pdf", "year": 2025, "institution": "Shandong University", "country": "China", "authors": ["Xinyue Liang", "Runnan Chen", "Guangshun Wei", "Shaojie Zhuang", "Yuanfeng Zhou"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0847", "x": 1.813, "y": 6.24, "title": "CXR-CML: Improved Zero-Shot Classification of Long-Tailed Multi-label Diseases in Chest X-Rays", "abstract": "Chest radiography (CXR) plays a crucial role in the diagnosis of various diseases. However, the inherent class imbalance in the distribution of clinical findings presents a significant challenge for current self-supervised deep learning models. These models often fail to accurately classify long-tailed classes. Current Vision-Language models such as Contrastive Language Image Pre-training (CLIP) models effectively model the manifold distribution of the latent space, enabling high zero-shot classification accuracies. Although CLIP performs well on most of the primary classes in the dataset, our work reveals that its effectiveness decreases significantly for classes with a long-tailed distribution. Our approach employs a class-weighting mechanism that directly aligns with the distribution of classes within the latent space. This method ensures a substantial improvement in overall classification performance, with particular emphasis on enhancing the recognition and accuracy of rarely observed classes. W e accomplish this by applying Gaussian Mixture Model (GMM) clustering to the latent space. The subsequent clusters are further refined by Student t-distribution, followed by a metric loss that utilizes the altered embeddings. Our approach facilitates stable and adaptive clustering of the features. This results in a notable average improvement of 7% points in zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from previous SOTA models.", "filename": "2025_0847.pdf", "year": 2025, "institution": "Friedrich-Alexander University", "country": "Germany", "authors": ["Rajesh Madhipati", "Sheethal Bhat", "Lukas Buess", "Andreas Maier"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0848", "x": 3.249, "y": 5.282, "title": "Deep Learning Framework for Managing Inter-reader Variability in Background Parenchymal Enhancement Classification for Contrast-Enhanced Mammography", "abstract": "Background parenchymal enhancement (BPE) classification for contrast-enhanced mammography (CEM) is highly affected by interreader variability. Traditional approaches aggregate expert annotations into a single consensus label to minimize individual subjectivity. By contrast, we propose a two-stage deep learning framework that explicitly models inter-reader variability through self-trained, reader-specific embeddings. In the first stage, the model learns discriminative image features while associating each reader with a dedicated embedding that captures their annotation signature, enabling personalized BPE classification. In the second stage, these embeddings can be calibrated using a small set of CEM cases selected through active learning and annotated by either a new reader or a consensus standard. This calibration process allows the model to adapt to new annotation st yles with minimal supervision and without extensive retraining. This work leverages a multi-site CEM dataset of 7,734 images, non-exhaustively annotated by several readers. Calibrating reader-specific embeddings using a set of 40 cases offers an average accuracy of 73.5%, outperforming the proposed baseline method based on reader consensus. This approach enhances robustness and generalization in clinical environments characterized by heterogeneous labeling patterns.", "filename": "2025_0848.pdf", "year": 2025, "institution": "GE HealthCare", "country": "France", "authors": ["Elodie Ripaud", "Clément Jailin", "Pablo Milioni De Carvalho", "Laurence Vancamberg", "Isabelle Bloch"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0849", "x": 0.698, "y": 6.772, "title": "Dia-LLaMA: Towards Large Language Model-Driven CT Report Generation", "abstract": "Medical report generation has made notable progress, but most studies focus on chest X-rays, leaving CT report generation largely underexplored. This task poses unique challenges, including sparse diseased regions due to high-dimensional volumes, imbalanced distributions of normal and abnormal samples leading to biased predictions, and excessive template sentences that may obscure critical findings. Recently, large language models (LLMs) have demonstrated strong instructionfollowing capabilities, producing reliable outputs when guided by welldesigned prompts, which provides a promising approach to address these issues. To this end, we propose Dia-LLaMA, a framework adapted from LLaMA2-7B for CT report generation with diagnostic guidance prompts. To enhance the focus on diseased areas, we introduce a disease-aware attention module to capture disease-specific information. Furthermore, we propose a disease prototype memory bank to capture common disease patterns, providing a reliable reference during diagnosis. Experiments on a large-scale chest CT report dataset demonstrated that our method outperforms previous approaches, achieving state-of-the-art results in both clinical efficacy and natural language generation metrics. The code is available at https://github.com/zhi-xuan-chen/Dia-LLaMA", "filename": "2025_0849.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Zhixuan Chen", "Luyang Luo", "Yequan Bie", "Hao Chen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0850", "x": 0.697, "y": 6.779, "title": "Diff-RRG: Longitudinal Disease-Wise Patch Difference as Guidance for LLM-Based Radiology Report Generation", "abstract": "Radiology report generation (RRG) is an emerging field that aims to automatically generate free-text clinical descriptions of radiographic images, incorporating temporal disease progression. However, existing methods rely on coarse-grained image representations and lack explicit mechanisms to integrate patients' historical information. To address these limitations, we propose a novel framework Diff-RRG that introduces longitudinal disease-wise patch Diff erence as guidance for large language model (LLM)-based Radiology Report Generation, aligning with the real-world diagnostic process. Our approach extracts disease-wise difference maps to identify fine-grained patches associated with specific diseases and to capture the difference between consecutive radiographs. Such information is fed into the LLM to pro vide direct guidance on disease progression. Accordingly, the resulting generated reports can be explained by pinpointing the related regions in the image, thereby enhancing explainability. In the extensive experiments, we have achieved state-of-the-art performance in most of the natural language generation and clinical efficacy metrics on the Longitudinal-MIMIC dataset. Our code is available at https://github.com/ku-milab/Diff-RRG.", "filename": "2025_0850.pdf", "year": 2025, "institution": "Korea University", "country": "Republic of Korea", "authors": ["Hannah Yun", "Junyeong Maeng", "Eunsong Kang", "Heung-Il Suk"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0851", "x": 3.097, "y": 6.359, "title": "Distilling Foundation Models for Robust and Efficient Models in Digital Pathology", "abstract": "In recent years, the advent of foundation models (FM) for digital pathology has relied heavily on scaling the pre-training datasets and the model size, yielding large and powerful models. While it resulted in improving the performance on diverse downstream tasks, it also introduced increased computational cost and inference time. In this work, we explore the distillation of a large foundation model into a smaller one, reducing the number of parameters by several orders of magnitude. Leveraging distillation techniques, our distilled model, H0mini, achieves comparable performance to large FMs at a significantly reduced inference cost on HEST and EVA public benchmarks. Additionally, we conduct robustness analyses on the PLISM-WSI dataset and a multi-scanner, multi-staining private breast cancer cohort. We demonstrate that our distilled model reaches excellent robustness to variations in staining and scanning conditions, significantly outperforming other state-of-the-art models. This opens new perspectives to design lightweight and robust models for digital pathology, without compromising on performance. We publicly release H0-mini (https://huggingface. co/bioptimus/H0-mini) along with plismbench (Available at https:// github.com/owkin/plism-benchmark), the first robustness benchmark of pathology foundation models based on the PLISM dataset.", "filename": "2025_0851.pdf", "year": 2025, "institution": "Owkin, Inc", "country": "USA", "authors": ["Alexandre Filiot", "Nicolas Dop", "Oussama Tchita", "Auriane Riou", "Rémy Dubois", "Thomas Peeters", "Daria Valter", "Marin Scalbert", "Charlie Saillard", "Geneviève Robin", "Antoine Olivier"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0852", "x": 2.258, "y": 5.258, "title": "Domain Generalization for Mammogram Classification by Suppressing Domain-Specific Features", "abstract": "Mammogram is the gold standard for early breast cancer screening, and its integration with deep learning-based computer-aided diagnosis (CAD) models has demonstrated significant advantages in improving the accuracy of breast cancer diagnosis. However, due to differences in mammography acquisition protocols and scanner models, significant inter-domain variations exist in images obtained from different mammography devices. As deep learning models tend to overfit to domain-specific feature representations during training, models trained on source domain often experience notable performance degradation when applied to cross-domain data, hindering their deployment in dynamic clinical settings. Therefore, this paper proposes a novel domain generalization approach for mammogram classification by suppressing domain-specific features (MC-SDS). MC-SDS first employs an adaptive channel filter to identify and drop channels that have a tendency to capture domain-specific features to suppress domain-specific features. Then, by perturbing the low-frequency components, the model is encouraged to learn from the high-frequency parts, further suppressing the domainspecific features present in the low-frequency components. Experiments conducted on a public dataset and two internal datasets demonstrate that MC-SDS outperforms other benchmark methods.", "filename": "2025_0852.pdf", "year": 2025, "institution": "China University of Petroleum (East China)", "country": "China", "authors": ["Jiqun Chen", "Luhao Sun", "Wenzong Jiang", "Weifeng Liu", "Chao Li", "Zhiyong Yu", "Baodi Liu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0853", "x": 6.896, "y": 4.819, "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "abstract": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that imposes a significant burden on public health. It shares clinical similarities with Alzheimer's disease (AD), as both progress through stages of normal cognition, mild cognitive impairment, and dementia. A major obstacle in LBD diagnosis is data scarcity, which limits the effectiveness of deep learning. In contrast, AD datasets are more abundant, offering potential for knowledge transfer. However, LBD and AD data are typically collected from different sites using different machines and protocols, resulting in a distinct domain shift. To effectively leverage AD data while mitigating domain shift, we propose a Transferability Aware Transformer (TAT) that adapts knowledge from AD to enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived from structural MRI as training data. Built on the attent ion mechanism, TAT adaptively assigns greater weights to disease-transferable features while suppressing domain-specific ones, thereby reducing domain shift and improving diagnostic accuracy with limited LBD data. The experimental results demonstrate the effectiveness of TAT. To the best of our knowledge, this is the first study to explore domain adaptation from AD to LBD under conditions of data scarcity and domain shift, providing a promising framework for domainadaptive diagnosis of rare diseases.", "filename": "2025_0853.pdf", "year": 2025, "institution": "University of Texas at Arlington", "country": "USA", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0854", "x": 1.733, "y": 4.482, "title": "DPGS-Net: Dual Prior-Guided Cross-Domain Adaptive Framework for Ultrasound Image Segmentation", "abstract": "Ultrasound image segmentation plays a critical role in medical-assisted diagnosis but suffers from inherent limitations, such as high noise, artifacts, and morphological diversity. Existing methods struggle to generalize with small-sample data due to feature contradictions from varying acquisition angles, limiting multi-center clinical use. To address these issues, we propose a dual prior-guided two-stage segmentation framework. In the first stage, the prior classification of small-sample data guides domain adaptation pretraining on large-scale datasets, employing dynamic class balancing to mitigate data distribution bias. The second stage features a multi-level feature fusion architecture with three core modules: First, we design a Multi-branch Convolutional Parallel Attention (MCPA) module that extracts contextual features via parallel dual attention to adaptively select multi-scale features.Next, we propose a Multi-scale Fusion Dilated Convolution (MFDC) module that enhances the encoder's capability to capture lesion boundaries across different receptive fields through hierarchical dilated convolutions. Finally, we introduce an Enhanced Feature Decoding module (EFD) in the decoder, em bedding a cross-layer compensation mechanism using shallow high-resolution features to recover spatial details lost. Furthermore, we propose an interactive dual-stream architecture that bridges prior-guided classification and segmentation tasks, where complementary features are fused through cross-task attention to optimize holistic semantic consistency and robustness. Experiments on the public dataset demonstrate our method's superiority over mainstream approaches. Ablation studies validate the effectiveness of our method, providing a solution for high-precision, high-availability small-sample ultrasound image segmentation. Code is on Github: https://github.com/ notchXie/DPGS-Net.", "filename": "2025_0854.pdf", "year": 2025, "institution": "Guangdong University of Foreign Studies", "country": "China", "authors": ["Weijie Zhang", "Lingfeng Xie", "Kun Zeng", "Xiaonan Luo", "Yongyi Gong"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0855", "x": 1.399, "y": 6.684, "title": "DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning", "abstract": "Medical image captioning via vision-language models has shown promising potential for clinical diagnosis assistance. However, generating contextually relevant descriptions with accurate modality recognition remains challenging. We present DualPrompt-MedCap, a novel dual-prompt enhancement framework that augments Large Vision-Language Models (LVLMs) through two specialized components: (1) a modality-aware prompt derived from a semi-supervised classification model pre-trained on medical question-answer pairs, and (2) a questionguided prompt leveraging biomedical language model embeddings. To address the lack of captioning ground truth, we also propose an evaluation framework that jointly considers spatial-semantic relevance and medical narrative quality. Experiments on multiple m edical datasets demonstrate that DualPrompt-MedCap outperforms the baseline BLIP-3 by achieving a 22% improvement in modality recognition accuracy while generating more comprehensive and question-aligned descriptions.Our method enables the generation of clinically accurate reports that can serve as medical experts' prior knowledge and automatic annotations for downstream vision-language tasks.", "filename": "2025_0855.pdf", "year": 2025, "institution": "University of Technology Sydney", "country": "Australia", "authors": ["Yining Zhao", "Mukesh Prasad", "Ali Braytee"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0856", "x": 0.836, "y": 6.547, "title": "Enhancing Radiology Report Interpretation through Modality-Specific RadGraph Fine-Tuning", "abstract": "Radiology reports contain free-form text that conveys critical clinical information derived from imaging studies and patient history. However, the unstructured nature of these reports, coupled with the complexity and ambiguity of natural language, poses significant challenges for automated information extraction, particularly in domains with limited labeled data. To address this, we introduce a novel expert-annotated dataset encompassing four new imaging modalities: cardiac magnetic resonance imaging (MRI), abdominal ultrasound, head computerized tomography (CT), and CT pulmonary angiography (CTPA). Leveraging this dataset, we developed transformer-based models optimized for entity recognition and relation extraction within specific modalities, enabling the generation of high-quality radiology annotations. Our evaluation of fine-tuning methods demonstrate that modality-specific models achieve a 12.5% macro F1 score improvement i n entity recognition and a 28.3% improvement on relation extraction tasks compared to prior approaches. These findings highlight the potential of fine-tuned, modality-specific models in enhancing automated radiology text processing and downstream applications. By releasing the model and datasets, we aim to foster research on wider modalities in medical natural language processing across a broader range of imaging modalities. The code is available at https://github.com/tonikroos7/RadGraph-Multimodality.", "filename": "2025_0856.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Haoyue Guan", "Yuwei Dai", "Shadi Afyouni", "Alec Kain", "Wen-Chi Hsu", "Jiashu Cheng", "Sophie Yao", "Yuli Wang", "Jing Wu", "Rishitha Pulakhandam", "Lin-Mei Zhao", "Chengzhang Zhu", "Zhicheng Jiao", "Craig Jones", "Harrison Bai"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0857", "x": 2.267, "y": 6.961, "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "abstract": "Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into realworld clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs' predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup lev el prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. The codebase can be found at https:// github.com/xingbpshen/medical-calibration-fairness-mllm.", "filename": "2025_0857.pdf", "year": 2025, "institution": "McGill Universit y", "country": "Canada", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0858", "x": 1.824, "y": 6.744, "title": "Few-Shot, Now for Real: Medical VLMs Adaptation Without Balanced Sets or Validation", "abstract": "Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of highperforming solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyperparameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios. Code is available: https://github.com/ jusiro/SS-Text.", "filename": "2025_0858.pdf", "year": 2025, "institution": "ÉTS Montréal", "country": "Canada", "authors": ["Julio Silva-Rodríguez", "Fereshteh Shakeri", "Houda Bahig", "Jose Dolz", "Ismail Ben Ayed"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0859", "x": -0.204, "y": 5.118, "title": "FluoroSAM: A Language-Promptable Foundation Model for Flexible X-Ray Image Segmentation", "abstract": "Language promptable X-ray image segmentation would enable greater flexibility for human-in-the-loop workflows in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving problems within a narrow scope, but expanding to broader use requires additional data, annotations, and training time. Recently, language-aligned foundation models (LFMs)machine learning models trained on large amounts of highly variable image and text data thus enabling broad applicability -have emerged as promising tools for automated image analysis. Existing foundation models for medical image analysis focus on scenarios and modalities where large, richly annotated datasets are available. However, the X-ray imaging modality features highly variable image appearance and applications, from diagnostic chest X-rays to interventional fluoroscopy, with varying availability of data. To pave the way toward an LFM for comprehensive and language-aligned analysis of arbitrary medical X-ray images, we introduce FluoroSAM, a language-promptable variant of the Segment-Anything Model, trained from scratch on 3M synthetic X-ray images from a wide variety of human anatomies, imaging geometries, and viewing angles. These include pseudo-ground truth masks for 128 organ types and 464 tools with associated text descriptions. FluoroSAM is capable of segmenting myriad anatomical structures and tools based on natural language prompts, thanks to the novel incorporation of vector quantization (VQ) of text embeddings in the training process. We demonstrate Fluo-roSAM's performance quantitatively on real X-ray images and showcase on several applications how FluoroSAM is a key enabler for rich humanmachine interaction in the X-ray image acquisition and analysis context. Information on data, weights, and code is available at https://github. com/arcadelab/fluorosam. Keywords: radiology • fluoroscopy • medical imaging AI • multimodal foundation model • segment any thing • machine learning • deep learning", "filename": "2025_0859.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Benjamin D Killeen", "Liam J Wang", "Blanca Iñígo", "Han Zhang", "Mehran Armand", "Russell H Taylor", "Greg Osgood", "Mathias Unberath"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0860", "x": 4.607, "y": 5.278, "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-parametric MRI", "abstract": "Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is essential for effective glioma management. Traditional methods rely on invasive tissue sampling, which may fail to capture a tumor's spatial heterogeneity. While deep learning models have shown promise in molecular profiling, their performance is often limited by scarce annotated data. In contrast, foundation deep learning models offer a more generalizable approach for glioma imaging biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch signals associated with IDH mutation. The model was trained and validated on a diverse, multi-center cohort of 1,705 glioma patients from six public datasets. Our model achieved AUCs of 90.58% ± 1.25, 88.08% ± 3.08, 65.41% ± 3.35, and 80.31% ± 1.09 on independent test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming baseline approaches (p ≤0.05). Ablation studies confirmed that both the TAFE and CMD modules are essential for improving predictive accuracy. By integrating large-scale pretraining and taskspecific fine-tuning, FoundBioNet enables generalizable glioma characterization. This approach enhances diagnostic accuracy and interpretability, with the potential to enable more personalized patient care.", "filename": "2025_0860.pdf", "year": 2025, "institution": "Tehran University of Medical Sciences", "country": "Iran", "authors": ["Somayeh Farahani", "Marjaneh Hejazi", "Antonio Di Ieva", "Sidong Liu"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0861", "x": 2.183, "y": 5.406, "title": "General Methods Make Great Domain-Specific Foundation Models: A Case-Study on Fetal Ultrasound", "abstract": "With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transferlearning from an existing generalist model? And, if a custom model is pretrained, are novel methods required? In this paper we explore these questions by conducting a case-study, in which we train a foundation model on a large regional fetal ultrasound dataset of 2M images. By selecting the well-established DINOv2 method for pretraining, we achieve stateof-the-art results on three fetal ultrasound datasets, covering data from different countries, classification, segmentation, and few-shot tasks. We compare against a series of models pretrained on natural images, ultrasound images, and supervised baselines. Our results demonstrate two key insights: (i) Pretraining on custom data is worth it, even if smaller models are trained on less data, as scaling in natural image pretraining does not translate to ultrasound performance. (ii) Well-tuned methods from computer vision are making it feasible to train custom foundation models for a given medical domain, requiring no hyperparameter tuning and little methodological adaptation. Given these findings, we argue that a bias towards methodological innovation should be avoided when developing domain specific foundation models under common computational resource constraints. (Code available at: https://github.com/jakobamb/ UltraDINO. Model weights are available given p ermission to access pretraining data.", "filename": "2025_0861.pdf", "year": 2025, "institution": "University of Copenhagen", "country": "Denmark", "authors": ["Jakob Ambsdorf", "Asbjørn Munk", "Sebastian Llambias", "Anders N Christensen", "Kamil Mikolaj", "Randall Balestriero", "Martin G Tolsgaard", "Aasa Feragen", "Mads Nielsen"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0862", "x": 2.898, "y": 4.391, "title": "Improving OCTA Imaging Through Cross-Domain Adaptation: A Noise-Guided Framework Using Intralipid-Enhanced Rat Data", "abstract": "Deep learning has been introduced into optical coherence tomography angiography (OCTA) imaging, which is a non-invasive technique for visualizing vascular structures. Intralipid injection has shown promise in improving blood cell scattering for better OCTA imaging. However, administering intralipid to human subjects for imaging purposes may raise ethical concerns. To address this challenge, we acquire intralipid-enhanced OCTA in rats and introduce cross-domain learning to address the domain shifts. Specifically, we collect data from eyes of anesthetized rats to obtain motion-free data and introduce a noise-guided self-training framework to bridge the domain gaps between rats and primates. Additionally, an en face enhancement loss is incorporated to further refine en face vectors during adaptation. Compared with other classical and fully supervised OCTA imaging algorithms, our method improves B-scan denoising performance by 53.1% and 65.0% on CNR and BRISQUE in human subjects respectively, while enhancing vessel contrast in en face images.", "filename": "2025_0862.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Bingyu Yang", "Bingyao Tan", "Zaiwang Gu", "Leopold Schmetterer", "Huiqi Li", "Jun Cheng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0863", "x": 2.138, "y": 6.345, "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "abstract": "Reliable out-of-distribution (OOD) detection is important for safe deployment of deep learning models in fetal ultrasound amidst heterogeneous image characteristics and clinical settings. OOD detection relies on estimating a classification model's uncertainty, which should increase for OOD samples. While existing research has largely focused on uncertainty quantification methods, this work investigates the impact of the classification task itself. Through experiments with eight uncertainty quantification methods across four classification tasks on the same image dataset, we demonstrate that OOD detection performance significantly varies with the task, and that the best task depends on the defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an image characteristic shift or ii) an anatomical feature shift. Furthermore, we reveal that superior OOD detection does not guarantee optimal abstained prediction, underscoring the necessity to align task selection and uncertainty strategies with the specific downstream application in medical image analysis. Code: https://github.com/wong-ck/ ood-fetal-us.", "filename": "2025_0863.pdf", "year": 2025, "institution": "Technical University of Denmark", "country": "Denmark", "authors": ["Chun Kit Wong", "Anders N Christensen", "Cosmin I Bercea", "Julia A Schnabel", "Martin G Tolsgaard", "Aasa Feragen"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0864", "x": 2.726, "y": 3.859, "title": "ISAC: Redefining the Vascular Segmentation Paradigm Through Mask Completion for Cross-Domain Generalization", "abstract": "Accurate vessel segmentation is critical for diagnosis. However, the annotation of vascular images cost a lot, and due to their diverse modalities and complex foreground structures, it is hard for learningbased methods to reduce annotation cost by training models of high domain generalization (DG) on partial modalities. To address this, we propose the Image-Sparse Annotation Completion (ISAC) segmentation model, which reformulates vascular segmentation as a mask completion task based on sparse-annotated supports. ISAC treats the segmentation task as incomplete mask reconstruction guided by image features and structural properties of the foreground in the sparse mask. Unlike pixel-wise classification, ISAC detects vessels according to the mask context supported regions, in which way the anatomical continuity of vascular foreground is improved. Additionally, to f urther avoid the reliance on high-cost manually annotated supports, we propose the Uncertaintyguided Patch Selection (UPS) module to extract high-quality supports from coarse pseudo labels, which enables ISAC to perform segmentation in zero-shot scenarios. Experiments on 7 vascular datasets across 3 modalities demonstrate that ISAC outperforms state-of-the-art methods in DG ability. The code is publicly available at https://github.com/ Architect15806/ISAC.", "filename": "2025_0864.pdf", "year": 2025, "institution": "Huazhong University of Science and Technology", "country": "China", "authors": ["Tianyu Zhao", "Zihang Huang", "Xixi Jiang", "Liang Zhang", "Xiaohuan Ding", "Xin Yang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0865", "x": 1.053, "y": 6.091, "title": "Longitudinal Anatomical Attention Maps for Recognizing Diagnostic Errors from Radiologists’ Eye Movements", "abstract": "With the rise in respiratory diseases, the workload on radiologists is increasing, leading to a higher risk of diagnostic errors. One approach to improve diagnostic processes is to reduce the frequency of cognitive and perceptual errors made by humans. This study aims to predict radiologists' diagnostic errors while interpreting chest X-rays using eyetracking technology. We propose a novel method that combines human attention, derived from the locations of gaze fixation points, with attention from transformer neural networks. The resulting attention maps are combined with the segmentation of anatomical structures, including the lungs, clavicles, hila, heart, mediastinum, and esophagus, which restricts the analysis for regions potentially relevant for thoracic disease diagnosis. Attention maps are computed for each gaze fixation point, creating a longitudinal path representing the X-ray reading pro cess. Finally, we applied Gated Recurrent Units (GRUs) to learn from the longitudinal attention maps and statistical gaze features to predict potential X-ray diagnostic errors. The proposed methodology was validated on 4, 000 chest X-ray readings performed by four radiologists. The model achieved an error detection accuracy of 0.79, measured as the area under the receiver operating characteristic (ROC) curve. The code is available at https://github.com/annshorn/TEGRU.", "filename": "2025_0865.pdf", "year": 2025, "institution": "University of C openhagen", "country": "Denmark", "authors": ["Anna Anikina", "Diliara Ibragimova", "Tamerlan Mustafaev", "Claudia Mello-Thoms", "Bulat Ibragimov"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0866", "x": 0.092, "y": 4.99, "title": "MAUP: Training-Free Multi-center Adaptive Uncertainty-Aware Prompting for Cross-Domain Few-Shot Medical Image Segmentation", "abstract": "Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations:(1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-a ware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: https:// github.com/YazhouZhu19/MAUP.", "filename": "2025_0866.pdf", "year": 2025, "institution": "Nanjing University of Science and Technology", "country": "China", "authors": ["Yazhou Zhu", "Haofeng Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0867", "x": 1.378, "y": 7.121, "title": "MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning", "abstract": "Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised finetuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.", "filename": "2025_0867.pdf", "year": 2025, "institution": "Technical University of Munich (TUM)", "country": "UK", "authors": ["Jiazhen Pan", "Che Liu", "Junde Wu", "Fenglin Liu", "Jiayuan Zhu", "Hongwei Bran Li", "Chen Chen", "Cheng Ouyang", "Daniel Rueckert"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0868", "x": 1.161, "y": 6.7, "title": "More Performant and Scalable: Rethinking Contrastive Vision-Language Pre-training of Radiology in the LLM Era", "abstract": "The emergence of Large Language Models (LLMs) presents unprecedented opportunities to revolutionize medical contrastive visionlanguage pre-training. In this paper, we show how LLMs can facilitate large-scale supervised pre-training, thereby advancing vision-language alignment. We begin by demonstrating that modern LLMs can automatically extract diagnostic labels from radiology reports with remarkable precision (>96% AUC in our experiments) without complex prompt engineering, enabling the creation of large-scale \"silver-standard\" datasets at a minimal cost ( $3 for 50k CT image-report pairs). Further, we find that vision encoders trained on this \"silver-standard\" dataset achieve performance comparable to those trained on labels extracted by specialized BERT-based models, thereby democratizing the access to large-scale supervised pre-training. Building on this foundation, we proceed to reveal that supervised pre-training fundamentally improves contrastive vision-language alignment. Our approach achieves state-ofthe-art performance using only a 3D ResNet-18 with vanilla CLIP training, including 83.8% AUC for zero-shot diagnosis on CT-RATE, 77.3% AUC on RAD-ChestCT, and substantial improvements in crossmodal retrieval (MAP@50=53.7% for image-image, Recall@100=52.2% for report-image). These results demonstrate the potential of utilizing LLMs to facilitate more performant and scalable medical AI systems. Our code is available at https://github.com/SigmaLDC/More- performant-and-scalable.", "filename": "2025_0868.pdf", "year": 2025, "institution": "University of Science and T echnology of China (USTC)", "country": "China", "authors": ["Yingtai Li", "Haoran Lai", "Xiaoqian Zhou", "Shuai Ming", "Wenxin Ma", "Wei Wei", "Shaohua Kevin Zhou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0869", "x": 4.584, "y": 3.661, "title": "Multi-agent Collaboration for Integrating Echocardiography Expertise in Multi-modal Large Language Models", "abstract": "Current echocardiography MLLMs rely on diagnosticfocused data lacking detailed image-text descriptions and systematic multi-modal cardiac knowledge, resulting in suboptimal performance across diverse echocardiography visual question answering tasks. Existing methods to integrate clinical expertise face three key challenges when adapting to echocardiography: labor-intensive curation processes, overlooking textual or diagrammatic knowledge sources essential in cardiac diagnosis, and incompatibility with pretrained MLLMs. To address these gaps, we propose Multi-Agent Collaborative Expertise Extractor (MACEE), a multi-agent framework employing MLLM-powered agents to extract echocardiography expertise from diverse sources. MACEE collects the EchoCardiography Expertise Database (ECED), the first comprehensive knowledge repository covering 100+ common and rare cardiac conditions from textbooks, guidelines, and case studies. To integrate ECED into MLLMs, we introduce Echocardiography Expertiseenhanced Visual Instruction Tuning (EEVIT), a lightweight training framework using expertise-guided instruction tuning. EEVIT employs adapters in vision and language modules, enabling efficient expertise integration while training less than 1% of the model's parameters. Experiments validate the effectiveness of these three components. Codes and license details: https://github.com/xmed-lab/ECED.", "filename": "2025_0869.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology", "country": "China", "authors": ["Yi Qin", "Dinusara Sasindu Gamage Nanayakkara", "Xiaomeng Li"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0870", "x": 6.843, "y": 3.74, "title": "Multi-subject Orthogonal Sparse Matrix Decomposition Method for Extracting Individual Brain Functional Networks", "abstract": "Brain functional network (FN) extraction is fundamental to advancing our understanding of brain function, providing critical insights into the neural mechanisms underlying cognition and behavior. Data-driven FN analysis methods have been developed to analyze functional magnetic resonance imaging (fMRI) data. However, to ensure cross-subject correspondence, group-level analyses of these methods sacrifice subject-specific variation. This trade-off between grouplevel alignment and subject-specific discrepancies hinders the accurate characterization of individual brain FNs. In this study, we propose a multi-subject orthogonal sparse matrix decomposition method without the need for group-level analysis, which simultaneously extracts both group-level FNs and individual FNs with cross-subject correspondence. We introduce a novel quasi-orthogonality constraint that enhances the linear independence of FNs, ensuring effective extraction of FNs, while enabling precise control over FN spatial scale. Additionally, by further incorporating a sparsity constraint, our method effectively minimizes spatial overlap between FNs, resulting in sparse representations. For simulated datasets, our method outperforms comparison methods, supporting its low parameter sensitivity and superior ability to extract FNs and time courses. Application to multi-site fMRI datasets, comprising 233 healthy controls (HCs) and 205 schizophrenia patients (SZs), validates the reproducibility of FNs extracted by our method. The results underscore the method's ability to preserve both cross-subject correspondence and individual variability. Overall, our method advances fMRI analytic capabilities by reconciling population-level consistency with individualized neural signatures, offering enhanced discriminative power for investigating neuropsychiatric disorder mechanisms and brain function.", "filename": "2025_0870.pdf", "year": 2025, "institution": "Shanxi University", "country": "China", "authors": ["Xingyu He", "Vince D Calhoun", "Theo G M Van Erp", "Yuhui Du"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0871", "x": 1.78, "y": 6.351, "title": "OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-adjustment", "abstract": "Contrastive Language-Image Pre-Training (CLIP) based models enable zero-shot classification in radiology but often struggle with detecting normal cases due to rigid intra-sample alignment, which leads to poor feature clustering and increased false positive and false negative rates. We propose OFF-CLIP, a simple and effective refinement that introduces an off-diagonal loss term to promote the clustering of normal samples explicitly. In addition, it applies sentence-level filtering to remove typical normal phrases embedded within abnormal reports. OFF-CLIP does not require architectural changes and does not compromise abnormal classification performance. In the VinDr-CXR dataset, normal classification shows a notable 0.61 AUC improvement over the state-ofthe-art baseline CARZero. It also improves zero-shot grounding performance by increasing pointing game accuracy and providing more reliable and precise anomaly localization. These results clearly demonstrate that OFF-CLIP serves as an efficient plug-and-play enhancement to existing medical vision-language models. The code and pre-trained models are publicly available at https://github.com/Junhyun-Park01/OFF-CLIP.", "filename": "2025_0871.pdf", "year": 2025, "institution": "DGIST", "country": "Republic of Korea", "authors": ["Junhyun Park", "Chanyu Moon", "Donghwan Lee", "Kyungsu Kim", "Minho Hwang"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0872", "x": -0.387, "y": 4.659, "title": "OralSAM: One-Shot Segmentation for Intraoral Ultrasound Videos with Adaptive Feature Correlation and Self-prompting Strategy", "abstract": "Periodontal disease is a leading cause of tooth loss and is linked to systemic conditions such as endocarditis, diabetes, cardiovascular disease, and osteoporosis. Intraoral ultrasound (IUS) videos offer a non-invasive means for diagnosing periodontal structures, but existing segmentation methods rely on extensive manual annotations. We propose OralSAM, a one-shot video segmentation network inspired by the Segment Anything Model (SAM), which requires annotation from only a single frame. Our network integrates an adaptive feature correlation module to capture temporal dependencies and refine segmentation consistency across frames. Additionally, we introduce a self-prompting strategy based on optical flow, dynamically adjusting point prompts based on motion cues in consecutive frames to improve segmentation accuracy. To further enhance robustness, we incorporate a self-correction mechanism that refines mask embeddings adaptively, reducing propagation errors in intermediate frames. The combination of these components ensures effective generalization t o unseen anatomical structures and improves temporal coherence in IUS videos. We evaluate OralSAM on both IUS and public datasets, demonstrating superior performance over state-of-theart methods. Unlike conventional methods, our approach significantly reduces annotation effort while maintaining high segmentation accuracy. Our approach provides a scalable solution for real-time clinical applications, enabling more efficient and accurate periodontal disease assessment. Code is available at https://github.com/BioMedCom/OralSAM. Logiraj Kumaralingam and Anparasy Siv aanpu shared first authorship.", "filename": "2025_0872.pdf", "year": 2025, "institution": "University of A lberta", "country": "Canada", "authors": ["Logiraj Kumaralingam", "Anparasy Sivaanpu", "Manh-Hai Hoang", "Javaneh Alavi", "Kim-Cuong T. Nguyen", "Kumaradevan Punithakumar", "Edmond H M Lou", "Paul Major", "Lawrence H Le"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0873", "x": 2.147, "y": 6.435, "title": "Out-of-Distribution Nuclei Segmentation in Histology Imaging via Liquid Neural Networks with Modern Hopfield Layer", "abstract": "Precise nuclei segmentation is crucial in histopathology but remains challenging due to variable tissue types, staining protocols, and imaging conditions. Traditional deep neural networks often perform inconsistently when presented with data distributions not seen during training. Existing approaches typically process multi-scale features sequentially but lack mechanisms to explicitly enforce robustness against distribution shifts. To address this, we propose a novel framework that integrates hierarchical feature learning with associative memory to enhance model adaptability. First, we extract multi-layer embeddings from an image encoder, then process them in reverse layer order (from deepest to shallowest) using Liquid Neural Network (LNN). This design allows the model to capture global features initially and then refine them with increasingly localized information. The image encoding and the LNN e ncoding is then concatenated in hidden space and passed through Hopfield layer that stabilizes and stores relevant patterns. This effectively enhances domain-invariant representations by filtering out spurious correlations. Our OOD experiments on nuclei segmentation benchmark datasets show that our approach achieves average improvement of 16.35% over baseline models. Our code will be released at https://github. com/CVPR-KIT/OOD-Nuclei-Segmentation-via-LNNs-with-MHN.", "filename": "2025_0873.pdf", "year": 2025, "institution": "Kumoh National Institute of Technology", "country": "Korea", "authors": ["Bishal Ranjan Swain", "Kyung Joo Cheoi", "Jaepil Ko"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0874", "x": 2.971, "y": 6.313, "title": "PathoCellBench: A Comprehensive Benchmark for Cell Phenotyping", "abstract": "Digital pathology has seen the advent of a wealth of foundational models (FMs), yet to date their performance on cell phenotyping has not been benchmarked in a unified manner. We therefore propose PathoCellBench: A comprehensive benchmark for cell phenotyping on Hematoxylin and Eosin (H&E) stained histopathology images. We provide both PathoCell , a new H&E dataset featuring 14 cell types identified via multiplexed imaging, and ready-to-use fine-tuning and benchmarking code that allows the systematic evaluation of multiple prominent pathology FMs in terms of dense cell phenotype predictions in a range of generalization scenarios. We perform extensive benchmarking of existing FMs, providing insights into their generalization behavior under technical vs. medical domain shifts. Furthermore, while FMs achieve macro F1 scores > 0.70 on previously established benchmarks such as Lizard and PanNuke, on PathoCell , we observe scores as low as 0.20. This indicates a much more challenging task not captured by previous benchmarks, establishing PathoCell as a prime asset for future benchmarking of FMs and supervised models alike. Code and data are available on GitHub .", "filename": "2025_0874.pdf", "year": 2025, "institution": "Helmholtz Imaging", "country": "Germany", "authors": ["Jérôme Lüscher", "Nora Koreuber", "Jannik Franzen", "Fabian H Reith", "Claudia Winklmayr", "Elias Baumann", "Christian M Schürch", "Dagmar Kainmüller", "Josef Lorenz Rumberger"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0875", "x": 2.994, "y": 6.064, "title": "Pathology-Aware Virtual H&E Staining of Section-Free Thick Tissues with Semantic Contrastive Guidance", "abstract": "The conventional histopathology paradigm, while remaining the gold standard for clinical diagnosis, is inherently constrained by its lengthy processing time. The emergence of virtual staining in computational histopathology has catalyzed significant research efforts toward developing rapid and chemical-free staining techniques. However, current methodologies are primarily applicable to well-prepared thin tissue sections and lack the capability to effectively process the section-free thick tissues. In this work, we present a novel approach that utilizes fluorescence light-sheet microscopy to directly image thick tissue samples, followed by image translation to generate virtually stained hematoxylin and eosin (H&E) images. To overcome the insufficient exploration of pathological features in current methods, we introduce Semantic Contrastive Guidance (SemCG), which enforces morphological consistency between fluorescence inputs and H&E outputs. Additionally, we incorporate subtype-aware classification to enhance the discriminator's ability to learn domain-specific pathological knowledge. Experimental results demonstrate that our proposed modules offer an advantage in generating high-quality images. We anticipate that this sectioning-free virtual staining framework will have significant potential for clinical rapid pathology applications, offering a transformative improvement to current histological workflows. Our code is available at https:// github.com/commashy/SemCG-Stain.", "filename": "2025_0875.pdf", "year": 2025, "institution": "The Hong Kong University of S cience and Technology", "country": "China", "authors": ["Jintaek Oh", "Lulin Shi", "Terence T W Wong"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0876", "x": 2.484, "y": 6.351, "title": "PathoPrompt: Cross-Granular Semantic Alignment for Medical Pathology Vision-Language Models", "abstract": "Pre-trained visual-language (V-L) models have demonstrated impressive generalization capabilities on various downstream tasks, yet their performance is significantly influenced by input text prompts. Previous studies, such as CoPrompt, have attempted to use detailed descriptions generated by large language models to assist model learning. For example, while a coarse-grained prompt like “A photo of debris.” may be less informative, a fine-grained description such as “Debris consists of dead cells and matrix fragments.” provides additional context, resulting in enhanced model performance. However, existing methods generally lack sensitivity to capture subtle semantic differences that are crucial for accurately classifying pathology images. To address this challenge, we introduce PathoPrompt, a framework that leverages cross-granular semantic alignment to improve sensitivity and refine the model’s ability to capture subtle semantic variations in pathology image classification. Specifically, we introduce token-level fine-grained alignment, enabling the model to capture subtle differences that are critical for accurate pathology image classification. Furthermore, cross-granular semantic distillation improves generalization by filtering out irrelevant information from both coarse- and fine-grained prompts. In addition, PathoPrompt employs a prototype-based cross-modal separation mechanism, promoting distinct class boundaries by separating image and text semantics for more effective multimodal representation learning. Experiments on five pathology datasets across three task types demonstrate that the proposed method achieves superior performance compared to existing approaches.", "filename": "2025_0876.pdf", "year": 2025, "institution": "Beijing Normal-Hong Kong Baptist University", "country": "China", "authors": ["Runlin Huang", "Haohui Liang", "Hongmin Cai", "Weipeng Zhuo", "Wentao Fan", "Weifeng Su"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0877", "x": 0.725, "y": 6.788, "title": "Phrase-Grounded Fact-Checking for Automatically Generated Chest X-Ray Reports", "abstract": "With the emergence of large-scale vision language models (VLM), it is now possible to produce realistic-looking radiology reports for chest X-ray images. However, their clinical translation has been hampered by the factual errors and hallucinations in the produced descriptions during inference. In this paper, we present a novel phrase-grounded fact-checking model (FC model) that detects errors in findings and their indicated locations in automatically generated chest radiology reports. Specifically, we simulate the errors in reports through a large synthetic dataset derived by perturbing findings and their locations in ground truth reports to form real and fake findings-location pairs with images. A new multi-label cross-modal contrastive regression network is then trained on this dataset. We present results demonstrating the robustness of our method in terms of accuracy of finding veracity prediction and localization on multiple X-ray datasets. We also show its effectiveness for error detection in reports of SOTA report generators on multiple datasets achieving a concordance correlation coefficient of 0.997 with ground truth-based verification, thus pointing to its utility during clinical inference in radiology workflows.", "filename": "2025_0877.pdf", "year": 2025, "institution": "Rensselaer Polytechnic Institute", "country": "USA", "authors": ["Razi Mahmood", "Diego Machado-Reyes", "Joy Wu", "Parisa Kaviani", "Ken C L Wong", "Niharika D’souza", "Mannudeep Kalra", "Ge Wang", "Pingkun Yan", "Tanveer Syeda-Mahmood"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0878", "x": 2.96, "y": 5.791, "title": "Physics-Guided Deep Image Prior Network for General Zero-Shot Stain Deconvolution", "abstract": "Many barriers remain before the clinical translation and deployment of prognostic and predictive models utilizing deep learning in digital pathology. In particular, models need to be generalizable to widespread variations in image characteristics resulting from differences in slide preparation protocols and inter-scanner variability. Yet, most existing stain deconvolution methods that correct for the variability in image appearances were developed and validated on specific datasets and perform poorly on unseen data. We developed Physics-Guided Deep Image Prior network for Stain deconvolution (PGDIPS), a self-supervised method guided by a novel optical physics model to perform zero-shot s tain deconvolution and normalization. PGDIPS outperformed state-of-the-art approaches for the deconvolution of conventional stain combinations, enabled analysis of previously unsupported special stains, and provided superior interpretability by explicitly encoding representations for stain properties and the light transmittance/absorbance process. PGDIPS is publicly available as an end-to-end off-the-shelf tool at https://github.com/GJiananChen/PGDIPS.", "filename": "2025_0878.pdf", "year": 2025, "institution": "University of Toronto", "country": "Canada", "authors": ["Jianan Chen", "Lydia Y Liu", "Wenchao Han", "Alison Cheung", "Hubert Tsui", "Anne L Martel"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0879", "x": 2.531, "y": 7.791, "title": "Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning", "abstract": "Federated learning (FL) provides a promising paradigm for collaboratively training machine learning models across distributed data sources while maintaining privacy. Nevertheless, real-world FL often faces major challenges including communication overhead during the transfer of large model parameters and statistical heterogeneity, arising from non-identical independent data distributions across clients. In this work, we propose an FL framework that 1) provides inherent interpretations using prototypes, and 2) tackles statistical heterogeneity by utilising lightweight adapter modules to act as compressed surrogates of local models and guide clients to achieve generalisation despite varying client distribution. Each client locally refines its model by aligning class embeddings toward prototype representations and simultaneously adjust the lightweight adapter. Our approach replaces the need to communicate entire model weights with prototypes and lightweight adapters. This design ensures that each client's model aligns with a globally shared structure while minimising communication load and providing inherent interpretations. Moreover, we conducted our experiments on a real-world retinal fundus image dataset, which provides clinical-site information. We demonstrate inherent interpretable capabilities and perform a classification task, which shows improvements in accuracy over baseline algorithms (Code is available at https://github.com/berenslab/ FedAdapterProto).", "filename": "2025_0879.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Samuel Ofosu Mensah", "Kerol Djoumessi", "Philipp Berens"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0880", "x": 1.468, "y": 4.168, "title": "PSAT: Pediatric Segmentation Approaches via Adult Augmentations and Transfer Learning", "abstract": "Pediatric medical imaging presents unique challenges due to significant anatomical and developmental differences compared to adults. Direct application of segmentation models trained on adult data often yields suboptimal performance, particularly for small or rapidly evolving structures. To address these challenges, several strategies leveraging the nnU-Net framework have been proposed, differing along four key axes: (i) the fingerprint dataset (adult, pediatric, or a combination thereof) from which the Training Plan-including the network architecture-is derived; (ii) the Learning Set (adult, pediatric, or mixed), (iii) Data Augmentation parameters, and (iv) the Transfer learning method (finetuning versus continual learning). In this work, we introduce PSAT (Pediatric Segmentation Approaches via Adult Augmentations and Transfer learning), a systematic study that investigates the impact of these axes on segmentation performance. We benchmark the derived strategies on two pediatric CT datasets and compare them with state-of-the-art methods, including a commercial radiotherapy solution. PSAT highlights key pitfalls and provides actionable insights for improving pediatric segmentation. Our experiments reveal that a training plan based on an adult fingerprint dataset is misaligned with pediatric anatomy-resulting in significant performance degradation, especially when segmenting fine structures-and that continual learning strategies mitigate institutional shifts, thus enhancing generalization across diverse pediatric datasets. The code is available at https://github.com/ICANS-Strasbourg/PSAT.", "filename": "2025_0880.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Tristan Kirscher", "Sylvain Faisan", "Xavier Coubez", "Loris Barrier", "Philippe Meyer"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0881", "x": 0.748, "y": 6.722, "title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment", "abstract": "Medical image interpretation and report generation are essential for radiologists to identify and communicate observable findings of diseases. Major efforts in image-to-report generation require heavy language model training yet still suffer from producing reports with factual errors. In this study, we present RadAlign, demonstrating that a concept-based vision-language model can improve both predictive accuracy and report factual correctness without extensive language model training. Our key innovation is aligning visual features with medical diagnostic criteria in a shared representation space. Such alignment introduces core knowledge supervision and creates interpretable intermediate diagnosis results for LLMs to refine report generation. We also propose a cross-modal retrieval mechanism to provide additional clinical cont ext of history cases for enhancing report generation accuracy. This unified approach achieves superior disease classification on MIMIC-CXR (average AUC: 0.885) and enables accurate report generation (GREEN score: 0.678 vs. SOTA: 0.634). RadAlign also demonstrates exceptional generalization capabilities, outperforming SOTA foundation and specialized models on the external OpenI dataset (AUC: 0.923 vs. 0.836). Code is available at https://github.com/difeigu/RadAlign.", "filename": "2025_0881.pdf", "year": 2025, "institution": "Rutgers University", "country": "USA", "authors": ["Difei Gu", "Yunhe Gao", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0882", "x": 0.505, "y": 6.858, "title": "RAPTOR: Generative AI for Parsing Colorectal Cancer Referrals to Streamline Faster Diagnostic Standard Pathways", "abstract": "Delays in processing urgent cancer referrals hinder Faster Diagnostic Standards (FDS), with manual extraction of patient data (demographics, symptoms and test results) remaining a bottleneck in colorectal two-week wait (2WW) pathways. We evaluate generative AI (GenAI) for automating structured data extraction from colorectal cancer (CRC) 2WW referrals, comparing the reasoning capabilities of GPT-4o-Mini and DeepSeek-R1 against clinician-led extraction. Both models achieved near-human precision (GPT-4o-Mini: 94.83%, DeepSeek-R1: 93.72%) while reducing the processing time by 10-fold. Key challenges included non-deterministic output, OCR noise (e.g. handwritten annotations, overlapping text), and contextual ambiguity, notably misclassified checkboxes, symptom misattribution, and numerical inconsistencies (e.g. fecal immunochemical test (FIT) unit conversions). We also proposed an uncertainty quantification mechanism to flag uncertain extractions for human review. Despite residual limitations, GenAI shows the potential to improve efficiency, standardisation, and equity in cancer pathways by alleviating administrative burdens. Future work should prioritise hybrid AI-clinician workflows, domain-specific fine-tuning, and real-world validation to ensure reliable clinical integration.", "filename": "2025_0882.pdf", "year": 2025, "institution": "Birmingham City University", "country": "UK", "authors": ["Sofiat Abioye", "Shazad Ashraf", "Junaid Qadir", "Adam Byfield", "Anusha Jose", "William Poulett", "Ben Wallace", "Adil Butt", "Colm Forde", "Marcus Mottershead", "Simon Fallis", "Andrew Beggs", "Aneel Bhangu", "Lukman Akanbi", "Muhammad Bilal"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0883", "x": 3.602, "y": 6.781, "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "abstract": "Digital pathology has revolutionized the field by enabling the digitization of tissue samples into whole slide images (WSIs). However, the high resolution and large size of WSIs present significant challenges when it comes to applying Deep Learning models. As a solution, WSIs are often divided into smaller patches with a global label (i.e., diagnostic) per slide, instead of a (too) costly pixel-wise annotation. By treating each slide as a bag of patches, Multiple Instance Learning (MIL) methods have emerged as a suitable solution for WSI classification. A major drawback of MIL methods is their high variability in performance across different runs, which can reach up to 10-15 AUC points on the test set, making it difficult to compare different MIL methods reliably. This variability mainly comes from three factors: i) weight initialization, ii) batch (shuffling) ordering, iii) and learning rate. To address that, we introduce a Multi-Fidelity, Model Fusion strategy for M IL methods. We first train multiple models for a few epochs and average the most stable and promising ones based on validation scores. This approach can be applied to any existing MIL model to reduce performance variability. It also simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. We extensively validate our approach on WSI classification tasks using 2 different datasets, 3 initialization strategies and 5 MIL methods, for a total of more than 2000 experiments.", "filename": "2025_0883.pdf", "year": 2025, "institution": "Institut Polytechnique de Paris", "country": "France", "authors": ["Ali Mammadov", "Loïc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0884", "x": 1.616, "y": 5.004, "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is increasingly attracting interest in medical imaging due to its effectiveness and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA) is a notable approach based on the assumption that the adaptation inherently occurs in a low-dimensional subspace. While it has shown good performance, its implementation requires a fixed and unalterable rank, which might be challenging to select given the unique complexities and requirements of each medical imaging downstream task. Inspired by advancements in natural image processing, we introduce a novel approach for medical image segmentation that dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank representation of the trainable weight matrices as a singular value decomposition, we introduce an l1 sparsity regularizer to the loss function, and tackle it with a proximal optimizer. The regularizer could be viewed as a penalty on the decomposition rank. Hence, its minimization enables to find task-adapted ranks automatically. Our method is evaluated in a realistic few-shot fine-tuning setting, where we compare it first to the standard LoRA and then to several other PEFT methods across two distinguishable tasks: base organs and novel organs. Our extensive experiments demonstrate the significant performance improvements driven by our method, highlighting its efficiency and robustness against suboptimal rank initialization. Our code is publicly available: https://github.com/ghassenbaklouti/ARENA.", "filename": "2025_0884.pdf", "year": 2025, "institution": "ÉTS Montréal", "country": "Canada", "authors": ["Ghassen Baklouti", "Julio Silva-Rodríguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0885", "x": 3.446, "y": 3.633, "title": "Resolving the Overlap of Macrovascular and Microvascular Flow Components in Digital Subtraction Angiography for Cerebral Reperfusion Assessment", "abstract": "Cerebral digital subtraction angiography (DSA) is an Xraybased imaging modality that provides high-resolution, real-time visualisation of cerebral vasculature, and is an established part of the standard treatment of stroke patients. Conventionally, DSA data are acquired as 2D images where vessel structures overlap with one another due to the penetrating nature of X-ray. Given the increasing recognition of the importance of microvasculatures in stroke, there is an unmet need to utilise DSA to accurately assess microvessels, unobstructed from overlapping macrovessels. This work proposes a novel Expectation-Maximisation algorithm integrated with anatomy-informed regularisation to disentangle macrovascular and microvascular flow component overlaps in a spatiotemporal Gamma mixture model for DSA. In-vivo experiments across 108 stroke patients demonstrate that the proposed method achieves robust estimation and provides clear separation of the macrovascular and microvascular flow components. Based on the proposed method, quantitative microvascular cerebral blood volume was derived from DSA images and shown to be significantly associated with the current gold-standard reperfusion metric.", "filename": "2025_0885.pdf", "year": 2025, "institution": "The University of Melbourne", "country": "Australia", "authors": ["Chengchuan Wu", "Catherine Davey", "Gagan Sharma", "Felix Ng"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0886", "x": 2.741, "y": 4.854, "title": "Revisiting Masked Image Modeling with Standardized Color Space for Domain Generalized Fundus Photography Classification", "abstract": "Diabetic retinopathy (DR) is a serious complication of diabetes, requiring rapid and accurate assessment through computer-aided grading of fundus photography. To enhance the practical applicability of DR grading, domain generalization (DG) and foundation models have been proposed to improve accuracy on data from unseen domains. Despite recent advancements, foundation models trained in a self-supervised manner still exhibit limited DG capabilities, as selfsupervised learning does not account for domain variations. In this paper, we revisit masked image modeling (MIM) in foundation models to advance DR grading for domain generalization. We introduce a MIM-based approach that transforms images to achieve standardized color representation across domains. By transforming images from various domains into this color space, the model can learn consistent representation even for unseen images, promoting domain-invariant feature learning. Additionally, we employ joint representation learning of both the original and transformed images, using cross-attention to integrate their respective strengths for DR classification. We showed a performance improvement of up to nearly 4% across the three datasets, positioning our method as a promising solution for domain-generalized medical image classification.", "filename": "2025_0886.pdf", "year": 2025, "institution": "DGIST", "country": "South Korea", "authors": ["Eojin Jang", "Myeongkyun Kang", "Soopil Kim", "Min Sagong", "Sang Hyun Park"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0887", "x": 4.517, "y": 2.769, "title": "Robust Fetal Pose Estimation Across Gestational Ages via Cross-Population Augmentation", "abstract": "Fetal motion is a critical indicator of neurological development and intrauterine health, yet its quantification remains challenging, particularly at earlier gestational ages (GA). Current methods track fetal motion by predicting the location of annotated landmarks on 3D echo planar imaging (EPI) time-series, primarily in third-trimester fetuses. The predicted landmarks enable simplification of the fetal body for downstream analysis. While these methods perform well within their training age distribution, they consistently fail to generalize to early GAs due to significant anatomical changes in both mother and fetus across gestation, as well as the difficulty of obtaining annotated early GA EPI data. In this work, we develop a cross-population data augmentation framework that enables pose estimation models to robustly generalize to younger GA clinical cohorts using only annotated images from older GA cohorts. Specifically, we in troduce a fetal-specific augmentation strategy that simulates the distinct intrauterine environment and fetal positioning of early GAs. Our experiments find that cross-population augmentation yields reduced variability and significant improvements across both older GA and challenging early GA cases. By enabling more reliable pose estimation across gestation, our work potentially facilitates early clinical detection and intervention in challenging 4D fetal imaging settings. Code is available at https://github.com/sebodiaz/cross-population-pose.", "filename": "2025_0887.pdf", "year": 2025, "institution": "MIT", "country": "USA", "authors": ["Sebastian Diaz", "Benjamin Billot", "Neel Dey", "Molin Zhang", "Esra Abaci Turk", "P Ellen Grant", "Polina Golland", "Elfar Adalsteinsson"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0888", "x": 4.182, "y": 3.631, "title": "Self-supervised Normality Learning and Divergence Vector-Guided Model Merging for Zero-Shot Congenital Heart Disease Detection in Fetal Ultrasound Videos", "abstract": "Congenital Heart Disease (CHD) is one of the leading causes of fetal mortality, yet the scarcity of labeled CHD data and strict privacy regulations surrounding fetal ultrasound (US) imaging present significant challenges for the development of deep learning-based models for CHD detection. Centralised collection of large real-world datasets for rare conditions, such as CHD, from large populations requires significant co-ordination and resource. In addition, data governance rules increasingly prevent data sharing between sites. To address these challenges, we introduce, for the first time, a novel privacy-preserving, zero-shot CHD detection framework that formulates CHD detection as a normality modeling problem integrated with model merging. In our framework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site first trains a sparse video tube-based self-supervised video anomaly detection (VAD) model on normal fetal heart US clips with self-distillation loss. This enables site-specific models to independently learn the distribution of healthy cases. To aggregate knowledge across the decentralized models while maintaining privacy, we propose a Divergence Vector-Guided Model Merging approach, DivMerge, that combines site-specific models into a single VAD model without data exchange. Our approach preserves domain-agnostic rich spatio-temporal representations, ensuring generalization to unseen CHD cases. We evaluated our approach on real-world fetal US data collected from 5 hospital sites. Our merged model out-P. Saha and D. Mishra-Equal contribution.", "filename": "2025_0888.pdf", "year": 2025, "institution": "University of Oxford", "country": "UK", "authors": ["Pramit Saha", "Divyanshu Mishra", "Netzahualcoyotl Hernandez-Cruz", "Olga Patey", "Aris T Papageorghiou", "Yuki M Asano", "J Alison Noble"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0889", "x": 2.098, "y": 5.035, "title": "Solving Medical Multi-Label Domain Adaptation via Wasserstein Adversarial Learning with Class-Level Alignment", "abstract": "In medical imaging, domain adaptation (DA) enables the transfer of knowledge from models trained on labeled source domains to unlabeled target domains that exhibit distribution shifts. In real world, medical images often contain multiple disease-related labels. However, existing multi-label domain adaptation (MLDA) algorithms face two primary challenges in addressing multi-label domain shifts: inadequate capture of disease features and insufficient integration of information from each individual class. To tackle these challenges, we propose a novel approach, Wasserstein Adversarial Learning with Class-Level Alignment, designed to align feature distributions for medical MLDA. By utilizing adversarial learning guided by W asserstein distance, our approach captures more complete domain-invariant representations of lesion region. Additionally, we introduce a class-level alignment loss that leverages individual class information to further reduce domain discrepancies. Extensive experiments on real medical datasets demonstrate that our method significantly enhances medical multi-label domain adaptation and outperforms existing state-of-the-art algorithms.", "filename": "2025_0889.pdf", "year": 2025, "institution": "University of Science and Technology of China", "country": "China", "authors": ["Wenjie Liu", "Fuyou Miao", "Xu Wang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0890", "x": 6.416, "y": 3.739, "title": "Sparsely Labeled fMRI Data Denoising with Meta-learning-Based Semi-supervised Domain Adaptation", "abstract": "Functional magnetic resonance imaging (fMRI) denoising is a crucial preprocessing step in neuroimaging studies, as noise degrades the reliability of downstream analyses. Previous approaches for fMRI denoising either rely on predefined noise patterns or train dataset-specific models, restricting their reliability across various datasets due to interdataset variations in scanner types, scanning protocols, and preprocessing pipelines. Additionally, applying previous approaches to new datasets requires extensive expert signal/noise annotations. To mitigate this reliance, leveraging existing datasets to train sparsely labeled datasets is a practical solution, but inconsistencies in labeling criteria hinder effective adaptation. To address these challenges, we propose a metalearning-based semi-supervised domain adaptation framework, enabling the learning of dataset-irrelev ant features from sparsely labeled datasets by leveraging existing labeled datasets with two key components: (1) a dataset-irrelevant feature extractor trained by meta-learning to capture noise patterns across multiple datasets, and (2) dataset-specific classifiers optimized by decoupled training to handle inconsistencies in labeling criteria. Our proposed approach shows outstanding performance on four fMRI datasets in both fully labeled and sparsely labeled conditions.", "filename": "2025_0890.pdf", "year": 2025, "institution": "Korea University", "country": "Republic of Korea", "authors": ["Keun-Soo Heo", "Ji-Wung Han", "Soyeon Bak", "Minjoo Lim", "Bogyeong Kang", "Sang-Jun Park", "Weili Lin", "Han Zhang", "Dinggang Shen", "Tae-Eui Kam"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0891", "x": 0.69, "y": 6.682, "title": "SPEC-CXR: Advancing Clinical Safety Through Entity-Level Performance Evaluation of Chest X-ray Report Generation", "abstract": "Automated chest X-ray report generation has great potential to improve healthcare efficiency, but rigorous validation is essential for safe clinical adoption. Existing evaluation metrics focus mainly on report-level scores, failing to provide actionable insights for clinicians.In this paper, we present SPEC-CXR (Safety-centered Performance Evaluation in Clinical Report for Chest X-Ray), an evaluation framework that integrates entity-level performance assessment with reportlevel error analysis using a large language model (LLM). In our approach, the LLM extracts and classifies entities-radiological findings and differential diagnoses-from both generated and reference reports based on a carefully curated entity set. Generated reports are then evaluated on entity presence, location, severity, and prior comparison, yielding structured outputs to calculate detailed entity-level scores (F1 for presence and a ccuracy for location, severity, and comparison).Our entity-level evaluation shows 91.8% accuracy compared to human evaluation for presence detection and 0.777 Kendall's tau-b correlation for report-level evaluation. Furthermore, our entity-level performance analysis uncovers critical limitations of current state-of-the-art report generation models across diverse entities, highlighting the urgent need for rigorous, safety-oriented evaluation metrics.Our framework is publicly available and usable: https://github.com/ lunit-io/spec-cxr.", "filename": "2025_0891.pdf", "year": 2025, "institution": "Lunit Inc", "country": "South Korea", "authors": ["Jung Oh Lee", "Junwoo Cho", "Junha Kim", "Laurent Dillard", "Tom Van Sonsbeek", "Arnaud A A Setio", "Hyeonsoo Lee", "Donggeun Yoo", "Taesoo Kim"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0892", "x": 4.289, "y": 6.614, "title": "Teaching Pathology Foundation Models to Accurately Predict Gene Expression with Parameter Efficient Knowledge Transfer", "abstract": "Gene expression profiling provides critical insights into cellular heterogeneity, biological processes, and disease mechanisms. There has been an increasing interest in computational approaches that can predict gene expression directly from digitalized histopathology images. While image foundation models have shown promise in a variety of pathology downstream analysis, their performances on gene expression prediction are still limited. Explicitly incorporating information from the transcriptomic models can help image models address domain shift, yet the fine-tuning and alignment of foundation models can be expensive. In this work, we propose Parameter Efficient Knowledge trAnsfer (PEKA), a novel framework that leverages Block-Affine Adaptation and integrates knowledge distillation and structure alignment losses for cross-modal knowledge transfer. We evaluated PEKA for gene expression prediction using multiple spatial transcriptomics datasets (comprising 206,123 image tiles with matched gene expression profiles) that included various types of tissue. PEKA achieved at least 5% performance improvement over baseline foundation models while also outperforming alternative parameter-efficient fine-tuning strategies. We have released the code, datasets and aligned models at Github to facilitate broader adoption and further development for parameter efficient model alignment.", "filename": "2025_0892.pdf", "year": 2025, "institution": "University College London", "country": "UK", "authors": ["Shi Pan", "Jianan Chen", "Maria Secrier"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0893", "x": 2.316, "y": 4.463, "title": "Test-Time Training with Local Contrast-Preserving Copy-Pasted Image for Domain Generalization in Retinal Vessel Segmentation", "abstract": "Accurate segmentation of retinal vessels is an important task. Deep learning-based approaches have achieved impressive segmentation performance on images with the same distribution as the training images. However, the performance significantly drops when there is a substantial disparity between the distributions of the training and testing data, which limits the practical applicability of these methods in real-world scenarios. In this paper, we propose a novel test-time training (TTT) strategy that employs a local contrast-preserving copy-paste (L2CP) method to generate synthetic images in the target domain style. Specifically, leveraging the thin nature of retinal vessel structures, we apply a simple morphological closing to remove these structures from the test image. This process yields a vessel-free image that retains the target domain's style, which we then employ as the background component for the synthetic image. To realistically integrate retinal vessels from source domain images into the background component, our L2CP method p astes the local contrast map of the vessels, rather than their grayscale values, onto the background component. This approach effectively mitigates the issue of significant disparities in grayscale distribution between the foreground and background across the source and target domains. Extensive TTT experiments on retinal vessel segmentation tasks demonstrate that the proposed L2CP consistently improves the model's generalization ability in retinal structure segmentation. The code of our implementation is available at https://github.com/GuGuLL123/L2CP.", "filename": "2025_0893.pdf", "year": 2025, "institution": "Wuhan University", "country": "China", "authors": ["Yuliang Gu", "Zhichao Sun", "Zelong Liu", "Yongchao Xu"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0894", "x": 4.752, "y": 5.555, "title": "Thread the Needle: Genomics-Guided Prompt-Bridged Attention Model for Survival Prediction of Glioma Based on MRI Images", "abstract": "Glioma remains one of the most lethal malignancy, making accurate prognosis crucial for personalized treatment and improved patient outcome. Existing models based on non-invasive magnetic resonance imaging (MRI) offer convenience, but they suffer from the poor performance and generalizability compared to genomic biomarkers, limiting their clinical adoption. Genomic biomarkers, such as IDH mutation and 1p/19q co-deletion, provide superior prognostic value but are restricted by their reliance on invasive surgical sampling. In this study, we hypothesize that these genomic biomarkers can guide the development of more robust MRI-based prognostic models, and propose a genomicsguided prompt learning framework that leverages both MRI and transcriptomic data to enhance survival prediction. Specifically, we introduce a novel visual modeling strategy for comprehensive glioma MRI representation and a Prompt-bridged Attention m echanism that can fuse multiple modalities during training and enhance visual representations during inference. Experimental results demonstrate that our proposed method achieves c-indeces of 0.6709 and 0.6904 on UCSF-PDGM and TCGA-GBM datasets, respectively, with highly significant p-values of 5.27 × 10 -14 and 6.72 × 10 -7 . These results substantially outperform existing methods, presenting a promising step toward reliable and noninvasive glioma prognosis prediction.", "filename": "2025_0894.pdf", "year": 2025, "institution": "Harbin Institute of Tec hnology", "country": "China", "authors": ["Yi Zhong", "Xubin Zheng", "Xiongri Shen", "Jiaqi Wang", "Leilei Zhao", "Zhenxi Song", "Zhiguo Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0895", "x": 0.593, "y": 5.024, "title": "Towards Robust Medical Image Referring Segmentation with Incomplete Textual Prompts", "abstract": "Recent advancements in medical vision-language models have increasingly accentuated the substantial potential of incorporating textual information for better medical image segmentation. However, existing language-guided segmentation models were developed under the assumption that the attributes/clauses of textual prompts are uniformly complete across all images, neglecting the unavoidable incompleteness of texts/reports in clinical applications and thus making them less feasible. To address this, we, for the first time, identify such incomplete textual prompts in medical image referring segmentation (MIRS) and propose an attribute robust segmentor (ARSeg) by constructing attribute-specific features and balancing the attribute learning procedure. Specifically, based on a U-shaped CNN network and a BERT-based text encoder, an attribute-specific cross-modal interaction module is introduced to establish attribute-specific features, thereby eliminating the dependency of decoding features on complete attributes. To prevent the model from being dominated by attributes with lower missing rates during training, an attribute consistency loss and an attribute imbalance loss are designed for balanced feature learning. Experimental results on two publicly available datasets demonstrate the superiority of ARSeg against SOTA approaches, especially under incomplete and imbalanced textual prompts. Code is available at https://github.com/w7jie/ARSeg.", "filename": "2025_0895.pdf", "year": 2025, "institution": "Huazhon University of S cience and Technology", "country": "China", "authors": ["Qijie Wang", "Xian Lin", "Zengqiang Yan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0896", "x": 0.612, "y": 6.826, "title": "TRRG: Towards Truthful Radiology Report Generation With Cross-Modal Disease Clue Enhanced Large Language Models", "abstract": "The vision-language capabilities of multi-modal large language models have gained attention, but radiology report generation still faces challenges due to imbalanced data distribution and weak alignment between reports and radiographs. To address these issue, we propose TRRG, a stage-wise training framework for truthful radiology report generation. In the pre-training stage, contrastive learning enhances the visual encoder's ability to capture fine-grained disease details. In the fine-tuning stage, our clue injection module improves disease perception by integrating robust zero-shot disease recognition. Finally, the crossmodal clue interaction module enables effective multi-granular fusion of visual and disease clue embeddings, significantly improving report generation and clinical effectiveness. Experiments on IU-Xray and MIMIC-CXR show that TRRG achieves state-of-the-art performance, enhancing disease perception and clinical utility.", "filename": "2025_0896.pdf", "year": 2025, "institution": "Great Ba y University", "country": "China", "authors": ["Yuhao Wang", "Yue Sun", "Tao Tan", "Chao Hao", "Yawen Cui", "Xinqi Su", "Weichen Xie", "Linlin Shen", "Zitong Yu"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0897", "x": 2.037, "y": 6.899, "title": "Trustworthy Few-Shot Transfer of Medical VLMs Through Split Conformal Prediction", "abstract": "Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities and are being increasingly adopted for data-efficient image classification. Despite its growing popularity, its reliability aspect remains largely unexplored. This work explores the split conformal prediction (SCP) framework to provide trustworthiness guarantees when transferring such models based on a small labeled calibration set. Despite its potential, the generalist nature of the VLMs' pre-training could negatively affect the properties of the predicted conformal sets for specific tasks. While common practice in transfer learning for discriminative purposes involves an adaptation stage, we observe that deploying such a solution for conformal purposes is suboptimal since adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP. To address this issue, we propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data. We present comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores. Our framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees. The code is publicly available: https://github.com/jusiro/SCA-T.", "filename": "2025_0897.pdf", "year": 2025, "institution": "ÉTS Montréal", "country": "Canada", "authors": ["Julio Silva-Rodríguez", "Ismail Ben Ayed", "Jose Dolz"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0898", "x": 1.309, "y": 7.0, "title": "Unleashing the Power of LLMs for Medical Video Answer Localization", "abstract": "Given an untrimmed medical instructional video and a textual question, medical video answer localization is to locate the precise temporal span that visually answers the question. Existing methods primarily rely on supervised learning to tackle this problem. This requires massive annotated data for training and shows limited flexibility in generalizing across different datasets, especially in the medical domain. With the remarkable advancements of large language models (LLMs) and their multimodal variants (MLLMs), we explore a Socratic approach to compose LLMs and MLLMs to achieve zero-shot video answer localization.Our method effectively takes advantage of the rich subtitles and visual descriptions i n instructional videos to prompt LLMs. We also develop a subtitle refinement and early fusion strategy for better performance. Experiments on MedVidQA and COIN-Med show that our method outperforms existing state-of-the-art (SOTA) zero-shot multimodal models significantly by 41.0% and 20.3% in mIoU, respectively. It even surpasses SOTA supervised methods, suggesting the strength of our approach.", "filename": "2025_0898.pdf", "year": 2025, "institution": "National University of Singapore", "country": "Singapore", "authors": ["Junbin Xiao", "Qingyun Li", "Yusen Yang", "Liang Qiu", "Angela Yao"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0899", "x": 0.616, "y": 4.576, "title": "Vector-Quantization-Driven Active Learning for Efficient Multi-modal Medical Segmentation with Cross-Modal Assistance", "abstract": "Multi-modal medical image segmentation leverages complementary information across different modalities to enhance diagnostic accuracy, but faces two critical challenges: the requirement for extensive paired annotations and the difficulty in capturing complex inter-modality relationships. While Active Learning (AL) can reduce annotation burden through strategic sample selection, conventional methods suffer from unreliable uncertainty quantification. Meanwhile, Vector Quantization (VQ) offers a mechanism for encoding inter-modality relationships, yet existing implementations struggle with codebook misalignment across modalities. To address these limitations, we propose a novel Vector Quantization -Bimodal Entropy-Guided Active Learning (VQ-BEGAL) framework that employs a dual-encoder architecture with VQ to discretize continuous features into distinct codewords, effectively preserving modality-specific information while mitigating feature co-linearity. Unlike conventional AL methods that separate sample selection from model training, our approach integrates feature-level uncertaint y estimation from cross-modal discriminator outputs into the training processstrategically allocating samples with different uncertainty characteristics to optimize specific network components, enhancing both feature extraction stability and decoder robustness. Experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art performance while requiring significantly fewer annotations, making it particularly valuable for real-world clinical applications where labeled data is scarce. The code is available at https://github.com/xf-DU/vq-begal.", "filename": "2025_0899.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Xiaofei Du", "Haoran Wang", "Manning Wang", "Zhijian Song"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0900", "x": 0.69, "y": 4.839, "title": "ViTexNet: Vision-Text Guided Dynamic Convolution Network for Medical Image Segmentation", "abstract": "Recent advancements in medical image segmentation have leveraged multi-modal learning, incorporating textual descriptions to enhance segmentation accuracy. However, existing approaches suffer from high computational costs and inefficient text-vision fusion mechanisms, necessitating a more accurate yet computationally efficient solution. To address this, we propose ViTexNet, a novel vision-language segmentation model that introduces Text-Guided Dynamic Convolution (TGDC) for effective and lightweight fusion of medical visual features and textual cues. Unlike standard cross-attention mechanisms, which impose high parameter complexity, TGDC dynamically refines image features by leveraging relevant textual semantics at each decoder stage, ensuring efficient feature modulation without excessive overhead. By adaptively emphasizing clinically significant regions based on textual descriptions, TGDC enhances segmentation performance while maintaining computational efficiency. Extensive evaluations o n QaTa-COV19 and MosMed-Data+ datasets demonstrate ViTexNet's state-of-the-art performance, achieving 90.76% Dice and 83.25% mIoU on QaTa-COV19, and 78.19% Dice and 64.04% mIoU on MosMedData+, while operating at just 11.5G FLOPs, substantially lower than competing models. Ablation studies confirm TGDC's superiority over cross-attention-based methods, highlighting its effectiveness in optimizing segmentation accuracy without computational trade-offs. The source code is made publicly available at: https://github.com/bhardwaj-rahul-rb/vitexnet.", "filename": "2025_0900.pdf", "year": 2025, "institution": "Indian Institute of Technology (IIT) Guwahati", "country": "India", "authors": ["Rahul Bhardwaj", "Utkarsh Yashwant Tambe", "Debanga Raj Neog"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0901", "x": 3.918, "y": 3.988, "title": "A Causal-Holistic Adaptive Intervention Network for Tailoring Automated Coronary Artery Disease Diagnosis to Individual Patients", "abstract": "Given the global prevalence and high mortality of coronary artery disease (CAD), automated CAD diagnosis should evolve toward personalized methods to maximize its clinical value. However, existing techniques have been limited to artery-level prediction, lacking patientlevel causality and failing to effectively account for individual patient confounders. In this work, for the first time, we introduce a Causal-Holistic Adaptive Intervention Network (CAIN) that tailors personalized CAD diagnosis for individual patients. CAIN generates semantic representations at both the patient and artery dual-levels for each case, constructing a holistic causal graph that captures individual-specific characteristics. It then implements adaptive causal intervention based on the patient's specific condition, using dynamically updated and differentiated intervention variables. Experimental results on CCTA scans from 602 patients and 6,830 coronary branches across three clinical centers show that CAIN outperforms state-of-the-art methods, offering personalized clinical guidance. The source code is available at (https://github. com/PerceptionComputingLab/CAIN).", "filename": "2025_0901.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Xinghua Ma", "Xingyu Qiu", "Yuetan Chu", "Kuanquan Wang", "Zhaowen Qiu", "Gongning Luo", "Xin Gao"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0902", "x": 3.873, "y": 3.976, "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "abstract": "Carotid atherosclerosis represents a significant health risk, with its early diagnosis primarily dependent on ultrasound-based assessments of carotid intima-media thickening. However, during carotid ultrasound screening, significant view variations cause style shifts, impairing content cues related to thickening, such as lumen anatomy, which introduces spurious correlations that hinder assessment. Therefore, we propose a novel causal-inspired method for assessing carotid intima-media thickening in frame-wise ultrasound videos, which focuses on two aspects: eliminating spurious correlations caused by style and enhancing causal content correlations. Specifically, we introduce a novel Spurious Correlation Elimination (SCE) module to remove non-causal style effects by enforcing prediction invariance with style perturbations. Simultaneously, we propose a Causal Equivalence Consolidation (CEC) module to strengthen causal content correlation through adversarial optimization during content randomization. Simultaneously, we design a Causal Transition Augmentation (CTA) module to ensure smooth causal flow by integrating an auxiliary pathway with text prompts and connecting it through contrastive learning. The experimental results on our inhouse carotid ultrasound video dataset achieved an accuracy of 86.93%, demonstrating the superior performance of the proposed method. Code is available at https://github.com/xielaobanyy/causal-imt.", "filename": "2025_0902.pdf", "year": 2025, "institution": "Southeast University", "country": "China", "authors": ["Shuo Gao", "Meng Yang", "Jun Xue", "Yang Chen", "Jingyang Zhang", "Guangquan Zhou"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0903", "x": 3.998, "y": 4.952, "title": "A Learning Framework for Predicting CT-Based PRM Biomarker from MRI Sequences in COPD", "abstract": "Image-based biomarkers provide non-invasive regional assessment of structural-functional abnormalities in Chronic Obstructive Pulmonary Disease (COPD). For example, quantitative computed tomography (QCT) identifies emphysema and small airway disease, while functional MRI measures lung ventilation and perfusion. In recent years, machine learning techniques have been introduced to predict quantitative indices from alternative imaging modalities, with the aim to reduce scanning time, radiation dose and/or costs in the clinical setting. However, most of those works focused on lung ventilation, while robust quantification of regional lung perfusion of dynamic contrast-enhanced (DCE) MRI remains a challenging task. In addition, previous studies focused only on learning from a single imaging modality. In this study, we explore a deep learning-based model to predict conventionally CT-based biomarkers, namely Parametric response mapping (PRM) classifications, from multi-sequence structural-functional MR images. Our proposed model achieves very strong correlations in predicting %PRM emphysema (Pearson correlation coefficient r = 0.91, p < 0.001 at patient level and r = 0.87, p < 0.001 at lung lobe level), and moderate to strong correlations in predicting %PRM normal (r = 0.60, p < 0.001 at patient level and r = 0.58, p < 0.001 at lung lobe level) in unseen COPD patients.", "filename": "2025_0903.pdf", "year": 2025, "institution": "University Hospital of Heidelberg", "country": "Germany", "authors": ["Yiling Xu", "Simon M F Triphan", "Julian Grolig", "Hanyi Zhang", "Jürgen Biederer", "Craig J Galbán", "Hans-Ulrich Kauczor", "Mark O Wielpütz", "Oliver Weinheimer"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0904", "x": 4.292, "y": 3.475, "title": "A Semi-Supervised Knowledge Distillation Framework for Left Ventricle Segmentation and Landmark Detection in Echocardiograms", "abstract": "Left ventricular segmentation and landmark detection from echocardiograms are routine practices in clinical settings for comprehensive evaluation of cardiovascular disease. Recently, deep learningbased models have been developed to interpret echocardiograms. However, existing methods face challenges in handling sparse annotations, limiting their clinical applicability. Additionally, their robustness can be significantly influenced by temporal inconsistency (i.e., abrupt prediction fluctuations between consecutive frames) and inter-task conflict (i.e., detected landmarks deviating from segmentation boundaries). To address these issues, we propose a novel semi-supervised framework that integrates: 1) a knowledge distillation method for generating pseudo labels of the numerous unlabeled frames to improve the performance; 2) a Task-aware Spatial-Temporal Network (TSTNet) along with consistency constraints that enhances robustness by enforcing temporal consistency across frames, and inter-task consistency between segmentation and landmark detection. Experimental results on two datasets (a public dataset with 500 subjects and a private dataset with 1,950 subjects) show that our proposed framework significantly outperforms the previous approaches. The source code and dataset are publicly available at https://github.com/chenhy-97/TSTNet.", "filename": "2025_0904.pdf", "year": 2025, "institution": "ShanghaiTech University", "country": "China", "authors": ["Haoyuan Chen", "Yonghao Li", "Long Yang", "Han Wu", "Lin Zhou", "Kaicong Sun", "Dinggang Shen"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0905", "x": 3.164, "y": 2.801, "title": "A Unified Missing Modality Imputation Model with Inter-modality Contrastive and Consistent Learning", "abstract": "Multi-modality magnetic resonance imaging (MRI) is widely used in the clinical diagnosis of brain tumors. However, the issue of missing modalities is frequently encountered in the real-world setting and can lead to the collapse of deep-learning-based automatic diagnosis algorithms that rely on full-modality images. To address this challenge, we propose a unified model capable of synthesizing missing modalities through any subsets of the full-modality images. Our method is a sequence-to-sequence prediction model that predicts the missing images by inter-modality correlation and modality-specific semantics. Specifically, we develop a dual-branch encoder, where both branches encode partially masked image tokens into low-dimensional features independently. A decoder then generates the target input images based on the fused encoder features. To strengthen the representative ability of encoder features, we propose a combination loss to improve the discriminative and consistency between diverse modality features. We evaluate our method on the BraTS 2023 dataset. Extensive quantitative and qualitative experiments demonstrate the high fidelity and utility of the synthesized images.", "filename": "2025_0905.pdf", "year": 2025, "institution": "Changchun University of Science and Technology", "country": "China", "authors": ["Liangce Qi", "Yusi Liu", "Yuqin Li", "Weili Shi", "Guanyuan Feng", "Zhengang Jiang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0906", "x": 1.845, "y": 2.562, "title": "A Uniform Multi-mode Fused Framework for Velocity Field Estimation in Ultrasound Imaging", "abstract": "Velocity field estimation, or motion tracking, is the key to characterizing tissue function in ultrasound imaging. Current velocity field estimation remains challenging in cross-range motion tracking due to the less sensitivity of ultrasound in this dimension. In addition, there is a lack of a uniform framework for different imaging schemes, such as linear array with rectangular scanning, phased array with sector scanning, and matrix array with volumetric scanning. This paper proposes a uniform multi-mode fused framework for tissue velocity field estimation. This framework integrates multiple modes of pair-wise optical flows, Doppler, and speckle consistency in ultrasound to improve the accuracy of cross-range velocity estimation. Furthermore, the uniform framework is adapted to different arrays and imaging schemes for various application scenarios. Extensive in-silico experiments on homemade and public datasets demonstrate the effectiveness of the proposed framework and the outperformance of our method when compared with a window-based method and an energy function optimization-based method. Particularly, our method improves the accuracy of cross-range velocity estimation by 8.84%, 19.21%, and 10.94% in three cross-sectional views of the public cardiac dataset when compared with the energy function optimizationbased method.", "filename": "2025_0906.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Hailong Li", "Liansheng Wang", "Yinran Chen"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0907", "x": 6.807, "y": 4.662, "title": "Alzheimer’s Disease Recognition Based on Adaptive Graph Normalization Flow for Incomplete Multimodal Data Fusion", "abstract": "Multimodal data holds significant value in the diagnosis of Alzheimer's disease (AD). However, in real-world applications, factors such as privacy protection, acquisition costs, and sensor failures often lead to data missingness, posing challenges for incomplete multimodal learning. Currently the artificial intelligence-based diagnostic methods for AD on incomplete multimodal data have gained increasing attention. However, existing approaches typically overlook modality distribution discrepancies and suffer from severe performance degradation under recovery paradigms lacking reconstruction experience. To address this challenge, we propose an Adaptive Graph Distribution Consistency Modal Recovery Network Based on Normalizing Flows (AGDiC) to tackle incomplete multimodal learning in neuroimaging. We develop a novel framework integrating adaptive graph learning with normalizing flows and a modality regularization strategy. This framework focuses adaptive graph attention features on modality distributions while ensuring distribution consistency of recovered data, and employs masked cross-attention to facilitate multimodal fusion. Unlike conventional methods, our model can handle arbitrary modality missingness during both training and inference phases without relying on reconstruction experience. Extensive experiments are conducted using three neuroimaging modalities from the ADNI dataset: sMRI, fMRI and PET. Results demonstrate that our method achieves state-of-the-art performance and exhibits remarkable stability across various random missing rates.", "filename": "2025_0907.pdf", "year": 2025, "institution": "Ningbo University", "country": "China", "authors": ["Yaqin Li", "Yihong Dong", "Yanan Wu", "Haihao Yan", "Linlin Gao"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0908", "x": 6.942, "y": 4.747, "title": "Anatomical Graph-Based Multilevel Distillation for Robust Alzheimer’s Disease Diagnosis with Missing Modalities", "abstract": "The multimodal model has shown superior potential for accurate Alzheimer's disease (AD) diagnosis; however, its reliance on complete modalities limits its use in a clinical setting. This study proposes a novel Anatomical Graph-based Multilevel Distillation (AGMD) framework that effectively transfers multimodal knowledge using layered modeling. Specifically, we develop a hierarchical distillation framework with three dedicated branches to explicitly capture the features of AD from multiple levels (local structural details, regional connectivity patterns, and global semantic information) to achieve complete knowledge transfer. Moreover, we introduce anatomical constraints to model the brain adjacent connection patterns to help better learn the relationships between key ROIs, particularly in disease-relevant regions, e.g., the hippocampus. The prediction entropy as regularization is introduced to refine instance-level knowledge, comprehensively alleviating the negative impact of the teacher's noisy information. Extensive experiments on the ADNI dataset demonstrate that AGMD achieves the best classification accuracy, with an improvement of 3.7% over the state-of-the-art methods, while significantly reducing the performance gap between teacher and student models. The code is available at https://github.com/LiuFei- AHU/AGMD.", "filename": "2025_0908.pdf", "year": 2025, "institution": "Monash University Malaysia", "country": "Malaysia", "authors": ["Fei Liu", "Huabin Wang", "Mohamed Hisham Jaward", "Shiuan-Ni Liang", "Huey Fang Ong", "Jiayuan Cheng"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0909", "x": 3.059, "y": 6.348, "title": "Automated Detection of BK Virus in H&E Whole-Slide Images Using Weakly-Supervised Deep Learning and Interpretable Morphological Biomarkers", "abstract": "Detecting BK Virus (BKV) is crucial for managing posttransplant outcomes in kidney patients. While BKV is typically identified using SV40 immunohistochemistry (IHC), this method is timeconsuming, limited by tissue availability and resource-intensive, especially in low-resource settings. Recent advances in computational pathology have shown potential for automating disease detection from Hematoxylin and Eosin (H&E)-stained images, though BKV detection remains understudied due to its low prevalence and limited data. We hypothesize that BKV-positive cells exhibit unique morphological patterns in H&E-stained tissue, detectable via computational methods. To address this, we developed BKVision, a weakly-supervised deep learning model for BKV detection in H&E whole-slide images (WSIs). Trained on 3,734 WSIs, BKVision achieves an F1-score of 0.984 ± 0.008 on a test cohort of 936 slides. Additionally, we conducted a morphological analysis on 774 H&E image patches, extracting 37 human interpretable features and validating them against IHC with pathologist guidance. This identified 11 cell attributes, such as nuclear enlargement and chromatin texture changes, that distinguish BKV-positive from negative cases. These findings highlight the potential to enhance BKV diagnostic criteria by integrating these identified morphological features. BKVision demonstrates the potential of computational methods to provide accurate, accessible, and interpretable BKV detection without the need for IHC, offering a S. Sahai and A. D. Ramos-Guerra-Equal contribution.", "filename": "2025_0909.pdf", "year": 2025, "institution": "Harvard Medical School", "country": "USA", "authors": ["Sharifa Sahai", "Ana D Ramos-Guerra", "Cristina Almagro-Pérez", "Guillaume Jaume", "Andrew Zhang", "Helmut Rennke", "Astrid Weins", "Juan E Ortuño", "Maria J Ledesma-Carbayo", "Faisal Mahmood"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0910", "x": 1.589, "y": 4.056, "title": "Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation", "abstract": "Accurate and reliable brain tumor segmentation, particularly when dealing with missing modalities, remains a critical challenge in medical image analysis. Previous studies have not fully resolved the challenges of tumor boundary segmentation insensitivity and feature transfer in the absence of key imaging modalities. In this study, we introduce MST-KDNet, aimed at addressing these critical issues. Our model features Multi-Scale Transformer Knowledge Distillation to effectively capture attention weights at various resolutions, Dual-Mode Logit Distillation to improve the transfer of knowledge, and a Global Style Matching Module that integrates feature matching with adversarial learning. Comprehensive experiments conducted on the BraTS and FeTS 2024 datasets demonstrate that MST-KDNet surpasses current leading methods in both Dice and HD95 scores, particularly in conditions with substantial modality loss. Our approach shows exceptional robustness and generalization potential, making it a promising candidate for real-world clinical applications. Our source code is available at https://github.com/Quanato607/MST-KDNet.", "filename": "2025_0910.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Shenghao Zhu", "Yifei Chen", "Weihong Chen", "Yuanhan Wang", "Chang Liu", "Shuo Jiang", "Feiwei Qin", "Changmiao Wang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0911", "x": 5.175, "y": 4.882, "title": "Causality-Inspired Graph Neural Network for Interpretable Strabismus Subtype Classification", "abstract": "Despite advances in deep learning, current automated methods for strabismus classification face two key challenges: limited interpretability and a lack of focus on strabismus subtypes. These issues undermine clinical trust, hinder practical adoption, and limit personalized treatment. To address this, we propose a Causality-Inspired Graph Neural Network (CI-GNN) framework that identifies causally related visual features from eye regions and constructs a graph structure for robust prediction, moving beyond reliance on raw image pixels. This causality-driven design enhances both interpretability and clinical relevance by providing more transparent diagnostic outcomes. We also establish a representative benchmark for strabismus subtype classification, focusing on deviation direction and horizontal angle variation (e.g., A/Vpattern). Experiments show that our method achieves state-of-the-art accuracy-89.8% and 88.1% on the two subtype tasks, respectively. Furthermore, by incorporating the SHapley explanation technique, CI-GNN offers clinician-friendly diagnostic evidence. Leveraging sparse causal features, the framework requires only 0.0003 GFLOPs, making it highly efficient and suitable for edge deployment. Overall, this work demonstrates the potential of integrating causal knowledge with GNNs to significantly enhance the performance, efficiency, and interpretability of strabismus diagnosis, offering promising directions for intelligent medical applications.", "filename": "2025_0911.pdf", "year": 2025, "institution": "Shantou University", "country": "China", "authors": ["Jiawen Zheng", "Li Luo", "Jiafan Zhuang", "Peiwei Wei", "Lihao Zhong", "Xiaoling Xie", "Jinming Guo", "Meng Xie", "Xiaoli Kang", "Jie Cen", "Lingyan Dong", "Ce Zheng", "Zhun Fan"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0912", "x": 1.503, "y": 5.678, "title": "CF-Seg: Counterfactuals Meet Segmentation", "abstract": "Segmenting anatomical structures in medical images plays an important role in the quantitative assessment of various diseases. However, accurate segmentation becomes significantly more challenging in the presence of disease. Disease patterns can alter the appearance of surrounding healthy tissues, introduce ambiguous boundaries, or even obscure critical anatomical structures. As such, segmentation models trained on real-world datasets may struggle to provide good anatomical segmentation, leading to potential misdiagnosis. In this paper, we generate counterfactual (CF) images to simulate how the same anatomy would appear in the absence of disease without altering the underlying structure. We then use these CF images to segment structures of interest, without requiring any changes to the underlying segmentation model. Our experiments on two real-world clinical chest X-ray datasets show that the use of counterfactual images improves anatomical segmentation, thereby aiding downstream clinical decision-making.", "filename": "2025_0912.pdf", "year": 2025, "institution": "Imperial College London", "country": "UK", "authors": ["Raghav Mehta", "Fabio De Sousa Ribeiro", "Tian Xia", "Mélanie Roschewitz", "Ainkaran Santhirasekaram", "Dominic C Marshall", "Ben Glocker"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0913", "x": 2.621, "y": 2.715, "title": "Coarse-to-Fine Medical Image Translation by Incorporating Deterministic Guidance and Probabilistic Refinement", "abstract": "In clinical diagnosis and treatment, traditional enhanced imaging techniques often suffer from inherent limitations such as high time costs and radiation risks. Therefore, medical image translation technology provides an efficient and cost-effective alternative. However, images generated by existing medical image generation methods still face challenges, such as a lack of structural consistency and blurred local details. Most methods struggle to simultaneously integrate deterministic structural information, such as anatomical priors, and probabilistic dynamic variations, such as blood flow changes, to guide image generation. To address these challenges, we propose a Coarse-to-Fine Medical Image Translation (C2FMIT) model, which incorporates Deterministic Guidance and Probabilistic Refinement to balance generation controllability and fidelity. First, we design a Deterministic Guidance Branch (DGB) to extract coarse-grained features, such as organ contours, to provide global structural constraints. Then, these deterministic priors are fused into our Probabilistic Refinement Branch (PRB), where the Brownian Bridge diffusion is employed for fine-grained optimization, enhancing microvascular textures and dynamic enhancement regions. Notably, we designed a Coarse-to-Fine Guided Attention Module (C2FGAM) to achieve progressive optimization from global structure to local details. Experimental results demonstrate that our method achieves superior performance across multiple modalities of functionally contrast-enhanced medical imaging on both public and in-house datasets.", "filename": "2025_0913.pdf", "year": 2025, "institution": "Jiangnan University", "country": "China", "authors": ["Hongnian Tian", "Tianxu Lv", "Jiansong Fan", "Delin Pan", "Lihua Li", "Xiang Pan"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0914", "x": 1.644, "y": 4.175, "title": "DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities", "abstract": "Accurate segmentation of brain images typically requires the integration of complementary information from multiple image modalities. However, clinical data for all modalities may not be available for every patient, creating a significant challenge. To address this, previous studies encode multiple modalities into a shared latent space. While somewhat effective, it remains suboptimal, as each modality contains distinct and valuable information. In this study, we propose DC-Seg (Disentangled Contrastive Learning for Segmentation), a new method that explicitly disentangles images into modality-invariant anatomical representation and modality-specific representation, by using anatomical contrastive learning and modality contrastive learning respectively. This solution improves the separation of anatomical and modality-specific features by considering the modality gaps, leading to more robust representations. Furthermore, we introduce a segmentation-based regularizer that enhances the model's robustness to missing modalities. Extensive experiments on the BraTS 2020 and a private white matter hyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms state-of-the-art methods in handling incomplete multimodal brain tumor segmentation tasks with varying missing modalities, while also demonstrate strong generalizability in WMH segmentation. The code is available at https://github.com/CuCl-2/DC-Seg.", "filename": "2025_0914.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Zhengyao Ding", "Zhengxing Huang"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0915", "x": 3.031, "y": 4.069, "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "abstract": "Coronary stenosis is a major risk factor for ischemic heart events leading to increased mortality, and medical treatments for this condition require meticulous, labor-intensive analysis. Coronary angiography provides critical visual cues for assessing stenosis, supporting clinicians in making informed decisions for diagnosis and treatment. Recent advances in deep learning have shown great potential for automated localization and severity measurement of stenosis. In real-world scenarios, however, the success of these competent approaches is often hindered by challenges such as limited labeled data and class imbalance. In this study, we propose a novel data augmentation approach that uses an inpainting method based on a diffusion model to generate realistic lesions, allowing user-guided control of severity. Extensive evaluations show that incorporating synthetic data during training enhances lesion detection and severity classification performance on both a large-scale inhouse dataset and a public coronary angiography dataset. Furthermore, our approach maintains high detection and classification performance even when trained with limited data, highlighting its clinical importance in improving the assessment of stenosis severity and optimizing data utilization for more reliable decision support.", "filename": "2025_0915.pdf", "year": 2025, "institution": "Medipixel, Inc", "country": "Republic of Korea", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0916", "x": 0.911, "y": 4.683, "title": "Edge-Semantic Synergy Fusion and Adaptive Noise-Aware for Weakly Supervised Pathological Tissue Segmentation", "abstract": "Existing studies on weakly supervised pathological tissue segmentation predominantly rely on class activation maps (CAMs) to generate pixel-level pseudo-masks from image-level labels. However, CAMs tend to emphasize only the most discriminative regions, resulting in boundary noise that undermines the quality of pseudo-masks and degrades segmentation performance. To address these challenges, we propose a novel weakly supervised pathological tissue segmentation framework: Edge-semantic Synergy Fusion and Adaptive Noise-aware (ESFAN) mechanism. In the classification phase, the Edge-semantic Synergy Fusion (ESF) improves the quality of pseudo-masks by incorporating four synergistic components. The hybrid edge-aware transformer refines boundaries, while the pyramid context integrator captures multiscale context. The context channel amplifier fine-tunes semantic features, and the adaptive fusion gating balances feature map contributions using learnable spatial weights. In the segmentation phase, we propose an Adaptive Noise-aware Mechanism (ANM) that incorporates adaptive weighted cross-entropy, uncertainty regularization, and spatial smoothing constraints to mitigate noise in pseudo-masks and enhance segmentation robustness. Extensive experiments on the LUAD-HistoSeg and BCSS datasets demonstrate that ESFAN significantly outperforms state-of-the-art methods. The code is available at: https://github.com/ Sameer-815/ESFAN.", "filename": "2025_0916.pdf", "year": 2025, "institution": "Guilin University of Electronic Technology", "country": "China", "authors": ["Hualong Zhang", "Siyang Feng", "Zihan Huan", "Huadeng Wang", "Zhenbing Liu", "Rushi Lan", "Xipeng Pan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0917", "x": 0.905, "y": 1.108, "title": "End-to-End 3D Tooth Landmark Detection with Fuzzy Tooth Localization", "abstract": "Accurate detection of tooth landmarks is crucial for computer-aided orthodontic treatment. Previous methods often employ segmentation to isolate individual teeth, but rely heavily on segmentation accuracy and require annotated data. In this paper, we introduce a two-stage framework for tooth localization and landmark detection, eliminating the need for segmentation based on mesh deep learning. First, we define the fuzzy tooth regions based on landmark positions. Binary masks are generated for the tooth regions located from the original jaw mesh. By combining local features of individual teeth with the global features of the jaw model, our method predicts multiple heatmaps and the corresponding probabilities of potential landmarks for each tooth. Finally, we design a bipartite matching loss for both tooth localization and landmark detection to align the prediction set with the ground truth, thereby facilitating end-to-end inference throughout the entire process. Experimental results on the Teeth3DS+ dataset demonstrate that our method effectively detects a variable number of landmarks. Furthermore, it significantly outperforms existing baseline methods, exhibiting robust generalization and superior performance.(The code will be released at https://github.com/sikingbo/ ToothLDNet.", "filename": "2025_0917.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Kaibo Shi", "Hairong Jin", "Youyi Zheng"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0918", "x": 6.24, "y": 4.565, "title": "Explainable ADHD Diagnostic Framework Using Weakly-Supervised Action Recognition", "abstract": "The clinical diagnosis of Attention Deficit Hyperactivity Disorder (ADHD) primarily relies on scale questionnaires, clinical interviews, and executive function tests, which face challenges including limited medical resources, low diagnostic efficiency, and high dependence on clinicians' subjective experience. Existing AI-assisted diagnostic approaches based on behavioral analysis lack sufficient result interpretability, hindering their integration with conventional diagnostic workflows and practical clinical application. This paper proposes EDWAR, an Explainable ADHD Diagnostic Framework Using Weakly-Supervised Action Recognition, which establishes a collaborative diagnostic mechanism integrating behavioral analysis with traditional test records. By employing weakly-supervised action recognition methodology requiring only diagnostic labels and video-level annotations of abnormal behaviors, our framework not only achieves high diagnostic accuracy but also provides transparent interpretation through both video-level and timestep-wise anomaly action recognition. Experimental results demonstrate that EDWAR attains superior diagnostic performance while offering convincing and explainable evidence.", "filename": "2025_0918.pdf", "year": 2025, "institution": "Zhejiang University", "country": "China", "authors": ["Ninghan Fan", "Ming Kong", "Jing Huang", "Bingdi Chen", "Qiang Zhu"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0919", "x": 0.391, "y": 4.839, "title": "FDAS: Foundation Model Distillation and Anatomic Structure-Aware Multi-task Learning for Self-Supervised Medical Image Segmentation", "abstract": "Self-Supervised Learning (SSL) has shown promising results in medical image segmentation, offering advanced performance with minimal annotations. However, the absence of semantics during pre-training limits the performance of downstream tasks (e.g., organ segmentation). To address this issue, we propose a novel SSL framework via Foundation model Distillation and Anatomic Structure-aware multi-task learning (FDAS) for medical image segmentation. Specifically, we distill knowledge from the Segment Anything Model (SAM) and propose SAM-guided anatomic Structure-aware Masked Image Modeling (S2MIM), which randomly masks multiple anatomic structures in the image to enrich representation learning. For better pre-training, we introduce anatomic structure-aware multi-task learning, which integrates reconstruction and segmentation of anatomic structure-fused images to capture richer semantic information, along with fusion-based contrastive learning to preserve the semantic integrity and discriminative power of the learned representations. Experiments on two applications (cardiac MRI segmentation and fetal brain MRI segmentation) demonstrate that our method effectively improved the representation learning and outperformed several stateof-the-art SSL methods. The code is available at https://github.com/ HiLab-git/FDAS.", "filename": "2025_0919.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Xiaoran Qi", "Guoning Zhang", "Jianghao Wu", "Shaoting Zhang", "Xiaorong Hou", "Guotai Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0920", "x": 2.636, "y": 7.708, "title": "FedAMM: Federated Learning for Brain Tumor Segmentation with Arbitrary Missing Modalities", "abstract": "Brain tumor segmentation and detection have advanced significantly with the introduction of multimodal magnetic resonance imaging. However, data privacy concerns restrict most studies to centralized environments, limiting their real-world applicability. While federated learning (FL) offers a privacy-preserving solution for cross-institutional brain tumor research, existing multimodal FL approaches primarily address scenarios wherein clients possess either a single modality or complete missing modality data. These methods fail to account for the modality heterogeneity caused by arbitrary missing modalities, a frequent challenge in clinical practice. To address this issue, we propose FedAMM, a novel FL framework designed for brain tumor segmentation under arbitrary missing modalities. FedAMM incorporates multiple strategies to mitigate discrepancies arising from varying modality combinations across clients. First, FedAMM introduces a unimodal prototype distillation technique during local training to balance the contributions of different modalities. Additionally, the server aggregates multimodal prototypes uploaded by clients to generate cluster centers that represent the global modality distribution, thereby guiding local training toward global optimality. Furthermore, we implement a weighted aggregation strategy based on modality proportions. Experimental results on the BraTS2020 dataset demonstrate that FedAMM outperforms existing methods in handling arbitrary missing modalities, highlighting its strong adaptability to imbalanced and heterogeneous federated systems. The code is available at https://github.com/13sky/FedAMM.git.", "filename": "2025_0920.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Yukun Shi", "Meiting Xue", "Yan Zeng", "Jilin Zhang", "Jian Wan", "Ye Zhou"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0921", "x": -0.286, "y": 4.851, "title": "GA-SAM: Geometry-Aware SAM Adaptation with Sparse Annotation-Driven Point Cloud Completion", "abstract": "Segment Anything Model (SAM) adaptation has shown remarkable performance in medical image segmentation, but typically relies on large and precisely annotated datasets. However, acquiring such dense annotation is a labor-intensive and time-consuming task that requires significant expertise. An effective direction is to focus on sparse annotation, where only a few slices are annotated. However, sparse annotations are insufficient for capturing the complete 3D anatomical structure. To address this limitation, we innovatively leverage point cloud completion to generate robust volumetric shape from sparse annotation, offering a promising solution to this challenge. In this paper, we propose a novel Geometry-Aware SAM adaptation framework (namely GA-SAM) that integrates point cloud shape generation module with cross-view segmentation supervision mechanism. Specifically, we train a point cloud completion network to infer the 3D structure of the target anatomy. The generated point cloud shapes are then used to produce pseudo-labels, guiding the adaptation of SAM via a geometry-aware shape constraints. Furthermore, we incorporate a cross-view supervision mechanism, leveraging multi-view consistency to ensure reliable segmentation across different planes. We demonstrate the effectiveness of our method on Pancreas-CT dataset, surpassing the state-of-the-art SAM adaptation method by a Dice score of 15.25% and significantly improving segmentation robustness. Our code is available at https://github. com/ShumengLI/GA-SAM.", "filename": "2025_0921.pdf", "year": 2025, "institution": "Nanjing University", "country": "China", "authors": ["Shumeng Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0922", "x": 0.788, "y": 3.722, "title": "IM-Fuse: A Mamba-Based Fusion Block for Brain Tumor Segmentation with Incomplete Modalities", "abstract": "Brain tumor segmentation is a crucial task in medical imaging that involves the integrated modeling of four distinct imaging modalities to identify tumor regions accurately. Unfortunately, in real-life scenarios, the full availability of such four modalities is often violated due to scanning cost, time, and patient condition. Consequently, several deep learning models have been developed to address the challenge of brain tumor segmentation under conditions of missing imaging modalities. However, the majority of these models have been evaluated using the 2018 version of the BraTS dataset, which comprises only 285 volumes. In this study, we reproduce and extensively analyze the most relevant models using BraTS2023, which includes 1, 251 volumes, thereby providing a more comprehensive and reliable comparison of their performance. Furthermore, we propose and evaluate the adoption of Mamba as an alternative fusion mechanism for brain tumor segmentation in the presence of missing modalities. Experimental results demonstrate that Transformer-based architectures achieve leading performance on BraTS2023, outperforming purely convolutional models that were instead superior in BraTS2018. Meanwhile, the proposed Mamba-based architecture exhibits promising performance in comparison to state-ofthe-art models, competing and even outperforming Transformers. The source code is publicly released alongside the benchmark developed for the evaluation: https://github.com/AImageLab-zip/IM-Fuse.", "filename": "2025_0922.pdf", "year": 2025, "institution": "University of Modena and Reggio Emilia", "country": "Italy", "authors": ["Vittorio Pipoli", "Alessia Saporita", "Kevin Marchesini", "Costantino Grana", "Elisa Ficarra", "Federico Bolelli"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0923", "x": 3.672, "y": 5.986, "title": "Information Bottleneck-Based Causal Attention for Multi-label Medical Image Recognition", "abstract": "Multi-label classification (MLC) of medical images aims to identify multiple diseases and holds significant clinical potential. A critical step is to learn class-specific features for accurate diagnosis and improved interpretability effectively. However, current works focus primarily on causal attention to learn class-specific features, yet they struggle to interpret the true cause due to the inadvertent attention to classirrelevant features. To address this challenge, we propose a new structural causal model (SCM) that treats class-specific attention as a mixture of causal, spurious, and noisy factors, and a novel Information Bottleneckbased Causal Attention (IBCA) that is capable of learning the discriminative class-specific attention for MLC of medical images. Specifically, we propose learning Gaussian mixture multi-label spatial attention to filter out class-irrelevant information and capture each class-specific attention pattern. Then a contrastive enhancement-based causal intervention is proposed to gradually mitigate the spurious attention and reduce noise information by aligning multi-head attention with the Gaussian mixture multi-label spatial. Quantitative and ablation results on Endo and MuReD show that IBCA outperforms all methods. Compared to the second-best results for each metric, IBCA achieves improvements of 6.35% in CR, 7.72% in OR, and 5.02% in mAP for MuReD, 1.47% in CR, and 1.65% in CF1, and 1.42% in mAP for Endo.", "filename": "2025_0923.pdf", "year": 2025, "institution": "Shandong University", "country": "China", "authors": ["Xiaoxiao Cui", "Yiran Li", "Kai He", "Shanzhi Jiang", "Mengli Xue", "Wentao Li", "Junhong Leng", "Zhi Liu", "Lizhen Cui", "Shuo Li"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0924", "x": 1.325, "y": 4.673, "title": "Inter-class Separability Loss for Weakly Supervised Mutually Exclusive Multiclass Segmentation of Brain Tumor Lesions", "abstract": "Medical image segmentation is essential for diagnosis and treatment planning, however fully supervised deep learning methods require expensive pixel-level annotations. Weakly supervised semantic segmentation (WSSS) using class activation mapping (CAM) reduces this burden by utilizing image-level labels. While binary CAM has shown promising results, multiclass CAM remains under-explored and suffers from reduced accuracy due to weak localization signals. To address this, we propose a novel approach that improves multiclass WSSS by leveraging binary CAM to guide multiclass CAM, enhancing feature representation, inter-class boundary segmentation and prediction accuracy. Additionally, we introduce novel inter-class separability loss and agreement loss designed to enhance multiclass CAM learning by enforcing spatial consistency and class separability. Experimental results on brain tumor segmentation (BraTS) datasets demonstrate that our approach significantly enhances multiclass weakly supervised segmentation accuracy, outperforming existing methods. Our code is available at https:// github.com/Vivek-Dhamale/WSS-Interclass-Sep.", "filename": "2025_0924.pdf", "year": 2025, "institution": "Indian Institute of Science", "country": "India", "authors": ["Vivek Dhamale", "Vaanathi Sundaresan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0925", "x": 0.288, "y": 4.836, "title": "Iterative Foundation-Dedicated Learning: Optimized Key Frames, Prompts and Memories for Semi-supervised Segmentation", "abstract": "Semi-supervised learning (SSL) can effectively reduce the labor-intensive labeling required for deep learning based medical image segmentation. The emergence of visual foundation models show zero-shot capability, offering a new way of SSL. In this paper, a novel SSL framework that combines foundation and dedicated models is proposed. Unlike most existing SSL methods, where the foundation model is manually prompted to generate pseudo-labels from unlabeled images for training the dedicated model in a one-way strategy without further refinement. In our framework, foundation (SAM2) and dedicated (UNet) models are in an iterative pipeline. Specifically, in each iteration, prompts from coarse segmentation results using UNet are calculated for SAM2 to generate pseudo-labels which are used to further train the UNet for better prompts in next iteration. In this way, the pseudo-labels and UNet can be mutually improved until convergence. To enhance the performance of SAM2 in medical image segmentation, a new uncertainty-aware module using historical cues is presented to optimize key frames selection and prompts generation for SAM2. Furthermore, a new semantic-aware memory bank is introduced, where memories in the memory bank of SAM2 are divided into semantic groups. In this way, anatomical prior knowledge can be leveraged by SAM2. In the experiment, our framework is evaluated using public and in-house datasets in the context of multi-label segmentation, and the experimental results demonstrate that our framework outperforms state-of-the-art SSL methods in both datasets.", "filename": "2025_0925.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Ziman Yin", "Dong Nie", "Shuo Li", "Junjun Pan", "Zhenyu Tang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0926", "x": 0.878, "y": 1.096, "title": "ITMatch: Arch-Guided Semi-supervised Tooth Arrangement via Iterative Confidence Evaluation", "abstract": "Automated tooth arrangement is a crucial stage in digital orthodontic planning. Existing learning-based methods are based on large-scale expert-designed treatment plans, but high-quality arrangement results are difficult to obtain. Semi-supervised learning is commonly applied in scenarios with limited labeled data. However, due to the challenge of evaluating the confidence of pseudo-labels, previous works have not effectively explored semi-supervised tooth arrangement as a regression problem. To address this, we propose a semi-supervised tooth arrangement framework guided by dental arch priors and iterative confidence evaluation. We establish a teacher-student-based semi-supervised framework and introduce a weak-to-strong consistency regularization tailored for 3D point clouds. Inspired by optimization problems, we iteratively analyze errors to assess the confidence of pseudo-labels generated by the teacher network, mitigating the challenge of filtering low-quality pseudo-labels in regression. In addition, we predict the dental arch width to reduce the complexity of learning intricate transformations and leverage it as orthodontic prior information to improve arrangement accuracy. Our framework fills a critical gap in the field, and its core ideas can be generalized to other regression tasks. On a high-quality dataset, our method achieves competitive results with minimal labeled data. Code and typical data are available at https://github.com/oblivionis- tgw/ITMatch.", "filename": "2025_0926.pdf", "year": 2025, "institution": "Tsinghua University", "country": "China", "authors": ["Chengyuan Wang", "Zhihui He", "Li Chen", "Shidong Yang", "Guiyu Sun", "Fan Duan", "Shuo Wang", "Yanheng Zhou"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0927", "x": 1.847, "y": 4.269, "title": "Last Layer Laplacian Pseudocoresets for Robust Medical Image Analysis", "abstract": "Developing robust machine learning algorithms is of utmost importance for their applications to biomedical imaging applications. This issue is non-trivial, as networks are generally trained with datasets taken from relatively homogeneous samples dominated by statistically more probable disease classes, leading to unbalanced class distributions. One possible solution is to resolve the intrinsic biases towards certain dominating classes in the training datasets through more data collection with a more diverse sample, which is often prohibitively expensive. Another solution is to directly implement established uncertainty estimation measures for more robust predictions, which are nevertheless computationally demanding and insensitive to class imbalance. To address this issue, we propose a novel class-aware and uncertainty-aware pseudocoreset framework consisting of the following components: 1) An efficient framework with last layer Laplacian approximation 2) Class-aware calibration with error-based regularization, and 3) a Wasserstein distancebased regularization which explicitly imposes uncertainty-awareness. We evaluate our method for In-Distribution calibration, Out-of-Distribution inference, and class balance evaluations in two public skin cancer datasets taken from samples from different geographical location with differing skin colors. Our method outperforms various baseline uncertainty quantification and Bayesian pseudocoreset methods.", "filename": "2025_0927.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Franciskus Xaverius Erick", "Johanna Paula Müller", "Zhe Li", "Bernhard Kainz"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0928", "x": 1.926, "y": 3.751, "title": "LVPNet: A Latent-Variable-Based Prediction-Driven End-to-End Framework for Lossless Compression of Medical Images", "abstract": "Autoregressive Initial Bits is a framework that integrates sub-image autoregression and latent variable modeling, demonstrating its advantages in lossless medical image compression. However, in existing methods, the image segmentation process leads to an even distribution of latent variable information across each sub-image, which in turn causes posterior collapse and inefficient utilization of latent variables. To deal with these issues, we propose a prediction-based end-to-end lossless medical image compression method named LVPNet, leveraging global latent variables to predict pixel values and encoding predicted probabilities for lossless compression. Specifically, we introduce the Global Multiscale Sensing Module (GMSM), which extracts compact and informative latent representations from the entire image, effectively capturing spatial dependencies within the latent space. Furthermore, to mitigate the information loss introduced during quantization, we propose the Quantization Compensation Module (QCM), which learns the distribution of quantization errors and refines the quantized features to compensate for quantization loss. Extensive experiments on challenging benchmarks demonstrate that our method achieves superior compression efficiency compared to state-of-the-art lossless image compression approaches, while maintaining competitive inference speed. The code is at https://github.com/scy- Jackel/LVPNet", "filename": "2025_0928.pdf", "year": 2025, "institution": "Harbin Institute of Technology", "country": "China", "authors": ["Chenyue Song", "Chen Hui", "Qing Lin", "Wei Zhang", "Siqiao Li", "Haiqi Zhu", "Shengping Zhang", "Zhixuan Li", "Shaohui Liu", "Feng Jiang", "Xiang Li"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0929", "x": 0.569, "y": 3.932, "title": "MAMBA-Based Weakly Supervised Medical Image Segmentation with Cross-Modal Textual Information", "abstract": "In medical image segmentation, obtaining pixel-level annotated data is costly. While semi-supervised and weakly-supervised methods reduce annotation dependence, they still require some pixel-level annotations. In contrast, leveraging textual descriptions corresponding to medical images as supervisory information for segmentation is more promising. Textual descriptions are easier to acquire, as users only need to provide location and appearance details of lesions. We present TIFC-Mamba, a Mamba-based architecture for text-image fusion segmentation. The framework processes images and texts in parallel to establish cross-modal correspondences, aligning CLIP-encoded features through contrastive learning. The architecture employs a Mamba-based image encoder that reduces computational complexity compared to traditional Transformer models. We propose Mamba Fusion (MF) module integrates text and image features through Bi-Dimension Fusion (BiDF), enabling both intra-modal refinement and inter-modal interaction while preserving computational efficiency. Experiments on polyp and skin lesion datasets demonstrate competitive performance against fully supervised methods and state-of-the-art weakly-supervised approaches. Code and dataset will be available at https://github.com/PZalio/TIFCMamba.", "filename": "2025_0929.pdf", "year": 2025, "institution": "Shandong Normal University", "country": "China", "authors": ["Zhen Pan", "Wenhui Huang", "Yuanjie Zheng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0930", "x": 1.141, "y": 4.716, "title": "Masked Contrastive Language-Image Modeling For Brain Segmentation", "abstract": "Self-supervised learning (SSL) has emerged as a powerful paradigm to mitigate neuroimaging analysis algorithms' reliance on annotated data. However, existing SSL methods for brain MRI often fail to incorporate anatomical priors inherent in brain MRI, limiting their effectiveness. Here, we present Masked Contrastive Language-Image Modeling (MCLIM), a novel SSL framework that integrates knowledge from brain atlases through text-guided representation learning. We first generate structure-specific textual descriptors based on brain atlases, with no need for manually collecting image-text pairs. Then MCLIM employs (1) an image restoration branch that reconstructs randomly masked image patches through an encoder-decoder network, and (2) a cross-modal alignment module that establishes semantic correspondences between image features and atlas-derived text embeddings. These two learning objectives enable the simultaneous capture of fine-grained intensity patterns and whole-brain topological relationships. The proposed method is fine-tuned and evaluated on three brain parcellation datasets with varying granularities and a brain lesion segmentation dataset. Experiment results demonstrate that MCLIM outperforms state-of-theart SSL methods and reduces annotation effort by at least 40%. Code and pre-trained models will be available at https://github.com/CRazorback/ MCLIM.", "filename": "2025_0930.pdf", "year": 2025, "institution": "Southern University of Science and Technology", "country": "China", "authors": ["Jianwen Liang", "Junyan Lyu", "Yixuan Yuan", "Xiaoying Tang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0931", "x": 2.593, "y": 5.249, "title": "MM-DINOv2: Adapting Foundation Models for Multi-modal Medical Image Analysis", "abstract": "Vision foundation models like DINOv2 demonstrate remarkable potential in medical imaging despite their origin in natural image domains. However, their design inherently works best for uni-modal image analysis, limiting their effectiveness for multi-modal imaging tasks that are common in many medical fields, such as neurology and oncology. While supervised models perform well in this setting, they fail to leverage unlabeled datasets and struggle with missing modalities-a frequent challenge in clinical settings. To bridge these gaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the pre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our approach incorporates multi-modal patch embeddings, enabling vision foundation models to effectively process multi-modal imaging data. To address missing modalities, we employ full-modality masking, which encourages the model to learn robust cross-modality relationships. Furthermore, we leverage semi-supervised learning to harness large unlabeled datasets, enhancing both the accuracy and reliability of medical predictions. We demonstrate our approach on glioma subtype classification from multi-sequence brain MRI, achieving a Matthews Correlation D. Rueckert and B. Wiestler-Contributed equally as senior authors.", "filename": "2025_0931.pdf", "year": 2025, "institution": "Technical University of Munich (TUM)", "country": "Germany", "authors": ["Daniel Scholz", "Ayhan Can Erdur", "Viktoria Ehm", "Anke Meyer-Baese", "Jan C Peeken", "Daniel Rueckert", "Benedikt Wiestler"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0932", "x": 4.439, "y": 4.096, "title": "MOSCARD - Multimodal Opportunistic Screening for Cardiovascular Adverse Events with Causal Reasoning and De-confounding", "abstract": "Major Adverse Cardiovascular Events (MACE) remain the leading cause of mortality globally, as reported in the Global Disease Burden Study 2021. Opportunistic screening leverages data collected from routine health check-ups and multimodal data can play a key role to identify at-risk individuals. Chest X-rays (CXR) provide insights into chronic conditions contributing to major adverse cardiovascular events (MACE), while 12-lead electrocardiogram (ECG) directly assesses cardiac electrical activity and structural abnormalities. Integrating CXR and ECG could offer a more comprehensive risk assessment than conventional models, which rely on clinical scores, computed tomography (CT) measurements, or biomarkers, which may be limited by sampling bias and single modality constraints. We propose a novel predictive modeling framework -MOSCARD, multimodal causal reasoning with co-attention to align two distinct modalities and simultaneously mitigate bias and confounders in opportunistic risk estimation. Primary technical contributions are -(i) multimodal alignment of CXR with ECG guidance; (ii) integration of causal reasoning; (iii) dual back-propagation graph for deconfounding. Evaluated on internal, shift data from emergency department (ED) and external MIMIC datasets, our model outperformed single modality and state-of-the-art foundational models -AUC: 0.75, 0.83, 0.71 respectively. Proposed cost-effective opportunistic screening enables early intervention, improving patient outcomes and reducing disparities (Code will be available at https://github.com/OrchidPi/MOSCARD).", "filename": "2025_0932.pdf", "year": 2025, "institution": "Arizona State University", "country": "USA", "authors": ["Jialu Pi", "Juan Maria Farina", "Rimita Lahiri", "Jiwoong Jeong", "Archana Gurudu", "Hyung-Bok Park", "Chieh-Ju Chao", "Chadi Ayoub", "Reza Arsanjani", "Imon Banerjee"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0933", "x": 5.581, "y": 3.859, "title": "Multi-masked Querying Network for Robust Emotion Recognition from Incomplete Multi-modal Physiological Signals", "abstract": "Emotion recognition from physiological data is crucial for mental health assessment, yet it faces two significant challenges: incomplete multi-modal signals and interference from body movements and artifacts. This paper presents a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by integrating multiple querying mechanisms into a unified framework. Specifically, it uses modality queries to reconstruct missing data from incomplete signals, category queries to focus on emotional state features, and interference queries to separate relevant information from noise. Extensive experiment results demonstrate the superior emotion recognition performance of MMQ-Net compared to existing approaches, particularly under high levels of data incompleteness.", "filename": "2025_0933.pdf", "year": 2025, "institution": "Chinese Academy of Sciences", "country": "China", "authors": ["Geng-Xin Xu", "Xiang Zuo", "Ye Li"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0934", "x": 4.461, "y": 4.518, "title": "Multimodal Imputation of Imaging-Derived Phenotypes from Genomic and Blood-Based Biomarkers Enhances Common Disease Discovery", "abstract": "Medical imaging provides a wealth of information about a patient's physical condition, and Imaging-derived phenotypes (IDPs) extracted from medical images have applications in various biomedical tasks such as disease prediction and phenotype association studies. For disease prediction tasks, the collection of multimodal imaging data and the conduct of long-term follow-ups are crucial; however, the low incidence rates of certain diseases make it challenging to acquire large-scale cohort data. On the other hand, cohorts that contain genomics and blood-based biomarkers are relatively extensive. Against this backdrop, large-scale cohort data from the UK Biobank (UKB) were leveraged to construct prediction models for 260 IDPs extracted from common brain MRI and cardiac MRI using machine learning methods combined with genomics and basic blood characteristics. We applied these models to impute IDPs in cohorts missing imaging data and utilized the imputed IDPs for IDP-disease association studies and disease prediction. Association study results demonstrate that the imputed IDPs can reveal numerous IDP-disease associations. Furthermore, the disease prediction models developed using imputed IDPs demonstrated significantly superior performance across 184 common diseases, as evidenced by higher overall AUC values when compared to models utilizing real IDPs (Wilcoxon signed-rank test, p < 0.001). These results clearly highlight the significant application value of our IDPs prediction models in the context of disease discovery.", "filename": "2025_0934.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Haoyang Zhang", "Yan Li", "Junhong Liu", "Lizhen Lan", "Zian Wang", "Longyu Sun", "Yuntong Lv", "Shengxiao Yang", "Qing Li", "Mengting Sun", "Yajing Zhang", "Binghua Chen", "Xionghui Zhou", "Lianming Wu", "Chengyan Wang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0935", "x": 2.956, "y": 2.595, "title": "Multi-modal MRI Translation via Evidential Regression and Distribution Calibration", "abstract": "Multi-modal Magnetic Resonance Imaging (MRI) translation leverages information from source MRI sequences to generate target modalities, enabling comprehensive diagnosis while overcoming the limitations of acquiring all sequences. While existing deep-learning-based multi-modal MRI translation methods have shown promising potential, they still face two key challenges: 1) lack of reliable uncertainty quantification for synthesized images, and 2) limited robustness when deployed across different medical centers. To address these challenges, we propose a novel framework that reformulates multi-modal MRI translation as a multi-modal evidential regression problem with distribution calibration. Our approach incorporates two key components: 1) an evidential regression module that estimates uncertainties from different source modalities and an explicit distribution mixture strategy for transparent multi-modal fusion, and 2) a distribution calibration mechanism that adapts to source-target mapping shifts to ensure consistent performance across different medical centers. Extensive experiments on three datasets from the BraTS2023 challenge demonstrate that our framework achieves superior performance and robustness across domains.", "filename": "2025_0935.pdf", "year": 2025, "institution": "Fudan University", "country": "China", "authors": ["Jiyao Liu", "Shangqi Gao", "Yuxin Li", "Lihao Liu", "Xin Gao", "Zhaohu Xing", "Junzhi Ning", "Yanzhou Su", "Xiao-Yong Zhang", "Junjun He", "Ningsheng Xu", "Xiahai Zhuang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0936", "x": 6.945, "y": 4.418, "title": "Multi-spatial Granger Causality Features Fusion Network for Alzheimer’s Disease Classification", "abstract": "By leveraging complementary Euclidean and graph-based spatial information from structural Magnetic Resonance Imaging (sMRI), the effective fusion of multi-spatial brain features holds the potential to enhance the classification accuracy for Alzheimer's Disease (AD). Existing deep learning models often rely on simplistic methods such as concatenation, weighted summation, and self-attention to integrate Euclidean and graph spatial features. However, these models neglect the causal relationships between feature domains and labels, resulting in redundancies and limiting the classification accuracy. In this study, we propose a Multi-Spatial Granger Causality Features Fusion Network (MSGCFNet). Specifically, the MSGCFNet consists of a Multi-Spatial Features Encoder (MSFEN) module that extracts Euclidean and graph spatial features, a Multi-Spatial Granger Causality Features Disentanglement (MSGCFD) module that uses Granger causality-based learning to disentangle the causal dependencies within Euclidean and graph spatial features, and a Multi-Spatial Features Fusion Classification (MSFFC) module that employs a bidirectional cross-attention mechanism to robustly fuse the disentangled features from the two spatial features. Additionally, we design a multi-spatial Granger causal contrast disentanglement loss function that effectively minimizes the bias and redundancy of the disentangled features. Experimental results demonstrate that MSGCFNet achieves classification accuracies of 93.6% for Alzheimer's Disease (AD) vs. Normal Controls (NC) and 83.4% for Early Mild Cognitive Impairment (EMCI) vs. Late Mild Cognitive Impairment (LMCI) tasks, highlighting its superior classification performance. The code is available at https://github.com/FindBrain/MSGCFNet.", "filename": "2025_0936.pdf", "year": 2025, "institution": "Beijing Normal University", "country": "China", "authors": ["Zhiwei Song", "Jingming Li", "Hu Yu", "Xiaojuan Guo"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0937", "x": 4.562, "y": 6.702, "title": "Neural Proteomics Fields for Super-Resolved Spatial Proteomics Prediction", "abstract": "Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task-Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.", "filename": "2025_0937.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Bokai Zhao", "Weiyang Shi", "Hanqing Chao", "Zijiang Yang", "Yiyang Zhang", "Ming Song", "Tianzi Jiang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0938", "x": 3.735, "y": 6.823, "title": "PCR-MIL: Phenotype Clustering Reinforced Multiple Instance Learning for Whole Slide Image Classification", "abstract": "Multiple instance learning (MIL) has proven effective in classifying whole slide images (WSIs), owing to its weakly supervised learning framework. However, existing MIL methods still face challenges, particularly over-fitting due to small sample sizes or limited WSIs (bags). Pseudo-bags enhance MIL's classification performance by increasing the number of training bags. However, these methods struggle with noisy labels, as positive patches often occupy small portions of tissue, and pseudo-bags are typically generated by random splitting. Additionally, they face difficulties with non-discriminative instance embeddings due to the lack of domain-specific feature extractors. To address these limitations, we propose Phenotype Clustering Reinforced Multiple Instance Learning (PCR-MIL), a novel MIL framework that integrates clusteringbased pseudo-bags to improve MIL's noise robustness and the discriminative power of instance embeddings. PCR-MIL introduces two key innovations: (i) Phenotype Clustering-based Feature Selection (PCFS) selects relevant instance embeddings for prediction. It clusters instances into phenotype-specific groups, assigns positive instances to each pseudo-bag, and then uses Grad-CAM to select the most relevant positive embeddings. This approach mitigates noisy label challenges and enhances MIL's robustness to noise; (ii) Reinforced Feature Extractor (RFE) uses reinforcement learning to train an extractor based on selected clean pseudobags instead of noisy samples. This approach improves the discriminative power of extracted instance embeddings and enhances the feature representation capabilities of MIL. Experimental results on the publicly available BRACS and CRC-DX datasets demonstrate that PCR-MIL outperforms state-of-the-art methods. The code is available at: https:// github.com/JingjiaoLou/PCR-MIL.", "filename": "2025_0938.pdf", "year": 2025, "institution": "Shandong University", "country": "China", "authors": ["Jingjiao Lou", "Qingtao Pan", "Qing Yang", "Bing Ji"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0939", "x": 2.562, "y": 5.023, "title": "ProTeUS: A Spatio-Temporal Enhanced Ultrasound-Based Framework for Prostate Cancer Detection", "abstract": "Deep learning holds significant promise for enhancing realtime ultrasound-based prostate biopsy guidance through precise and effective tissue characterization. Despite recent advancements, prostate cancer (PCa) detection using ultrasound imaging still faces two critical challenges: (i) limited sensitivity to subtle tissue variations essential for detecting clinically significant disease, and (ii) weak and noisy labeling resulting from reliance on coarse annotations in histopathological reports. To address these issues, we introduce ProTeUS, an innovative spatio-temporal framework that integrates clinical metadata with comprehensive spatial and temporal ultrasound features extracted by a foundation model. Our method includes a novel hybrid, cancer involvementaware loss function designed to enhance resilience against label noise and effectively learn distinct PCa signatures. Furthermore, we employ a progressive training strategy that initially prioritizes high-involvement cases and gradually incorporates lower-involvement samples. These advancements significantly improve the model's robustness to noise and mitigate the limitations posed by weak labels, achieving state-of-the-art PCa detection performance with an AUROC of 86.9%. Our code is publicly accessible at https://github.com/DeepRCL/ProTeUS.", "filename": "2025_0939.pdf", "year": 2025, "institution": "The University of British Columbia", "country": "Canada", "authors": ["Tarek Elghareb", "Mohamed Harmanani", "Minh Nguyen Nhat To", "Paul Wilson", "Amoon Jamzad", "Fahimeh Fooladgar", "Baraa Abdelsamad", "Obed Dzikunu", "Samira Sojoudi", "Gabrielle Reznik", "Michael Leveridge", "Robert Siemens", "Silvia Chang", "Peter Black", "Parvin Mousavi", "Purang Abolmaesumi"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0940", "x": 0.713, "y": 5.091, "title": "R1Seg-3D: Rethinking Reasoning Segmentation for Medical 3D CTs", "abstract": "The explosive development of large-scale model technology has provided strong support for achieving more intelligent, robust, and precise segmentation techniques. However, owing to the unique challenges posed by medical domain data, the typical 3D medical image-text alignment model, 3D CLIP, struggles to match the performance of its natural scene counterpart. This limitation hinders the application of CLIP-based text-image reasoning in medical segmentation tasks. Furthermore, CLIP has been shown to rely on high-level semantic alignment between vision and text, lacking effective support for local visual features that are crucial for dense prediction tasks. Existing reasoning segmentation methods often adopt a redundant design with two visual encoders-one from CLIP and the other from large vision models for downstream dense tasks. This adversely affects model efficiency and complicates the training process. To address these challenges, we propose a novel framework, R1Seg-3D, which unifies a visual encoder. Our approach achieves a three-way alignment of dense visual, text reasoning, and mask decoding features within a shared latent space. Compared with previous methods, R1Seg-3D implicitly incorporates more detailed spatial features into the reasoning path. Therefore, it can strengthen the reasoning ability by incorporating additional visual spatial details and directly enhances the mask decoding process. The R1Seg-3D architecture is more concise and easier to be trained. Extensive evaluations on 25 diverse datasets demonstrate that R1Seg-3D outperforms state-of-the-art methods in both performance and stability. This work advances intelligent medical imaging and lays a foundation for future research in inference-driven segmentation. Our code and models are available at https://git hub.com/lihaoqin168/R1Seg-3D.", "filename": "2025_0940.pdf", "year": 2025, "institution": "Xinjiang University", "country": "China", "authors": ["Qin Hao", "Long Yu", "Shengwei Tian", "Xujiong Ye", "Lei Zhang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0941", "x": 2.149, "y": 5.827, "title": "Rethinking Multi-view Mammogram Representation Learning via Counterfactual Reasoning with Kolmogorov-Arnold Theorem", "abstract": "Cross-view interference caused by impure shared information for multiview mammogram representation. Existing methods are accustomed to assuming that purely complementary shared information are provided between multiple views, ignoring the negative side of the shared information. To address this issue, we propose the first Dual-view Mammography Causal Graph (DMCG) to model multi-view representation by capturing direct and mediation effects. Based on DMCG, we propose MammoCRKAN, the first counterfactual reasoning paradigm integrating the Kolmogorov-Arnold theorem for decoupling interfering information. MammoCRKAN comprises two key modules: the Spherical Sample Module (SSM), which enhances the direct effect of tumor features by aligning consistent geometric representations, and the Kolmogorov-Arnold Aggregate Module (KAAM), which decomposes complex joint causality into univariate effects to mitigate negative side of mediation effects. Moreover, We find that heterogeneous channel allocations across views outperform fixed matching channels. Extensive experiments on four publicly available mammogram datasets demonstrate the effectiveness of MammoCRKAN. Code is available at https://guoli-w.github.io/ MammoCRKAN.", "filename": "2025_0941.pdf", "year": 2025, "institution": "Shandong University of Traditional Chinese Medicine", "country": "China", "authors": ["Guoli Wang", "Benzheng Wei", "Shuo Li"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0942", "x": 0.885, "y": 4.759, "title": "Revisiting 3D Medical Scribble Supervision: Benchmarking Beyond Cardiac Segmentation", "abstract": "Scribble supervision has emerged as a promising approach for reducing annotation costs in medical 3D segmentation by leveraging sparse annotations instead of voxel-wise labels. While existing methods report strong performance, a closer analysis reveals that the majority of research is confined to the cardiac domain, predominantly using ACDC and MSCMR datasets. This over-specialization may contribute to overfitting, overly optimistic performance claims, and limited generalization across broader segmentation tasks. In this work, we formulate a set of key requirements for practical scribble supervision and introduce ScribbleBench, a comprehensive benchmark spanning over seven diverse medical imaging datasets, to systematically evaluate the fulfillment of these requirements. Consequently, we uncover a general failure of methods to generalize across tasks and that many widely used novelties degrade performance outside of the cardiac domain, whereas simpler overlooked approaches achieve superior generalization. Finally, we raise awareness for a strong yet overlooked baseline, nnU-Net coupled with a partial loss, which consistently outperforms specialized methods across a diverse range of tasks. By identifying fundamental limitations in existing research and establishing a new benchmark-driven evaluation standard, this work aims to steer scribble supervision toward more practical, robust, and generalizable methodologies for medical image segmentation.", "filename": "2025_0942.pdf", "year": 2025, "institution": "Division of Medical Image Computing", "country": "Germany", "authors": ["Karol Gotkowski", "Klaus H Maier-Hein", "Fabian Isensee"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0943", "x": 3.602, "y": 4.855, "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation", "abstract": "Ophthalmologists often rely on multimodal data to improve diagnostic precision. However, data on complete modalities are rare in real applications due to a lack of medical equipment and data privacy concerns. Traditional deep learning approaches usually solve these problems by learning representations in latent space. However, we highlight two critical limitations of these current approaches: (i) Task-irrelevant redundant information existing in complex modalities (e.g., massive slices) leads to a significant amount of redundancy in latent space representations. (ii) Overlapping multimodal representations make it challenging to extract features that are unique to each modality. To address these, we introduce the Essence-Point and Disentangle Representation Learning (EDRL) strategy that integrates a self-distillation mechanism into an end-to-end framework to enhance feature selection and disentanglement for robust multimodal learning. Specifically, Essence-Point Representation Learning module selects discriminative features that enhance disease grading performance. Moreover, the Disentangled Representation Learning module separates multimodal data into modality-common and modality-unique representations, reducing feature entanglement and enhancing both robustness and interpretability in ophthalmic disease diagnosis. Experiments on ophthalmology multimodal datasets demonstrate that the proposed EDRL strategy outperforms the state-of-the-art methods significantly. Code is available at GitHub Repository.", "filename": "2025_0943.pdf", "year": 2025, "institution": "MBZUAI", "country": "United Arab Emirates", "authors": ["Xinkun Wang", "Yifang Wang", "Senwei Liang", "Feilong Tang", "Chengzhi Liu", "Ming Hu", "Chao Hu", "Junjun He", "Zongyuan Ge", "Imran Razzak"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0944", "x": 3.571, "y": 2.703, "title": "SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging", "abstract": "Magnetic resonance imaging (MRI) provides detailed softtissue characteristics that assist in disease diagnosis and screening. However, the accuracy of clinical practice is often hindered by missing or unusable slices due to various factors. Volumetric MRI synthesis methods have been developed to address this issue by imputing missing slices from available ones. The inherent 3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR), poses significant challenges for missing slice imputation approaches, including (1) the difficulty of modeling local inter-slice correlations and dependencies of volumetric slices, and (2) the limited exploration of crucial 3D spatial information and global context. In this study, to mitigate these issues, we present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the dependency on complete volumetric data, featuring two main innovations: (1) a volumetric slice graph completion module that incorporates the inter-slice relationships into a graph structure, and (2) a volumetric spatial adapter component that enables our model to effectively capture and utilize various forms of 3D spatial context. Extensive experiments on cardiac MRI datasets demonstrate that SAGCNet is capable of synthesizing absent CMR slices, outperforming competitive state-of-the-art MRI synthesis methods both quantitatively and qualitatively. Notably, our model maintains superior performance even with limited slice data. Code is available at https://github.com/JK-Liu7/SAGCNet.", "filename": "2025_0944.pdf", "year": 2025, "institution": "University of Birmingham", "country": "UK", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N Arvanitis", "Stefan K Piechnik", "Joao A C Lima", "Steffen E Petersen", "Le Zhang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0945", "x": 2.913, "y": 4.795, "title": "Scalp Diagnostic System with Label-Free Segmentation and Training-Free Image Translation", "abstract": "Scalp disorders are highly prevalent worldwide, yet remain underdiagnosed due to limited access to expert evaluation and the high cost of annotation. Although AI-based approaches hold great promise, their practical deployment is hindered by challenges such as severe data imbalance and the absence of pixel-level segmentation labels. To address these issues, we propose \"ScalpVision\", an AI-driven system for the holistic diagnosis of scalp diseases. In ScalpVision, effective hair segmentation is achieved using pseudo image-label pairs and an innovative prompting method in the absence of traditional hair masking labels. Additionally, ScalpVision introduces DiffuseIT-M, a generative model adopted for dataset augmentation while maintaining hair information, facilitating improved predictions of scalp disease severity. Our experimental results affirm ScalpVision's efficiency in diagnosing a variety of scalp conditions, showcasing its potential as a valuable tool in dermatological care. Our code is available at https://github.com/winston1214/ScalpVision.", "filename": "2025_0945.pdf", "year": 2025, "institution": "Yonsei University", "country": "Korea", "authors": ["Youngmin Kim", "Saejin Kim", "Hoyeon Moon", "Youngjae Yu", "Junhyug Noh"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0946", "x": 5.738, "y": 4.116, "title": "Self-supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making", "abstract": "High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rulebased detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors. The code is available at https://github. com/roychowdhuryresearch/SS2LD.", "filename": "2025_0946.pdf", "year": 2025, "institution": "Samueli School of Engineering", "country": "USA", "authors": ["Yipeng Zhang", "Yuanyi Ding", "Chenda Duan", "Atsuro Daida", "Hiroki Nariai", "Vwani Roychowdhury"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_0947", "x": 0.674, "y": 4.647, "title": "Semi-supervised Multi-modal Medical Image Segmentation for Complex Situations", "abstract": "Semi-supervised learning addresses the issue of limited annotations in medical images effectively, but its performance is often inadequate for complex backgrounds and challenging tasks. Multimodal fusion methods can significantly improve the accuracy of medical image segmentation by providing complementary information. However, they face challenges in achieving significant improvements under semi-supervised conditions due to the challenge of effectively leveraging unlabeled data. There is a significant need to create an effective and reliable multi-modal learning strategy for leveraging unlabeled data in semi-supervised segmentation. To address these issues, we propose a novel semi-supervised multi-modal medical image segmentation approach, which leverages complementary multi-modal information to enhance performance with limited labeled data. Our approach employs a multi-stage multi-modal fusion and enhancement strategy to fully utilize complementary multi-modal information, while reducing feature discrepancies and enhancing feature sharing and alignment. Furthermore, we effectively introduce contrastive mutual learning to constrain prediction consistency across modalities, thereby facilitating the robustness of segmentation results in semi-supervised tasks. Experimental results on two multi-modal datasets demonstrate the superior performance and robustness of the proposed framework, establishing its valuable potential for solving medical image segmentation tasks in complex scenarios. The code is available at: https://github.com/DongdongMeng/SMMS.", "filename": "2025_0947.pdf", "year": 2025, "institution": "Peking University", "country": "China", "authors": ["Dongdong Meng", "Sheng Li", "Hao Wu", "Guoping Wang", "Xueqing Yan"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0948", "x": 0.334, "y": 4.543, "title": "Spatial Aggregation for Semi-supervised Active Learning in 3D Medical Image Segmentation", "abstract": "Existing active learning (AL)-based 3D medical image segmentation methods often select images, slices, or patches as isolated entities, overlooking inter-slice spatial relationships in 3D images. Additionally, AL methods train the segmentation model on labeled data only and ignore valuable unlabeled data. Both factors limit its ability to further reduce labeled data needs. To address these problems, we propose a novel semi-supervised AL approach termed SpaTial AggRegation (STAR), which enables the model to learn from unlabeled data beyond annotated samples by leveraging spatial correlations between slices, reducing labeling costs. In each AL iteration, STAR employs a spatial cross-attention mechanism to transfer relevant knowledge from adjacent labeled slices to unlabeled ones by generating pseudo-labels. These pseudo-labeled slices and queried slices are used to train the segmentation model. The experimental results indicate that STAR outperforms other state-of-the-art AL methods, achieving fully supervised 3D segmentation performance with as little as 18%-19% of the labeled data. The code is available at https://github.com/HelenMa9998/STAR.", "filename": "2025_0948.pdf", "year": 2025, "institution": "University College Dublin", "country": "Ireland", "authors": ["Siteng Ma", "Honghui Du", "Dairui Liu", "Kathleen M Curran", "Aonghus Lawlor", "Ruihai Dong"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0949", "x": 3.135, "y": 2.496, "title": "Structure-Aware MRI Translation: Multi-modal Latent Diffusion Model with Arbitrary Missing Modalities", "abstract": "Multi-modal Magnetic Resonance Imaging (MRI) plays a crucial role in clinical diagnosis by providing complementary anatomy and pathology information. However, incomplete acquisitions remain common due to practical constraints such as cost, scan time and image corruption. Recently, the diffusion model has shown significant potential in the medical image-to-image translation task. However, most existing diffusion-based synthesis models are constrained to fixed inputoutput modality pairs, lacking the flexibility to handle arbitrary missing scenarios. Furthermore, these approaches inevitably sacrifice anatomical structures consistency and degrade critical texture details during generation, potentially leading to the misdiagnosis of subtle pathological patterns. To address these issues, we propose MISA-LDM, the first many-to-many MRI synthesis framework with modality-invariant structure awareness based on the latent diffusion model. Our approach enables the synthesis of missing modalities within a single model by utilizing any available combinations of modalities. Meanwhile, we introduce a Structure-Preserving Module (SPM) that employs a disentanglement strategy to obtain modality-invariance structural representation and use high-frequency information as a supplement. We use the anatomical priors obtained by SPM to guide the diffusion process, preserving anatomical structures integrity. Extensive experiments conducted on the BraTS2020 and BraTS2021 datasets demonstrate the superiority of our method. The result confirms the necessity of introducing more comprehensive anatomical priors for preserving generation consistency in multimodal MRI translation. The source code is available at https://github. com/yichen-byte/misa-ldm.", "filename": "2025_0949.pdf", "year": 2025, "institution": "Northeastern University", "country": "China", "authors": ["Xinzhe Zhang", "Junjie Liang", "Peng Cao", "Jinzhu Yang", "Osmar R Zaiane"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0950", "x": 2.434, "y": 4.974, "title": "Subtyping Breast Lesions via Generative Augmentation Based Long-Tailed Recognition in Ultrasound", "abstract": "Accurate identification of breast lesion subtypes can facilitate personalized treatment and interventions. Ultrasound (US), as a safe and accessible imaging modality, is extensively employed in breast abnormality screening and diagnosis. However, the incidence of different subtypes exhibits a skewed long-tailed distribution, posing significant challenges for automated recognition. Generative augmentation provides a promising solution to rectify data distribution. Inspired by this, we propose a dual-phase framework for long-tailed classification that mitigates distributional bias through high-fidelity data synthesis while avoiding overuse that corrupts holistic performance. The framework incorporates a reinforcement learning-driven adaptive sampler, dynamically calibrating synthetic-real data ratios by training a strategic multi-agent to compensate for scarcities of real data while ensuring stable discriminative capability. Furthermore, our class-controllable synthetic network integrates a sketch-grounded perception branch that harnesses anatomical priors to maintain distinctive class features while enabling annotationfree inference. Extensive experiments on an in-house long-tailed and a public imbalanced breast US datasets demonstrate that our method achieves promising performance compared to state-of-the-art approaches. More synthetic images can be found at https://github.com/Stinalalala/ Breast-LT-GenAug.", "filename": "2025_0950.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Shijing Chen", "Xinrui Zhou", "Yuhao Wang", "Yuhao Huang", "Ao Chang", "Dong Ni", "Ruobing Huang"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0951", "x": 0.675, "y": 4.713, "title": "Synergy-Guided Regional Supervision of Pseudo Labels for Semi-supervised Medical Image Segmentation", "abstract": "Semi-supervised learning has received considerable attention for its potential to leverage abundant unlabeled data to enhance model robustness. Despite the widespread adoption of pseudo labeling in semisupervised learning, existing methods often suffer from noise contamination, which can undermine the robustness of the model. To tackle this challenge, we introduce a novel Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework. Built upon the mean teacher network, we employ a Mix Augmentation module to enhance the unlabeled data. By evaluating the synergy before and after augmentation, we strategically partition the pseudo labels into distinct regions. Additionally, we introduce a Region Loss Evaluation module to assess the loss across each delineated area. Extensive experiments conducted on the LA, Pancreas-CT and BraTS2019 dataset have demonstrated superior performance over current state-of-the-art techniques, underscoring the efficiency and practicality of our framework. The code is available at https://github.com/ortonwang/SGRS-Net.", "filename": "2025_0951.pdf", "year": 2025, "institution": "Fuzhou University", "country": "China", "authors": ["Tao Wang", "Xinlin Zhang", "Yuanbin Chen", "Yuanbo Zhou", "Longxuan Zhao", "Tao Tan", "Tong Tong"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0952", "x": 1.673, "y": 5.758, "title": "Synthetic Ground Truth Counterfactuals for Comprehensive Evaluation of Causal Generative Models in Medical Imaging", "abstract": "Counterfactuals in medical imaging are synthetic representations of how an individual's medical image might appear under alternative, typically unobservable conditions, which have the potential to address data limitations and enhance interpretability. However, counterfactual images, which can be generated by causal generative models (CGMs), are inherently hypothetical-raising questions of how to properly validate that they are realistic and accurately reflect the intended modifications. A common approach for quantitatively evaluating CGMgenerated counterfactuals involves using a discriminative model as a 'pseudo-oracle' to assess whether interventions on specific variables are effective. However, this method is not well-suited for in-depth error identification and analysis of CGMs. To address this limitation, we propose to leverage synthetic, 'ground truth' counterfactual datasets as a novel approach for debugging and evaluating CGMs. These synthetic datasets enable the computation of global performance metrics and precise localization of CGM failure modes. To further quantify failures, we introduce a novel metric, the Triangulation of Effectiveness and Amplification (TEA), which precisely quantifies the effectiveness of target variable interventions and the additional amplification of unintended effects. We test and validate our evaluation framework on two state-of-the-art CGMs where the results demonstrate the utility of synthetic datasets in identifying failure modes of CGMs, and highlight the potential of the proposed TEA metric as a robust tool for evaluation of their performance. Code and data are available at https://github.com/ucalgary-miplab/TEA.", "filename": "2025_0952.pdf", "year": 2025, "institution": "University of Calgary", "country": "Canada", "authors": ["Emma A M Stanley", "Vibujithan Vigneshwaran", "Erik Y Ohara", "Finn G Vamosi", "Nils D Forkert", "Matthias Wilms"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0953", "x": 3.596, "y": 2.669, "title": "Temporal Modulated Multi-scale Deformation Fusion via Knowledge Distillation for 4D Medical Image Interpolation", "abstract": "The acquisition of 4D medical images, which are crucial for monitoring disease progression, poses significant challenges due to the expensive cost and the imaging mechanism constraints. Existing solutions attempt to interpolate the volumes between the acquired volumes with linearly scaling the initial bidirectional deformation between two distant phases like end-systole and end-diastole, to generate detailed 4D image. However, the simple linear motion assumption fails to accurately model the anisotropic deformation induced by respiration and heartbeat. In this paper, we propose a temporal modulated multi-scale deformation fusion framework for 4D medical image interpolation via knowledge distillation, to directly generate the bidirectional deformation and volume at any intermediate time without the sub-optimal linear motion assumption. Guided by the teacher model with extensive priors, the student model, modulated by surrogate timestamps, learns to approximate the deformation modeling ability of teacher without any need for intermediate volumes. Particularly, a multi-scale deformation fusion decoder is proposed including the temporal modulated deformation feature generator and the deformation fusion module. The former generates modulation parameters with timestamps for temporal-aware transformation and then models the bidirectional deformation in a coarse-to-fine manner. While the latter adaptively fuses deformation features at different scales to improve the accuracy of predicted deformation. Compared with nine state-of-the-art methods, the proposed method achieves superior performance on two public datasets, fully demonstrating its effectiveness and generalization.", "filename": "2025_0953.pdf", "year": 2025, "institution": "Beijing Institute of Technology", "country": "China", "authors": ["Jiaju Zhang", "Danni Ai", "Zhikun Gan", "Tianyu Fu", "Jingfan Fan", "Hong Song", "Deqiang Xiao", "Jian Yang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0954", "x": 3.408, "y": 1.794, "title": "Towards Globally Predictable k-Space Interpolation: A White-Box Transformer Approach", "abstract": "Interpolating missing data in k-space is essential for accelerating imaging. However, existing methods, including convolutional neural network-based deep learning, primarily exploit local predictability while overlooking the inherent global dependencies in k-space. Recently, Transformers have demonstrated remarkable success in natural language processing and image analysis due to their ability to capture long-range dependencies. This inspires the use of Transformers for k-space interpolation to better exploit its global structure. However, their lack of interpretability raises concerns regarding the reliability of interpolated data. To address this limitation, we propose GPI-WT, a white-box Transformer framework based on Globally Predictable Interpolation (GPI) for k-space. Specifically, we formulate GPI from the perspective of annihilation as a novel k-space structured low-rank (SLR) model. The global annihilation filters in the SLR model are treated as learnable parameters, and the subgradients of the SLR model naturally induce a learnable attention mechanism. By unfolding the subgradient-based optimization algorithm of SLR into a cascaded network, we construct the first whitebox Transformer specifically designed for accelerated MRI. Experimental results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in k-space interpolation accuracy while providing superior interpretability.", "filename": "2025_0954.pdf", "year": 2025, "institution": "Inner Mongolia University", "country": "China", "authors": ["Chen Luo", "Qiyu Jin", "Taofeng Xie", "Xuemei Wang", "Huayu Wang", "Congcong Liu", "Liming Tang", "Guoqing Chen", "Zhuo-Xu Cui", "Dong Liang"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_0955", "x": 3.078, "y": 6.46, "title": "Training State-of-the-Art Pathology Foundation Models with Orders of Magnitude Less Data", "abstract": "The field of computational pathology has recently seen rapid advances driven by the development of modern vision foundation models (FMs), typically trained on vast collections of pathology images. Recent studies demonstrate that increasing the training data set and model size and integrating domain-specific image processing techniques can significantly enhance the model's performance on downstream tasks. Building on these insights, our work incorporates several recent modifications to the standard DINOv2 framework from the literature to optimize the training of pathology FMs. We also apply a post-training procedure for fine-tuning models on higher-resolution images to further enrich the information encoded in the embeddings. We present three novel pathology FMs trained on up to two orders of magnitude fewer WSIs than those used to train other state-of-the-art FMs while demonstrating a comparable or superior performance on downstream tasks. Even the model trained on TCGA alone (12k WSIs) outperforms most existing FMs and, on average, matches Virchow2, the second-best FM published to date. This suggests that there remains a significant potential for further improving the models and algorithms used to train pathology FMs to take full advantage of the vast data collections.", "filename": "2025_0955.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Mikhail Karasikov", "Joost Van Doorn", "Nicolas Känzig", "Melis Erdal Cesur", "Hugo Mark Horlings", "Robert Berke", "Fei Tang", "Sebastian Otálora"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0956", "x": 1.074, "y": 4.849, "title": "Tree-Based Semantic Losses: Application to Sparsely-Supervised Large Multi-class Hyperspectral Segmentation", "abstract": "Hyperspectral imaging (HSI) shows great promise for surgical applications, offering detailed insights into biological tissue differences beyond what the naked eye can perceive. Refined labelling efforts are underway to train vision systems to distinguish large numbers of subtly varying classes. However, commonly used learning methods for biomedical segmentation tasks penalise all errors equivalently and thus fail to exploit any inter-class semantics in the label space. In this work, we introduce two tree-based semantic loss functions which take advantage of a hierarchical organisation of the labels. We further incorporate our losses in a recently proposed approach for training with sparse, backgroundfree annotations. Extensive experiments demonstrate that our proposed method reaches state-of-the-art performance on a sparsely annotated HSI dataset comprising 107 classes organised in a clinically-defined semantic tree structure. Furthermore, our method enables effective detection of out-of-distribution (OOD) pixels without compromising segmentation performance on in-distribution (ID) pixels.", "filename": "2025_0956.pdf", "year": 2025, "institution": "King's College London", "country": "UK", "authors": ["Junwen Wang", "Oscar Maccormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0957", "x": 2.839, "y": 3.335, "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "abstract": "Tubular tree structures, such as blood vessels and airways, are essential in human anatomy, and accurately tracking them while preserving their topology is crucial for various downstream tasks. Trexplorer is a recurrent model designed for centerline tracking in 3D medical images, but it is prone to predicting duplicate branches and terminating tracking prematurely. To address these issues, we present Trexplorer Super, an enhanced version that substantially improves performance through several novel advancements. Evaluating centerline tracking models is challenging due to the lack of public benchmark datasets. To enable thorough evaluation, we develop three centerline datasets, one synthetic and two real, each with increasing difficulty. Using these datasets, we perform a comprehensive comparison of existing state-of-the-art (SOTA) models with our approach. Trexplorer Super outperforms previous SOTA models on every dataset. Our results also highlight that strong performance on synthetic data does not necessarily translate to real datasets.", "filename": "2025_0957.pdf", "year": 2025, "institution": "Chalmers University of Technology", "country": "Sweden", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alvén", "Lennart Svensson", "Fredrik Kahl"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0958", "x": 2.077, "y": 3.088, "title": "UltrON: Ultrasound Occupancy Networks", "abstract": "In free-hand ultrasound imaging, sonographers rely on expertise to mentally integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can assist clinicians in this process. Central to this task is the choice of shape representation, as it determines how accurately and efficiently the structure can be visualized, analyzed, and interpreted. Implicit representations, such as SDF and occupancy function, offer a powerful alternative to traditional voxel-or mesh-based methods by modeling continuous, smooth surfaces with compact storage, avoiding explicit discretization. Recent studies demonstrate that SDF can be effectively optimized using annotations derived from segmented Bmode ultrasound images. Yet, these approaches hinge on precise annotations, overlooking the rich acoustic information embedded in B-mode intensity. Moreover, implicit representation approaches struggle with the ultrasound's view-dependent nature and acoustic shadowing artifacts, which impair reconstruction. To address the problems resulting from occlusions and annotation dependency, we propose an occupancy-based representation and introduce Ultrasound Occupancy Network (UltrON) that leverages acoustic features to improve geometric consistency in weakly-supervised optimization regime. We show that these features can be obtained from B-mode images without additional annotation cost. Moreover, we propose a novel loss function that compensates for viewdependency in the B-mode images and facilitates occupancy optimization from multiview ultrasound. By incorporating acoustic properties, (UltrON) generalizes to shapes of the same anatomy. We show that (UltrON) mitigates the limitations of occlusions and sparse labeling and paves the way for more accurate 3D reconstruction. Code and dataset is available at https://github.com/magdalena-wysocki/ultron.", "filename": "2025_0958.pdf", "year": 2025, "institution": "Technical University of Munich", "country": "Germany", "authors": ["Magdalena Wysocki", "Felix Duelmer", "Ananya Bal", "Nassir Navab", "Mohammad Farid Azampour"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0959", "x": -0.237, "y": 4.73, "title": "UM-SAM: Unsupervised Medical Image Segmentation Using Knowledge Distillation from Segment Anything Model", "abstract": "Despite the success of deep learning in automatic medical image segmentation, it heavily relies on manual annotations for training that are time-consuming to obtain. Unsupervised segmentation approaches have shown potential in eliminating manual annotations, while they often struggle to capture distinctive features for low-contrast and inhomogeneous regions, limiting their performance. To address this, we propose UM-SAM, a novel unsupervised medical image segmentation framework that harnesses Segment Anything Model (SAM)'s capabilities for pseudo-label generation and segmentation network training. Specifically, class-agnostic pseudo-labels are generated via SAM's everything mode, followed by a shape prior-based filtering strategy to select valid pseudo-labels. Given SAM's lack of class information, a shape-agnostic clustering technique based on ROI pooling is proposed to identify targetrelevant pseudo-labels based on their proximity. To reduce the impact of noise in pseudo-labels, a triple Knowledge Distillation (KD) strategy is proposed to transfer knowledge from SAM to a lightweight task-specific segmentation model, including pseudo-label KD, class-level feature KD, and class-level contrastive KD. Extensive experiments on fetal brain and prostate segmentation tasks demonstrate that UM-SAM significantly outperforms existing unsupervised and prompt-based methods, achieving state-of-the-art performance without requiring manual annotations.", "filename": "2025_0959.pdf", "year": 2025, "institution": "University of Electronic Science and Technology of China", "country": "China", "authors": ["Jia Fu", "He Li", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0960", "x": 0.959, "y": 1.147, "title": "VBCD: A Voxel-Based Framework for Personalized Dental Crown Design", "abstract": "The design of restorative dental crowns from intraoral scans is labor-intensive for dental technicians. To address this challenge, we propose a novel voxel-based framework for automated dental crown design (VBCD). The VBCD framework generates an initial coarse dental crown from voxelized intraoral scans, followed by a fine-grained refiner incorporating distance-aware supervision to improve accuracy and quality. During the training stage, we employ the Curvature and Margin line Penalty Loss (CMPL) to enhance the alignment of the generated crown with the margin line. Additionally, a positional prompt based on the FDI tooth numbering system is introduced to further improve the accuracy of the generated dental crowns. Evaluation on a large-scale dataset of intraoral scans demonstrated that our approach outperforms existing methods, providing a robust solution for personalized dental crown design. The related code is in: https://github.com/lullcant/VBCD.", "filename": "2025_0960.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "Hong Kong", "authors": ["Linda Wei", "Chang Liu", "Wenran Zhang", "Zengji Zhang", "Shaoting Zhang", "Hongsheng Li"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0961", "x": 3.699, "y": 6.533, "title": "Weakly Semi-supervised Cervical Lesion Cell Detection via Twin-Memory Augmented Multiple Instance Learning", "abstract": "Deep learning methods have demonstrated promising results in cervical lesion cell detection. Training detection models that generalize well typically require a large amount of cell-level annotations that are expensive and time-consuming to obtain. Instead, weak slide-level annotations, which entail assigning a gigapixel whole slide image (WSI) with a single label, are easier to acquire. However, due to significant differences in annotation scales, they cannot be directly utilized to assist in the training of cervical cell detectors. To address this challenge, we propose a Twin-memory augmented Multiple Instance Learning (Twin-MIL) framework to refine cervical lesion cell detection. Firstly, we utilize the multiple instance learning to bridge the gap between cell-level and slide-level tasks. Then, we reduce false positives in conventional MIL by introducing a twin-memory module, which improves the classification capability by capturing more discriminative patterns of positive and negative cells. We also propose uncertainty-regulated negative instance learning to enhance the robustness of negative latent space against noisy instances and its separability from the positive one. Experiments indicate that our method is effective in enhancing different detection models trained on the datasets with varying annotation levels.", "filename": "2025_0961.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Manman Fei", "Zhiyun Song", "Zhenrong Shen", "Mengjun Liu", "Qian Wang", "Lichi Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0962", "x": 1.326, "y": 2.078, "title": "Weakly-Supervised 2D/3D Image Registration via Differentiable X-ray Rendering and ROI Segmentation", "abstract": "Accurate registration between intraoperative 2D images and preoperative 3D anatomical structures is a prerequisite for image-guided minimally invasive surgery. Existing approaches for 2D/3D rigid registration, particularly those for X-ray to CT image registration, primarily rely on grayscale-based image similarity metrics. However, such metrics often fail to capture the optimal projection transformation due to their limited contextual information. To address this issue, we propose a novel and intuitive correspondence representation: the overlap of multiple corresponding Regions of Interest (ROIs). By introducing the differentiable Dice coefficient computed on the projection image, we establish a direct link between segmentation and registration within our weakly supervised 2D/3D registration framework. This framework comprises two stages-a learning-based preoperative stage and an optimization-based intraoperative stage-both of which leverage the ROI-based Dice score as a differentiable supervision signal. Additionally, we integrate automatic segmentation methods (e.g., UNet) and prompt-based methods (e.g., MedSAM) into the framework to investigate the impact of different segmentation approaches on registration performances. Furthermore, we validate the generalization ability of the proposed framework by integrating the ROI-based similarity with various similarity measures. Extensive experiments conducted on the DeepFluoro dataset yielded an mTRE of 0.67 ± 1.34 mm, with rotational and translational error values being 0.2 ± 0.5 • and 1.6 ± 2.9 mm respectively, outperforming existing state-of-the-art methods. The codes are available at https://github.com/ CYXYZ/WSReg.", "filename": "2025_0962.pdf", "year": 2025, "institution": "Shandong University", "country": "China", "authors": ["Yuxin Cui", "Null- H Meng", "Zhe Min"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0963", "x": 0.973, "y": 1.23, "title": "3D Dynamic Prediction of Missing Teeth in Diverse Patterns via Centroid-Prompted Diffusion Model", "abstract": "Dental implantation restores missing teeth through surgical insertion of artificial roots, relying on preoperative digital planning to ensure precision and efficiency. However, critical challenges persist in virtual tooth positioning: this process demands extensive clinical expertise and time-consuming manual adjustments due to ambiguous anatomical references from missing teeth. To address these limitations, we propose a unified framework that accurately predicts the original three dimensional (3D) shapes and positions of missing teeth in diverse patterns, enabling anatomy-aware preoperative planning. Our proposal introduces two technical innovations: (1) A dynamic iterative generation strategy is proposed to progressively predict multiple missing teeth one by one using a target tooth identification module, accommodating arbitrary tooth loss patterns without case-specific retraining; (2) A tooth-centroid-prompted conditional diffusion model is developed to leverage geometric constraints from predicted tooth centroid and adjacent teeth to generate high-fidelity point cloud reconstructions. Extensive experiments show that our model outperforms conventional U-net based framework in predicting multiple missing teeth, achieving a prediction accuracy of 1.30 mm (Chamfer Distance) and an angular error of 5.42 • . This improvement has the potential to enhance the accuracy and efficiency of dental implant planning by providing precise anatomical references for missing teeth, potentially revolutionizing digital dentistry workflows.", "filename": "2025_0963.pdf", "year": 2025, "institution": "Tongji University", "country": "China", "authors": ["Zongrui Ji", "Na Li", "Peng Xue", "Yi Dong", "Lei Ma"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0964", "x": 0.421, "y": 2.186, "title": "6D Object Pose Tracking for Orthopedic Surgical Training Using Visual-Inertial Sensor Fusion", "abstract": "Digital training simulators play a growing role in orthopedic surgery, offering realistic, standardized, and risk-free learning environments without the need for constant expert supervision. To enable simulators with realistic tactile feedback and haptic sensations, accurate tracking of surgical tools and anatomical structures in real-time is required. However, existing object tracking solutions are often expensive, difficult to integrate into training workflows, or lack robustness. To address these limitations, we propose a novel visual-inertial 6D object pose tracking system for orthopedic surgical training. Our approach features a custom fiducial object that combines multiple ArUco markers with an Inertial Measurement Unit, a dual-camera setup to improve occlusion robustness, and a sensor fusion algorithm that integrates highfrequency IMU data with vision-based tracking while ensuring precise coordinate and time synchronization. In our evaluation, we achieve a fiducial object pose accuracy of 0.9 mm/0.5 • and extract drill hole metrics in a mock surgical procedure with average position, angle, and length errors of 1.7 mm, 2.0 • , and 1.0 mm, respectively, while demonstrating low occlusion rates. Our cost-effective and easily integrated solution meets clinical training requirements and marks a step towards scalable and widely accessible digital orthopedic simulators. The tracking code is available at https://github.com/MountainCoot/fusionpose.", "filename": "2025_0964.pdf", "year": 2025, "institution": "ETH Zurich", "country": "Switzerland", "authors": ["Maarten Hogenkamp", "Tobias Stauffer", "Quentin Lohmeyer", "Mirko Meboldt"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0965", "x": 1.276, "y": 2.318, "title": "A Novel Framework for Integrating 3D Ultrasound Into Percutaneous Liver Tumour Ablation", "abstract": "3D ultrasound (US) imaging has shown significant benefits in enhancing the outcomes of percutaneous liver tumour ablation. Its clinical integration is crucial for transitioning 3D US into the therapeutic domain. However, challenges of tumour identification in US images continue to hinder its broader adoption. In this work, we propose a novel framework for integrating 3D US into the standard ablation workflow. We present a key component, a clinically viable 2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to reduce registration complexity. To facilitate efficient verification of the registration workflow, we also propose an intuitive multimodal image visualization technique. In our study, 2D US-CT/MRI registration achieved a landmark distance error of ∼2-4 mm with a runtime of 0.22 s per image pair. Additionally, non-rigid registration reduced the mean alignment error by ∼40% compared to rigid registration. Results demonstrated the efficacy of the proposed 2D US-CT/MRI registration workflow. Our integration framework advanced the capabilities of 3D US imaging in improving percutaneous tumour ablation, demonstrating the potential to expand the therapeutic role of 3D US in clinical interventions.", "filename": "2025_0965.pdf", "year": 2025, "institution": "Western University", "country": "Canada", "authors": ["Shuwei Xing", "Derek W Cool", "David Tessier", "Elvis C S Chen", "Terry M Peters", "Aaron Fenster"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0966", "x": 3.772, "y": 5.77, "title": "Asymmetric Matching in Abdominal Lymph Nodes of Follow-up CT Scans", "abstract": "Accurate tracking of abdominal lymph nodes (LN) across follow-up computed tomography (CT) scans is crucial for colorectal cancer staging and treatment response evaluation. However, establishing reliable LN correspondences remains underexplored due to challenges including scale variations, low resolution, difficulty distinguishing nodes from adjacent structures, inability to handle tissue deformation, and dynamic visibility. To address these challenges, we propose an asymmetric matching framework that strikes a balance between enhancing LN specificity and contextual correlations. For specificity, we achieve crossdimensional feature consistency and generate discriminative LN features via self-supervised learning on orthogonal 2D projections of 3D node volumes. For correlation, we develop a graph model capturing lymphatic topology within scans, reinforced by temporal contrastive learning that encourages consistency between matched node pairs across CT. To balance specificity and correlation, we propose a multi-module architecture that integrates volumetric LN features with projection embeddings through attention-based fusion, enabling confidence-calibrated similarity assessment across temporal scans. Experimental results demonstrate that our solution provides reliable lymph node correspondence for clinical follow-up and disease monitoring. Code is available at https://github. com/maoyij/Asymmetric-Matching.", "filename": "2025_0966.pdf", "year": 2025, "institution": "Sichuan University", "country": "China", "authors": ["Yiji Mao", "Yi Zhang", "Xinyu Zou", "Yuling Zheng", "Hao Huang", "Haixian Zhang"], "topic_id": 3, "base_color": "hsl(52.5, 70%, 50%)"}, {"id": "2025_0967", "x": 0.776, "y": 2.099, "title": "Augmented Reality-Based Guidance with Deformable Registration in Head and Neck Tumor Resection", "abstract": "Head and neck squamous cell carcinoma has one of the highest rates of recurrence. Recurrence rates can be reduced by accurate localization of positive margins. While frozen section analysis of resected specimens provides accurate intraoperative margin assessment, complex 3D anatomy and significant shrinkage of resected specimens complicate margin relocation from the specimen back to the post-resection cavity. We propose a novel deformable registration framework that uses both the pre-resection external surface and the post-resection cavity of the specimen to incorporate thickness information. In tongue specimens, the proposed framework improved the target registration error (TRE) by up to 33% as compared to using the post-resection cavity alone. We found distinct deformation behaviors in skin, buccal, and tongue specimens, highlighting the need for tailored deformation strategies. Notably, tongue specimens hold the highest clinical need for improvement among head and neck specimens. To further aid intraoperative visualization, we also integrated this framework into an augmented reality-based guidance system. This system can automatically overlay the deformed 3D specimen mesh with positive margin annotation onto the post-resection cavity. The integrated system improved a surgeon and a trainee's average relocation error from 9.8 mm to 4.8 mm in a pilot study. Our implementation code for AR guidance and generating the target point cloud is available at https://github.com/vu-maple-lab/Head-and-Neck-Tumor-Resection- Guidance.", "filename": "2025_0967.pdf", "year": 2025, "institution": "Vanderbilt University", "country": "USA", "authors": ["Qingyun Yang", "Fangjie Li", "Jiayi Xu", "Zixuan Liu", "Sindhura Sridhar", "Whitney Jin", "Jennifer Du", "Jon Heiselman", "Michael Miga", "Michael Topf", "Jie Ying Wu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0968", "x": 0.794, "y": 2.127, "title": "Automated Integration of Surgical Implants Into Digital Twins for Trauma Surgery", "abstract": "A digital twin (DT) is a dynamic virtual model that mirrors a physical system, with promising applications in surgical planning, guidance, and outcome assessment. While DTs can represent various key aspects of surgery, such as patient anatomy and surgical tools, implants remain difficult to integrate due to tracking challenges related to occlusions by soft tissue and their small size. Consequently, current surgical DTs lack implant integration, a critical limitation in trauma surgery. To address this challenge, this work presents an automated method to integrate surgical implants-plates and screws-into DTs during bone fracture platings. The solution leverages surgical tracking data to analyze interactions between surgical tools and patient anatomy. By combining deterministic algorithms with a machine learning-based activity classification model, DTs of implants can be reconstructed without requiring direct tracking. A study involving 28 participants-5 medical students, 12 residents, and 11 attending physicians-evaluated detection reliability and geometric accuracy on a comminuted ulnar fracture. Results showed a screw detection rate of 96.4 % and a plate detection rate of 100 % across 112 screws and 28 plates. Screw and plate placement had Root Mean Square Errors of 1.52 mm and 0.94 mm respectively-comparable to or better than existing surgical DTs. These findings confirm the feasibility of dynamic implant integration, marking a significant step toward comprehensive DT solutions for trauma surgery. This advancement has the potential to enhance intraoperative visualization and postoperative assessment, ultimately improving patient care.", "filename": "2025_0968.pdf", "year": 2025, "institution": "ETH Zürich", "country": "Switzerland", "authors": ["Tobias Stauffer", "Manuel Reber", "Léon Fellmann", "Reto Babst", "Mirko Meboldt", "Quentin Lohmeyer"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0969", "x": 1.159, "y": 2.098, "title": "Automatic Deep Deformable Registration Using Domain Adaptation and Run-Time Optimisation", "abstract": "Augmented reality from preoperative 3D model registration is promising to assist navigation in minimally-invasive liver surgery. The current registration methods are either accurate, but require surgeon interactions to annotate anatomical landmarks, or are fully automatic, but inaccurate. We propose a two-step automatic and accurate registration method. Step 1) segments the registration landmarks with a neural method. Step 2) estimates the 3D model deformation from the landmarks. The task is challenging because of the defects of the automatically segmented landmarks and the impossibility to label registration for training. We handle it by combining supervised training from synthetic transformations with domain adaptation and a novel robust Run-Time Optimisation (RTO). Our method outperforms existing ones, both with manual and automatic landmark segmentations, improving both automation and accuracy.", "filename": "2025_0969.pdf", "year": 2025, "institution": "Institut Pascal", "country": "France", "authors": ["Emilien Gadoux", "Adrien Bartoli"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0970", "x": 0.274, "y": 2.575, "title": "BREA-Depth: Bronchoscopy Realistic Airway-Geometric Depth Estimation", "abstract": "Monocular depth estimation in bronchoscopy can significantly improve real-time navigation accuracy and enhance the safety of interventions in complex, branching airways. Recent advances in depth foundation models have shown promise for endoscopic scenarios, yet these models often lack anatomical awareness in bronchoscopy, overfitting to local textures rather than capturing the global airway structureparticularly under ambiguous depth cues and poor lighting. To address this, we propose Brea-Depth, a novel framework that integrates airwayspecific geometric priors into foundation model adaptation for bronchoscopic depth estimation. Our method introduces a depth-aware Cycle-GAN, refining the translation between real bronchoscopic images and airway geometries from anatomical data, effectively bridging the domain gap. In addition, we introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. By incorporating anatomical priors, Brea-Depth enhances model generalization and yields more robust, accurate 3D airway reconstructions. To assess anatomical realism, we introduce Airway Depth Structure Evaluation, a new metric for structural consistency. We validate BREA-Depth on a collected ex-vivo human lung dataset and an open bronchoscopic dataset, where it outperforms existing methods in anatomical depth preservation.", "filename": "2025_0970.pdf", "year": 2025, "institution": "University of Edinburgh", "country": "UK", "authors": ["Francis Xiatian Zhang", "Emile Mackute", "Mohammadreza Kasaei", "Kevin Dhaliwal", "Robert Thomson", "Mohsen Khadem"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0971", "x": -0.041, "y": 5.061, "title": "CA-SAM2: SAM2-Based Context-Aware Network with Auto-prompting for Nuclei Instance Segmentation", "abstract": "Nuclei instance segmentation is crucial for biomedical research and disease diagnosis. Pathologists utilize information such as color, shape, and the surrounding tissue microenvironment to distinguish nuclei. However, existing models are limited as they rely solely on features from the current patch, neglecting contextual information from neighboring patches. This limitation impedes the model's ability to accurately identify nuclei. To address this issue, we propose CA-SAM2, a novel framework that enhances the prompt propagation capability of the Segment Anything Model 2 (SAM2) through a Context Injection Module(CIM), integrating surrounding contextual information during segmentation. Additionally, to adapt SAM2 to the pathology image domain, we introduce a convolutional branch to extract domain-specific features from pathological images. We further design a Multi-Level Feature Refinement Block (MFRB) to refine the prior features extracted by SAM2 and integrate domain features. Finally, we incorporate a regression head and a classification head after the convolutional branch to automatically generate point prompts, eliminating the need for manual annotation. Extensive evaluations of CA-SAM2 on the MoNuSeg and CPM-17 datasets demonstrate its effectiveness and practicality in enhancing nuclei segmentation. The code is available at https://github. com/HanbinHuang123/CA-SAM2.", "filename": "2025_0971.pdf", "year": 2025, "institution": "Soochow University", "country": "China", "authors": ["Hanbin Huang", "Hongliang He", "Liying Xu", "Xudong Zhu", "Siwei Feng", "Guohong Fu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0972", "x": -0.311, "y": 2.79, "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "abstract": "Understanding the intricate workflows of cataract surgery requires modeling complex interactions between surgical tools, anatomical structures, and procedural techniques. Existing datasets primarily address isolated aspects of surgical analysis, such as tool detection or phase segmentation, but lack comprehensive representations that capture the semantic relationships between entities over time. This paper introduces the Cataract Surgery Scene Graph (CAT-SG) dataset, the first to provide structured annotations of tool-tissue interactions, procedural variations, and temporal dependencies. By incorporating detailed semantic relations, CAT-SG offers a holistic view of surgical workflows, enabling more accurate recognition of surgical phases and techniques. Additionally, we present a novel scene graph generation model, CatSGG, which outperforms current methods in generating structured surgical representations. The CAT-SG dataset is designed to enhance AIdriven surgical training, real-time decision support, and workflow analysis, paving the way for more intelligent, context-aware systems in clinical practice. The dataset is available at github.com/felixholm/CAT-SG.", "filename": "2025_0972.pdf", "year": 2025, "institution": "Technical University Munich", "country": "Germany", "authors": ["Felix Holm", "Gözde Ünver", "Ghazal Ghazaei", "Nassir Navab"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0973", "x": -0.1, "y": 3.416, "title": "CholecMamba: A Mamba-Based Multimodal Reasoning Model for Cholecystectomy Surgery", "abstract": "Automatic analysis of cholecystectomy surgical videos has significant clinical value. However, current models are limited to simple tasks like single-frame phase recognition and multi-tool classification, failing to effectively utilize video context for complex clinical reasoning. They lack the ability to integrate medical textual knowledge with cholecystectomy images and long surgical videos. We propose Cholec-Mamba, a model that compresses video feature sequences through the Mamba architecture and deeply integrates with large-scale reasoning language models to achieve multimodal reasoning capabilities for surgical videos. Our main contributions include: 1) Designing a novel architecture that enables visual feature compression and knowledge feature injection, supporting multi-task video analysis of varying lengths; 2) Innovatively incorporating segmentation category information generated by large language models into the decoder, enhancing surgical video understanding and reasoning segmentation capabilities through medical knowledge logical reasoning; 3) Proposing the Surgical Reasoning Synthesis method, which leverages physician annotations and reinforcement learning with large language models to create the CholecReason dataset containing 49K multi-round dialogues, establishing a new benchmark for surgical video understanding and reasoning segmentation. Experimental results demonstrate that our model achieves optimal performance on existing datasets and CholecReason, with a closed-test score of 0.822, significantly outperforming the best competing model's score of 0.728. Our code is available at https://github.com/displaywz/CholecMamba.", "filename": "2025_0973.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Zipei Wang", "Sitian Pan", "Mengjie Fang", "Ruofan Zhang", "Jie Tian", "Di Dong"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0974", "x": 1.353, "y": 3.341, "title": "Conformal Forecasting for Surgical Instrument Trajectory", "abstract": "Forecasting surgical instrument trajectories and predicting the next surgical action recently started to attract attention from the research community. Both these tasks are crucial for automation and assistance in endoscopy surgery. Given the safety-critical nature of these tasks, reliable uncertainty quantification is essential. Conformal prediction is a fast-growing and widely recognized framework for uncertainty estimation in machine learning and computer vision, offering distribution-free, theoretically valid prediction intervals. In this work, we explore the application of standard conformal prediction and conformalized quantile regression to estimate uncertainty in forecasting surgical instrument motion, i.e., predicting direction and magnitude of surgical instruments' future motion. We analyze and compare their coverage and interval sizes, assessing the impact of multiple hypothesis testing and correction methods. Additionally, we show how these techniques can be employed to produce useful uncertainty heatmaps. To the best of our knowledge, this is the first study applying conformal prediction to surgical guidance, marking an initial step toward constructing principled prediction intervals with formal coverage guarantees in this domain (The code is available at this link).", "filename": "2025_0974.pdf", "year": 2025, "institution": "ETH Zurich", "country": "Switzerland", "authors": ["Sara Sangalli", "Gary Sarwin", "Ertunc Erdil", "Carlo Serra", "Alessandro Carretta", "Victor Staartjes", "Ender Konukoglu"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0975", "x": 4.298, "y": 3.591, "title": "Contrastive Masked Video Modeling for Coronary Angiography Diagnosis", "abstract": "Patients with valvular heart disease often exhibit motion characteristics such as artery movements and anatomic characteristics, thus extracting dynamic features from coronary angiography (CAG) is of great significance for diagnosing. Given the challenge of limited annotated medical imaging data, we propose a novel self-supervised learning framework that integrates masked video modeling (MVM) and video contrastive learning, enabling the model to learn representations with both strong instance discriminability between video segments and local perceptibility between neighboring frames. Specifically, our framework consists of three key components: an off-the-shelf frozen encoder, an online encoder-decoder following the MVM pipeline and a momentum encoder composed of an exponential moving average of previous students. We enhance the integration of contrastive learning and MVM in mainly two ways: the frozen encoder converts the supervision of masked reconstruction from low-level pixels to high-level features; an augmentation strategy called frame shifting, is introduced specifically for video contrastive learning. To validate the effectiveness of our proposed method, we first conducted self-supervised pre-training on over 50,000 self-collected, unlabeled CAG sequences. Subsequently, we performed supervised fine-tuning using two small-scale labeled CAG diagnostic datasets, achieving state-of-the-art performance (98.1% and 75.0% F1-Score, respectively) in both supervised and self-supervised video recognition domains. Our code is publicly available at: https://github.com/ ZmingShao/ConMVM.", "filename": "2025_0975.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Zhiming Shao", "Yingqian Zhang", "Zechen Wei", "Yong Ge", "Chen Wang", "Guodong Ding", "Lei Gao", "Liwei Zhang", "Yundai Chen", "Jie Tian", "Hui Hui"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_0976", "x": -0.443, "y": 3.164, "title": "CSAP-Assist: Instrument-Agent Dialogue Empowered Vision-Language Models for Collaborative Surgical Action Planning", "abstract": "Visual Planning for Assistance (VPA) in Robot-Assisted Minimally Invasive Surgery (RMIS) holds significant potential for introoperative guidance and procedural automation. This paper presents the Collaborative Surgical Action Planning (CSAP) task, which focuses on generating cooperative action plans based on linguistic surgical goals, highlighting the crucial need for coordinated multi-tool interactions in surgical procedures. CSAP task emphasizes two core challenges: understanding tool-action interdependencies in the timeline and managing concurrent multi-tool interactions. To address these challenges, we propose CSAP-Assist, a VLM-based framework consisting of two key modules: a Recency-Centric Focus Memory Module (ReFocus-MM), which prioritizes recent surgical history while summarizing distant events to improve performance in complex scenes and long sequences; and a Hybrid Multi-Agent Module (HMM), featuring a central agent that provides an initial plan, prompting a dialogue with local agent instruments to iteratively refine their collaborative actions. We evaluated CSAP-Assist on datasets that include phantom and real surgical scenarios. Our extensive experiments show that CSAP-Assist substantially outperforms the baseline method, achieving a 15% higher planning precision for surgical action planning. The source code and dataset are available at https://github. com/einnullnull/Collaborative-Surgical-Action-Planning-Assist.", "filename": "2025_0976.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Jie Zhang", "Mengya Xu", "Yiwei Wang", "Qi Dou"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0977", "x": -0.404, "y": 3.297, "title": "CurConMix: A Curriculum Contrastive Learning Framework for Enhancing Surgical Action Triplet Recognition", "abstract": "Accurately recognizing surgical action triplets in surgical videos is crucial for advancing context-aware systems that deliver realtime feedback, enhancing surgical safety and efficiency. However, recognizing surgical action triplets instrument, verb, target is challenging due to subtle variations, complex interdependencies, and severe class imbalance. Most existing approaches focus on individual triplet components while overlooking their interdependencies and the inherent class imbalance in triplet distributions. To address these challenges, we propose a novel framework, Curriculum Contrastive learning with feature Mixup (CurConMix). During pre-training, we employ curriculum contrastive learning, which progressively captures relationships among triplet components and distinguishes fine-grained variations through hard pair sampling and synthetic hard negative generation. In the finetuning stage, we further refine the model using self-distillation and mixup strategies to alleviate class imbalance. We evaluate our framework on the CholecT45 dataset using 5-fold cross-validation. Experimental results demonstrate that our approach surpasses existing methods across various model sizes and input resolutions. Moreover, our findings underscore the importance of capturing interdependency among triplet components, highlighting the effectiveness of our proposed framework in addressing key challenges in surgical action recognition. The official implementation is available at https://github.com/MIDAS-SurgAI/CurConMix.", "filename": "2025_0977.pdf", "year": 2025, "institution": "Sungkyunkwan University", "country": "Republic of Korea", "authors": ["Yongjun Jeon", "Jongmin Shin", "Seonmin Park", "Bogeun Kim", "Kanggil Park", "Namkee Oh", "Kyu-Hwan Jung"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0978", "x": 0.068, "y": 2.217, "title": "D 4 Recon: Dual-Stage Deformation and Dual-Scale Depth Guidance for Endoscopic Reconstruction", "abstract": "Deformable tissue reconstruction in endoscopy is vital for surgery, yet current methods struggle with high-fidelity reconstruction of irreversible tissue deformations. To this end, we present D 4 Recon, a novel framework for real-time and high-fidelity endoscopic reconstruction, addressing crucial challenges in surgical applications. A Dualstage Deformation modeling and a Dual-scale Depth guidance (D 4 ) are proposed in a dynamic 3D Gaussian Splatting paradigm along with lightweight multi-layer perception (MLP) to model dynamics in endoscopic scenes. In the dual-stage deformation modeling, we introduce a spatial deformation model to correct multiview inconsistencies, accompanied by a temporal deformation model that accurately represents tissue distortion and dynamic tissue interaction with surgical tools in the reference frames. In the dual-scale depth guidance, we propose to balance local error correction with absolute depth consistency, enabling precise depth refinement while preserving fine-grained color accuracy. D 4 Recon generates accurate 3D reconstructions with superior PSNR, SSIM, and LPIPS scores, outperforming existing methods in terms of geometric coherence and photorealism with real-time rendering speed, as demonstrated by extensive experiments on diverse endoscopic datasets. Reconstruction videos are in the supplementary file. Website.", "filename": "2025_0978.pdf", "year": 2025, "institution": "Stony Brook University", "country": "USA", "authors": ["Hritam Basak", "Zhaozheng Yin"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0979", "x": -0.007, "y": 2.413, "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video", "abstract": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's perception and therefore plays a vital role in various computer-assisted surgery tasks. However, achieving scale-consistent reconstruction remains an open challenge due to inherent issues in endoscopic videos, such as dynamic deformations and textureless surfaces. Despite recent advances, current methods either rely on calibration or instrument priors to estimate scale, or employ SfM-like multi-stage pipelines, leading to error accumulation and requiring offline optimization. In this paper, we present Endo3R, a unified 3D foundation model for online scale-consistent reconstruction from monocular surgical video, without any priors or extra optimization. Our model unifies the tasks by predicting globally aligned pointmaps, scale-consistent video depths, and camera parameters without any offline optimization. The core contribution of our method is expanding the capability of the recent pairwise reconstruction model to long-term incremental dynamic reconstruction by an uncertainty-aware dual memory mechanism. The mechanism maintains history tokens of both short-term dynamics and long-term spatial consistency. Notably, to tackle the highly dynamic nature of surgical scenes, we measure the uncertainty of tokens via Sampson distance and filter out tokens with high uncertainty. Regarding the scarcity of endoscopic datasets with ground-truth depth and camera poses, we further devise a self-supervised mechanism with a novel dynamics-aware flow loss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our superior performance in zero-shot surgical video depth prediction and camera pose estimation with online efficiency. Project page: https://wrld.github.io/Endo3R/.", "filename": "2025_0979.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Jiaxin Guo", "Wenzhen Dong", "Tianyu Huang", "Hao Ding", "Ziyi Wang", "Haomin Kuang", "Qi Dou", "Yun-Hui Liu"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0980", "x": -0.143, "y": 2.185, "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "abstract": "Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve highquality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.", "filename": "2025_0980.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0981", "x": 0.031, "y": 2.455, "title": "EndoDAV: Depth Any Video in Endoscopy with Spatiotemporal Accuracy", "abstract": "Video depth estimation has been applied to various endoscopy tasks, such as reconstruction, navigation, and surgery. Recently, many methods focus on directly applying or adapting depth estimation foundation models to endoscopy scenes. However, these methods do not consider temporal information, leading to an inconsistent prediction. We propose Endoscopic Depth Any Video (EndoDAV) to estimate spatially accurate and temporally consistent endoscopic video depth, which significantly expands the usability of depth estimation in downstream tasks. Specifically, we parameterefficiently finetune a video depth estimation foundation model to endoscopy scenes, utilizing a self-supervised depth estimation framework which simultaneously learns depth and camera pose. Considering the distinct characteristics of endoscopic videos compared to common videos, we further design a novel loss function and a depth alignment inference strategy to enhance the temporal consistency. Experiments on two public endoscopy datasets demonstrate that our method presents superior performance in both spatial accuracy and temporal consistency. Code is available at https://github.com/Zanue/ EndoDAV.", "filename": "2025_0981.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Zanwei Zhou", "Chen Yang", "Piao Yang", "Xiaokang Yang", "Wei Shen"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0982", "x": -0.114, "y": 2.103, "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting", "abstract": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and slightly dynamic surgical scenes. Our code is available at https:// github.com/vamWu/EndoFlow-SLAM.", "filename": "2025_0982.pdf", "year": 2025, "institution": "Xi'an Jiaotong Liverpool University", "country": "China", "authors": ["Taoyu Wu", "Yiyi Miao", "Zhuoxiao Li", "Haocheng Zhao", "Kang Dang", "Jionglong Su", "Limin Yu", "Haoang Li"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0983", "x": -0.12, "y": 2.283, "title": "Endo-GSMT: Endoscopic Monocular Scene Reconstruction with Dynamic Gaussian Splatting and Motion Tracking", "abstract": "Limited perspectives and complex tissue deformations pose significant challenges in accurately reconstructing monocular dynamic surgical scene. Many existing methods fail to fully exploit inter-frame relationships, resulting in suboptimal performance in processing complex tissue deformations and synthesizing novel views. To address these challenges, we propose Endo-GSMT, an accurate and high-quality method for dynamic endoscopic reconstruction from monocular surgical videos. Our method begins by comprehensively extracting both intra-frame information and inter-frame relationships from the raw monocular videos. We incorporate monocular depth priors and dense displacement field priors to generate the pixel-wise 3D trajectories during the training phase. Then, we design a set of compact and low-dimensional Sim(3) motion bases, with each point's motion represented as a weighted combination of these motion bases. Furthermore, we develop a novel depth loss function to address the scale inconsistency inherent in monocular depth priors. We evaluate our method using two distinct evaluation strategies, the experimental results demonstrate that our method achieves state-of-theart reconstruction quality. The code is available at https://github.com/ M11pha/Endo-GSMT.", "filename": "2025_0983.pdf", "year": 2025, "institution": "Hangzhou Dianzi University", "country": "China", "authors": ["Hao Gou", "Changmiao Wang", "Jiahao Yang", "Yaoqun Liu", "Fucang Jia", "Deqiang Xiao", "Feiwei Qin", "Huoling Luo"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0984", "x": -0.042, "y": 3.039, "title": "EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training", "abstract": "Endoscopic video-based tasks, such as visual navigation and surgical phase recognition, play a crucial role in minimally invasive surgeries by providing real-time assistance. While recent video foundation models have shown promise, their applications are hindered by (1) computational inefficiencies and (2) suboptimal performance caused by limited data for pre-training in endoscopy. To address these issues, we present EndoMamba, a foundation model designed for real-time inference while learning generalized spatiotemporal representations. First, to mitigate computational inefficiencies, we propose the EndoMamba backbone, optimized for real-time inference. Inspired by recent advancements in state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial modeling within individual frames and vanilla Mamba blocks for past-to-present reasoning across the temporal domain. This design enables both strong spatiotemporal modeling and efficient inference in online video streams. Second, to improve data efficiency, we propose a self-supervised hierarchical pre-training diagram that enhances EndoMamba's representation learning. Specifically, our approach combines masked reconstruction with auxiliary supervision, leveraging lowlevel reconstruction to capture spatial-temporal structures and highlevel alignment to transfer broader knowledge from a pretrained generalvideo domain foundation model. Extensive experiments on four downstream tasks-classification, segmentation, surgical phase recognition, and localization-demonstrate that EndoMamba outperforms existing foundation models and task-specific methods while maintaining real-time inference speed. The source code is available at https://github.com/ TianCuteQY/EndoMamba.", "filename": "2025_0984.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Qingyao Tian", "Huai Liao", "Xinyan Huang", "Bingyu Yang", "Dongdong Lei", "Sebastien Ourselin", "Hongbin Liu"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0985", "x": 0.069, "y": 2.333, "title": "Enforcing Geometric Constraints of Surface Normal and Pose for Self-supervised Monocular Depth Estimation on Laparoscopic Images", "abstract": "Depth information is essential for 3D reconstruction in surgical scenes. Depth-pose-based self-supervised monocular depth estimation has advanced significantly but faces two challenges in laparoscopic scenes, leading to unreliable pixel matching during training. This also results in depth maps failing to preserve geometric structure when back-projected into 3D space. Second, limited movement space necessitates that laparoscopic motion involves pure complex rotations. It further complicates the relative pose estimation between adjacent views. To address these issues, we propose a novel self-supervised monocular depth estimation method guided by geometric constraints. We incorporate surface normal estimation with depth-normal consistency to establish a geometric constraint for predicted depth maps. Furthermore, we propose an uncertainty measure based on the distance from 3D points to a synthesized plane, reducing conversion bias from depth to normals. Moreover, we optimize pose estimation using a feature-matching process with a 4D score volume. Our method reduced absolute relative error by 19.0% and 3D completeness by 23.9% over the baseline. Our code is available at https://github.com/MoriLabNU/GSPDepthL.", "filename": "2025_0985.pdf", "year": 2025, "institution": "Nagoya University", "country": "Japan", "authors": ["Wenda Li", "Yuichiro Hayashi", "Masahiro Oda", "Takayuki Kitasaka", "Kazunari Misawa", "Kensaku Mori"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0986", "x": 0.649, "y": 3.809, "title": "F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery", "abstract": "Pituitary tumors often cause deformation or encapsulation of adjacent vital structures. Anatomical structure segmentation can provide surgeons with early warnings of regions that pose surgical risks, thereby enhancing the safety of pituitary surgery. However, pixel-level annotated video stream datasets for pituitary surgeries are extremely rare. To address this challenge, we introduce a new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845 timecoherent images extracted from 120 videos. To mitigate class imbalance, we apply data augmentation techniques that simulate the presence of surgical instruments in the training data. One major challenge in pituitary anatomy segmentation is the inconsistency in feature representation due to occlusions, camera motion, and surgical bleeding. By incorporating a Feature Fusion module, F2PASeg is proposed to refine anatomical structure segmentation by leveraging both high-resolution image features and deep semantic embeddings, enhancing robustness against intraoperative variations. Experimental results demonstrate that F2PASeg consistently segments critical anatomical structures in real time, providing a reliable solution for intraoperative pituitary surgery planning. Code: https:// github.com/paulili08/F2PASeg.", "filename": "2025_0986.pdf", "year": 2025, "institution": "Centre for Artificial Intelligence and Robotics", "country": null, "authors": ["Lumin Chen", "Zhiying Wu", "Tianye Lei", "Xuexue Bai", "Ming Feng", "Yuxi Wang", "Gaofeng Meng", "Zhen Lei", "Hongbin Liu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0987", "x": 2.891, "y": 4.452, "title": "Fairness-Aware vCDR-Controlled Generation for Glaucoma Diagnosis", "abstract": "Glaucoma is a leading cause of irreversible blindness, and early diagnosis is crucial for effective treatment. However, AI-assisted glaucoma diagnosis faces challenges in fairness and data scarcity, because AI model biases can lead to disparities across demographic groups. To address this, we propose GlaucoDiff, a diffusion-based generative model that synthesizes SLO images with precise control over the vertical cupto-disc ratio. Unlike previous methods, GlaucoDiff enables bidirectional synthesis, generating both healthy and glaucomatous samples of varying severity, thus enhancing the dataset diversity. To ensure anatomical fidelity, GlaucoDiff leverages real fundus backgrounds while generating the optic nerve head regions. We also introduce a sample selection strategy that filters generated images based on the alignment agreement percentage, compared with target optic structures, ensuring the high-quality of the synthetic data. Experiments on two public ophthalmic datasets demonstrate that GlaucoDiff outperforms state-of-the-art approaches in both diagnosis and fairness measurement settings. Two independent ophthalmologists' evaluations confirm the clinical relevance of the generated images, highlighting GlaucoDiff's potential for improving AI-driven glaucoma diagnosis. Our code is available (https://github.com/WANG- ZIHENG/GlaucoDiff).", "filename": "2025_0987.pdf", "year": 2025, "institution": "University of Exeter", "country": "UK", "authors": ["Ziheng Wang", "Shuran Yang", "Wen Chen", "Zhen Zhang", "Mengyu Wang", "Feixiang Zhou", "Yu Tian", "Meng Wang", "Yitian Zhao", "Yalin Zheng", "Yanda Meng"], "topic_id": 6, "base_color": "hsl(105.0, 70%, 50%)"}, {"id": "2025_0988", "x": 1.466, "y": 3.322, "title": "FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation", "abstract": "Synthesizing high-quality medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixellevel guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at here.", "filename": "2025_0988.pdf", "year": 2025, "institution": "Beihang University", "country": "China", "authors": ["Huihan Wang", "Zhiwen Yang", "Hui Zhang", "Dan Zhao", "Bingzheng Wei", "Yan Xu"], "topic_id": 2, "base_color": "hsl(275.0, 70%, 50%)"}, {"id": "2025_0989", "x": 0.733, "y": 4.947, "title": "Frequency-Domain Multi-modal Fusion for Language-Guided Medical Image Segmentation", "abstract": "Automatically segmenting infected areas in radiological images is essential for diagnosing pulmonary infectious diseases. Recent studies have demonstrated that the accuracy of the medical image segmentation can be improved by incorporating clinical text reports as semantic guidance. However, the complex morphological changes of lesions and the inherent semantic gap between vision-language modalities prevent existing methods from effectively enhancing the representation of visual features and eliminating semantically irrelevant information, ultimately resulting in suboptimal segmentation performance. To address these problems, we propose a Frequency-domain Multi-modal Interaction model (FMISeg) for language-guided medical image segmentation. FMISeg is a late fusion model that establishes interaction between linguistic features and frequency-domain visual features in the decoder. Specifically, to enhance the visual representation, our method introduces a Frequency-domain Feature Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain features. Furthermore, a Languageguided Frequency-domain Feature Interaction (LFFI) module is incorporated within the decoder to suppress semantically irrelevant visual features under the guidance of linguistic information. Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method outperforms the state-of-the-art methods qualitatively and quantitatively.", "filename": "2025_0989.pdf", "year": 2025, "institution": "Anhui University", "country": "China", "authors": ["Bo Yu", "Jianhua Yang", "Zetao Du", "Yan Huang", "Chenglong Li", "Liang Wang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0990", "x": 0.526, "y": 3.64, "title": "FSA-Net: Fractal-Driven Synergistic Anatomy-Aware Network for Segmenting White Line of Toldt in Laparoscopic Images", "abstract": "Accurate automatic segmentation of the White Line of Toldt (WLT) is crucial for guiding colorectal cancer surgeries and improving patient outcomes. However, the complex anatomical structures and low signal-to-noise ratio involved in relevant regions of WLT pose significant challenges to existing segmentation models. Recent studies highlight fractal dimension as a powerful tool for analyzing the complexity of topological structures, offering an effective approach to representing anatomical features in medical images. Building on its success, we present the first well-annotated laparoscopic WLT segmentation (LTS) dataset and propose FSA-Net, a fractal-driven synergistic anatomy-aware network, specially designed for laparoscopic WLT segmentation. Specifically, FSA-Net consists of two core modules: the local texture-aware convolution (LTC) module and the fractal-guided anatomy-consistent attention (FAA) module. The LTC module adaptively adjusts the convolutional kernel offsets based on fractal dimensions to capture intra-anatomical features, while the FAA module employs a fractal-driven key-value pair filtering strategy to enhance the modeling of correlations across interanatomical structures. Extensive experimental results validate the effectiveness of our method. The resources will be available at https://github. com/Bigmouth233/FSA-Net.", "filename": "2025_0990.pdf", "year": 2025, "institution": "The Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Kecheng Wu", "Zhaohu Xing", "Zerong Cai", "Feng Gao", "Wenxue Li", "Lei Zhu"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0991", "x": -0.218, "y": 4.271, "title": "HalF-SAM: SAM-Based Haustral Fold Detection in Colonoscopy with Debris Suppression and Temporal Consistency", "abstract": "Haustral folds can serve as important landmarks to localize and navigate colonoscopes through the colon. Fold edges can be utilized for tracking in 3D reconstruction algorithms to generate colonoscopy coverage maps and ultimately reduce missed lesions. Current haustral fold detection models struggle with debris-filled colonoscopy videos and fail to maintain high temporal consistency due to their single-frame input. We introduce HalF-SAM, a Haustral Fold detection model utilizing the Segment Anything Model (SAM) image encoder, which suppresses edges from specular reflection and fecal debris. The SAM2-based memory module enhances temporal consistency, which is essential for tracking. Our experiments have shown significant improvements in haustral fold extraction accuracy and stability. We also release a training dataset with automatically annotated haustral fold edges in debris-filled high-fidelity colon phantom videos. The dataset and code will be available at: https:// github.com/DurrLab/HalFSAM.", "filename": "2025_0991.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Mayank Golhar", "Luojie Huang", "Nicholas J Durr"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_0992", "x": -0.446, "y": 2.856, "title": "HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation", "abstract": "Surgical Video Synthesis has emerged as a promising research direction following the success of diffusion models in generaldomain video generation. Although existing approaches achieve highquality video generation, most are unconditional and fail to maintain consistency with surgical actions and phases, lacking the surgical understanding and fine-grained guidance necessary for factual simulation. We address these challenges by proposing HieraSurg, a hierarchy-aware surgical video generation framework consisting of two specialized diffusion models. Given a surgical phase and an initial frame, HieraSurg first predicts future coarse-grained semantic changes through a segmentation prediction model. The final video is then generated by a secondstage model that augments these temporal segmentation maps with finegrained visual features, leading to effective texture rendering and integration of semantic information in the video space. Our approach leverages surgical information at multiple levels of abstraction, including surgical phase, action triplets, and panoptic segmentation maps. The experimental results on Cholecystectomy Surgical Video Generation demonstrate that the model significantly outperforms prior work both quantitatively and qualitatively, showing strong generalization capabilities and the ability to generate higher frame-rate videos. The model exhibits particularly fine-grained adherence when provided with existing segmentation maps, suggesting its potential for practical surgical applications. Project Webpage: diegobiagini.github.io/HieraSurg/.", "filename": "2025_0992.pdf", "year": 2025, "institution": "TU Munich", "country": "Germany", "authors": ["Diego Biagini", "Nassir Navab", "Azade Farshad"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0993", "x": 0.207, "y": 1.875, "title": "ICE-PoGO: Improving Dynamic Panoramic Reconstruction of 4D ICE Imaging Through Pose Graph Optimization", "abstract": "Intracardiac echocardiography (ICE) has the potential to play a crucial role in structural heart disease (SHD) interventions by providing high-quality imaging in real time, without many of the key drawbacks of established imaging modalities. However, ICE's limited field-ofview (FoV) requires continuous readjustments of the catheter position to fully visualize the dynamic cardiac environment, which impairs spatial navigation and increases procedure time and complexity. Dynamic panoramic reconstruction can mitigate this limitation. However, stateof-the-art methods depend on precise catheter tracking, the accuracy of which is affected by the presence of noise and anatomical motion. While registration can correct these errors, existing approaches are computationally prohibitive for large imaging volumes due to repeated iterations over image data, further amplified by the added time dimension. To address these challenges, we present a novel method for truly dynamic panoramic reconstruction by leveraging the repetitive nature of cardiac motion under a cyclic environment assumption. To our knowledge, our method is the first to employ dynamic pose graph optimization (PGO) specifically designed for 4D ICE tracking. Our results demonstrate enhanced tracking accuracy and improved panoramic reconstruction quality, potentially providing real-time, dynamic anatomical guidance for clinicians. The improved alignment of overlapping ICE volumes and increased temporal tracking resolution represent a substantial advancement in 4D ICE imaging, enhancing navigation and decisionmaking during complex cardiac interventions.", "filename": "2025_0993.pdf", "year": 2025, "institution": "Technical University of, Munich", "country": "Ireland", "authors": ["Sebastian Herz", "Magdalena Wysocki", "Felix Tristram", "Julia Hickler", "Lydia Neary-Zajiczek", "Christoph Hennersperger", "Nassir Navab", "Stefan Wörz"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0994", "x": 0.452, "y": 2.52, "title": "KidneyDepth: A Synthetic Kidney Dataset for Metric Depth Estimation in Ureteroscopy", "abstract": "Monocular Metric Depth Estimation (MDE) in endoscopic images is a crucial step to improve navigation during medical procedures, as it enables the estimation of dense, real-scale 3D maps of the organs. For instance, in monocular flexible ureteroscopy (fURS), accurate navigation and real-scale information are essential for locating and removing kidney stones efficiently. Currently, the most promising approach to infer depth from single passive cameras is by supervised training of large neural networks, so-called foundation models for MDE. However, the depth output of these models is biased when the training data domain does not fit the goal domain (both camera and scene). At the same time, one of the greatest challenges in medical imaging is the lack of annotated datasets, as obtaining real ground-truth (e.g., depth data) is difficult. To overcome this, simulation has become a valuable tool in ureteroscopic imaging research. In this study, we introduce KidneyDepth, a synthetic dataset designed to reduce the gap between simulated and real-world 3D imaging. It includes a variety of shapes (e.g. mesh from CT scan, geometric primitive forms) along with different textures and lighting conditions, generated by BlenderProc2 [ 7]. To assess the effectiveness of KidneyDepth, we fine-tune two state-of-the-art MDE models (Depth Anything V2 and ZoeDepth) and test their performance on both simulated and real ureteroscopic images. Additionally, we evaluate the validity of their output by using the inferred depths in the context of a RGB-D SLAM system. Our results show that training models on a synthetic dataset with diverse structures and lighting conditions improves depth estimation in real endoscopic images and our simulations show that these RGB-D images enhance overall SLAM accuracy. The KidneyDepth dataset can be found at https://zenodo.org/records/14893421.", "filename": "2025_0994.pdf", "year": 2025, "institution": "Institute of Robotics and Mechatronics", "country": "Germany", "authors": ["Laura Oliva-Maza", "Florian Steidle", "Julian Klodmann", "Klaus Strobl", "Arkadiusz Miernik", "Rudolph Triebel"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0995", "x": 1.541, "y": 6.947, "title": "Localization Lens for Improving Medical Vision-Language Models", "abstract": "Medical Vision-Language Models (Med-VLMs) have demonstrated strong capabilities in clinical tasks. However, they often struggle to understand anatomical structures and spatial positioning, which are crucial for medical reasoning. To address this, we propose a localization-aware enhancement to the Med-VLM pipeline, introducing improvements at three levels: data, architecture, and alignment. First, we introduce localization lens, a set of expert-validated representations that provide richer anatomical and positional context. However, as these representations increase input complexity, we integrate pixel shuffle within the model architecture to filter and refine representations, enhancing spatial information processing while preserving anatomical continuity. Lastly, to effectively align the localization lens representations with textual features, we incorporate decoupled contrastive loss (DCL) alongside the standard loss function. This ensures better feature discrimination and robustness, particularly in data-limited medical settings. Through extensive evaluations on medical visual question answering (Med-VQA) datasets, we show that our methodology improves localization-driven performance across different Med-VLM architectures. Our analysis of localization-based questions further reveals that improvements in anatomy and spatial reasoning directly enhance the overall accuracy of Med-VQA up to 6.2%. The proposed approach is modelagnostic and can be seamlessly integrated into existing Med-VLM pipelines. The dataset, code, and trained models will be made publicly available at https://github.com/CVLABLUMS/localizationlens", "filename": "2025_0995.pdf", "year": 2025, "institution": "Lahore University of Management Sciences", "country": "Pakistan", "authors": ["Hasan Farooq", "Murtaza Taj", "Mehwish Nasim", "Arif Mahmood"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0996", "x": 0.596, "y": 2.107, "title": "Marker-Less Head Pose Tracking for Image-Guided Cerebral Artery Navigation", "abstract": "Image-guided cerebral artery navigation (CAN) system can provide precise guidance for intracranial artery examination and surgery by aligning 3D medical data with patient's head observed by a depth sensor. Existing CAN systems generally suffer from either susceptibility to location marker offset or weak efficiency. This paper presents a real-time marker-less method to track the patient's head pose based on the MRI data for CAN. Briefly, the 3D facial model is constructed from the patient's MRI data in the pre-operative stage. Then, a 3D local description is proposed to encode the local geometry of the facial model via thin plate spline function. Subsequently, according to the local description of the facial model, the patient's head observed by an RGBD camera is registered with the facial model by maximum weight matching. Eventually, the head pose is accurately tracked in real-time via square-root cubature Kalman filter (SCKF) and iterative closest point algorithm (ICP) during navigation. With each estimated head pose, the patient's vessels in MRI data are visualized onto the RGB image of the patient's head for CAN. The proposed method is evaluated on comprehensive experiments, showing the best core performance metrics than all comparison methods. The average rotational and translational errors of our method are 2.6° and 1.9 mm respectively on the BIWI dataset. The average tracking rate achieves 0.06 s.", "filename": "2025_0996.pdf", "year": 2025, "institution": "University of Chinese Academy of Sciences", "country": "China", "authors": ["Qiuying Wang", "Pandeng Zhang", "Dewei Chen", "Hao Tang", "Chang Liu", "Jia Liu"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_0997", "x": 1.219, "y": 6.869, "title": "MedAgentSim: Self-evolving Multi-agent Simulations for Realistic Clinical Interactions", "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-ofthought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM's ability to engage in dynamic, context-aware diagnostic interactions. While MedA-gentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our codebase, simulation environment, and benchmark datasets are publicly available on the project", "filename": "2025_0997.pdf", "year": 2025, "institution": "University of Artificial Intelligence", "country": "UAE", "authors": ["Mohammad Almansoori", "Komal Kumar", "Hisham Cholakkal"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_0998", "x": -0.278, "y": 2.773, "title": "Motion-Boundary-Driven Unsupervised Surgical Instrument Segmentation in Low-Quality Optical Flow", "abstract": "Unsupervised video-based surgical instrument segmentation has the potential to accelerate the adoption of robot-assisted procedures by reducing the reliance on manual annotations. However, the generally low quality of optical flow in endoscopic footage poses a great challenge for unsupervised methods that rely heavily on motion cues. To overcome this limitation, we propose a novel approach that pinpoints motion boundaries, regions with abrupt flow changes, while selectively discarding frames with globally low-quality flow and adapting to varying motion patterns. Experiments on the EndoVis2017 VOS and EndoVis2017 Challenge datasets show that our method achieves mean Intersection-over-Union (mIoU) scores of 0.75 and 0.72, respectively, effectively alleviating the constraints imposed by suboptimal optical flow. This enables a more scalable and robust surgical instrument segmentation solution in clinical settings. The code is publicly available at https://github.com/ wpr1018001/Rethinking-Low-quality-Optical-Flow.git.", "filename": "2025_0998.pdf", "year": 2025, "institution": "King's College London", "country": "UK", "authors": ["Yang Liu", "Peiran Wu", "Jiayu Huo", "Gongyu Zhang", "Zhen Yuan", "Christos Bergeles", "Rachel Sparks", "Prokar Dasgupta", "Alejandro Granados", "Sebastien Ourselin"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_0999", "x": 4.403, "y": 3.557, "title": "MReg: A Novel Regression Model with MoE-Based Video Feature Mining for Mitral Regurgitation Diagnosis", "abstract": "Color Doppler echocardiography is a crucial tool for diagnosing mitral regurgitation (MR). Recent studies have explored intelligent methods for MR diagnosis to minimize user dependence and improve accuracy. However, these approaches often fail to align with clinical workflow and may lead to suboptimal accuracy and interpretability. In this study, we introduce an automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color Doppler echocardiography video (A4C-CDV). It follows comprehensive feature mining strategies to detect MR and assess its severity, considering clinical realities. Our contribution is threefold. First, we formulate the MR diagnosis as a regression task to capture the continuity and ordinal relationships between categories. Second, we design a feature selection and amplification mechanism to imitate the sonographer's diagnostic logic for accurate MR grading. Third, inspired by the Mixture-of-Experts concept, we introduce a feature summary module to extract the category-level features, enhancing the representational capacity for more accurate grading. We trained and evaluated our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases with three graded regurgitation labels. Compared to other weakly supervised video anomaly detection and supervised classification methods, MReg demonstrated superior performance in MR diagnosis. Our code is available at: https://github.com/cskdstz/MReg.", "filename": "2025_0999.pdf", "year": 2025, "institution": "Shenzhen University", "country": "China", "authors": ["Zhe Liu", "Yuhao Huang", "Lian Liu", "Chengrui Zhang", "Haotian Lin", "Tong Han", "Zhiyuan Zhu", "Yanlin Chen", "Ruiyue Chen", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_1000", "x": 6.03, "y": 4.591, "title": "Multi-modal Progressive Fusion for ASD Screening Using Smartphone Video", "abstract": "Screening for Autism Spectrum Disorder (ASD) is an important yet challenging task. Traditional screening tools, such as questionnaires and other technical methods, face difficulties in large-scale implementation, such as primary healthcare and home monitoring settings. To address this issue, we develop a smartphone application to highlight atypical eye movement behaviors in children with ASD and extract multi-modal features, including eye movements, head pose, and emotional expressions, from smartphone videos to characterize the subjects' viewing behavior. Additionally, we propose a multi-modal progressive fusion framework to comprehensively integrate the relationships between different modalities. The progressive fusion strategy combines multi-modal features at multiple scales to achieve attention-based deep fusion. Moreover, we develop a global intra-and inter-modality interaction (GIIMI) module to enhance competition and interaction within and between modalities. In the experiment, we constructed a smartphone video dataset of 124 children aged 3 to 6 years and validated the performance advantages of the proposed algorithm.", "filename": "2025_1000.pdf", "year": 2025, "institution": "Northwestern Polytechnical University", "country": "China", "authors": ["Wenqi Zhong", "Bohan Li", "Chen Xia", "Kuan Li", "Dingwen Zhang"], "topic_id": 0, "base_color": "hsl(0.0, 70%, 50%)"}, {"id": "2025_1001", "x": 0.526, "y": 2.662, "title": "Navigational Bronchoscopy in Critical Care via End-to-End Pose Regression", "abstract": "Bronchoscopy is a minimally invasive procedure for diagnosing and treating lung conditions, but accurate navigation remains challenging and resource-intensive due to reliance on preoperative imaging, sensor-based tracking, and the low-saliency visual environment of the airways. To address these limitations, we propose a novel Navigational Bronchoscopy framework that enables real-time guidance and repeatable interventions without requiring external sensors or CT scans, making it particularly suitable for mechanically ventilated patients in critical care units with limited access to preoperative imaging. Our approach leverages deep learning, combining airway landmark recognition with deep visual features and a Vision Transformer (ViT)-based pose regression network to track bronchoscope motion. The framework is deployed on a commercially available bronchoscope and validated through trials in both a phantom lung model and a mechanically ventilated ex-vivo human lung. Results show that our ViT-based model achieves the lowest pose estimation errors among tested methods. Furthermore, in ex-vivo trials, our system successfully guided the bronchoscope to predefined targets, achieving high similarity scores for reliable landmark identification. These findings highlight the feasibility of our approach for real-world clinical applications.", "filename": "2025_1001.pdf", "year": 2025, "institution": "University of Edinburgh", "country": "UK", "authors": ["Emile Mackute", "Francis Xiatian Zhang", "Kevin Dhaliwal", "Mohsen Khadem"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1002", "x": -0.359, "y": 2.973, "title": "Operating Room Workflow Analysis via Reasoning Segmentation over Digital Twins", "abstract": "Analyzing operating room (OR) workflows to derive quantitative insights into OR efficiency is important for hospitals to maximize patient care and financial sustainability. Prior work on OR-level workflow analysis has relied on end-to-end deep neural networks. While these approaches work well in constrained settings, they are limited to the conditions specified at development time and do not offer the flexibility necessary to accommodate the OR workflow analysis needs of various OR scenarios (e.g., large academic center vs. rural provider) without data collection, annotation, and retraining. Reasoning segmentation (RS) based on foundation models offers this flexibility by enabling automated analysis of OR workflows from OR video feeds given only an implicit text query related to the objects of interest. Due to the reliance on large language model (LLM) fine-tuning, current RS approaches struggle with reasoning about semantic/spatial relationships and show limited generalization to OR video due to variations in visual characteristics and domain-specific terminology. To address these limitations, we first propose a novel digital twin (DT) representation that preserves both semantic and spatial relationships between the various OR components. Then, building on this foundation, we propose ORDiRS (Operating Room Digital twin representation for Reasoning Segmentation), an LLM-tuning-free RS framework that reformulates RS into a \"reason-retrieval-synthesize\" paradigm. Finally, we present ORDiRS-Agent, an LLM-based agent that decomposes OR workflow analysis queries into manageable RS sub-queries and generates responses by combining detailed textual explanations with supporting visual evidence from RS. Experimental results on both an inhouse and a public OR dataset demonstrate that our ORDiRS achieves a cIoU improvement of 6.12%-9.74% compared to the existing state-ofthe-arts.", "filename": "2025_1002.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Yiqing Shen", "Chenjia Li", "Bohan Liu", "Cheng-Yi Li", "Tito Porras", "Mathias Unberath"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1003", "x": -0.363, "y": 2.837, "title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model", "abstract": "In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatialtemporal knowledge from a T2V model pre-trained on natural videotext datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/mar-cry/Ophora.", "filename": "2025_1003.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Wei Li", "Ming Hu", "Guoan Wang", "Lihao Liu", "Kaijing Zhou", "Junzhi Ning", "Xin Guo", "Zongyuan Ge", "Lixu Gu", "Junjun He"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1004", "x": 2.013, "y": 5.868, "title": "Pattern-Anchored Adaptive Prototype Learning for Gastroscopic Lesion Detection and Beyond", "abstract": "Gastroscopic Lesion Detection (GLD) is one of the critical tasks within computer-assisted gastroscopic diagnostics. Endoscopists adopt a pattern-based philosophy for GLD: they identify and summarize typical sub-category patterns with specific medical meanings and conduct GLD based on these patterns. However, the current gastroscopic lesion detectors follow the classical data-driven deep-learningbased training paradigm, which differs from the endoscopists' diagnosis process and leads to low interpretability, limiting their performance and potential for daily clinical practice and patient care. The intuitive data-driven solution with sub-category pattern labels may work but it requires expensive annotation costs. In this work, we imitate the patternbased philosophy with limited labels and propose a Pattern-Anchored Adaptive Prototype Learning (PAAPL) for Gastroscopic Lesion Detection. PAAPL consists of a Prototype-based Gastroscopic Lesion Detector (PGLD) and a Pattern-Anchored Adaptive Learning (PAAL) strategy. PGLD achieves sub-category pattern detection based on similarity to prototypes. PAAL proposes a vector-wise prototype formulation and an adaptive prototype update strategy to anchor prototypes to limitedannotated patterns with specific medical meanings and adaptively learn pattern characteristics from unannotated data in GLD datasets. We evaluate PAAPL on the LGLDD and Endo21 datasets, demonstrating its ability to learn and detect sub-category patterns trained with limited annotations. By doing this, PAAPL enhances detector interpretability and yields significant performance improvement (+3.7AP on LGLDD/+5.4AP on Endo21).", "filename": "2025_1004.pdf", "year": 2025, "institution": "Shenzhen Research Institute of Big Data", "country": "China", "authors": ["Xuanye Zhang", "Xiaoqing Hu", "Guanbin Li", "Si-Qi Liu", "Xiang Wan", "Yuanhuan Xiong"], "topic_id": 7, "base_color": "hsl(242.6, 70%, 50%)"}, {"id": "2025_1005", "x": -0.342, "y": 2.901, "title": "Phase-Informed Tool Segmentation for Manual Small-Incision Cataract Surgery", "abstract": "Cataract surgery is the most common surgical procedure globally, with a disproportionately higher burden in developing countries. While automated surgical video analysis has been explored in general surgery, its application to ophthalmic procedures remains limited. Existing research primarily focuses on Phaco cataract surgery, an expensive technique not accessible in regions where cataract treatment is most needed. In contrast, Manual Small-Incision Cataract Surgery (MSICS) is the preferred low-cost alternative in high-volume settings and for complex cases. However, no dataset exists for MSICS. To address this gap, we introduce Sankara-MSICS, the first comprehensive dataset containing 53 surgical videos annotated for 18 surgical phases and 3,527 frames with 13 surgical tools at the pixel level. We also present ToolSeg, a novel framework that enhances tool segmentation with a phase-conditional decoder and a semi-supervised setup leveraging pseudo-labels from foundation models. Our approach significantly improves segmentation performance, achieving a 38.1% increase in mean Dice scores, with notable gains for smaller and less prevalent tools. The code is available at https://github. com/Sri-Kanchi-Kamakoti-Medical-Trust/ToolSeg.", "filename": "2025_1005.pdf", "year": 2025, "institution": "Microsoft Research", "country": "India", "authors": ["Bhuvan Sachdeva", "Naren Akash", "Tajamul Ashraf", "Simon Müller", "Thomas Schultz", "Maximilian W M Wintergerst", "Niharika Singri", "Kaushik Murali", "Mohit Jain"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1006", "x": 1.033, "y": 2.215, "title": "RadGS-Reg: Registering Spine CT with Biplanar X-Rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration", "abstract": "Computed Tomography (CT)/X-ray registration in imageguided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional \"render and compare\" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: github.com/shenao1995/RadGS_Reg.", "filename": "2025_1006.pdf", "year": 2025, "institution": "Hohai University (HHU)", "country": "China", "authors": ["Ao Shen", "Xueming Fu", "Junfeng Jiang", "Qiang Zeng", "Ye Tang", "Zhengming Chen", "Luming Nong", "Feng Wang", "Shaohua Kevin Zhou"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_1007", "x": -0.552, "y": 3.133, "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-Time Adaptation and Task-Graph Guided Refinement", "abstract": "The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institutionspecific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multimodal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data.", "filename": "2025_1007.pdf", "year": 2025, "institution": "University of Strasbourg", "country": "France", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joël L Lavanchy", "Christian Heiliger", "Ege Özsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1008", "x": -0.283, "y": 4.755, "title": "SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "abstract": "Hyperspectral imaging (HSI) provides rich spectral information for medical imaging, yet encounters significant challenges due to data limitations and hardware variations. We introduce SAMSA, a novel interactive segmentation framework that combines an RGB foundation model with spectral analysis. SAMSA efficiently utilizes user clicks to guide both RGB segmentation and spectral similarity computations. The method addresses key limitations in HSI segmentation through a unique spectral feature fusion strategy that operates independently of spectral band count and resolution. Performance evaluation on publicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on a neurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperative porcine hyperspectral dataset. Experimental results demonstrate SAMSA's effectiveness in few-shot and zero-shot learning scenarios and using minimal training examples. Our approach enables seamless integration of datasets with different spectral characteristics, providing a flexible framework for hyperspectral medical image analysis.", "filename": "2025_1008.pdf", "year": 2025, "institution": "The Hamlyn Centre for Robotic Surgery", "country": "UK", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S Elson", "Stamatia Giannarou"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_1009", "x": 1.053, "y": 5.555, "title": "Self-adaptive Vision-Language Model for 3D Segmentation of Pulmonary Artery and Vein", "abstract": "Accurate segmentation of pulmonary structures is crucial in clinical diagnosis, disease study, and treatment planning. Significant progress has been made in deep learning-based segmentation techniques, but most require large amount of labeled data for training. Consequently, developing precise segmentation methods that demand fewer labeled datasets is paramount in medical image analysis. We constructed PAV-Seg3D, the largest Pulmonary Arteriovenous 3D Segmentation Dataset to date (718 scans). The emergence of pre-trained vision-language foundation models, such as CLIP, recently opened the door for universal computer vision tasks. However, exploring these models for pulmonary artery-vein segmentation is still limited. This paper proposes a novel framework called LA-CAF, which adopts pre-trained CLIP as a strong feature extractor for generating the segmentation of 3D CT scans, while adaptively aggregating the cross-modality of text and image representations. We propose a specially designed adapter module to fine-tune pre-trained CLIP with a self-adaptive learning strategy to effectively fuse the two modalities of embeddings. We validate LA-CAF on two datasets: PAV-Seg3D and the public PARSE2022 dataset. The experiments show that our method outperformed other state-of-the-art methods by a large", "filename": "2025_1009.pdf", "year": 2025, "institution": "National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital and Shenzhen Hospital", "country": "China", "authors": ["Xiaotong Guo", "Deqian Yang", "Dan Wang", "Ying Zhu", "Haochen Zhao", "Yuan Li", "Zhilin Sui", "Tao Zhou", "Lijun Zhang", "Hui Meng", "Yanda Meng"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_1010", "x": 0.829, "y": 5.513, "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance", "abstract": "Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among nonexpert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to non-expert users and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for nonexpert users, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for non-expert users. Project page: https://noseefood.github.io/us-scene-graph/.", "filename": "2025_1010.pdf", "year": 2025, "institution": "TU Munich", "country": "Germany", "authors": ["Xuesong Li", "Dianye Huang", "Yameng Zhang", "Nassir Navab", "Zhongliang Jiang"], "topic_id": 1, "base_color": "hsl(137.5, 70%, 50%)"}, {"id": "2025_1011", "x": -0.324, "y": 2.673, "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "abstract": "Surgical simulation plays a pivotal role in training novice surgeons, accelerating their learning curve and reducing intra-operative errors. However, conventional simulation tools fall short in providing the necessary photorealism and the variability of human anatomy. In response, current methods are shifting towards generative model-based simulators. Yet, these approaches primarily focus on using increasingly complex conditioning for precise synthesis while neglecting the fine-grained human control aspect. To address this gap, we introduce SG2VID, the first diffusion-based video model that leverages Scene Graphs for both precise video synthesis and fine-grained human control. We demonstrate SG2VID's capabilities across three public datasets featuring cataract and cholecystectomy surgery. While SG2VID outperforms previous methods both qualitatively and quantitatively, it also enables precise synthesis, providing accurate control over tool and anatomy's size and movement, entrance of new tools, as well as the overall scene layout. We qualitatively motivate how SG2VID can be used for generative augmentation and present an experiment demonstrating its ability to improve a downstream phase detection task when the training set is extended with our synthetic videos. Finally, to showcase SG2VID's ability to retain human control, we interact with the Scene Graphs to generate new video samples depicting major yet rare intra-operative irregularities. Project", "filename": "2025_1011.pdf", "year": 2025, "institution": "", "country": null, "authors": ["Ssharvien Kumar Sivakumar", "Yannik Frisch", "Ghazal Ghazaei", "Anirban Mukhopadhyay"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1012", "x": -0.421, "y": 3.315, "title": "Smarter Self-distillation: Optimizing the Teacher for Surgical Video Applications", "abstract": "Surgical workflow analysis poses significant challenges due to complex imaging conditions, annotation ambiguities, and the large number of classes in tasks such as action recognition. Self-distillation (SD) has emerged as a promising technique to address these challenges by leveraging soft labels, but little is known about how to optimize the quality of these labels for surgical scene analysis. In this work, we thoroughly investigate this issue. First, we show that the quality of soft labels is highly sensitive to several design choices and that relying on a single topperforming teacher selected based on validation performance often leads to suboptimal results. Second, as a key technical innovation, we introduce a multi-teacher distillation strategy that ensembles checkpoints across seeds and epochs within a training phase where soft labels maintain an optimal balance-neither underconfident nor overconfident. By ensembling at the teacher level rather than the student level, our approach reduces computational overhead during inference. Finally, we validate our approach on three benchmark datasets, where it demonstrates consistent improvements over existing SD methods. Notably, our method sets a new state-of-the-art (SOTA) performance on the CholecTriplet benchmark, achieving a 43.1% mean Average Precision (mAP) score and real-time inference time, thereby establishing a new standard for surgical video analysis in challenging and ambiguous environments. Code available at https://github.com/IMSY-DKFZ/self-distilled-swin.", "filename": "2025_1012.pdf", "year": 2025, "institution": "NCT Heidelberg, a Partnership Between DKFZ and University Medical Center Heidelberg", "country": "Germany", "authors": ["Amine Yamlahi", "Piotr Kalinowski", "Patrick Godau", "Rayan Younis", "Martin Wagner", "Beat Müller", "Lena Maier-Hein"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1013", "x": 1.011, "y": 2.243, "title": "Sparse-XM: Spine Pose Adjustment with RGB-D Bone Segmentation via Cross-Modality Label Transfer", "abstract": "In open spine surgery, navigation requires registration between the surgical field and volumetric CT. The spine pose changes between preoperative CT (pCT) and intraoperative patient positioning, and can further change after intraoperative CT (iCT) during surgery, degrading navigation accuracy. In this study, we developed a novel, fully automated end-to-end system for spine pose adjustment driven by intraoperative stereovision imaging (iSV) images. Our system includes three innovative modules. First, we present a method to automatically generate weak bone labels in stereo images via co-registration with iCT images. The automated labeling process addresses the labor and expertise-intensive challenges associated with supervised bone segmentation models that typically require manually segmented labels for training. Second, we train a fully convolutional deep learning method that integrates complementary information from the color (RBG) and depth (D) images to automatically segment bone using the weak labels. Finally, the segmented bone structures are used to perform a pose-adjusted registration. Data collected from 5 porcine cadavers were used for training and validation, and data from 2 porcine cadavers were used for independent testing. Pose-adjusted registration accuracy across all lumbar levels of test specimens was 2.0 ± 1.1 mm, compared to 2.5 ± 1.5 mm using manual segmentation, and 9.1 ± 6.8 mm using a commercially available navigation system. The fully automated pose-adjusting registration framework compensated for spine motion between pCT and intraoperative positioning and overall achieved clinically acceptable accuracy. Our approach was not user or expertise-dependent and holds potential for wider adoptions in open spinal procedures for intraoperative spine motion correction. Code is available at https://github.com/wRossw/Sparse-XM-Spine-Pose-Adjustment.", "filename": "2025_1013.pdf", "year": 2025, "institution": "Dartmouth College", "country": "USA", "authors": ["William R Warner", "Indrani Bhattacharya", "Linton T Evans", "Sohail K Mirza", "Keith D Paulsen", "Xiaoyao Fan"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_1014", "x": 0.819, "y": 1.048, "title": "STEAM: Self-supervised TEeth Analysis and Modeling for Point Cloud Segmentation", "abstract": "Accurate segmentation of 3D tooth point clouds from intraoral scanner (IOS) data is crucial for orthodontic applications. While current methods show promise, their reliance on high-quality labeled datasets is limited due to costly annotation processes, which further constrain their practical generalizability. We address this challenge with STEAM, a self-supervised learning framework that learns comprehensive features from large-scale unlabeled tooth point clouds. Built upon the masked autoencoder, our framework incorporates two key innovations: Gradient-guided Adaptive Masking (GAM), which adaptively identifies and prioritizes challenging regions by analyzing local feature variations during the training process, and Multi-attribute Geometric Reconstruction (MGR), which reconstructs multiple geometric attributes including point distributions, normals, and curvatures to capture geometric features of different granularity. Through extensive experiments on public datasets, our approach demonstrates superior performance in downstream segmentation tasks with minimal labeled data, achieving significant improvements over existing methods. The results validate STEAM effectiveness in maximizing the utility of limited labeled data for practical dental applications.", "filename": "2025_1014.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Yifan Liu", "Chen Yang", "Weihao Yu", "Xinyu Liu", "Hui Chen", "Null- H Meng", "Yixuan Yuan"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_1015", "x": -0.361, "y": 3.759, "title": "StepAL: Step-Aware Active Learning for Cataract Surgical Videos", "abstract": "Active learning (AL) can reduce annotation costs in surgical video analysis while maintaining model performance. However, traditional AL methods, developed for images or short video clips, are suboptimal for surgical step recognition due to inter-step dependencies within long, untrimmed surgical videos. These methods typically select individual frames or clips for labeling, which is ineffective for surgical videos where annotators require the context of the entire video for annotation. To address this, we propose StepAL, an active learning framework designed for full video selection in surgical step recognition. StepAL integrates a step-aware feature representation, which leverages pseudo-labels to capture the distribution of predicted steps within each video, with an entropy-weighted clustering strategy. This combination prioritizes videos that are both uncertain and exhibit diverse step compositions for annotation. Experiments on two cataract surgery datasets (Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms existing active learning approaches, achieving higher accuracy in step recognition with fewer labeled videos. StepAL offers an effective approach for efficient surgical video analysis, reducing the annotation burden in developing computer-assisted surgical systems.", "filename": "2025_1015.pdf", "year": 2025, "institution": "Johns Hopkins University", "country": "USA", "authors": ["Nisarg A Shah", "Bardia Safaei", "Shameema Sikder", "S Swaroop Vedula", "Vishal M Patel"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1016", "x": -0.399, "y": 3.318, "title": "Surgical Action Planning with Large Language Models", "abstract": "We introduce the Surgical Action Planning (SAP) task for cholecystectomy procedures, which generates future action plans from visual inputs to address the absence of intraoperative predictive planning in current intelligent applications. SAP shows great potential for enhancing intraoperative guidance and automating procedures. However, it faces challenges such as understanding instrument-tissue relationships and tracking surgical progress. Large Language Models (LLMs) show promise in understanding surgical video content but remain underexplored for predictive decision-making in SAP, as they focus mainly on retrospective analysis. Challenges like data privacy, computational demands, and modality-specific constraints further highlight significant research gaps. To tackle these challenges, we introduce LLM-SAP, a Large Language Model-based Surgical Action Planning framework that predicts future actions and generates text responses by interpreting natural language prompts of surgical goals. The text responses potentially support surgical education, intraoperative decision-making, procedure documentation, and skill analysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory Module (NHF-MM) for modeling historical states and the prompts factory for action planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset using models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in nextaction prediction. Pre-trained LLMs are tested in a zero-shot setting, and supervised fine-tuning (SFT) with LoRA is implemented. Our experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy. The source code and dataset are available at https:// github.com/XuMengyaAmy/SAP.", "filename": "2025_1016.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Mengya Xu", "Zhongzhen Huang", "Jie Zhang", "Xiaofan Zhang", "Qi Dou"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1017", "x": -0.15, "y": 3.517, "title": "Surgical-MambaLLM: Mamba2-Enhanced Multimodal Large Language Model for VQLA in Robotic Surgery", "abstract": "In recent years, Visual Question Localized-Answering in robotic surgery (Surgical-VQLA) has gained significant attention for its potential to assist medical students and junior doctors in understanding surgical scenes. Recently, the rapid development of Large Language Models (LLMs) has provided more promising solutions for this task. However, current methods struggle to establish complex dependencies between text and visual details, and have difficulty perceiving the spatial information of surgical scenes. To address these challenges, we propose a novel method, Surgical-MambaLLM, which is the first to combine Mamba2 with LLM in the surgical domain, that leverages Mamba2's ability to effectively capture cross-modal dependencies and perceive spatial information in surgical scenes, thereby enhancing the LLMs' understanding of surgical images. Specifically, we propose the Cross-modal Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective multimodal fusion, with its cross-modal integration capabilities. Additionally, tailored to the geometric characteristics of surgical scenes, we design the Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the surgical images, enhancing the model's spatial understanding of the surgical scene. Extensive experiments demonstrate that our Surgical-MambaLLM model outperforms the state-of-the-art methods on the EndoVis17-VQLA and EndoVis18-VQLA datasets, significantly improving the performance of the Surgical-VQLA task.", "filename": "2025_1017.pdf", "year": 2025, "institution": "Hong Kong University of Science and Technology (Guangzhou)", "country": "China", "authors": ["Pengfei Hao", "Hongqiu Wang", "Shuaibo Li", "Zhaohu Xing", "Guang Yang", "Kaishun Wu", "Lei Zhu"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1018", "x": -0.258, "y": 2.35, "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "abstract": "In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more indepth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. Our code is available at: https://github. com/lastbasket/SurgTPGS.", "filename": "2025_1018.pdf", "year": 2025, "institution": "The Chinese University of Hong Kong", "country": "China", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarak I Hoque", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1019", "x": -0.226, "y": 2.259, "title": "$$\\text {T}^{2}$$GS: Comprehensive Reconstruction of Dynamic Surgical Scenes with Gaussian Splatting", "abstract": "Surgical scene reconstruction from endoscopic video is crucial for many applications in computer-and robot-assisted surgery. However, existing methods primarily focus on soft tissue deformation while often neglecting the dynamic motion of surgical tools, limiting the completeness of the reconstructed scene. To bridge the aforementioned research gap, we propose T 2 GS, a novel and efficient surgical scene reconstruction framework that enables efficient spatio-temporal modelling of both deformable tissues and dynamically interacting surgical tools. T 2 GS leverages Gaussian Splatting for dynamic scene reconstruction, and it integrates a recent tissue deformation modelling technique while most importantly, introduces a novel efficient tool motion model (ETMM). At its core, ETMM disambiguates the modelling process of tool's motion as global trajectory modelling and local shape-change modelling. We additionally propose pose-informed pointcloud fusion (PIPF), holistically initialized of tools' gaussians for improved tool motion reconstruction. Extensive experiments on public datasets demonstrate T 2 GS's superior performance for comprehensive endoscopic scene reconstruction compared to previous methods. Moreover, as we specifically design our method with efficiency in concern, T 2 GS also showcases promising reconstruction efficiency (3mins) and rendering speed (71fps), highlighting its potential for intraoperative applications. Our code is available at https:// gitlab.com/nct tso public/ttgs.", "filename": "2025_1019.pdf", "year": 2025, "institution": "University Hospital Carl Gustav Carus", "country": "Germany", "authors": ["Jinjing Xu", "Chenyang Li", "Peng Liu", "Micha Pfeiffer", "Liwen Liu", "Reuben Docea", "Martin Wagner", "Stefanie Speidel"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1020", "x": 3.834, "y": 2.516, "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "abstract": "Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes are available at https://github. com/AlexYouXin/Mo-Diff", "filename": "2025_1020.pdf", "year": 2025, "institution": "Shanghai Jiao Tong University", "country": "China", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "topic_id": 4, "base_color": "hsl(190.0, 70%, 50%)"}, {"id": "2025_1021", "x": -0.395, "y": 2.702, "title": "Towards Holistic Surgical Scene Graph", "abstract": "Surgical scene understanding is crucial for computer-assisted intervention systems, requiring visual comprehension of surgical scenes that involves diverse elements such as surgical tools, anatomical structures, and their interactions. To effectively represent the complex information in surgical scenes, graph-based approaches have been explored to structurally model surgical entities and their relationships. Previous surgical scene graph studies have demonstrated the feasibility of representing surgical scenes using graphs. However, certain aspects of surgical scenes-such as diverse combinations of tool-action-target and the identity of the hand operating the tool-remain underexplored in graph-based representations, despite their importance. To incorporate these aspects into graph representations, we propose Endoscapes-SG201 dataset, which includes annotations for toolaction target combinations and hand identity. We also introduce SSG-Com, a graph-based method designed to learn and represent these critical elements. Through experiments on downstream tasks such as critical view of safety assessment and action triplet recognition, we demonstrated the importance of integrating these essential scene graph components, highlighting their significant contribution to surgical scene understanding. The code and dataset are available at https://github.com/ailab-kyunghee/SSG-Com", "filename": "2025_1021.pdf", "year": 2025, "institution": "Samsung Medical Center", "country": "Republic of Korea", "authors": ["Jongmin Shin", "Enki Cho", "Ka Young Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1022", "x": 0.673, "y": 2.104, "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue", "abstract": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is a promising method with high translational potential. Unlike bone-mounted tracking devices, markerless tracking can reduce operating time and complexity. However, its use has been limited to cadaveric studies. This paper introduces the first real-world clinical RGB-D dataset for spine surgery and develops SpineAlign, a system for capturing deformation between preoperative and intraoperative spine states. We also present an intraoperative segmentation network trained on this data and introduce CorrespondNet, a multi-task framework for predicting key regions for registration in both intraoperative and preoperative scenes.", "filename": "2025_1022.pdf", "year": 2025, "institution": "The Hamlyn Centre for Robotic Surgery", "country": "UK", "authors": ["Connor Daly", "Elettra Marconi", "Marco Riva", "Jinendra Ekanayake", "Daniel S Elson", "Ferdinando Rodriguez Y Baena"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_1023", "x": 0.97, "y": 2.095, "title": "Towards Patient-Specific Deformable Registration in Laparoscopic Surgery", "abstract": "Unsafe surgical care is a critical health concern, often linked to limitations in surgeon experience, skills, and situational awareness. Integrating patient-specific 3D models into the surgical field can enhance visualization, provide real-time anatomical guidance, and reduce intraoperative complications. However, reliably registering these models in general surgery remains challenging due to mismatches between preoperative and intraoperative organ surfaces, such as deformations and noise. To overcome these challenges, we introduce the first deep learning-based non-rigid point cloud registration method that is genuinely patient-specific, being both trained and tested on the same individual's anatomy. Our approach combines a Transformer encoderdecoder architecture with overlap estimation and a dedicated matching module to predict dense correspondences, followed by a physics-based algorithm for registration. Experimental results on both synthetic and real data demonstrate that our patient-specific method significantly outperforms traditional agnostic approaches, achieving 45% Matching Score with 92% Inlier Ratio on synthetic data, highlighting its potential to improve surgical care.", "filename": "2025_1023.pdf", "year": 2025, "institution": "Istituto Italiano di Tecnologia", "country": "Italy", "authors": ["Alberto Neri", "Nazim Haouchine", "Veronica Penza", "Leonardo S Mattos"], "topic_id": 8, "base_color": "hsl(20.1, 70%, 50%)"}, {"id": "2025_1024", "x": 3.941, "y": 2.834, "title": "Unsupervised Cardiac Video Translation Via Motion Feature Guided Diffusion Model", "abstract": "This paper presents a novel motion feature guided diffusion model for unpaired video-to-video translation (MFD-V2V), designed to synthesize dynamic, high-contrast cine cardiac magnetic resonance (CMR) from lower-contrast, artifact-prone displacement encoding with stimulated echoes (DENSE) CMR sequences. To achieve this, we first introduce a Latent Temporal Multi-Attention (LTMA) registration network that effectively learns more accurate and consistent cardiac motions from cine CMR image videos. A multi-level motion feature guided diffusion model, equipped with a specialized Spatio-Temporal Motion Encoder (STME) to extract hierarchical coarse-to-fine motion conditioning, is then developed to improve synthesis quality and fidelity. We evaluate our method, MFD-V2V, on a comprehensive cardiac dataset, demonstrating superior performance over the state-of-the-art in both quantitative metrics and qualitative assessments. Furthermore, we show the benefits of our synthesized cine CMRs improving downstream clinical and analytical tasks, underscoring the broader impact of our approach. Our code is publicly available at https://github.com/SwaksharDeb/MFD- V2V.", "filename": "2025_1024.pdf", "year": 2025, "institution": "University of Virginia", "country": "USA", "authors": ["Swakshar Deb", "Nian Wu", "Frederick H Epstein", "Miaomiao Zhang"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_1025", "x": 0.162, "y": 2.503, "title": "Unsupervised Structure-Geometric Consistency for Monocular Endoscopic Depth Overestimation", "abstract": "Monocular endoscopic depth estimation is a key to expand the surgical field and visually navigate the endoscope, augmenting the perception of surgeons and reducing inadvertent damages during robotic surgery. Unfortunately, current deep learning methods still suffer from a limited field of view, moving and limited artificial optic-fiber light sources (illumination variations), and weak textures or structures in monocular endoscopic video images collected from complex surgical scenarios, as well as they also get trapped in depth overestimation. This work first explores a small deep learning model of densely convolved pyramid transformer to simultaneously predict monocular depth and pose of the endoscope without using any annotation data. Specifically, this small model employs dense convolution and hierarchical transformer to encode multiscale local and global features, while it uses residual attention to effectively fuse or decode these features. Then, a photometric structure-aware consistency mechanism is introduced to deal with the problems of weak texture and depth overestimation, refining endoscopic depth and pose estimation. We evaluated our methods on both synthetic and clinical colonoscopic video images, with the experimental results showing that our unsupervised learning methods can attain higher accurate depth distribution and more sufficient textures, and better qualitative and quantitative results than state-of-the-art monocular depth estimation models.", "filename": "2025_1025.pdf", "year": 2025, "institution": "Xiamen University", "country": "China", "authors": ["Wenkang Fan", "Enqi Qiu", "Hongzhi Xu", "Xiongbiao Luo"], "topic_id": 5, "base_color": "hsl(327.5, 70%, 50%)"}, {"id": "2025_1026", "x": 3.187, "y": 3.386, "title": "Untangling Vascular Trees for Surgery and Interventional Radiology", "abstract": "The diffusion of minimally invasive, endovascular interventions motivates the development of visualization methods for complex vascular networks. We propose a planar representation of blood vessel trees which preserves the properties that are most relevant to catheter navigation: topology, length and curvature. Taking as input a threedimensional digital angiography, our algorithm produces a faithful twodimensional map of the patient's vessels within a few seconds. To this end, we propose optimized implementations of standard morphological filters and a new recursive embedding algorithm that preserves the global orientation of the vascular network. We showcase our method on peroperative images of the brain, pelvic and knee artery networks. On the clinical side, our method simplifies the choice of devices prior to and during the intervention. This lowers the risk of failure during navigation or device deployment and may help to reduce the gap between expert and common intervention centers. From a research perspective, our method simulates the cadaveric display of artery trees from anatomical dissections. This opens the door to large population studies on the branching patterns and tortuosity of fine human blood vessels. Our code is released under the permissive MIT license as part of the scikit-shapes Python library (https://scikit-shapes.github.io.", "filename": "2025_1026.pdf", "year": 2025, "institution": "Inria", "country": "France", "authors": ["Guillaume Houry", "Tom Boeken", "Stèphanie Allassonnière", "Jean Feydy"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}, {"id": "2025_1027", "x": 4.354, "y": 4.035, "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy", "abstract": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37% (.p < 0.001), with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a tradeoff between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.", "filename": "2025_1027.pdf", "year": 2025, "institution": "King's College London", "country": "UK", "authors": ["Harry Robertshaw", "Han-Ru Wu", "Alejandro Granados", "Thomas C Booth"], "topic_id": 9, "base_color": "hsl(157.6, 70%, 50%)"}]